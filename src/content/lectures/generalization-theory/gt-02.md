# 편향-분산 분해: 모델 복잡도의 이중 칼날

## 왜 편향-분산 트레이드오프를 이해해야 하는가

gt-01에서 정의한 일반화 갭은 "오차가 왜 발생하는가"에 대한 구조적 답을 제공하지 않습니다. 편향-분산 분해는 테스트 오차를 **근원적으로 다른 두 가지 원인** — 모델의 표현력 부족(편향)과 데이터의 유한성에 의한 변동(분산) — 으로 분리하여, 모델 선택에 대한 근본적 통찰을 제공합니다.

---

## 1. 편향-분산 분해의 유도

회귀 문제에서 참함수 $f(x)$와 잡음 $\epsilon \sim (0, \sigma^2)$에 대해 $y = f(x) + \epsilon$이라 하겠습니다. 훈련 집합 $S$에 대해 학습된 모델 $\hat{f}_S(x)$의 기대 테스트 오차를 분해합니다:

$$\mathbb{E}_S\left[\mathbb{E}_{x,y}[(y - \hat{f}_S(x))^2]\right]$$

$\bar{f}(x) = \mathbb{E}_S[\hat{f}_S(x)]$를 정의하면, 다음이 성립합니다:

$$\mathbb{E}_S[(y - \hat{f}_S(x))^2] = \underbrace{(f(x) - \bar{f}(x))^2}_{\text{편향}^2} + \underbrace{\mathbb{E}_S[(\hat{f}_S(x) - \bar{f}(x))^2]}_{\text{분산}} + \underbrace{\sigma^2}_{\text{돌이킬 수 없는 오차}}$$

**증명 스케치:**

$$y - \hat{f}_S = (y - f) + (f - \bar{f}) + (\bar{f} - \hat{f}_S)$$

제곱 후 기대값을 취하면 교차항이 소거됩니다. $(y - f) = \epsilon$은 $\hat{f}_S$와 독립이고, $\mathbb{E}_S[\hat{f}_S - \bar{f}] = 0$이므로:

$$\mathbb{E}[(y-\hat{f}_S)^2] = \sigma^2 + (f-\bar{f})^2 + \mathbb{E}_S[(\hat{f}_S - \bar{f})^2]$$

> **핵심 직관**: 편향은 "평균적으로 얼마나 틀리는가", 분산은 "데이터에 따라 얼마나 달라지는가"를 측정합니다. 둘 다 줄이고 싶지만, 일반적으로 하나를 줄이면 다른 하나가 증가합니다.

---

## 2. 트레이드오프의 기하학적 이해

모델 복잡도 $d$에 따른 각 항의 전형적 거동을 정리합니다:

| 복잡도 $d$ | 편향$^2$ | 분산 | 총 오차 | 상태 |
|-----------|---------|------|---------|------|
| 매우 작음 | 높음 | 낮음 | 높음 | 과소적합 |
| 최적 | 중간 | 중간 | 최소 | 최적점 |
| 매우 큼 | 낮음 | 높음 | 높음 | 과적합 |

최적 복잡도 $d^*$는 다음을 만족합니다:

$$d^* = \arg\min_d \left[\text{Bias}^2(d) + \text{Var}(d)\right]$$

```python
import numpy as np

def bias_variance_simulation(X_train_sets, y_train_sets, X_test, f_true, degrees):
    """여러 훈련 셋에 대한 편향-분산 시뮬레이션"""
    results = {}
    for d in degrees:
        predictions = []
        for X_tr, y_tr in zip(X_train_sets, y_train_sets):
            coeffs = np.polyfit(X_tr.ravel(), y_tr, d)
            pred = np.polyval(coeffs, X_test.ravel())
            predictions.append(pred)
        predictions = np.array(predictions)
        f_bar = predictions.mean(axis=0)           # 평균 예측
        bias_sq = np.mean((f_true - f_bar)**2)     # 편향^2
        variance = np.mean(predictions.var(axis=0)) # 분산
        results[d] = {'bias2': bias_sq, 'var': variance, 'total': bias_sq + variance}
    return results
```

> **핵심 직관**: 다항식 차수 1(직선)은 복잡한 함수를 표현하지 못해 편향이 크고, 차수 20은 데이터 점마다 급격히 변해 분산이 큽니다.

---

## 3. 분류 문제에서의 편향-분산 분해

회귀와 달리 분류에서의 분해는 더 미묘합니다. 0-1 손실의 경우 Domingos(2000)의 분해를 따릅니다:

$$\mathbb{E}_S[\ell_{0-1}(\hat{f}_S(x), y)]$$

주 예측(main prediction) $\hat{y}^*(x) = \arg\max_y \Pr_S[\hat{f}_S(x) = y]$를 정의하면:

$$\text{오차} = \underbrace{\ell_{0-1}(\hat{y}^*(x), f(x))}_{\text{편향}} + \underbrace{\Pr_S[\hat{f}_S(x) \neq \hat{y}^*(x)]}_{\text{분산}} - \underbrace{\text{상호작용항}}_{\text{편향-분산 공분산}}$$

| 항목 | 회귀 (MSE) | 분류 (0-1 손실) |
|------|-----------|---------------|
| 분해 깔끔함 | 깔끔한 가법적 분해 | 상호작용항 존재 |
| 편향 정의 | $(f - \bar{f})^2$ | $\ell_{0-1}(\hat{y}^*, f)$ |
| 분산 정의 | $\mathbb{E}[(\hat{f}_S - \bar{f})^2]$ | $\Pr[\hat{f}_S \neq \hat{y}^*]$ |
| 돌이킬 수 없는 오차 | $\sigma^2$ | 베이즈 오차율 |

> **핵심 직관**: 분류에서는 편향과 분산이 서로 상쇄하거나 증폭할 수 있습니다. 이것이 앙상블 방법(배깅은 분산 감소, 부스팅은 편향 감소)의 이론적 근거입니다.

---

## 4. 모델 복잡도의 측정

"복잡도"는 다양하게 정의됩니다. 각 측도의 장단점을 비교합니다:

| 복잡도 측도 | 정의 | 한계 |
|-----------|------|------|
| 파라미터 수 $p$ | 모델의 자유도 | 과매개변수 모델에서 실패 |
| VC 차원 | gt-03에서 상세 | 계산 불가능한 경우 多 |
| Rademacher 복잡도 | gt-05에서 상세 | 데이터 의존적 |
| 기술 길이 (description length) | 모델 인코딩 비트 수 | 인코딩 스킴 의존 |
| 유효 자유도 (effective DOF) | $\text{tr}(H)$, $H$는 hat matrix | 선형 모델에만 정확 |

선형 회귀에서 유효 자유도는 hat matrix의 대각합으로 주어집니다:

$$\text{df}(\lambda) = \text{tr}(X(X^TX + \lambda I)^{-1}X^T) = \sum_{i=1}^{p} \frac{\sigma_i^2}{\sigma_i^2 + \lambda}$$

여기서 $\sigma_i$는 $X$의 특이값이고 $\lambda$는 정규화 계수입니다. $\lambda = 0$이면 $\text{df} = p$, $\lambda \to \infty$이면 $\text{df} \to 0$입니다.

```python
import numpy as np

def effective_dof(X, lam):
    """릿지 회귀의 유효 자유도 계산"""
    _, s, _ = np.linalg.svd(X, full_matrices=False)
    return np.sum(s**2 / (s**2 + lam))

# 예: X가 100x50 행렬일 때
# effective_dof(X, 0.0) -> 50 (파라미터 수)
# effective_dof(X, 1.0) -> 약 30 (정규화 효과)
```

> **핵심 직관**: 정규화는 유효 복잡도를 줄여 편향은 약간 높이지만 분산을 크게 낮춥니다. gt-06에서 이를 심도 있게 다룹니다.

---

## 5. 이중 하강과 편향-분산의 확장

고전적 편향-분산 트레이드오프는 **단조적** 관계를 예측합니다: 복잡도가 증가하면 분산이 단조 증가한다. 그러나 현대 연구는 이 가정이 깨지는 영역을 발견했습니다.

보간 임계점 $p = n$ (파라미터 수 = 샘플 수) 근처에서:

$$\text{Var}(\hat{f}_S(x)) \to \infty \quad \text{as} \quad p \to n$$

그러나 $p \gg n$인 과매개변수 영역에서는 분산이 다시 감소합니다:

$$\text{Var}(\hat{f}_S(x)) \propto \frac{n}{p} \quad \text{when} \quad p \gg n$$

이를 통해 총 오차는 **이중 하강(double descent)** 곡선을 그립니다:

| 영역 | $p$ vs $n$ | 거동 | 지배 요인 |
|------|-----------|------|----------|
| 과소매개변수 | $p \ll n$ | 고전적 U자 | 편향 ↔ 분산 |
| 보간 임계점 | $p \approx n$ | 분산 발산 | 분산 폭발 |
| 과매개변수 | $p \gg n$ | 오차 재감소 | 분산 감소 |

이 현상은 gt-07에서 상세히 분석합니다.

> **핵심 직관**: 고전적 편향-분산 트레이드오프는 과매개변수 영역에서 수정이 필요합니다. 현대 딥러닝의 성공은 이 "두 번째 하강"에 위치합니다.

---

## 6. 편향-분산과 앙상블 방법

편향-분산 분해는 앙상블 학습의 이론적 기반을 제공합니다.

$M$개의 독립적 모델 $\hat{f}_1, \ldots, \hat{f}_M$을 평균낸 앙상블:

$$\hat{f}_{\text{ens}}(x) = \frac{1}{M}\sum_{m=1}^{M}\hat{f}_m(x)$$

이 앙상블의 분산은:

$$\text{Var}[\hat{f}_{\text{ens}}] = \frac{1}{M}\text{Var}[\hat{f}_1] + \frac{M-1}{M}\text{Cov}[\hat{f}_i, \hat{f}_j]$$

모델 간 상관이 $\rho$이면:

$$\text{Var}[\hat{f}_{\text{ens}}] = \rho \cdot \sigma^2_f + \frac{1-\rho}{M}\sigma^2_f$$

| 앙상블 방법 | 편향 영향 | 분산 영향 | 상관 $\rho$ 감소 전략 |
|-----------|---------|---------|-------------------|
| 배깅 (Bagging) | 거의 불변 | 감소 | 부트스트랩 샘플링 |
| 랜덤 포레스트 | 약간 증가 | 크게 감소 | 특성 부분 집합 |
| 부스팅 (Boosting) | 크게 감소 | 증가 가능 | 순차적 잔차 학습 |

```python
import numpy as np

def ensemble_variance_reduction(n_models, individual_var, correlation):
    """앙상블의 분산 감소 계산"""
    return correlation * individual_var + (1 - correlation) / n_models * individual_var

# 상관 0.3인 100개 모델 앙상블
# ensemble_variance_reduction(100, 1.0, 0.3) -> 0.307 (원래의 30.7%)
```

> **핵심 직관**: 배깅은 분산만 줄이므로 이미 분산이 높은 불안정한 학습기(결정 트리 등)에 효과적입니다. 부스팅은 편향을 줄이므로 약한 학습기를 강하게 만듭니다.

---

## 핵심 정리

- **편향-분산 분해는 테스트 오차를 모델의 표현력 부족(편향)과 데이터 변동에 대한 민감성(분산)으로 분리합니다**
- **고전적 트레이드오프에서 최적 복잡도 $d^*$는 편향$^2$과 분산의 합을 최소화하는 지점에 존재합니다**
- **유효 자유도 $\text{tr}(H)$는 정규화된 모델의 실질적 복잡도를 측정하며, 파라미터 수와 다를 수 있습니다**
- **과매개변수 영역($p \gg n$)에서 분산이 재감소하는 이중 하강 현상은 고전적 트레이드오프를 넘어서는 현상입니다**
- **앙상블 방법은 편향-분산 분해에 기반하며, 배깅은 분산을, 부스팅은 편향을 주로 감소시킵니다**
