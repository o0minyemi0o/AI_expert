# VC 차원: 가설 공간의 표현력 측정

## 왜 VC 차원이 중요한가

gt-01에서 유한 가설 클래스의 일반화 bound를 유도했지만, 실제 학습 문제의 가설 공간은 대부분 무한합니다(예: 모든 선형 분류기). 파라미터 수만으로는 표현력을 정확히 측정할 수 없습니다. **VC(Vapnik-Chervonenkis) 차원**은 가설 공간의 조합론적 복잡도를 측정하여, 무한 가설 클래스에 대해서도 일반화 보장을 제공합니다.

---

## 1. 성장 함수와 산산조각 내기

가설 클래스 $\mathcal{H} \subseteq \{h: \mathcal{X} \to \{0,1\}\}$가 점 집합 $\{x_1, \ldots, x_n\} \subset \mathcal{X}$ 위에서 만들 수 있는 이진 레이블 패턴의 수를 생각합니다.

**정의 (제한, Restriction):** 점 집합 $C = \{x_1,\ldots,x_n\}$ 위에서 $\mathcal{H}$의 제한은:

$$\mathcal{H}|_C = \{(h(x_1), \ldots, h(x_n)) : h \in \mathcal{H}\} \subseteq \{0,1\}^n$$

**정의 (성장 함수, Growth Function):**

$$\Pi_{\mathcal{H}}(n) = \max_{C \subset \mathcal{X}, |C|=n} |\mathcal{H}|_C|$$

**정의 (산산조각 내기, Shattering):** $\mathcal{H}$가 $C$를 산산조각 낸다 $\iff$ $|\mathcal{H}|_C| = 2^n$

| 점 개수 $n$ | 최대 가능 패턴 수 | 산산조각 조건 |
|------------|----------------|------------|
| 1 | 2 | $\Pi_{\mathcal{H}}(1) = 2$ |
| 2 | 4 | $\Pi_{\mathcal{H}}(2) = 4$ |
| 3 | 8 | $\Pi_{\mathcal{H}}(3) = 8$ |
| $n$ | $2^n$ | $\Pi_{\mathcal{H}}(n) = 2^n$ |

```python
import numpy as np
from itertools import product

def count_dichotomies(H_func, points):
    """가설 클래스가 점 집합 위에서 만드는 이분법 수 계산"""
    dichotomies = set()
    for h in H_func:
        labeling = tuple(h(x) for x in points)
        dichotomies.add(labeling)
    return len(dichotomies)

def is_shattered(H_func, points):
    """산산조각 여부 확인"""
    return count_dichotomies(H_func, points) == 2**len(points)
```

> **핵심 직관**: 성장 함수는 "이 가설 클래스가 $n$개 점을 얼마나 다양하게 분류할 수 있는가"를 측정합니다. 산산조각은 모든 $2^n$개 분류가 가능한 극단적 경우입니다.

---

## 2. VC 차원의 정의

**정의 (VC 차원):** 가설 클래스 $\mathcal{H}$의 VC 차원은 $\mathcal{H}$가 산산조각 낼 수 있는 최대 점 집합의 크기입니다:

$$\text{VCdim}(\mathcal{H}) = \max\{n : \Pi_{\mathcal{H}}(n) = 2^n\}$$

중요한 점: VC 차원이 $d$라 함은:
1. 크기 $d$인 **어떤** 점 집합을 산산조각 낼 수 있다 (존재 조건)
2. 크기 $d+1$인 **모든** 점 집합을 산산조각 낼 수 없다 (보편 조건)

| 가설 클래스 | VC 차원 | 직관 |
|-----------|---------|------|
| $\{h_0\}$ (상수 분류기) | 0 | 아무 점도 산산조각 불가 |
| 양의 반직선 $\{x \leq \theta\}$ | 1 | 1개 점은 되지만 2개는 불가 |
| $\mathbb{R}^d$의 선형 분류기 | $d+1$ | 일반 위치의 $d+1$개 점 |
| $\sin(\omega x)$ 분류기 | $\infty$ | 모든 유한 집합 산산조각 |

> **핵심 직관**: VC 차원은 "모든 배치"가 아닌 "최적 배치"에서의 표현력입니다. 하나라도 산산조각 가능한 크기-$d$ 집합이 존재하면 $\text{VCdim} \geq d$입니다.

---

## 3. 선형 분류기의 VC 차원

**정리:** $\mathbb{R}^d$에서 동차(homogeneous) 선형 분류기 $h_w(x) = \text{sign}(w^Tx)$의 VC 차원은 $d$입니다. 비동차(affine) 분류기 $h_{w,b}(x) = \text{sign}(w^Tx + b)$의 VC 차원은 $d+1$입니다.

**증명 ($\text{VCdim} \geq d+1$):**

$\mathbb{R}^d$에서 표준 기저 벡터 $e_1, \ldots, e_d$와 원점 $0$으로 구성된 $d+1$개 점을 고려합니다. 임의의 레이블 $(y_0, y_1, \ldots, y_d) \in \{-1,+1\}^{d+1}$에 대해:

$$w = \sum_{i=1}^{d} y_i e_i, \quad b = y_0$$

로 설정하면 모든 점을 올바르게 분류합니다.

**증명 ($\text{VCdim} \leq d+1$):**

Radon의 정리에 의해, $\mathbb{R}^d$의 임의의 $d+2$개 점은 두 집합 $A, B$로 분할되어 $\text{conv}(A) \cap \text{conv}(B) \neq \emptyset$입니다. 초평면은 이 두 집합을 분리할 수 없습니다. $\blacksquare$

```python
import numpy as np

def check_linear_shattering_2d(points, labels_list):
    """2D 선형 분류기로 모든 레이블을 실현 가능한지 확인"""
    from scipy.optimize import linprog
    success_count = 0
    for labels in labels_list:
        # y_i * (w^T x_i + b) > 0 풀기
        n = len(points)
        A = -np.column_stack([labels.reshape(-1,1) * points,
                              labels.reshape(-1,1)])
        b_ub = -np.ones(n) * 1e-6
        c = np.zeros(3)
        res = linprog(c, A_ub=A, b_ub=b_ub, bounds=[(-10,10)]*3)
        if res.success:
            success_count += 1
    return success_count == len(labels_list)
```

> **핵심 직관**: 2차원 평면에서 초평면(직선)은 일반 위치의 3개 점을 산산조각 낼 수 있지만, 4개 점에 대해서는 XOR 패턴이 불가능합니다.

---

## 4. Sauer-Shelah 보조정리

VC 차원이 유한하면 성장 함수의 증가율이 지수적에서 다항식적으로 전환됩니다.

**정리 (Sauer-Shelah):** $\text{VCdim}(\mathcal{H}) = d$이면:

$$\Pi_{\mathcal{H}}(n) \leq \sum_{i=0}^{d}\binom{n}{i} \leq \left(\frac{en}{d}\right)^d$$

이것은 **위상 전이**를 의미합니다:

| $n$ vs $d$ | $\Pi_{\mathcal{H}}(n)$ | 증가율 |
|-----------|----------------------|-------|
| $n \leq d$ | $= 2^n$ (가능) | 지수적 |
| $n > d$ | $\leq \left(\frac{en}{d}\right)^d$ | 다항식적 |

**증명의 핵심 아이디어:** 귀납법을 사용합니다. $n$개 점에서 하나를 제거하면:

$$\Pi_{\mathcal{H}}(n) \leq \Pi_{\mathcal{H}}(n-1) + \Pi_{\mathcal{H}'}(n-1)$$

여기서 $\mathcal{H}'$는 제거한 점에서 이분법이 달라지는 가설들의 제한으로 $\text{VCdim}(\mathcal{H}') \leq d-1$입니다.

> **핵심 직관**: VC 차원이 유한하다는 것은 성장 함수가 "부서지는"(break) 지점이 존재한다는 뜻입니다. 이 전환이 균일 수렴을 가능하게 합니다.

---

## 5. VC 일반화 bound

이제 핵심 결과인 VC bound를 진술합니다.

**정리 (VC Bound):** 손실이 $[0,1]$에 값을 가지면, 확률 $1 - \delta$ 이상으로 모든 $h \in \mathcal{H}$에 대해:

$$R(h) \leq \hat{R}_S(h) + \sqrt{\frac{d \ln(2en/d) + \ln(4/\delta)}{n}}$$

여기서 $d = \text{VCdim}(\mathcal{H})$입니다.

이를 **표본 복잡도** 형태로 다시 쓰면: $R(h) \leq \hat{R}_S(h) + \epsilon$을 확률 $1-\delta$로 보장하려면:

$$n \geq \frac{C}{\epsilon^2}\left(d\ln\frac{1}{\epsilon} + \ln\frac{1}{\delta}\right)$$

| 구성 요소 | bound에 미치는 영향 | 해석 |
|---------|-------------------|------|
| $d$ (VC 차원) | $\sqrt{d/n}$ | 복잡도 ↑ → bound 느슨 |
| $n$ (샘플 수) | $1/\sqrt{n}$ | 데이터 ↑ → bound 촘촘 |
| $\delta$ (실패 확률) | $\sqrt{\ln(1/\delta)/n}$ | 높은 신뢰도 → bound 느슨 |

```python
import numpy as np

def vc_bound(n, d, delta, empirical_risk):
    """VC 일반화 bound 계산"""
    complexity_term = np.sqrt((d * np.log(2*np.e*n/d) + np.log(4/delta)) / n)
    return empirical_risk + complexity_term

# 예: 선형 분류기(d=3), n=1000, delta=0.05
# vc_bound(1000, 3, 0.05, 0.01) -> 약 0.12 (상당히 느슨함)
```

> **핵심 직관**: VC bound는 분포 무관(distribution-free)이라 매우 보수적입니다. gt-05에서 다룰 Rademacher 복잡도는 데이터 분포를 활용하여 더 타이트한 bound를 제공합니다.

---

## 6. VC 이론의 한계

VC 이론은 강력하지만, 현대 머신러닝에서 여러 한계가 있습니다:

$$\text{VCdim}(\text{ReLU 신경망}) = O(WL\log W)$$

여기서 $W$는 가중치 수, $L$은 층 수입니다. 전형적 신경망에서 $W \sim 10^6$이면:

$$\text{VC bound} \sim \sqrt{\frac{10^6}{n}}$$

$n = 50000$ (CIFAR-10)이면 bound $\sim 4.5$로 자명합니다 — 0-1 손실이 최대 1인데 bound가 4.5입니다.

| 한계 | 설명 | 대안 |
|------|------|------|
| 과도하게 느슨함 | 실제 일반화 갭보다 수십 배 큼 | Rademacher, PAC-Bayes |
| 분포 무관 | 데이터 구조를 활용하지 않음 | 데이터 의존적 bound |
| 최악의 경우 | 평균적 거동을 포착하지 못함 | 알고리즘 의존적 bound |
| 과매개변수 설명 불가 | 높은 VC 차원에도 일반화하는 현상 | gt-08~10의 현대 이론 |

> **핵심 직관**: VC 차원은 가설 공간의 용량을 측정하지만, 학습 알고리즘이 그 공간의 어디를 탐색하는지는 고려하지 않습니다. 이것이 현대 이론의 출발점입니다.

---

## 핵심 정리

- **VC 차원은 가설 클래스가 산산조각 낼 수 있는 최대 점 집합의 크기로, 무한 가설 공간의 조합론적 복잡도를 측정합니다**
- **$\mathbb{R}^d$의 아핀 선형 분류기의 VC 차원은 $d+1$이며, Radon 정리에 의해 상한이 증명됩니다**
- **Sauer-Shelah 보조정리는 유한 VC 차원 $d$일 때 성장 함수가 $(en/d)^d$로 상한되어 다항식적으로 증가함을 보입니다**
- **VC bound는 일반화 갭을 $O(\sqrt{d\log n / n})$으로 제어하며, 분포 무관 보장을 제공합니다**
- **현대 신경망의 높은 VC 차원에도 불구한 일반화 성공은 VC 이론의 한계를 드러내며, 알고리즘 의존적 이론의 필요성을 제기합니다**
