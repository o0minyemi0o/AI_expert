# 경사 하강법의 암묵적 편향: 최적화가 일반화를 결정한다

## 왜 최적화 알고리즘의 선택이 일반화에 중요한가

gt-03~05의 고전 이론은 가설 공간의 복잡도에 초점을 맞추지만, 과매개변수 모델에서는 가설 공간에 무한히 많은 보간 해가 존재합니다. 어떤 해를 선택하느냐에 따라 일반화 성능이 결정되며, 이 선택은 **최적화 알고리즘**이 수행합니다. 경사 하강법(GD)과 그 변형들이 특정 구조의 해를 암묵적으로 선호하는 현상 — **암묵적 편향(implicit bias)** — 이 현대 일반화 이론의 핵심입니다.

---

## 1. 보간 해의 다양성과 알고리즘의 역할

$p > n$인 과매개변수 설정에서 $Xw = y$의 해 집합은 $(p-n)$차원 아핀 부분공간입니다:

$$\mathcal{W} = \{w : Xw = y\} = w_{\text{특수}} + \text{null}(X)$$

이 무한한 해 중 어떤 것을 선택하느냐가 일반화를 결정합니다.

| 해 선택 기준 | 수학적 형태 | 일반화 효과 |
|------------|-----------|-----------|
| 최소 $\ell_2$ 노름 | $\arg\min_{Xw=y} \|w\|_2$ | 매끄러운 함수 |
| 최소 $\ell_1$ 노름 | $\arg\min_{Xw=y} \|w\|_1$ | 희소 함수 |
| 최대 마진 | $\arg\max_{Xw=y} \min_i y_i w^Tx_i / \|w\|$ | 견고한 분류 |
| 최소 랭크 | $\arg\min_{XW=Y} \text{rank}(W)$ | 저차원 구조 |

**핵심 질문:** GD는 어떤 해를 선택하는가?

$$w_{\text{GD}}(t) = w_0 - \eta \sum_{\tau=0}^{t-1} \nabla \hat{R}_S(w_\tau)$$

> **핵심 직관**: 명시적 정규화 없이도 GD는 "가장 자연스러운" 해를 선택합니다. 이 자연스러움의 정의가 문제 구조와 매개변수화에 의존합니다.

---

## 2. 선형 모델에서의 최소 노름 해

**정리:** $w_0 = 0$에서 시작하는 경사 하강법은, 과매개변수 선형 회귀에서 최소 $\ell_2$ 노름 해로 수렴합니다:

$$w_{\text{GD}} \to w^+ = X^T(XX^T)^{-1}y = \arg\min_{Xw=y}\|w\|_2$$

**증명:** GD의 업데이트 $w_{t+1} = w_t - \eta X^T(Xw_t - y)$에서:

$$w_t = X^T \alpha_t, \quad \alpha_t \in \mathbb{R}^n$$

이것은 $w_t$가 항상 $X$의 행 공간에 있음을 의미합니다.

$$w_t \in \text{row}(X) = \text{null}(X)^\perp$$

row space의 해가 정확히 최소 노름 해입니다. $\blacksquare$

```python
import numpy as np

def gd_implicit_bias_linear(X, y, lr=0.01, max_iter=10000, tol=1e-10):
    """GD의 암묵적 편향: 최소 노름 해로의 수렴 확인"""
    n, p = X.shape
    w = np.zeros(p)  # 원점에서 시작
    for t in range(max_iter):
        grad = X.T @ (X @ w - y) / n
        w -= lr * grad
        if np.linalg.norm(X @ w - y) < tol:
            break

    # 최소 노름 해와 비교
    w_min_norm = X.T @ np.linalg.solve(X @ X.T, y)
    print(f"GD 해 노름: {np.linalg.norm(w):.6f}")
    print(f"최소 노름 해: {np.linalg.norm(w_min_norm):.6f}")
    print(f"차이: {np.linalg.norm(w - w_min_norm):.8f}")
    return w
```

> **핵심 직관**: GD의 그래디언트 $X^T(Xw-y)$는 항상 $X$의 행 공간에 속합니다. 원점에서 시작하면 $w$는 절대 null space에 진입하지 않으며, 이것이 최소 노름 해를 선택하는 메커니즘입니다.

---

## 3. 선형 분류에서의 최대 마진

분류 문제에서 GD의 암묵적 편향은 더 흥미롭습니다.

**정리 (Soudry et al., 2018):** 분리 가능한 데이터에서 로지스틱 회귀를 GD로 학습하면:

$$\frac{w_t}{\|w_t\|} \to \hat{w}_{\text{SVM}} = \frac{w_{\text{SVM}}}{\|w_{\text{SVM}}\|}$$

여기서 $w_{\text{SVM}}$은 하드 마진 SVM 해입니다.

수렴 속도: $\|w_t\| \sim \log t$ (로그적 발산)

$$w_t = \hat{w}_{\text{SVM}} \cdot \log t + O(\log\log t)$$

| 손실 함수 | 꼬리 감쇠 | 수렴하는 해 | 수렴 속도 |
|---------|---------|-----------|---------|
| 로지스틱 $\log(1+e^{-z})$ | 지수적 | $\ell_2$ 최대 마진 | $O(1/\log t)$ |
| 지수 $e^{-z}$ | 지수적 | $\ell_2$ 최대 마진 | $O(1/\log t)$ |
| 다항식 $(1+z)^{-q}$ | 다항식적 | $\ell_2$ 최대 마진 | $O(1/t^{1/q})$ |

**최대 마진의 일반화 의미:**

마진 $\gamma$에 대한 일반화 bound:

$$R(h) \leq \hat{R}_\gamma(h) + O\left(\frac{1}{\gamma\sqrt{n}}\right)$$

여기서 $\hat{R}_\gamma$는 마진 손실입니다.

> **핵심 직관**: GD가 최대 마진 해를 선택한다는 것은 SVM의 성공을 GD의 관점에서 재해석한 것입니다. 명시적 마진 최대화 없이도 GD가 암묵적으로 마진을 최대화합니다.

---

## 4. 미러 디센트와 다른 기하학

GD의 암묵적 편향은 사용하는 알고리즘에 따라 변합니다.

**미러 디센트 (Mirror Descent):**

$$w_{t+1} = \arg\min_w \left[\eta \langle \nabla \hat{R}(w_t), w \rangle + D_\psi(w, w_t)\right]$$

여기서 $D_\psi$는 Bregman 발산입니다.

| 미러 맵 $\psi(w)$ | Bregman 발산 | 암묵적 편향 | 대응 정규화 |
|------------------|-------------|-----------|-----------|
| $\frac{1}{2}\|w\|_2^2$ | $\frac{1}{2}\|w-w'\|_2^2$ (GD) | 최소 $\ell_2$ 노름 | L2 |
| $\sum w_j \log w_j$ | KL 발산 (지수 GD) | 최소 $\ell_1$ 노름 | L1 |
| $-\sum \log w_j$ | Burg 엔트로피 | 비음수 + 작은 노름 | 비음수 제약 |
| $\frac{1}{2}\|w\|_p^2$ | $p$-노름 거리 | 최소 $\ell_q$ 노름 | $L_q$ ($1/p+1/q=1$) |

**정리 (Gunasekar et al., 2018):** 미러 디센트는 보간 해 중 초기점으로부터의 Bregman 발산을 최소화하는 해로 수렴합니다:

$$w_{\text{MD}} \to \arg\min_{Xw=y} D_\psi(w, w_0)$$

```python
import numpy as np

def mirror_descent_exp(X, y, lr=0.01, max_iter=10000):
    """지수 경사 하강법 (L1 암묵적 편향)"""
    n, p = X.shape
    w = np.ones(p) / p  # 균일 초기화
    for t in range(max_iter):
        grad = X.T @ (X @ w - y) / n
        # 곱셈적 업데이트 (미러 디센트의 지수 맵)
        w = w * np.exp(-lr * grad)
        w = w / np.sum(w) * np.sum(np.ones(p)/p)  # 선택적 정규화
    return w
```

> **핵심 직관**: 알고리즘을 바꾸면 암묵적 편향이 바뀝니다. GD는 L2, 지수적 GD는 L1, 좌표 하강법은 또 다른 편향을 가집니다. 문제에 맞는 알고리즘 선택이 곧 정규화 선택입니다.

---

## 5. 깊은 신경망에서의 암묵적 편향

비선형 신경망에서의 암묵적 편향은 선형 모델보다 훨씬 풍부합니다.

**행렬 분해 (깊이 2):** $W = UV$에 대해 $\min_{UV} \|XUV-y\|^2$

GD의 암묵적 편향:

$$UV_{\text{GD}} \to \arg\min_{XW=y} \|W\|_* \quad \text{(핵 노름, 즉 저랭크 선호)}$$

| 매개변수화 | 목적 함수 | 암묵적 편향 |
|-----------|---------|-----------|
| $w$ (직접) | $\min \|Xw-y\|^2$ | $\min \|w\|_2$ |
| $w = u^2$ (재매개변수화) | $\min \|Xu^2-y\|^2$ | 희소 해 선호 |
| $W = UV$ (행렬 분해) | $\min \|XUV-y\|^2$ | $\min \|W\|_*$ (저랭크) |
| $W = U_1U_2\cdots U_L$ | $\min \|XU_1\cdots U_L-y\|^2$ | 더 강한 저랭크 |

**깊이의 효과:**

$$\text{깊이 } L \text{ 행렬 분해}: \quad \text{암묵적 편향} \to \min \sum_i \sigma_i^{2/L}$$

$L \to \infty$에서 랭크 최소화에 접근합니다.

> **핵심 직관**: 깊이는 단순한 표현력 증가가 아니라 암묵적 정규화를 강화합니다. 깊은 매개변수화가 더 "단순한" 해를 선호하게 만듭니다.

---

## 6. SGD의 암묵적 편향: 평탄한 최소점 선호

gt-06에서 언급한 SGD 노이즈의 정규화 효과를 형식화합니다.

**SGD의 연속 시간 근사 (SDE):**

$$dw = -\nabla \hat{R}(w)dt + \sqrt{\frac{\eta}{B}\Sigma(w)} dB_t$$

여기서 $\Sigma(w)$는 그래디언트 공분산, $B$는 배치 크기입니다.

**정상 분포 (Stationary Distribution):**

$$p_{\text{stat}}(w) \propto \exp\left(-\frac{2B}{\eta}\hat{R}(w) - \frac{B}{\eta}\ln\det\Sigma(w)\right)$$

두 번째 항이 핵심입니다: $\ln\det\Sigma(w) \approx \ln\det\nabla^2\hat{R}(w)$ (0 근처)

$$\text{SGD 선호} \propto \exp\left(-\frac{B}{\eta}\sum_i \ln\lambda_i(\nabla^2\hat{R})\right)$$

이는 **Hessian의 고유값이 작은** (평탄한) 최소점을 선호합니다.

| 요소 | 효과 | 평탄한 최소점 선호 강도 |
|------|------|-------------------|
| 학습률 $\eta$ ↑ | 노이즈 ↑ | 강해짐 |
| 배치 크기 $B$ ↓ | 노이즈 ↑ | 강해짐 |
| $\eta/B$ (노이즈 온도) | 핵심 비율 | 클수록 강함 |

```python
import numpy as np

def sgd_flatness_comparison(model, X, y, lr, batch_sizes, n_epochs=100):
    """배치 크기에 따른 Hessian 스펙트럼 비교"""
    results = {}
    for B in batch_sizes:
        model.reset()
        for epoch in range(n_epochs):
            indices = np.random.permutation(len(X))
            for i in range(0, len(X), B):
                batch_idx = indices[i:i+B]
                grad = model.compute_gradient(X[batch_idx], y[batch_idx])
                model.update(-lr * grad)
        # Hessian의 트레이스 (평탄도 측도)
        hessian_trace = model.compute_hessian_trace(X, y)
        results[B] = {'hessian_trace': hessian_trace,
                       'test_error': model.evaluate(X_test, y_test)}
    return results  # 작은 B -> 작은 Hessian trace (더 평탄)
```

> **핵심 직관**: SGD의 노이즈는 "나쁜 국소 최소점 탈출기"입니다. 날카로운 최소점은 탈출하기 쉽고 평탄한 최소점에 머물기 쉬워, SGD는 자연스럽게 평탄한 최소점을 선택합니다. 이것이 gt-06에서 다룬 암묵적 정규화의 메커니즘입니다.

---

## 7. 평탄도와 일반화의 연결

평탄한 최소점이 왜 잘 일반화하는지를 형식화합니다.

**PAC-Bayes를 통한 연결 (gt-11에서 상세):**

$w^*$ 주변의 가우시안 섭동 $Q = \mathcal{N}(w^*, \sigma^2 I)$에 대해:

$$R(\mathbb{E}_{w\sim Q}[h_w]) \leq \hat{R}_S(\mathbb{E}_{w\sim Q}[h_w]) + \sqrt{\frac{\|w^*\|^2/(2\sigma^2) + \ln(n/\delta)}{2n}}$$

평탄한 최소점에서는 $\sigma$를 크게 해도 성능이 유지되므로:

$$\text{평탄함} \implies \text{큰 } \sigma \text{ 허용} \implies \text{작은 } \frac{\|w^*\|^2}{2\sigma^2} \implies \text{타이트한 bound}$$

> **핵심 직관**: 평탄한 최소점은 "파라미터를 흔들어도 출력이 안정적"이라는 뜻이며, 이는 자연스럽게 압축 가능성과 일반화 보장으로 이어집니다.

---

## 핵심 정리

- **과매개변수 모델에서 GD는 무한히 많은 보간 해 중 최소 $\ell_2$ 노름 해를 암묵적으로 선택하며, 이것이 명시적 정규화 없이도 일반화하는 메커니즘입니다**
- **선형 분류에서 GD는 방향적으로 최대 마진 (SVM) 해에 수렴하며, 로지스틱 손실의 꼬리 감쇠 특성이 이를 결정합니다**
- **미러 디센트는 Bregman 발산에 따라 다른 암묵적 편향을 유도하며, 알고리즘 선택이 곧 암묵적 정규화 선택입니다**
- **깊은 매개변수화($W = U_1\cdots U_L$)는 저랭크 해를 선호하는 더 강한 암묵적 편향을 유도하며, 깊이가 정규화 강도를 높입니다**
- **SGD의 미니배치 노이즈는 Hessian의 고유값이 작은 평탄한 최소점을 선호하며, 학습률/배치크기 비율 $\eta/B$가 이 정규화의 강도를 결정합니다**
