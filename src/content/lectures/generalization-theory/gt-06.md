# 정규화의 이론: 복잡도 제어를 통한 일반화

## 왜 정규화 이론이 중요한가

gt-03~05에서 다룬 일반화 bound들은 모두 가설 공간의 복잡도가 작을수록 타이트합니다. **정규화(regularization)** 는 이 복잡도를 명시적으로 제어하여 일반화를 개선하는 가장 실용적인 도구입니다. L1/L2 벌점화부터 조기 종료, 드롭아웃까지 — 다양한 정규화 기법이 왜 작동하는지를 통합적 관점에서 이론적으로 분석합니다.

---

## 1. 정규화된 위험 최소화

ERM에 복잡도 벌점을 추가한 **정규화된 위험 최소화(Regularized ERM)** 를 정의합니다:

$$h_\lambda = \arg\min_{h \in \mathcal{H}} \left[\hat{R}_S(h) + \lambda \Omega(h)\right]$$

여기서 $\Omega: \mathcal{H} \to \mathbb{R}_{\geq 0}$는 정규화 함수, $\lambda > 0$는 정규화 강도입니다.

**구조적 위험 최소화(SRM)** 와의 관계:

$$\text{SRM}: \quad \mathcal{H}_1 \subset \mathcal{H}_2 \subset \cdots, \quad h^* = \arg\min_k \left[\min_{h \in \mathcal{H}_k}\hat{R}_S(h) + \text{pen}(k, n, \delta)\right]$$

| 방법 | 복잡도 제어 | 연속적 | 이론적 보장 |
|------|-----------|-------|-----------|
| ERM | 없음 | - | VC bound |
| SRM | 가설 클래스 중첩 | 이산적 | 각 클래스별 bound |
| 정규화 ERM | 벌점 함수 $\Omega$ | 연속적 | 정규화 의존 bound |

> **핵심 직관**: 정규화는 SRM의 연속적 이완(relaxation)으로 볼 수 있습니다. $\lambda$를 키우면 효과적으로 더 작은 $\mathcal{H}_k$에서 탐색하는 것과 동일합니다.

---

## 2. L2 정규화 (릿지)의 일반화 효과

선형 모델 $h_w(x) = w^Tx$에 대해 L2 정규화를 적용합니다:

$$w_\lambda = \arg\min_w \frac{1}{n}\|Xw - y\|^2 + \lambda\|w\|_2^2$$

닫힌 형태 해: $w_\lambda = (X^TX + n\lambda I)^{-1}X^Ty$

**일반화 bound (gt-05의 Rademacher 결과 적용):**

$\|w\|_2 \leq B_\lambda$이면:

$$R(h_{w_\lambda}) \leq \hat{R}_S(h_{w_\lambda}) + \frac{2B_\lambda C}{\sqrt{n}} + \sqrt{\frac{\ln(2/\delta)}{2n}}$$

L2 정규화의 효과를 편향-분산으로 분해합니다:

$$\text{편향}^2(\lambda) = \lambda^2 w^{*T}(X^TX + n\lambda I)^{-2}X^TX w^*$$

$$\text{분산}(\lambda) = \frac{\sigma^2}{n}\text{tr}\left[(X^TX + n\lambda I)^{-2}X^TX\right]$$

| $\lambda$ | 편향 | 분산 | 유효 자유도 |
|-----------|------|------|-----------|
| $0$ | 최소 | 최대 | $p$ |
| 적절한 값 | 약간 증가 | 크게 감소 | $< p$ |
| $\to \infty$ | 최대 | $\to 0$ | $\to 0$ |

```python
import numpy as np

def ridge_bias_variance(X, w_true, sigma, lam):
    """릿지 회귀의 편향과 분산 계산"""
    n, p = X.shape
    XtX = X.T @ X
    inv_term = np.linalg.inv(XtX + n * lam * np.eye(p))

    bias_sq = lam**2 * w_true @ inv_term @ XtX @ inv_term @ w_true
    variance = sigma**2 / n * np.trace(inv_term @ XtX @ inv_term @ XtX)
    eff_dof = np.trace(XtX @ inv_term)

    return {'bias2': bias_sq, 'variance': variance, 'eff_dof': eff_dof}
```

> **핵심 직관**: L2 정규화는 가중치를 0으로 수축(shrinkage)시킵니다. gt-02에서 다룬 유효 자유도가 $\text{tr}(H_\lambda)$로 감소하며, 이것이 분산 감소의 메커니즘입니다.

---

## 3. L1 정규화 (Lasso)의 희소성과 일반화

L1 정규화는 다른 기하학적 성질을 가집니다:

$$w_\lambda = \arg\min_w \frac{1}{n}\|Xw - y\|^2 + \lambda\|w\|_1$$

**일반화 bound:** 참 가중치가 $s$-희소적이면 ($\|w^*\|_0 = s$):

$$\|w_\lambda - w^*\|_2 \leq C\lambda\sqrt{s}$$

표본 복잡도: $n = O(s \log p)$이면 충분합니다.

| 비교 | L2 (릿지) | L1 (Lasso) |
|------|---------|-----------|
| 벌점 | $\|w\|_2^2$ | $\|w\|_1$ |
| 기하학 | $\ell_2$ 공 | $\ell_1$ 다면체 |
| 해의 특성 | 수축 (모든 성분) | 희소 (일부 성분 = 0) |
| 변수 선택 | 불가 | 가능 |
| 표본 복잡도 | $O(p)$ | $O(s\log p)$ |
| 일반화 메커니즘 | 분산 감소 | 차원 축소 |

L1의 희소성이 일반화에 도움이 되는 이유:

$$\text{유효 복잡도} \propto s\log p \ll p \quad \text{(}s \ll p\text{일 때)}$$

```python
import numpy as np

def soft_threshold(w, lam):
    """L1 근접 연산자 (소프트 임계화)"""
    return np.sign(w) * np.maximum(np.abs(w) - lam, 0)

def lasso_coordinate_descent(X, y, lam, max_iter=1000):
    """Lasso의 좌표 하강법 구현"""
    n, p = X.shape
    w = np.zeros(p)
    for _ in range(max_iter):
        for j in range(p):
            r = y - X @ w + X[:, j] * w[j]
            w[j] = soft_threshold(X[:, j] @ r / n, lam)
    return w
```

> **핵심 직관**: L1 정규화의 $\ell_1$ 공은 꼭짓점이 축 위에 있어서, 해가 좌표축에 "끌려가" 정확히 0인 성분이 생깁니다. 이 희소성이 고차원 문제에서 차원의 저주를 극복합니다.

---

## 4. 조기 종료의 정규화 효과

경사 하강법의 반복 횟수 $T$를 제한하는 **조기 종료(early stopping)** 도 정규화입니다.

선형 회귀에서 경사 하강법의 $T$-단계 해:

$$w_T = \sum_{t=0}^{T-1} \eta(I - \eta X^TX)^t X^Ty$$

이것의 스펙트럼 분석:

$$w_T = \sum_{j=1}^{p} \left(1 - (1-\eta\sigma_j^2)^T\right) \frac{u_j^Ty}{\sigma_j} v_j$$

여기서 $\sigma_j$는 $X$의 특이값, $u_j, v_j$는 좌/우 특이벡터입니다.

릿지 회귀의 스펙트럼 필터와 비교합니다:

| 필터 | 릿지 ($\lambda$) | 조기 종료 ($T$) |
|------|----------------|---------------|
| 필터 함수 | $\frac{\sigma_j^2}{\sigma_j^2 + \lambda}$ | $1 - (1-\eta\sigma_j^2)^T$ |
| 큰 특이값 | $\approx 1$ | $\approx 1$ |
| 작은 특이값 | $\approx \sigma_j^2/\lambda$ | $\approx T\eta\sigma_j^2$ |
| 대응 관계 | $\lambda$ ↑ → 강한 정규화 | $T$ ↓ → 강한 정규화 |

$T\eta \approx 1/\lambda$의 대응 관계가 성립합니다.

> **핵심 직관**: 조기 종료와 L2 정규화는 스펙트럼 공간에서 거의 동일한 필터링을 수행합니다. GD의 초기 반복은 큰 특이값(신호) 방향을 먼저 학습하고, 나중에 작은 특이값(잡음)을 학습합니다.

---

## 5. 드롭아웃의 정규화 이론

드롭아웃은 훈련 중 뉴런을 확률 $p$로 제거하는 기법입니다.

**정리 (Wager et al., 2013):** 선형 모델에서 드롭아웃은 적응적 L2 정규화와 등가입니다:

$$\mathbb{E}_{\text{mask}}[\hat{R}_{\text{dropout}}(w)] = \hat{R}(w) + \frac{p}{1-p}\sum_{j=1}^{p} w_j^2 \text{Var}[x_j]$$

일반 신경망에서는:

$$\text{드롭아웃} \approx \text{가중치 감소} + \text{데이터 증강} + \text{앙상블}$$

| 정규화 기법 | 등가 해석 | 일반화 메커니즘 |
|-----------|---------|--------------|
| L2 | 가중치 수축 | 분산 감소 |
| L1 | 희소화 | 차원 축소 |
| 드롭아웃 | 적응적 L2 + 앙상블 | 공동적응 방지 |
| 배치 정규화 | 노이즈 주입 | 지형 평탄화 |
| 데이터 증강 | 불변성 주입 | 유효 데이터 증가 |

```python
import numpy as np

def dropout_regularization_equiv(X, w, drop_rate=0.5):
    """드롭아웃의 등가 정규화 계수 계산"""
    feature_variances = np.var(X, axis=0)
    adaptive_lambda = drop_rate / (1 - drop_rate) * feature_variances
    # 등가 벌점: sum(adaptive_lambda * w**2)
    penalty = np.sum(adaptive_lambda * w**2)
    return adaptive_lambda, penalty
```

> **핵심 직관**: 드롭아웃이 특히 효과적인 이유는 분산이 큰 특성에 대해 더 강한 정규화를 자동으로 적용하기 때문입니다. 이는 고정된 $\lambda$를 사용하는 L2보다 적응적입니다.

---

## 6. 암묵적 정규화

명시적 정규화 항 없이도 **학습 알고리즘 자체**가 정규화 효과를 가질 수 있습니다.

**경사 하강법의 암묵적 정규화 (gt-10에서 상세):**

- 선형 분류: GD는 최대 마진 해로 수렴 → L2 노름 최소화
- 행렬 분해: GD는 낮은 랭크 해를 선호 → 핵 노름 정규화
- 깊은 신경망: 가중치 초기화 근처에 머무름 → 유효 복잡도 제한

**SGD의 노이즈에 의한 정규화:**

미니배치 SGD의 그래디언트 노이즈:

$$g_B = \nabla\hat{R}_B(w) = \nabla\hat{R}_S(w) + \xi_B, \quad \text{Cov}[\xi_B] \approx \frac{1}{B}\Sigma$$

이 노이즈는 날카로운 최소점에서 탈출하게 하여 **평탄한 최소점(flat minima)** 을 선호합니다:

$$\text{SGD noise} \propto \frac{\eta}{B}\text{tr}(\nabla^2 R(w))$$

| 암묵적 정규화 원천 | 메커니즘 | 선호하는 해 |
|----------------|---------|-----------|
| GD 경로 | 초기점에서 최소 거리 | 최소 노름 |
| SGD 노이즈 | 날카로운 최소점 탈출 | 평탄한 최소점 |
| 배치 크기 | 노이즈 크기 $\propto 1/B$ | 작은 $B$ → 강한 정규화 |
| 학습률 | 탈출 에너지 $\propto \eta$ | 큰 $\eta$ → 강한 정규화 |
| 아키텍처 | CNN, RNN의 구조 | 불변성, 패턴 공유 |

> **핵심 직관**: 현대 딥러닝의 일반화를 설명하는 핵심 열쇠는 명시적 정규화가 아닌 암묵적 정규화에 있습니다. gt-08~10에서 이를 깊이 탐구합니다.

---

## 핵심 정리

- **정규화된 위험 최소화는 $\hat{R}_S(h) + \lambda\Omega(h)$를 최소화하며, $\lambda$는 적합도와 복잡도 사이의 트레이드오프를 제어합니다**
- **L2 정규화는 가중치를 수축시켜 분산을 감소시키며, 스펙트럼 필터링 관점에서 작은 특이값 방향의 기여를 억제합니다**
- **L1 정규화는 희소한 해를 유도하여 유효 차원을 $s\log p$로 줄이며, 고차원 문제에서 차원의 저주를 극복합니다**
- **조기 종료는 L2 정규화와 스펙트럼적으로 등가이며, 반복 횟수 $T$와 $1/\lambda$ 사이에 대응 관계가 성립합니다**
- **SGD의 미니배치 노이즈와 학습률은 암묵적 정규화를 제공하며, 평탄한 최소점을 선호하여 일반화를 개선합니다**
