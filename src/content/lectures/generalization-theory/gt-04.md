# PAC 학습 프레임워크: 학습 가능성의 수학적 정의

## 왜 PAC 학습이 중요한가

gt-01에서 일반화의 개념을, gt-03에서 VC 차원을 다루었습니다. 그러나 "언제 학습이 가능한가"라는 근본적 질문에 대한 형식적 답은 아직 주어지지 않았습니다. **PAC(Probably Approximately Correct) 학습 프레임워크**는 Valiant(1984)가 도입한 계산 학습 이론의 기초로, 학습 가능성을 정확도($\epsilon$)와 신뢰도($\delta$)의 함수로 정량화합니다.

---

## 1. PAC 학습의 형식적 정의

**정의 (PAC 학습 가능성):** 가설 클래스 $\mathcal{H}$가 **PAC 학습 가능(PAC learnable)** 하다 함은, 알고리즘 $\mathcal{A}$가 존재하여 임의의 $\epsilon > 0$, $\delta > 0$, 임의의 분포 $\mathcal{D}$에 대해:

$$n \geq n_0(\epsilon, \delta) \implies \Pr_{S \sim \mathcal{D}^n}\left[R(h_S) - \min_{h \in \mathcal{H}}R(h) \leq \epsilon\right] \geq 1 - \delta$$

여기서 $n_0(\epsilon, \delta)$는 $\text{poly}(1/\epsilon, 1/\delta)$입니다.

| 용어 | 기호 | 의미 |
|------|------|------|
| Probably | $1-\delta$ | 높은 확률로 |
| Approximately | $\epsilon$ | 근사적으로 |
| Correct | $R(h_S) \leq \epsilon$ | 정확하게 |

두 가지 변형을 구분합니다:

- **실현 가능 (Realizable):** $\min_{h \in \mathcal{H}} R(h) = 0$ 가정
- **불가지론적 (Agnostic):** $\min_{h \in \mathcal{H}} R(h) \geq 0$ 허용

> **핵심 직관**: PAC 프레임워크는 "이 문제를 풀 수 있는가?"가 아니라 "주어진 샘플 수로 이 정확도를 이 확률로 달성할 수 있는가?"를 묻습니다.

---

## 2. 유한 가설 클래스의 표본 복잡도

가장 단순한 경우인 유한 $\mathcal{H}$, 실현 가능 설정에서 시작합니다.

**정리:** $|\mathcal{H}| < \infty$이고 실현 가능 설정에서, ERM 알고리즘의 표본 복잡도는:

$$n \geq \frac{1}{\epsilon}\left(\ln|\mathcal{H}| + \ln\frac{1}{\delta}\right)$$

**증명:**
$h^*$를 참 가설이라 합니다 ($R(h^*) = 0$). "나쁜" 가설 $h$는 $R(h) > \epsilon$이지만 $\hat{R}_S(h) = 0$인 가설입니다.

개별 나쁜 가설에 대해:

$$\Pr[\hat{R}_S(h) = 0] = \Pr[\forall i: h(x_i) = y_i] = (1-R(h))^n \leq (1-\epsilon)^n \leq e^{-n\epsilon}$$

합집합 한계(union bound)를 적용하면:

$$\Pr[\exists \text{나쁜 } h : \hat{R}_S(h)=0] \leq |\mathcal{H}| \cdot e^{-n\epsilon}$$

$|\mathcal{H}| \cdot e^{-n\epsilon} \leq \delta$를 풀면 결과를 얻습니다. $\blacksquare$

```python
import numpy as np

def sample_complexity_finite(H_size, epsilon, delta):
    """유한 가설 클래스의 PAC 표본 복잡도"""
    return int(np.ceil((np.log(H_size) + np.log(1/delta)) / epsilon))

# 예: |H| = 1000, epsilon = 0.1, delta = 0.05
# sample_complexity_finite(1000, 0.1, 0.05) -> 99
```

> **핵심 직관**: 표본 복잡도는 $\ln|\mathcal{H}|$에 비례합니다 — $|\mathcal{H}|$ 자체가 아닌 로그에 비례하므로, 가설 공간이 지수적으로 커져도 표본 수는 선형으로만 증가합니다.

---

## 3. 불가지론적 PAC 학습

실현 가능 가정을 제거하면 문제가 근본적으로 어려워집니다.

**정의 (불가지론적 PAC):** 임의의 분포 $\mathcal{D}$에 대해:

$$\Pr\left[R(h_S) \leq \min_{h \in \mathcal{H}} R(h) + \epsilon\right] \geq 1 - \delta$$

유한 가설 클래스에 대한 불가지론적 bound는 Hoeffding 부등식과 합집합 한계를 결합합니다:

$$n \geq \frac{2}{\epsilon^2}\left(\ln|\mathcal{H}| + \ln\frac{2}{\delta}\right)$$

| 설정 | 표본 복잡도 | $\epsilon$ 의존성 | 차이 원인 |
|------|-----------|----------------|---------|
| 실현 가능 | $\frac{1}{\epsilon}(\ln\|\mathcal{H}\| + \ln\frac{1}{\delta})$ | $O(1/\epsilon)$ | 경험적 위험이 0인 가설만 고려 |
| 불가지론적 | $\frac{2}{\epsilon^2}(\ln\|\mathcal{H}\| + \ln\frac{2}{\delta})$ | $O(1/\epsilon^2)$ | 균일 수렴 필요 |

차이가 $1/\epsilon$ vs $1/\epsilon^2$인 이유는, 실현 가능 설정에서는 단측(one-sided) 집중만 필요하지만, 불가지론적 설정에서는 양측(two-sided) 균일 수렴이 필요하기 때문입니다.

> **핵심 직관**: 불가지론적 설정이 훨씬 현실적입니다 — 실제로 참 함수가 가설 클래스에 정확히 속하는 경우는 드뭅니다. 그 대가로 $1/\epsilon^2$의 더 높은 표본 복잡도를 지불합니다.

---

## 4. 학습 가능성의 근본 정리

gt-03에서 예고한 핵심 결과를 진술합니다.

**정리 (Fundamental Theorem of Statistical Learning):** 이진 분류에서 손실 함수가 0-1 손실일 때, 가설 클래스 $\mathcal{H}$에 대해 다음이 동치입니다:

1. $\mathcal{H}$는 (균일 수렴) 성질을 가진다
2. $\mathcal{H}$는 불가지론적 PAC 학습 가능하다
3. $\mathcal{H}$는 PAC 학습 가능하다
4. $\text{VCdim}(\mathcal{H}) < \infty$

더 정확히, VC 차원 $d$와 표본 복잡도의 관계는:

$$\frac{d-1}{32\epsilon} \leq n_{\mathcal{H}}(\epsilon, \delta) \leq \frac{C}{\epsilon^2}\left(d + \ln\frac{1}{\delta}\right)$$

| 방향 | 내용 | 증명 도구 |
|------|------|----------|
| $(1) \Rightarrow (2)$ | 균일 수렴 → PAC 학습 | ERM의 성공 보장 |
| $(2) \Rightarrow (3)$ | 불가지론적 → 실현 가능 | 정의의 특수화 |
| $(3) \Rightarrow (4)$ | PAC 학습 → 유한 VC | No Free Lunch 정리 |
| $(4) \Rightarrow (1)$ | 유한 VC → 균일 수렴 | Sauer 보조정리 + 대칭화 |

> **핵심 직관**: 이 정리는 "학습 가능성"이라는 개념 자체를 하나의 수학적 조건(유한 VC 차원)으로 완전히 특성화합니다.

---

## 5. No Free Lunch 정리

학습 가능성에는 근본적 한계가 존재합니다.

**정리 (No Free Lunch):** 임의의 학습 알고리즘 $\mathcal{A}$에 대해, 어떤 분포 $\mathcal{D}$가 존재하여:

1. $\exists f^* : R_{\mathcal{D}}(f^*) = 0$ (실현 가능)
2. $n \leq |\mathcal{X}|/2$이면 $\mathbb{E}_S[R_{\mathcal{D}}(\mathcal{A}(S))] \geq 1/4$

이 정리의 함의: **보편적으로 좋은 학습 알고리즘은 존재하지 않습니다.** 모든 학습은 암묵적으로든 명시적으로든 가정(귀납적 편향)에 의존합니다.

```python
def no_free_lunch_demo(domain_size, n_samples, n_algorithms=100):
    """NFL 정리 시연: 어떤 알고리즘도 모든 분포에서 이길 수 없음"""
    import numpy as np
    results = np.zeros(n_algorithms)
    # 모든 가능한 참 함수에 대해 평균 성능은 동일
    all_functions = np.array(list(
        np.ndindex(*([2]*domain_size))
    ))  # 모든 이진 함수
    for alg_idx in range(n_algorithms):
        total_error = 0
        for f in all_functions:
            # 알고리즘은 훈련 데이터에서 학습 후 나머지에서 평가
            train_idx = np.random.choice(domain_size, n_samples, replace=False)
            test_idx = np.setdiff1d(range(domain_size), train_idx)
            # 최선의 경우에도 미관측 점에서는 무작위
            total_error += 0.5 * len(test_idx)
        results[alg_idx] = total_error / (len(all_functions) * domain_size)
    return results  # 모든 알고리즘이 비슷한 평균 오차
```

> **핵심 직관**: NFL 정리는 "가정 없이는 학습 없다"를 수학적으로 확인합니다. 가설 클래스 $\mathcal{H}$의 선택 자체가 귀납적 편향이며, 이것이 일반화를 가능하게 합니다.

---

## 6. 계산 효율적 PAC 학습

PAC 학습 가능성은 정보 이론적 질문이지만, 실용적으로는 **계산 효율성**도 중요합니다.

**정의 (효율적 PAC 학습):** $\mathcal{H}$가 효율적으로 PAC 학습 가능하다 함은, $\text{poly}(n, 1/\epsilon, 1/\delta)$ 시간에 실행되는 PAC 학습 알고리즘이 존재하는 것입니다.

| 가설 클래스 | PAC 학습 가능 | 효율적 PAC 학습 가능 | 비고 |
|-----------|------------|-------------------|------|
| 축 정렬 사각형 | 예 | 예 | $O(n)$ 알고리즘 |
| 선형 분류기 | 예 | 예 (LP/QP) | 분리 가능 시 |
| 3-CNF 공식 | 예 | 예 ($\text{VCdim} = O(k\log k)$) | $k$는 변수 수 |
| 일반 DNF | 예 | 미해결 | 암호학적 가정 하에 불가능 추정 |
| 일반 신경망 | 예 (유한 VC) | NP-hard 일반적 | 특수 구조에서 가능 |

정보-계산 갭은 현대 이론의 중요한 연구 주제입니다.

$$n_{\text{info}}(\epsilon, \delta) \leq n_{\text{comp}}(\epsilon, \delta)$$

> **핵심 직관**: "학습 가능하지만 효율적이지 않은" 문제들은 현대 암호학과 깊이 연결됩니다. 학습의 계산적 어려움이 암호 시스템의 안전성을 보장할 수 있습니다.

---

## 7. PAC 프레임워크의 확장

기본 PAC 프레임워크를 넘어서는 중요한 확장들이 있습니다:

**PAC-Bayes 확장 (gt-11에서 상세):**

$$R(h) \leq \hat{R}_S(h) + \sqrt{\frac{\text{KL}(Q\|P) + \ln(n/\delta)}{2n}}$$

여기서 $Q$는 사후 분포, $P$는 사전 분포입니다.

**균일 안정성 (Uniform Stability):**

알고리즘 $\mathcal{A}$가 $\beta$-균일 안정적이면:

$$|\mathbb{E}_S[\text{Gen}(\mathcal{A}(S))]| \leq \beta$$

SGD의 안정성은 gt-10에서 자세히 다룹니다.

| 확장 | 핵심 아이디어 | 장점 |
|------|------------|------|
| PAC-Bayes | 사전/사후 분포 | 비진공 bound |
| 안정성 기반 | 알고리즘 민감도 | 알고리즘 의존적 |
| 압축 기반 | 유효 파라미터 | 과매개변수 설명 |
| 정보 이론적 | 상호 정보량 | 분포 의존적 |

> **핵심 직관**: PAC 프레임워크의 핵심 통찰 — "정확도, 신뢰도, 표본 수의 트레이드오프" — 은 모든 확장에서 보존됩니다. 확장들은 더 타이트한 bound를 제공할 뿐입니다.

---

## 핵심 정리

- **PAC 학습은 "높은 확률($1-\delta$)로 근사적으로($\epsilon$) 정확한" 학습을 요구하며, 표본 복잡도로 학습 난이도를 정량화합니다**
- **유한 가설 클래스의 표본 복잡도는 $O(\frac{1}{\epsilon}\ln|\mathcal{H}|)$(실현 가능) 또는 $O(\frac{1}{\epsilon^2}\ln|\mathcal{H}|)$(불가지론적)입니다**
- **학습 가능성의 근본 정리는 PAC 학습 가능성과 유한 VC 차원의 동치성을 확립합니다**
- **No Free Lunch 정리는 보편적 학습 알고리즘의 불가능성을 증명하여, 귀납적 편향의 필수성을 확인합니다**
- **정보 이론적 학습 가능성과 계산적 학습 가능성의 갭은 현대 이론 컴퓨터 과학의 핵심 미해결 문제입니다**
