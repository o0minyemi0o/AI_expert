# 이중 하강 현상: U자 곡선을 넘어서

## 왜 이중 하강이 일반화 이론을 뒤흔드는가

gt-02에서 다룬 편향-분산 트레이드오프는 복잡도가 증가하면 결국 테스트 오차가 증가한다고 예측합니다. 그러나 2019년 Belkin 등의 연구는 **보간 임계점(interpolation threshold)** 을 지나면 테스트 오차가 다시 감소하는 **이중 하강(double descent)** 현상을 체계적으로 보고했습니다. 이 발견은 고전적 U자 곡선 패러다임에 근본적 수정을 요구합니다.

---

## 1. 보간 임계점과 이중 하강의 정의

모델의 파라미터 수 $p$가 증가할 때, 훈련 데이터를 완벽히 적합(보간)할 수 있는 지점을 **보간 임계점**이라 합니다.

**정의 (보간 임계점):** 모든 $(x_i, y_i) \in S$에 대해 $h(x_i) = y_i$를 만족하는 가설이 존재하는 최소 복잡도:

$$p^* = \min\{p : \exists h \in \mathcal{H}_p, \hat{R}_S(h) = 0\}$$

일반적으로 $p^* \approx n$입니다.

| 영역 | 파라미터 범위 | 테스트 오차 거동 | 지배 현상 |
|------|------------|--------------|---------|
| 과소매개변수 | $p \ll n$ | 고전적 U자 곡선 | 편향-분산 트레이드오프 |
| 보간 임계점 | $p \approx n$ | 피크 (최대 오차) | 분산 폭발 |
| 과매개변수 | $p \gg n$ | 단조 감소 | "양성" 과적합 |

$$\text{테스트 오차}(p) = \begin{cases} \text{U자 곡선} & p < p^* \\ \text{피크} & p \approx p^* \\ \text{단조 감소} & p > p^* \end{cases}$$

> **핵심 직관**: 보간 임계점에서 모델은 "겨우" 데이터를 적합할 수 있어서, 해가 극도로 불안정합니다. $p \gg n$에서는 많은 보간 해 중 "좋은" 해를 선택할 여유가 생깁니다.

---

## 2. 선형 회귀에서의 이중 하강: 정밀 분석

가장 깔끔한 분석이 가능한 선형 회귀에서 시작합니다.

설정: $y = X w^* + \epsilon$, $X \in \mathbb{R}^{n \times p}$, $\epsilon \sim \mathcal{N}(0, \sigma^2 I)$

최소 노름 보간 해 (min-norm interpolator):

$$\hat{w} = X^T(XX^T)^{-1}y \quad (p > n \text{일 때})$$

**정리 (Hastie et al., 2022):** 공분산 $\Sigma = \text{diag}(\lambda_1, \ldots, \lambda_p)$인 등방성 특성에서, $p/n \to \gamma > 1$일 때:

$$R(\hat{w}) \to \frac{\sigma^2}{(\gamma-1)} + \|w^*\|^2 \cdot g(\gamma)$$

여기서 첫 번째 항은 분산, 두 번째 항은 편향이며, $\gamma \to 1^+$에서 분산이 발산합니다.

```python
import numpy as np

def double_descent_linear(n, p_values, w_star, sigma, n_trials=100):
    """선형 회귀에서 이중 하강 시뮬레이션"""
    results = []
    d_true = len(w_star)
    for p in p_values:
        test_errors = []
        for _ in range(n_trials):
            X = np.random.randn(n, p) / np.sqrt(p)
            w_full = np.zeros(p)
            w_full[:d_true] = w_star
            y = X @ w_full + sigma * np.random.randn(n)

            if p <= n:
                w_hat = np.linalg.lstsq(X, y, rcond=None)[0]
            else:
                # 최소 노름 해
                w_hat = X.T @ np.linalg.solve(X @ X.T, y)

            X_test = np.random.randn(1000, p) / np.sqrt(p)
            y_test = X_test @ w_full + sigma * np.random.randn(1000)
            test_errors.append(np.mean((X_test @ w_hat - y_test)**2))
        results.append((p, np.mean(test_errors)))
    return results
```

| $p/n$ ($\gamma$) | 분산 항 | 편향 항 | 총 오차 |
|-----------------|--------|--------|--------|
| 0.5 | 작음 | 큼 (과소적합) | 중간 |
| 0.9 | 큼 | 작음 | 큼 |
| 1.0 | $\to \infty$ | 작음 | $\to \infty$ |
| 2.0 | $\sigma^2$ | 감소 | 감소 중 |
| 10.0 | $\sigma^2/9$ | 작음 | 작음 |

> **핵심 직관**: $\gamma = p/n \to 1^+$에서 분산이 폭발하는 이유는 $XX^T$의 최소 고유값이 0에 접근하기 때문입니다. $(XX^T)^{-1}$의 조건수가 무한대로 발산합니다.

---

## 3. 랜덤 특성 모델에서의 이중 하강

비선형 모델의 분석 가능한 대리(proxy)로 **랜덤 특성 모델(random features model)** 을 사용합니다:

$$h_w(x) = \sum_{j=1}^{p} w_j \phi(a_j^T x)$$

여기서 $a_j$는 고정된 랜덤 벡터, $\phi$는 비선형 활성화입니다.

Mei와 Montanari(2022)의 정밀 분석:

$$R(\hat{w}) = B(\gamma) + V(\gamma) + \sigma^2$$

| $\gamma = p/n$ | $B(\gamma)$ (편향) | $V(\gamma)$ (분산) |
|---------------|--------------------|-------------------|
| $< 1$ | $\frac{1}{1-\gamma}\to\infty$ (at $\gamma \to 1^-$) | 유한 |
| $= 1$ | 발산 | 발산 |
| $> 1$ | 단조 감소 | $\frac{1}{\gamma-1} \to 0$ |
| $\to \infty$ | $\to$ 커널 편향 | $\to 0$ |

**핵심 발견:** $\gamma \to \infty$에서 랜덤 특성 모델은 대응하는 **커널 회귀**에 수렴합니다:

$$\hat{f}(x) \to K(x, X)(K(X,X))^{-1}y$$

이것은 gt-09에서 다룰 NTK와 직접 연결됩니다.

> **핵심 직관**: 랜덤 특성 모델은 선형 회귀의 이중 하강을 비선형 영역으로 확장하며, 과매개변수 극한에서 커널 방법과 동일한 해를 제공합니다.

---

## 4. 에포크 단위 이중 하강

모델 복잡도뿐 아니라 **훈련 시간**에 대해서도 이중 하강이 관측됩니다.

Nakkiran 등(2021)의 에포크 단위 이중 하강:

고정된 모델 크기에서 에포크 수 $T$가 증가할 때:
1. 초기: 훈련/테스트 오차 모두 감소
2. 중기: 테스트 오차 증가 (과적합 시작)
3. 보간 도달 시점: 테스트 오차 피크
4. 이후: 테스트 오차 재감소

$$\text{에포크 이중 하강}: T_{\text{optimal}} \text{는 두 개 존재 (조기 종료 지점과 } T \to \infty\text{)}$$

| 하강 축 | 조절 변수 | 임계점 | 실용적 함의 |
|--------|---------|-------|-----------|
| 모델 크기 | 파라미터 수 $p$ | $p \approx n$ | 충분히 큰 모델 사용 |
| 에포크 수 | 훈련 시간 $T$ | 보간 시점 | 매우 오래 훈련하거나 조기 종료 |
| 데이터 크기 | 샘플 수 $n$ | 라벨 잡음 의존 | 샘플 추가가 해로울 수 있음 |

```python
import numpy as np

def epoch_wise_double_descent(model, X_train, y_train, X_test, y_test,
                                max_epochs=5000, eval_every=10):
    """에포크 단위 이중 하강 추적"""
    train_losses, test_losses = [], []
    for epoch in range(max_epochs):
        model.train_one_epoch(X_train, y_train)
        if epoch % eval_every == 0:
            train_losses.append(model.evaluate(X_train, y_train))
            test_losses.append(model.evaluate(X_test, y_test))
    return train_losses, test_losses
```

> **핵심 직관**: 에포크 이중 하강은 gt-06에서 다룬 조기 종료의 관점을 복잡하게 합니다. 최적 중단 시점이 두 개 존재할 수 있으며, 충분히 오래 훈련하면 두 번째 최적점에 도달합니다.

---

## 5. 이중 하강의 메커니즘: 왜 발생하는가

보간 임계점에서의 분산 폭발 메커니즘을 분석합니다.

**메커니즘 1: 조건수 발산**

$p \approx n$에서 설계 행렬의 최소 특이값 $\sigma_{\min}(X) \to 0$:

$$\|w_{\text{min-norm}}\|_2 \propto \frac{1}{\sigma_{\min}(X)} \to \infty$$

노름이 큰 해는 작은 입력 변동에도 크게 반응하여 분산이 폭발합니다.

**메커니즘 2: 보간 해의 다양성**

$p > n$에서 보간 해의 공간은 $(p-n)$차원입니다. 최소 노름 해는:

$$\hat{w} = \arg\min_w \|w\|_2 \quad \text{s.t.} \quad Xw = y$$

$p \gg n$이면 이 해의 노름이 작아져 정규화 효과가 생깁니다:

$$\|\hat{w}\|_2^2 \approx \frac{n}{p}\|w^*\|_2^2 + \frac{n\sigma^2}{p-n}$$

| 메커니즘 | 임계점 근처 | 과매개변수 영역 |
|---------|-----------|-------------|
| 해의 노름 | 발산 ($\to \infty$) | 감소 ($\propto n/p$) |
| 조건수 | 발산 ($\to \infty$) | 안정화 |
| 해의 자유도 | $\approx 0$ (유일 해) | $p - n$ (많은 해) |
| 유효 정규화 | 없음 | 최소 노름 = 암묵적 정규화 |

> **핵심 직관**: $p \gg n$에서 모델은 데이터를 보간하면서도 "가장 단순한" (최소 노름) 방식으로 합니다. 이 암묵적 단순성 선호가 일반화의 열쇠이며, gt-10에서 깊이 다룹니다.

---

## 6. 라벨 잡음과 이중 하강의 관계

이중 하강의 강도는 라벨 잡음 $\sigma^2$에 의존합니다.

**정리:** 잡음이 없는 경우 ($\sigma^2 = 0$), 보간 임계점의 피크가 사라집니다:

$$\sigma^2 = 0 \implies V(\gamma) = 0 \quad \forall \gamma$$

이 결과는 이중 하강이 본질적으로 **잡음 보간**의 결과임을 시사합니다.

$$\text{피크 높이} \propto \frac{\sigma^2}{\gamma - 1} \quad \text{for } \gamma \to 1^+$$

| 잡음 수준 $\sigma^2$ | 이중 하강 | 최적 전략 |
|---------------------|---------|---------|
| 0 (무잡음) | 없음 | 보간이 최적 |
| 작음 | 약한 피크 | 과매개변수화 유리 |
| 큼 | 강한 피크 | 정규화 또는 과매개변수화 |

> **핵심 직관**: 라벨 잡음이 클수록 보간 임계점의 피크가 뾰족해집니다. 실제 데이터에서 잡음이 불가피하므로, 임계점을 크게 넘어서는 과매개변수 모델이 유리합니다.

---

## 핵심 정리

- **이중 하강은 테스트 오차가 보간 임계점($p \approx n$)에서 피크 후 재감소하는 현상으로, 고전적 U자 곡선을 넘어서는 패러다임입니다**
- **보간 임계점에서의 분산 폭발은 설계 행렬의 최소 특이값이 0에 접근하여 해의 노름이 발산하기 때문입니다**
- **과매개변수 영역($p \gg n$)에서 최소 노름 보간 해의 노름은 $O(n/p)$로 감소하여 암묵적 정규화 효과를 제공합니다**
- **에포크 단위 이중 하강은 훈련 시간 축에서도 동일한 현상이 발생함을 보이며, 조기 종료의 최적 시점이 두 개 존재할 수 있습니다**
- **이중 하강의 강도는 라벨 잡음 $\sigma^2$에 비례하며, 무잡음 설정에서는 이중 하강이 사라집니다**
