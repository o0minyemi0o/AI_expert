# Rademacher 복잡도: 데이터 의존적 일반화 bound

## 왜 Rademacher 복잡도가 필요한가

gt-03에서 다룬 VC bound는 분포에 무관하여 보편적이지만, 그만큼 느슨합니다. 실제 데이터가 저차원 매니폴드에 집중되어 있거나 특정 구조를 가지고 있다면, 이를 활용한 더 타이트한 bound가 가능합니다. **Rademacher 복잡도**는 가설 클래스가 **관측된 데이터**에서 무작위 잡음을 얼마나 잘 적합할 수 있는지를 측정하여, 데이터 의존적 일반화 보장을 제공합니다.

---

## 1. Rademacher 복잡도의 정의

**정의 (경험적 Rademacher 복잡도):** 고정된 샘플 $S = \{x_1, \ldots, x_n\}$에 대해:

$$\hat{\mathfrak{R}}_S(\mathcal{H}) = \mathbb{E}_{\sigma}\left[\sup_{h \in \mathcal{H}} \frac{1}{n}\sum_{i=1}^{n}\sigma_i h(x_i)\right]$$

여기서 $\sigma_1, \ldots, \sigma_n$은 **Rademacher 확률 변수**로, $\Pr[\sigma_i = +1] = \Pr[\sigma_i = -1] = 1/2$ i.i.d.입니다.

**정의 (Rademacher 복잡도):** 분포 $\mathcal{D}$에 대한 기대값:

$$\mathfrak{R}_n(\mathcal{H}) = \mathbb{E}_{S \sim \mathcal{D}^n}[\hat{\mathfrak{R}}_S(\mathcal{H})]$$

| 직관 | 수식 해석 |
|------|---------|
| $\sigma_i$는 무작위 레이블 | 순수 잡음에 대한 적합 |
| $\sup_{h}$는 최선의 가설 선택 | 가설 클래스의 최대 적합 능력 |
| $\frac{1}{n}\sum$는 상관 계수 | 잡음과의 최대 상관 |

```python
import numpy as np

def empirical_rademacher(H_predictions, n_trials=10000):
    """경험적 Rademacher 복잡도 몬테카를로 추정
    H_predictions: (|H|, n) 행렬 - 각 가설의 각 점에서의 예측
    """
    n_hypotheses, n_samples = H_predictions.shape
    complexities = []
    for _ in range(n_trials):
        sigma = np.random.choice([-1, 1], size=n_samples)
        correlations = H_predictions @ sigma / n_samples
        complexities.append(np.max(correlations))
    return np.mean(complexities)
```

> **핵심 직관**: Rademacher 복잡도는 "이 가설 클래스가 순수한 잡음을 얼마나 잘 암기할 수 있는가"를 측정합니다. 잡음을 잘 적합할 수 있다면 실제 데이터도 과적합할 위험이 높습니다.

---

## 2. Rademacher 복잡도 기반 일반화 bound

**정리 (Rademacher 일반화 bound):** 손실 함수 $\ell \in [0, 1]$에 대해, 확률 $1 - \delta$ 이상으로 모든 $h \in \mathcal{H}$에 대해:

$$R(h) \leq \hat{R}_S(h) + 2\mathfrak{R}_n(\mathcal{H}) + \sqrt{\frac{\ln(2/\delta)}{2n}}$$

경험적 버전:

$$R(h) \leq \hat{R}_S(h) + 2\hat{\mathfrak{R}}_S(\mathcal{H}) + 3\sqrt{\frac{\ln(2/\delta)}{2n}}$$

**증명 스케치:**

1단계: 대칭화(symmetrization)로 일반화 갭을 Rademacher 복잡도에 연결:

$$\mathbb{E}_S\left[\sup_{h \in \mathcal{H}}(R(h) - \hat{R}_S(h))\right] \leq 2\mathfrak{R}_n(\mathcal{H})$$

2단계: McDiarmid 집중 부등식으로 기대값에서 고확률 bound로 전환:

$$\Pr\left[\sup_h(R(h) - \hat{R}_S(h)) - \mathbb{E}[\sup_h(R(h)-\hat{R}_S(h))] > t\right] \leq e^{-2nt^2}$$

| 단계 | 기법 | 역할 |
|------|------|------|
| 대칭화 | 고스트 샘플 도입 | 참위험 제거 |
| Rademacher화 | $\sigma_i$ 도입 | 대칭성 활용 |
| 집중 | McDiarmid | 기대값 → 고확률 |

> **핵심 직관**: 대칭화 트릭의 핵심은 "두 개의 독립 샘플의 차이"가 "하나의 샘플과 Rademacher 변수의 곱"과 분포적으로 동일하다는 것입니다.

---

## 3. 주요 가설 클래스의 Rademacher 복잡도

중요한 가설 클래스들의 Rademacher 복잡도를 계산합니다.

**유한 가설 클래스:**

$$\mathfrak{R}_n(\mathcal{H}) \leq \sqrt{\frac{2\ln|\mathcal{H}|}{n}}$$

증명: Massart 보조정리를 적용합니다.

**노름 제한 선형 분류기:** $\mathcal{H} = \{x \mapsto w^Tx : \|w\|_2 \leq B\}$이고 $\|x_i\|_2 \leq C$이면:

$$\hat{\mathfrak{R}}_S(\mathcal{H}) = \frac{B}{n}\left\|\sum_{i=1}^n \sigma_i x_i\right\|_2 \leq \frac{BC}{\sqrt{n}}$$

**커널 방법:** 재생핵 힐베르트 공간 $\mathcal{H}_K$에서 $\|f\|_{\mathcal{H}_K} \leq B$이면:

$$\hat{\mathfrak{R}}_S(\mathcal{H}_K) \leq \frac{B}{n}\sqrt{\sum_{i=1}^{n}K(x_i, x_i)} = \frac{B\sqrt{\text{tr}(K_S)}}{n}$$

| 가설 클래스 | Rademacher 복잡도 | 비고 |
|-----------|-----------------|------|
| 유한 $\|\mathcal{H}\|$ | $O(\sqrt{\ln\|\mathcal{H}\|/n})$ | VC bound 회복 |
| 노름 제한 선형 | $O(BC/\sqrt{n})$ | 차원 무관! |
| RKHS | $O(B\sqrt{\text{tr}(K)/n})$ | 커널에 의존 |
| 신경망 (노름 제한) | $O(B^L \prod \|W_l\| / \sqrt{n})$ | 층별 스펙트럼 노름 |

```python
import numpy as np

def rademacher_linear(X, B):
    """노름 제한 선형 분류기의 Rademacher 복잡도 추정"""
    n = X.shape[0]
    n_trials = 10000
    values = []
    for _ in range(n_trials):
        sigma = np.random.choice([-1, 1], size=n)
        v = X.T @ sigma  # d차원 벡터
        values.append(B * np.linalg.norm(v) / n)
    return np.mean(values)
```

> **핵심 직관**: 노름 제한 선형 분류기의 Rademacher 복잡도가 입력 차원 $d$에 무관하다는 사실은 놀랍습니다. 이는 고차원 문제에서도 정규화만 적절히 하면 일반화가 가능함을 시사합니다.

---

## 4. 축소 보조정리와 합성

복잡한 가설 클래스의 Rademacher 복잡도를 계산하기 위한 도구들입니다.

**Talagrand의 축소 보조정리 (Contraction Lemma):** $\phi$가 $L$-Lipschitz이면:

$$\hat{\mathfrak{R}}_S(\phi \circ \mathcal{H}) \leq L \cdot \hat{\mathfrak{R}}_S(\mathcal{H})$$

이것이 중요한 이유: 신경망은 **층별 합성**으로 구성됩니다. 각 층의 비선형 활성화 $\phi$(ReLU는 1-Lipschitz)를 적용할 때 Rademacher 복잡도가 증가하지 않습니다.

**합성 규칙:**

| 연산 | Rademacher 관계 |
|------|----------------|
| 합집합 $\mathcal{H}_1 \cup \mathcal{H}_2$ | $\leq \max(\mathfrak{R}(\mathcal{H}_1), \mathfrak{R}(\mathcal{H}_2))$ |
| 합 $\mathcal{H}_1 + \mathcal{H}_2$ | $\leq \mathfrak{R}(\mathcal{H}_1) + \mathfrak{R}(\mathcal{H}_2)$ |
| 볼록 껍질 $\text{conv}(\mathcal{H})$ | $= \mathfrak{R}(\mathcal{H})$ |
| $L$-Lipschitz 합성 | $\leq L \cdot \mathfrak{R}(\mathcal{H})$ |
| 스케일링 $a\mathcal{H}$ | $= |a| \cdot \mathfrak{R}(\mathcal{H})$ |

> **핵심 직관**: 축소 보조정리 덕분에 깊은 신경망의 Rademacher 복잡도를 각 층의 가중치 노름의 곱으로 상한할 수 있습니다. gt-06에서 이 결과를 정규화와 연결합니다.

---

## 5. VC 차원과의 관계

Rademacher 복잡도와 VC 차원은 밀접히 연결되어 있습니다.

**정리:** 이진 분류 가설 클래스 $\mathcal{H}$에 대해:

$$\mathfrak{R}_n(\mathcal{H}) \leq \sqrt{\frac{2d\ln(en/d)}{n}}$$

여기서 $d = \text{VCdim}(\mathcal{H})$입니다.

역방향도 성립합니다 (Dudley의 엔트로피 적분 경유):

$$\text{VCdim}(\mathcal{H}) \leq c \cdot n \cdot \mathfrak{R}_n(\mathcal{H})^2$$

| 비교 항목 | VC 차원 | Rademacher 복잡도 |
|---------|---------|-----------------|
| 데이터 의존성 | 없음 (최악 경우) | 있음 (관측 데이터) |
| 계산 가능성 | 일반적으로 어려움 | 몬테카를로 추정 가능 |
| 적용 범위 | 이진 분류 | 실수 값 함수까지 |
| 타이트함 | 느슨 | 상대적으로 타이트 |
| 분포 활용 | 불가 | 가능 |

```python
import numpy as np

def compare_vc_rademacher_bounds(X, y, vc_dim, H_predictions, delta=0.05):
    """VC bound와 Rademacher bound 비교"""
    n = len(y)

    # VC bound
    vc_complexity = np.sqrt((vc_dim * np.log(2*np.e*n/vc_dim)
                             + np.log(4/delta)) / n)

    # Rademacher bound (몬테카를로)
    rad = empirical_rademacher(H_predictions)  # 위의 함수 사용
    rad_complexity = 2 * rad + 3 * np.sqrt(np.log(2/delta) / (2*n))

    return {'vc_bound': vc_complexity, 'rad_bound': rad_complexity}
```

> **핵심 직관**: Rademacher 복잡도는 VC 차원의 "데이터 의존적 세분화"로 이해할 수 있습니다. 데이터가 특정 부분 공간에 집중되면 Rademacher bound가 VC bound보다 훨씬 타이트합니다.

---

## 6. Gaussian 복잡도와 커버링 수

Rademacher 복잡도의 변형과 관련 개념을 소개합니다.

**Gaussian 복잡도:** $\sigma_i$ 대신 $g_i \sim \mathcal{N}(0,1)$을 사용합니다:

$$\hat{\mathfrak{G}}_S(\mathcal{H}) = \mathbb{E}_g\left[\sup_{h \in \mathcal{H}} \frac{1}{n}\sum_{i=1}^n g_i h(x_i)\right]$$

비교 부등식: $\hat{\mathfrak{R}}_S \leq \sqrt{\pi/2} \cdot \hat{\mathfrak{G}}_S$

**Dudley의 엔트로피 적분:**

$$\hat{\mathfrak{R}}_S(\mathcal{H}) \leq \frac{C}{\sqrt{n}}\int_0^{\infty}\sqrt{\ln \mathcal{N}(\mathcal{H}|_S, \epsilon, \|\cdot\|_2)} \, d\epsilon$$

여기서 $\mathcal{N}(\cdot, \epsilon, \|\cdot\|_2)$는 $\epsilon$-커버링 수입니다.

| 복잡도 측도 | 정의 | 관계 |
|-----------|------|------|
| Rademacher | $\pm 1$ 확률 변수 | 기본 |
| Gaussian | $\mathcal{N}(0,1)$ 변수 | $\leq \sqrt{\pi/2} \cdot$ Gaussian |
| 커버링 수 | $\epsilon$-net 크기 | Dudley 적분으로 연결 |
| 메트릭 엔트로피 | $\ln \mathcal{N}(\cdot,\epsilon)$ | 커버링 수의 로그 |

> **핵심 직관**: 이 모든 복잡도 측도는 "가설 공간이 얼마나 풍부한가"를 서로 다른 각도에서 측정합니다. Dudley 적분은 가설 공간의 기하학적 구조(메트릭 엔트로피)를 직접 활용합니다.

---

## 핵심 정리

- **Rademacher 복잡도는 가설 클래스가 무작위 잡음에 얼마나 잘 상관할 수 있는지를 측정하며, 데이터 의존적 일반화 bound를 제공합니다**
- **일반화 bound $R(h) \leq \hat{R}_S(h) + 2\mathfrak{R}_n(\mathcal{H}) + O(1/\sqrt{n})$은 대칭화와 McDiarmid 집중 부등식으로 증명됩니다**
- **노름 제한 선형 분류기의 Rademacher 복잡도 $O(BC/\sqrt{n})$은 입력 차원에 무관하여, 고차원에서의 일반화를 설명합니다**
- **축소 보조정리는 Lipschitz 비선형 함수(ReLU 등)의 합성이 Rademacher 복잡도를 증가시키지 않음을 보장합니다**
- **VC 차원은 Rademacher 복잡도의 상한을 제공하지만, 데이터 의존적 Rademacher bound가 실제로는 더 타이트합니다**
