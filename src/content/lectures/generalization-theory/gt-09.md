# 신경 접선 커널 (NTK): 무한 너비 극한의 일반화 이론

## 왜 신경 접선 커널이 중요한가

gt-08에서 과매개변수 신경망이 커널 방법으로 환원될 수 있음을 언급했습니다. **신경 접선 커널(Neural Tangent Kernel, NTK)** 은 Jacot 등(2018)이 도입한 프레임워크로, 무한 너비 신경망의 학습 동역학을 선형화하여 분석 가능하게 합니다. NTK는 "왜 경사 하강법으로 학습된 신경망이 일반화하는가"에 대한 최초의 엄밀한 답을 제공합니다.

---

## 1. 신경망의 선형화와 NTK의 정의

파라미터 $\theta \in \mathbb{R}^P$를 가진 신경망 $f(x; \theta)$의 학습 동역학을 분석합니다.

초기 파라미터 $\theta_0$ 근방에서 1차 테일러 전개:

$$f(x; \theta) \approx f(x; \theta_0) + \nabla_\theta f(x; \theta_0)^T (\theta - \theta_0)$$

**정의 (신경 접선 커널):**

$$K^{(\text{NTK})}(x, x') = \nabla_\theta f(x; \theta)^T \nabla_\theta f(x'; \theta) = \sum_{p=1}^{P}\frac{\partial f(x;\theta)}{\partial \theta_p}\frac{\partial f(x';\theta)}{\partial \theta_p}$$

| 구성 요소 | 의미 | 차원 |
|---------|------|------|
| $\nabla_\theta f(x; \theta)$ | 파라미터 공간에서의 특성 맵 | $P$차원 |
| $K^{(\text{NTK})}$ | 그래디언트 공간의 내적 | $n \times n$ 행렬 |
| $P$ | 총 파라미터 수 | $\sum_l d_l \cdot d_{l+1}$ |

```python
import numpy as np

def compute_ntk(model, X, params):
    """신경 접선 커널 행렬 계산 (유한 너비)"""
    n = X.shape[0]
    # 야코비안 계산: J[i, p] = df(x_i)/d(theta_p)
    jacobian = compute_jacobian(model, X, params)  # (n, P)
    ntk = jacobian @ jacobian.T  # (n, n)
    return ntk

def ntk_two_layer_relu(X1, X2, sigma_w=1.0, sigma_b=0.0):
    """2층 ReLU 네트워크의 해석적 NTK (무한 너비)"""
    # Σ = sigma_w^2 * X1 @ X2.T / d + sigma_b^2
    d = X1.shape[1]
    S = sigma_w**2 * X1 @ X2.T / d + sigma_b**2
    norms1 = np.sqrt(np.diag(sigma_w**2 * X1 @ X1.T / d + sigma_b**2))
    norms2 = np.sqrt(np.diag(sigma_w**2 * X2 @ X2.T / d + sigma_b**2))
    cos_theta = S / np.outer(norms1, norms2)
    cos_theta = np.clip(cos_theta, -1, 1)
    theta = np.arccos(cos_theta)
    # NTK for ReLU
    K = S * (np.pi - theta) / (2 * np.pi)
    return K
```

> **핵심 직관**: NTK는 신경망을 "파라미터 공간의 특성 맵"으로 해석합니다. 각 파라미터가 하나의 특성이 되어, 총 $P$개의 특성을 가진 커널 방법으로 봅니다.

---

## 2. NTK의 핵심 정리

**정리 (Jacot et al., 2018):** NTK 매개변수화 하에서, 너비 $m \to \infty$일 때:

1. **초기화 시 수렴:** $K^{(\text{NTK})}_m(x,x') \xrightarrow{m \to \infty} K^{(\text{NTK})}_\infty(x,x')$ (확정적 커널)

2. **학습 중 불변성:** 연속 시간 경사 하강법 하에서 $K^{(\text{NTK})}_m(t) \to K^{(\text{NTK})}_\infty$ (시간에 무관)

3. **학습 동역학의 선형화:** 잔차 $r(t) = f(X;\theta(t)) - y$는:

$$\frac{dr}{dt} = -K^{(\text{NTK})}_\infty \cdot r(t)$$

$$r(t) = e^{-K^{(\text{NTK})}_\infty t} r(0)$$

| 성질 | 유한 너비 | 무한 너비 |
|------|---------|---------|
| NTK 초기값 | 확률적 | 확정적 $K_\infty$ |
| 학습 중 NTK | 변화함 | 고정 (불변) |
| 학습 동역학 | 비선형 | 선형 (지수 감쇠) |
| 전역 수렴 | 보장 안됨 | $\lambda_{\min}(K_\infty) > 0$이면 보장 |

**수렴 속도:**

$$\|r(t)\|^2 \leq e^{-2\lambda_{\min}(K_\infty)t} \|r(0)\|^2$$

> **핵심 직관**: 무한 너비에서 신경망은 "게으른 학습(lazy training)"을 합니다 — 파라미터가 초기값에서 거의 움직이지 않고, 선형화된 모델로 학습합니다.

---

## 3. NTK 레짐에서의 일반화

NTK 레짐에서 학습된 함수는 커널 회귀와 동일합니다:

$$f_\infty(x) = K_\infty(x, X) K_\infty(X, X)^{-1} y$$

이것은 RKHS $\mathcal{H}_{K_\infty}$에서의 최소 노름 보간 해입니다:

$$f_\infty = \arg\min_{f \in \mathcal{H}_{K_\infty}} \|f\|_{\mathcal{H}_{K_\infty}} \quad \text{s.t.} \quad f(x_i) = y_i$$

**일반화 bound:** RKHS 노름 $\|f_\infty\|_{\mathcal{H}_K}$에 의존합니다:

$$R(f_\infty) \leq \hat{R}_S(f_\infty) + \frac{2\|f_\infty\|_{\mathcal{H}_K}\sqrt{\text{tr}(K_S)}}{\sqrt{n}} + \sqrt{\frac{\ln(2/\delta)}{2n}}$$

gt-05에서 다룬 Rademacher bound를 적용한 것입니다.

| NTK 일반화 요인 | 역할 | 제어 방법 |
|---------------|------|---------|
| $\|f_\infty\|_{\mathcal{H}_K}$ | 함수 복잡도 | 커널/아키텍처 선택 |
| $\text{tr}(K_S)$ | 데이터 커버리지 | 데이터 분포 |
| $\lambda_{\min}(K_S)$ | 보간 안정성 | 정규화 |
| 커널의 고유값 감쇠 | 유효 차원 | 아키텍처 깊이 |

```python
import numpy as np

def ntk_generalization_bound(K_train, y_train, K_test_train, delta=0.05):
    """NTK 레짐에서의 일반화 bound 계산"""
    n = K_train.shape[0]
    alpha = np.linalg.solve(K_train, y_train)
    rkhs_norm = np.sqrt(y_train @ alpha)
    trace_K = np.trace(K_train)

    rademacher = 2 * rkhs_norm * np.sqrt(trace_K) / np.sqrt(n)
    confidence = np.sqrt(np.log(2/delta) / (2*n))
    return rademacher + confidence
```

> **핵심 직관**: NTK 관점에서 과매개변수 신경망의 일반화는 RKHS 노름의 최소화로 설명됩니다. 이는 gt-06의 정규화 관점과 연결되며, 아키텍처가 암묵적 정규화를 제공합니다.

---

## 4. NTK의 스펙트럼 분석

NTK 커널의 고유값 분해가 일반화 성능을 결정합니다.

NTK의 Mercer 분해: $K(x,x') = \sum_{k=1}^{\infty} \mu_k \phi_k(x)\phi_k(x')$

**정리:** 깊이 $L$의 완전 연결 ReLU 네트워크의 NTK 고유값은:

$$\mu_k \sim k^{-(2L+d-1)/d}$$

여기서 $d$는 입력 차원입니다. 깊이가 증가하면 고유값이 더 빠르게 감쇠합니다.

| 깊이 $L$ | 감쇠율 | 유효 차원 | RKHS 매끄러움 |
|---------|-------|---------|-------------|
| 1 | $k^{-(d+1)/d}$ | 높음 | Sobolev $H^{1/2}$ |
| 2 | $k^{-(d+3)/d}$ | 중간 | Sobolev $H^{3/2}$ |
| $L$ | $k^{-(2L+d-1)/d}$ | 낮음 | Sobolev $H^{L-1/2}$ |

$$\text{깊은 NTK} \implies \text{더 매끄러운 함수} \implies \text{더 강한 암묵적 정규화}$$

> **핵심 직관**: 깊은 네트워크의 NTK는 더 "매끄러운" 함수 클래스에 대응하며, 이것이 깊은 네트워크의 일반화 이점을 부분적으로 설명합니다.

---

## 5. NTK 이론의 한계와 특성 학습

NTK 이론은 강력하지만 중요한 한계가 있습니다.

**게으른 학습 vs 풍부한 학습:**

| 비교 | NTK (게으른) 레짐 | 특성 학습 (풍부한) 레짐 |
|------|----------------|---------------------|
| 특성 변화 | $\nabla_\theta f$ 고정 | $\nabla_\theta f$ 변화 |
| 너비 | $m \to \infty$ | 유한 $m$ |
| 성능 | 커널 방법 수준 | 커널 방법 초월 |
| 깊이 이점 | 매끄러움만 | 계층적 특성 |
| 예시 | 랜덤 특성 | CNN, 트랜스포머 |

**특성 학습이 필수적인 문제:**

"고차 교호작용 탐지" 문제: $y = \text{sign}(x_1 \cdot x_2 \cdot x_3)$

- NTK (고정 특성): $O(d^3)$ 샘플 필요
- 특성 학습: $O(d)$ 샘플로 충분

$$\text{특성 학습 이점} = \frac{n_{\text{NTK}}}{n_{\text{특성 학습}}} = O(d^2) \to \infty$$

```python
import numpy as np

def compare_ntk_vs_feature_learning(d, n_samples, n_trials=50):
    """NTK vs 특성 학습의 표본 효율성 비교"""
    ntk_errors, feat_errors = [], []
    for _ in range(n_trials):
        # 교호작용 문제: y = sign(x1 * x2 * x3)
        X = np.random.randn(n_samples, d)
        y = np.sign(X[:, 0] * X[:, 1] * X[:, 2])

        # NTK (선형 커널 근사) - 교호작용 포착 불가
        # 3차 다항 커널이 필요 -> 표본 복잡도 O(d^3)
        ntk_error = estimate_kernel_error(X, y, kernel='ntk_linear')

        # 특성 학습 (2층 NN) - 관련 방향 발견
        feat_error = estimate_nn_error(X, y, hidden_dim=100)

        ntk_errors.append(ntk_error)
        feat_errors.append(feat_error)
    return np.mean(ntk_errors), np.mean(feat_errors)
```

> **핵심 직관**: NTK 이론은 신경망의 일반화를 설명하는 첫 단계이지만, 실제 딥러닝의 핵심 강점인 **특성 학습(feature learning)** 은 포착하지 못합니다. gt-10의 암묵적 편향 이론이 이 갭을 부분적으로 메웁니다.

---

## 6. 유한 너비 보정과 평균장 이론

NTK의 무한 너비 극한을 넘어서는 이론적 접근들을 소개합니다.

**유한 너비 보정:**

$$K_m^{(\text{NTK})}(x, x') = K_\infty(x, x') + \frac{1}{\sqrt{m}}\Delta K(x, x') + O(1/m)$$

$\Delta K$는 특성 학습에 의한 보정 항입니다.

**평균장 이론 (Mean-Field Theory):**

다른 스케일링에서 신경망은 NTK가 아닌 **평균장** 레짐에 들어갑니다:

$$\frac{1}{m}\sum_{j=1}^{m} a_j \sigma(w_j^T x) \to \int a\sigma(w^Tx) d\rho(a, w)$$

여기서 $\rho$는 뉴런의 분포입니다.

| 스케일링 | 레짐 | 특성 학습 | 분석 가능성 |
|---------|------|---------|-----------|
| NTK ($1/\sqrt{m}$) | 게으른 | 없음 | 높음 (선형) |
| 평균장 ($1/m$) | 풍부한 | 있음 | 중간 (PDE) |
| $\mu$P | 최대 특성 학습 | 최대 | 낮음 |
| 실제 학습 | 혼합 | 있음 | 어려움 |

> **핵심 직관**: NTK, 평균장, $\mu$P 스케일링은 같은 신경망의 서로 다른 극한입니다. 실제 신경망은 이 극한들 사이 어딘가에서 작동하며, 이를 정밀하게 분석하는 것은 미해결 과제입니다.

---

## 핵심 정리

- **NTK는 무한 너비 신경망의 학습을 선형 동역학 $dr/dt = -K_\infty r$로 환원하며, 커널이 학습 중 불변임이 핵심 성질입니다**
- **NTK 레짐에서 학습된 함수는 RKHS $\mathcal{H}_{K_\infty}$에서의 최소 노름 보간 해이며, RKHS 노름이 일반화를 제어합니다**
- **깊이 $L$ 네트워크의 NTK 고유값은 $k^{-(2L+d-1)/d}$로 감쇠하여, 깊은 네트워크가 더 매끄러운 함수 클래스에 대응합니다**
- **NTK 이론의 근본적 한계는 특성 학습을 포착하지 못한다는 것이며, 실제 신경망의 성공에서 특성 학습이 핵심적 역할을 합니다**
- **평균장 이론과 $\mu$P 스케일링은 NTK 너머의 특성 학습 레짐을 분석하는 대안적 프레임워크입니다**
