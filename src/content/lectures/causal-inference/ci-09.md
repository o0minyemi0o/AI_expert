# 인과 발견 알고리즘

## 왜 데이터에서 인과 구조를 학습해야 하는가

ci-02~ci-04에서는 DAG가 이미 주어진 상태에서 인과 효과를 식별하는 방법을 다루었습니다. 그러나 실무에서는 DAG 자체를 모르는 경우가 대부분입니다. 인과 발견(causal discovery)은 관측 데이터로부터 변수 간의 인과 구조(DAG)를 학습하는 알고리즘적 접근법입니다. 도메인 지식을 보완하거나 새로운 인과 가설을 생성하는 데 활용됩니다.

---

## 1. 인과 발견의 기본 아이디어

인과 발견은 관측 데이터의 조건부 독립 패턴 또는 함수적 관계를 분석하여 DAG를 추론합니다.

$$
\text{데이터 } \mathcal{D} = \{(x_1^{(i)}, \ldots, x_p^{(i)})\}_{i=1}^{n} \quad \Rightarrow \quad \text{DAG } \mathcal{G}
$$

| 접근법 | 핵심 아이디어 | 대표 알고리즘 |
|--------|------------|-------------|
| 제약 기반 (Constraint-based) | 조건부 독립 검정 활용 | PC, FCI |
| 점수 기반 (Score-based) | DAG 점수 함수 최적화 | GES, FGES |
| 함수 기반 (Functional) | 잔차의 비대칭성 활용 | LiNGAM, ANM |
| 하이브리드 | 위 방법들의 결합 | GFCI, 최신 방법들 |

> **핵심 직관**: 상관관계만으로는 $X \to Y$와 $Y \to X$를 구분할 수 없지만, ci-04의 마르코프 조건과 충실성 가정 하에서 조건부 독립 패턴은 인과 방향에 대한 정보를 제공합니다.

---

## 2. PC 알고리즘

PC 알고리즘(Peter-Clark algorithm)은 가장 대표적인 제약 기반 인과 발견 알고리즘입니다. 조건부 독립 검정을 체계적으로 수행하여 DAG의 골격(skeleton)과 방향을 추론합니다.

**알고리즘 단계:**

$$
\text{1단계: 완전 비방향 그래프에서 시작}
$$
$$
\text{2단계: 조건부 독립 검정으로 간선 제거}
$$
$$
\text{3단계: v-구조 (충돌자) 식별로 방향 부여}
$$
$$
\text{4단계: 방향 전파 규칙 적용}
$$

| 단계 | 연산 | 결과 |
|------|------|------|
| 골격 학습 | $X \perp\!\!\!\perp Y \mid \mathbf{S}$ 검정 | 비방향 그래프 |
| v-구조 식별 | $A - C - B$에서 $C \notin \text{Sep}(A,B)$ | $A \to C \leftarrow B$ |
| 방향 전파 | Meek 규칙 적용 | 가능한 방향 추가 |
| 최종 결과 | CPDAG (마르코프 동치류) | 방향 + 비방향 간선 |

> **핵심 직관**: PC 알고리즘은 $X$와 $Y$가 독립이면 간선을 제거하고, $A-C-B$ 구조에서 $A$와 $B$를 $C$ 없이 분리할 수 없으면 $A \to C \leftarrow B$ (충돌자)로 방향을 부여합니다.

```python
import numpy as np

# PC 알고리즘의 핵심 로직 시뮬레이션
np.random.seed(42)
n = 5000

# 진정한 DAG: A → B → C, A → C
A = np.random.normal(0, 1, n)
B = 0.7 * A + np.random.normal(0, 0.5, n)
C = 0.5 * B + 0.3 * A + np.random.normal(0, 0.5, n)

# Step 1: 무조건부 상관으로 골격 추정
from scipy.stats import pearsonr
pairs = [('A','B',A,B), ('A','C',A,C), ('B','C',B,C)]
for name1, name2, x, y in pairs:
    r, p = pearsonr(x, y)
    print(f"{name1}-{name2}: r={r:.3f}, p={p:.6f} → {'간선 유지' if p < 0.05 else '간선 제거'}")

# Step 2: 조건부 독립 검정 (A와 C | B)
from numpy.linalg import lstsq
residA = A - lstsq(B.reshape(-1,1), A, rcond=None)[0][0] * B
residC = C - lstsq(B.reshape(-1,1), C, rcond=None)[0][0] * B
r_partial, p_partial = pearsonr(residA, residC)
print(f"\nA-C | B: partial r={r_partial:.3f}, p={p_partial:.6f}")
print("→ A⊥C|B이면 A→B→C (체인), 아니면 A→C 직접 경로 존재")
```

---

## 3. FCI 알고리즘과 숨은 변수

PC 알고리즘은 숨은 변수(latent variable)가 없다고 가정합니다. FCI(Fast Causal Inference) 알고리즘은 이 제한을 극복하여 관측되지 않은 교란 변수가 존재할 수 있는 상황에서도 인과 구조를 학습합니다.

$$
\text{FCI 출력}: \quad \text{PAG (Partial Ancestral Graph)}
$$

| 간선 표기 | 의미 |
|----------|------|
| $A \to B$ | $A$는 $B$의 원인 (확실) |
| $A \leftrightarrow B$ | 숨은 공통 원인 존재 |
| $A \circ\!\!\to B$ | $A$는 원인이거나 숨은 공통 원인 |
| $A \circ\!\!-\!\!\circ B$ | 방향 불확실 |

> **핵심 직관**: FCI는 "모르는 것은 모른다고 표현"합니다. 숨은 변수가 있을 수 있으면 양방향 화살표($\leftrightarrow$)로, 방향을 결정할 수 없으면 원($\circ$) 표기로 불확실성을 명시합니다.

---

## 4. GES 알고리즘 (점수 기반)

GES(Greedy Equivalence Search)는 DAG의 점수 함수를 탐욕적으로 최적화하는 점수 기반 알고리즘입니다.

$$
\text{Score}(\mathcal{G}) = \sum_{i=1}^{p} \text{Score}(X_i \mid \text{Pa}_{\mathcal{G}}(X_i))
$$

일반적으로 BIC(Bayesian Information Criterion)를 점수 함수로 사용합니다:

$$
\text{BIC}(\mathcal{G}) = -2 \log L(\mathcal{G}) + k \log n
$$

| 단계 | 연산 | 목표 |
|------|------|------|
| Forward phase | 간선 추가 | 점수 증가할 때까지 |
| Backward phase | 간선 제거 | 점수 증가할 때까지 |
| 방향 결정 | CPDAG 출력 | 마르코프 동치류 반환 |

> **핵심 직관**: GES는 빈 그래프에서 시작하여 간선을 추가하고, 그 후 불필요한 간선을 제거합니다. BIC의 복잡도 패널티가 과적합을 방지합니다. si-05에서 다룬 모델 선택 기준과 동일한 원리입니다.

```python
import numpy as np

# 점수 기반 인과 발견의 핵심 아이디어
np.random.seed(42)
n = 1000
p = 3

# 진정한 DAG: X1 → X2 → X3
X1 = np.random.normal(0, 1, n)
X2 = 0.8 * X1 + np.random.normal(0, 0.5, n)
X3 = 0.6 * X2 + np.random.normal(0, 0.5, n)
data = np.column_stack([X1, X2, X3])

# BIC 점수 비교 (간단한 예시)
from sklearn.linear_model import LinearRegression

def bic_score(X, y, parents_idx):
    if len(parents_idx) == 0:
        resid = y - y.mean()
    else:
        reg = LinearRegression().fit(X[:, parents_idx], y)
        resid = y - reg.predict(X[:, parents_idx])
    rss = np.sum(resid**2)
    k = len(parents_idx) + 1
    return n * np.log(rss / n) + k * np.log(n)

# DAG 1: X1→X2→X3 (올바른 구조)
score1 = (bic_score(data, data[:,1], [0]) +
          bic_score(data, data[:,2], [1]) +
          bic_score(data, data[:,0], []))

# DAG 2: X1→X3→X2 (잘못된 구조)
score2 = (bic_score(data, data[:,2], [0]) +
          bic_score(data, data[:,1], [2]) +
          bic_score(data, data[:,0], []))

print(f"올바른 DAG BIC: {score1:.1f}")
print(f"잘못된 DAG BIC: {score2:.1f}")
print(f"BIC 낮은 쪽이 선호됩니다")
```

---

## 5. 마르코프 동치류와 식별 한계

순수 관측 데이터만으로는 DAG를 유일하게 결정할 수 없는 경우가 많습니다. 같은 조건부 독립 관계를 만족하는 DAG들의 집합을 **마르코프 동치류(Markov equivalence class)**라 합니다.

$$
\mathcal{G}_1 \sim \mathcal{G}_2 \iff \text{같은 골격 + 같은 v-구조}
$$

$$
\text{CPDAG (Completed Partially Directed Acyclic Graph)}: \text{동치류의 대표 그래프}
$$

| 구조 | 식별 가능 여부 | 이유 |
|------|-------------|------|
| $X \to Y \to Z$ vs $X \leftarrow Y \leftarrow Z$ | 불가 | 동일한 조건부 독립 |
| $X \to Y \to Z$ vs $X \leftarrow Y \to Z$ | 불가 | 동일한 조건부 독립 |
| $X \to Z \leftarrow Y$ (v-구조) | 가능 | 고유한 조건부 독립 |
| 개입 데이터 추가 시 | 완전 식별 가능 | 추가 정보 |

> **핵심 직관**: $X \to Y$와 $X \leftarrow Y$는 관측 데이터만으로 구분할 수 없습니다. 이를 구분하려면 개입 실험(ci-03의 do-operator)이나 함수적 가정(LiNGAM 등)이 추가로 필요합니다.

---

## 6. 함수 기반 방법과 최신 동향

비가우시안 분포나 비선형 관계를 활용하면 마르코프 동치류 내에서도 방향을 식별할 수 있습니다.

$$
\text{LiNGAM}: \quad X_i = \sum_{j \in \text{Pa}(i)} b_{ij} X_j + e_i, \quad e_i \sim \text{non-Gaussian}
$$

| 방법 | 가정 | 식별 수준 | 장점 |
|------|------|----------|------|
| LiNGAM | 선형 + 비가우시안 | 완전 DAG | 동치류 넘어 식별 |
| ANM | 가산 잡음 | 쌍별 방향 | 비선형 허용 |
| NOTEARS | 연속 최적화 | DAG 제약 연속 완화 | 확장 가능 |
| DECI | 신경망 기반 | 유연한 함수형 | ci-11의 인과 표현 학습 |

> **핵심 직관**: LiNGAM은 "원인의 잔차는 비가우시안이지만, 결과의 잔차(원인 제거 후)는 독립"이라는 비대칭성을 활용합니다. 가우시안 데이터에서는 작동하지 않습니다.

```python
import numpy as np

# LiNGAM 핵심 아이디어: 비가우시안 잔차의 비대칭성
np.random.seed(42)
n = 5000

# X → Y (진정한 방향)
e_x = np.random.laplace(0, 1, n)  # 비가우시안 잡음
X = e_x
Y = 0.8 * X + np.random.laplace(0, 0.5, n)

# 방향 1: X → Y → 잔차 = Y - aX
resid_xy = Y - np.polyfit(X, Y, 1)[0] * X
# 방향 2: Y → X → 잔차 = X - bY
resid_yx = X - np.polyfit(Y, X, 1)[0] * Y

# 독립성 검정 (상호 정보량으로 대략적 판단)
from scipy.stats import pearsonr
_, p_xy = pearsonr(X, resid_xy)   # X와 잔차 독립?
_, p_yx = pearsonr(Y, resid_yx)   # Y와 잔차 독립?

print(f"X→Y 모델: X와 잔차 상관 p={p_xy:.4f}")
print(f"Y→X 모델: Y와 잔차 상관 p={p_yx:.4f}")
print("p-value가 높은 방향이 올바른 인과 방향입니다")
```

---

## 7. 실무 적용 시 고려사항

인과 발견 알고리즘을 실무에 적용할 때는 여러 한계를 인식해야 합니다.

| 고려사항 | 설명 | 권장 사항 |
|---------|------|----------|
| 표본 크기 | 변수 수 대비 충분한 관측 필요 | $n \gg p^2$ |
| 충실성 위반 | 경로 효과 상쇄 시 오류 | 다중 알고리즘 비교 |
| 측정 오차 | 관측 변수의 노이즈 | 잠재 변수 모델 활용 |
| 시간 해상도 | 인과 관계의 시간 지연 | ts-01에서 다룰 그랜저 인과 |
| 도메인 지식 | 알고리즘 결과의 해석 | 전문가와 결과 검토 |

> **핵심 직관**: 인과 발견 알고리즘은 DAG를 "발견"하는 것이 아니라 "제안"합니다. 결과는 도메인 지식으로 검증하고, 가능하면 ci-03의 개입 실험으로 확인해야 합니다.

---

## 핵심 정리

- **PC 알고리즘은 조건부 독립 검정을 체계적으로 수행하여 DAG의 골격과 v-구조를 학습합니다**: 충실성 가정 하에서 마르코프 동치류를 올바르게 식별합니다
- **FCI 알고리즘은 관측되지 않은 교란 변수가 존재해도 적용 가능한 인과 발견 방법입니다**: PAG로 불확실성을 명시적으로 표현합니다
- **GES는 BIC 등의 점수 함수를 탐욕적으로 최적화하여 DAG를 학습합니다**: 대규모 변수에 FGES로 확장됩니다
- **마르코프 동치류의 존재로 인해, 순수 관측 데이터만으로는 모든 인과 방향을 식별할 수 없습니다**: v-구조만 확실히 식별됩니다
- **LiNGAM 등 함수 기반 방법은 비가우시안 가정 하에서 동치류를 넘어 완전한 DAG를 식별할 수 있습니다**
