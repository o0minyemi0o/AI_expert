# 경향 점수와 매칭

## 왜 경향 점수가 필요한가

ci-05에서 배운 무교란성 가정 하에서 인과 효과를 추정하려면 공변량 $X$를 통제해야 합니다. 그러나 $X$의 차원이 높으면 직접 층화(stratification)가 불가능합니다. 경향 점수(Propensity Score)는 고차원 공변량을 하나의 스칼라 값으로 요약하여 교란을 효과적으로 통제하는 핵심 도구입니다.

---

## 1. 경향 점수의 정의와 성질

경향 점수(propensity score)는 공변량 $X$가 주어졌을 때 치료를 받을 확률입니다.

$$
e(X) = P(T=1 \mid X)
$$

Rosenbaum & Rubin(1983)의 핵심 정리:

$$
(Y(0), Y(1)) \perp\!\!\!\perp T \mid X \quad \Rightarrow \quad (Y(0), Y(1)) \perp\!\!\!\perp T \mid e(X)
$$

| 성질 | 수학적 표현 | 의미 |
|------|------------|------|
| 밸런싱 성질 | $X \perp\!\!\!\perp T \mid e(X)$ | 같은 경향 점수에서 공변량 분포 동일 |
| 차원 축소 | $\mathbb{R}^p \to [0,1]$ | 고차원 공변량을 스칼라로 압축 |
| 충분 통계량 | 교란 통제에 충분 | $e(X)$만 조건화하면 됨 |

> **핵심 직관**: 경향 점수가 0.6인 치료군 환자와 경향 점수가 0.6인 대조군 환자는, 관측된 공변량 분포가 평균적으로 동일합니다. 이것이 "유사 무작위 배정"을 만들어냅니다.

```python
import numpy as np
from sklearn.linear_model import LogisticRegression

# 경향 점수 추정
np.random.seed(42)
n = 5000

X1 = np.random.normal(0, 1, n)
X2 = np.random.binomial(1, 0.5, n)
logit = -0.5 + 0.8 * X1 + 0.6 * X2
T = np.random.binomial(1, 1 / (1 + np.exp(-logit)), n)

# 로지스틱 회귀로 경향 점수 추정
X = np.column_stack([X1, X2])
ps_model = LogisticRegression()
ps_model.fit(X, T)
ps_hat = ps_model.predict_proba(X)[:, 1]

print(f"경향 점수 범위: [{ps_hat.min():.3f}, {ps_hat.max():.3f}]")
print(f"경향 점수 평균 (치료군): {ps_hat[T==1].mean():.3f}")
print(f"경향 점수 평균 (대조군): {ps_hat[T==0].mean():.3f}")
```

---

## 2. 경향 점수 매칭 (PSM)

경향 점수 매칭은 치료군의 각 개체에 대해 경향 점수가 가장 유사한 대조군 개체를 매칭하는 방법입니다.

$$
\hat{\tau}_{\text{ATT}} = \frac{1}{n_1} \sum_{i: T_i=1} \left[ Y_i - Y_{j(i)} \right]
$$

여기서 $j(i) = \arg\min_{j: T_j=0} |e(X_i) - e(X_j)|$

| 매칭 방법 | 설명 | 장단점 |
|----------|------|--------|
| 1:1 최근접 | 가장 가까운 1개 매칭 | 단순하지만 정보 손실 |
| 1:k 매칭 | 가장 가까운 k개 매칭 | 편향-분산 트레이드오프 |
| 캘리퍼 매칭 | 거리 $< \delta$ 내에서만 매칭 | 나쁜 매칭 방지 |
| 커널 매칭 | 모든 대조군을 가중 평균 | 정보 손실 최소화 |

> **핵심 직관**: 매칭 후 남은 표본에서 치료군과 대조군의 공변량 분포가 유사해야 합니다. 이를 "밸런스 검증"이라 하며, 표준화 평균 차이(SMD)로 확인합니다.

```python
import numpy as np
from scipy.spatial.distance import cdist

# 1:1 최근접 경향 점수 매칭
np.random.seed(42)
n = 3000
X = np.random.normal(0, 1, (n, 3))
logit = X @ [0.5, -0.3, 0.8]
ps = 1 / (1 + np.exp(-logit))
T = np.random.binomial(1, ps, n)
Y = 2 * T + X @ [1, 0.5, -0.5] + np.random.normal(0, 1, n)

# 매칭
treated_idx = np.where(T == 1)[0]
control_idx = np.where(T == 0)[0]
ps_treated = ps[treated_idx].reshape(-1, 1)
ps_control = ps[control_idx].reshape(-1, 1)

distances = cdist(ps_treated, ps_control, metric='euclidean')
matched_control = control_idx[distances.argmin(axis=1)]

# ATT 추정
att_psm = (Y[treated_idx] - Y[matched_control]).mean()
print(f"PSM ATT 추정: {att_psm:.3f}")  # 진정한 ATT ≈ 2.0

# 밸런스 검증 (SMD)
for j in range(3):
    smd_before = (X[treated_idx, j].mean() - X[control_idx, j].mean()) / X[:, j].std()
    smd_after = (X[treated_idx, j].mean() - X[matched_control, j].mean()) / X[:, j].std()
    print(f"X{j+1} SMD: 매칭 전 {smd_before:.3f} → 매칭 후 {smd_after:.3f}")
```

---

## 3. 역확률 가중치 (IPW)

역확률 가중치(Inverse Probability Weighting, IPW)는 경향 점수의 역수를 가중치로 사용하여 유사-모집단을 구성합니다.

$$
\hat{\tau}_{\text{IPW}} = \frac{1}{n} \sum_{i=1}^{n} \left[ \frac{T_i Y_i}{e(X_i)} - \frac{(1-T_i) Y_i}{1-e(X_i)} \right]
$$

정규화된 IPW (Hajek estimator):

$$
\hat{\tau}_{\text{Hajek}} = \frac{\sum_i T_i Y_i / e(X_i)}{\sum_i T_i / e(X_i)} - \frac{\sum_i (1-T_i) Y_i / (1-e(X_i))}{\sum_i (1-T_i) / (1-e(X_i))}
$$

| IPW 변형 | 특징 | 주의점 |
|----------|------|--------|
| Horvitz-Thompson | 비정규화, 비편향 | 분산 큼 |
| Hajek | 정규화, 약간 편향 | 분산 작음 |
| 안정화 가중치 | $w = P(T) / e(X)$ | 극단 가중치 완화 |
| 트리밍 | $e(X) \in [\alpha, 1-\alpha]$ | ci-05의 양성성 보장 |

> **핵심 직관**: IPW의 직관은 "치료받기 어려운 조건에서 치료받은 사람에게 높은 가중치를, 치료받기 쉬운 조건에서 치료받지 않은 사람에게 높은 가중치를 부여"하는 것입니다.

```python
import numpy as np

# IPW ATE 추정
np.random.seed(42)
n = 10000
X = np.random.normal(0, 1, (n, 2))
ps_true = 1 / (1 + np.exp(-(X @ [0.8, 0.5])))
T = np.random.binomial(1, ps_true, n)
Y = 1.5 * T + X @ [1, 0.5] + np.random.normal(0, 1, n)

# IPW 추정 (진정한 경향 점수 사용)
ipw_ate = np.mean(T * Y / ps_true - (1 - T) * Y / (1 - ps_true))

# 트리밍 적용
trim = (ps_true > 0.1) & (ps_true < 0.9)
ipw_trimmed = np.mean(
    T[trim] * Y[trim] / ps_true[trim] -
    (1 - T[trim]) * Y[trim] / (1 - ps_true[trim])
)

print(f"IPW ATE: {ipw_ate:.3f}")
print(f"IPW ATE (트리밍): {ipw_trimmed:.3f}")
print(f"진정한 ATE: 1.500")
```

---

## 4. 이중 강건 추정 (Doubly Robust Estimation)

이중 강건(Doubly Robust, DR) 추정은 경향 점수 모델과 결과 모델 중 **하나만 올바르면** 일치 추정량(consistent estimator)이 되는 방법입니다.

$$
\hat{\tau}_{\text{DR}} = \frac{1}{n} \sum_{i=1}^{n} \left[ \hat{\mu}_1(X_i) - \hat{\mu}_0(X_i) + \frac{T_i(Y_i - \hat{\mu}_1(X_i))}{\hat{e}(X_i)} - \frac{(1-T_i)(Y_i - \hat{\mu}_0(X_i))}{1-\hat{e}(X_i)} \right]
$$

| 상황 | $\hat{e}(X)$ 올바름 | $\hat{\mu}(X)$ 올바름 | DR 일치성 |
|------|------|------|------|
| 경우 1 | O | O | O |
| 경우 2 | O | X | O |
| 경우 3 | X | O | O |
| 경우 4 | X | X | X |

> **핵심 직관**: DR 추정은 "보험"과 같습니다. 두 모델 중 하나가 틀려도 다른 하나가 보정해줍니다. ci-11에서 다룰 이중 머신러닝(DML)은 이 아이디어를 ML과 결합한 발전형입니다.

```python
import numpy as np
import statsmodels.api as sm

# 이중 강건 추정
np.random.seed(42)
n = 5000
X = np.random.normal(0, 1, (n, 3))
ps_true = 1 / (1 + np.exp(-(X @ [0.5, 0.3, -0.4])))
T = np.random.binomial(1, ps_true, n)
Y = 2.0 * T + X @ [1, -0.5, 0.8] + np.random.normal(0, 1, n)

# Step 1: 경향 점수 추정
from sklearn.linear_model import LogisticRegression
ps_model = LogisticRegression()
ps_model.fit(X, T)
ps_hat = ps_model.predict_proba(X)[:, 1]
ps_hat = np.clip(ps_hat, 0.05, 0.95)  # 트리밍

# Step 2: 결과 모델 추정
X_with_const = sm.add_constant(X)
mu1_model = sm.OLS(Y[T==1], X_with_const[T==1]).fit()
mu0_model = sm.OLS(Y[T==0], X_with_const[T==0]).fit()
mu1_hat = mu1_model.predict(X_with_const)
mu0_hat = mu0_model.predict(X_with_const)

# Step 3: DR 추정
dr_ate = np.mean(
    mu1_hat - mu0_hat
    + T * (Y - mu1_hat) / ps_hat
    - (1 - T) * (Y - mu0_hat) / (1 - ps_hat)
)
print(f"DR ATE 추정: {dr_ate:.3f}")  # ≈ 2.0
```

---

## 5. 경향 점수 진단과 밸런스 검증

경향 점수 분석의 품질은 반드시 진단 과정을 거쳐야 합니다.

| 진단 항목 | 방법 | 기준 |
|----------|------|------|
| 경향 점수 겹침 | 히스토그램 비교 | 양 그룹의 분포가 충분히 겹침 |
| 공변량 밸런스 | 표준화 평균 차이 (SMD) | $\|SMD\| < 0.1$ |
| 분산 비율 | 치료군/대조군 분산 비 | $0.5 < VR < 2.0$ |
| 모델 적합도 | C-statistic (AUC) | $0.6 \sim 0.8$ 적정 |

> **핵심 직관**: 경향 점수 모델의 예측력이 너무 높으면(AUC > 0.9) 양성성 위반 가능성이 있고, 너무 낮으면(AUC < 0.5) 공변량이 충분하지 않을 수 있습니다.

```python
import numpy as np

# 밸런스 진단: SMD 계산
def standardized_mean_diff(x_treat, x_control):
    pooled_std = np.sqrt((x_treat.var() + x_control.var()) / 2)
    return (x_treat.mean() - x_control.mean()) / pooled_std

# 매칭/가중 전후 SMD 비교
np.random.seed(42)
n = 3000
X = np.random.normal(0, 1, (n, 3))
T = np.random.binomial(1, 1 / (1 + np.exp(-(X @ [1, 0.5, -0.3]))), n)

print("매칭 전 SMD:")
for j in range(3):
    smd = standardized_mean_diff(X[T==1, j], X[T==0, j])
    status = "OK" if abs(smd) < 0.1 else "불균형!"
    print(f"  X{j+1}: SMD = {smd:.3f} [{status}]")
```

---

## 6. 경향 점수 방법의 한계와 대안

경향 점수 기반 방법은 관측된 공변량만으로 교란을 통제하므로, 근본적인 한계가 존재합니다.

| 한계 | 설명 | 대안 |
|------|------|------|
| 관측되지 않은 교란 | 경향 점수로 통제 불가 | ci-07 도구 변수, ci-08 RDD/DID |
| 모델 의존성 | 경향 점수 모델 오명세 | DR 추정, ci-11 DML |
| 양성성 위반 | 극단 가중치 문제 | 트리밍, 안정화 가중치 |
| 차원의 저주 | 공변량 많으면 매칭 품질 저하 | 기계학습 경향 점수 |

> **핵심 직관**: 경향 점수는 만능 해결책이 아닙니다. "관측되지 않은 교란이 없다"는 가정은 데이터만으로 검증할 수 없으므로, 민감도 분석(sensitivity analysis)을 반드시 수행해야 합니다.

---

## 핵심 정리

- **경향 점수 $e(X) = P(T=1 \mid X)$는 고차원 공변량을 스칼라로 압축하여 교란을 통제합니다**: 밸런싱 성질에 의해 $e(X)$만 조건화하면 무교란성이 유지됩니다
- **경향 점수 매칭(PSM)은 유사한 경향 점수를 가진 개체끼리 매칭하여 인과 효과를 추정합니다**: SMD < 0.1로 밸런스를 검증해야 합니다
- **역확률 가중치(IPW)는 경향 점수의 역수로 가중하여 유사-모집단을 구성합니다**: 극단 가중치에 대한 트리밍이 필수적입니다
- **이중 강건(DR) 추정은 경향 점수 모델과 결과 모델 중 하나만 올바르면 일치 추정량이 됩니다**: 실무에서 가장 권장되는 방법입니다
- **경향 점수 방법은 관측된 공변량만 통제하므로, 관측되지 않은 교란에 대한 민감도 분석이 반드시 필요합니다**
