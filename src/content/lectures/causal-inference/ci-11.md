# 인과 추론과 ML의 결합

## 왜 인과 추론에 머신러닝이 필요한가

ci-06~ci-08에서 다룬 전통적 인과 추론 방법은 모수적 가정(선형성 등)에 의존하는 경우가 많습니다. 현대의 고차원 데이터에서는 유연한 ML 모델이 필요하지만, 단순히 ML을 적용하면 정규화 편향(regularization bias)이 발생합니다. 이중 머신러닝(Double/Debiased Machine Learning)은 ML의 유연성과 인과 추론의 엄밀성을 결합한 프레임워크입니다.

---

## 1. 이중 머신러닝 (Double Machine Learning, DML)

Chernozhukov et al.(2018)이 제안한 DML은 고차원 교란 변수를 ML로 유연하게 통제하면서도 인과 효과의 $\sqrt{n}$-일치성과 점근 정규성을 보장합니다.

**부분 선형 모델:**

$$
Y = \theta T + g(X) + \epsilon, \quad E[\epsilon \mid X, T] = 0
$$
$$
T = m(X) + \eta, \quad E[\eta \mid X] = 0
$$

**DML 추정 절차:**

$$
\hat{\theta}_{\text{DML}} = \frac{\sum_i (T_i - \hat{m}(X_i))(Y_i - \hat{g}(X_i))}{\sum_i (T_i - \hat{m}(X_i))^2}
$$

| 단계 | 연산 | ML 모델 |
|------|------|--------|
| 1. 결과 잔차 | $\tilde{Y} = Y - \hat{g}(X)$ | 랜덤 포레스트, Lasso 등 |
| 2. 치료 잔차 | $\tilde{T} = T - \hat{m}(X)$ | 로지스틱 회귀, GBM 등 |
| 3. 인과 효과 | $\hat{\theta} = \text{OLS}(\tilde{Y} \sim \tilde{T})$ | 잔차 간 회귀 |

> **핵심 직관**: DML의 핵심 아이디어는 "먼저 ML로 교란 변수의 효과를 제거한 뒤, 남은 잔차들의 관계에서 인과 효과를 추정"하는 것입니다. ci-06의 이중 강건 추정을 고차원으로 확장한 것입니다.

```python
import numpy as np
from sklearn.ensemble import GradientBoostingRegressor, GradientBoostingClassifier
from sklearn.model_selection import KFold

# DML 구현
np.random.seed(42)
n = 5000
p = 20
X = np.random.normal(0, 1, (n, p))
T = (X @ np.random.normal(0, 0.3, p) + np.random.normal(0, 1, n) > 0).astype(float)
theta_true = 2.0
Y = theta_true * T + np.sin(X[:, 0]) + X[:, 1]**2 + np.random.normal(0, 1, n)

# Cross-fitting (필수!)
kf = KFold(n_splits=5, shuffle=True, random_state=42)
Y_resid = np.zeros(n)
T_resid = np.zeros(n)

for train_idx, test_idx in kf.split(X):
    # 결과 모델
    g_model = GradientBoostingRegressor(n_estimators=100, max_depth=3)
    g_model.fit(X[train_idx], Y[train_idx])
    Y_resid[test_idx] = Y[test_idx] - g_model.predict(X[test_idx])

    # 치료 모델
    m_model = GradientBoostingRegressor(n_estimators=100, max_depth=3)
    m_model.fit(X[train_idx], T[train_idx])
    T_resid[test_idx] = T[test_idx] - m_model.predict(X[test_idx])

# DML 추정
theta_dml = np.sum(T_resid * Y_resid) / np.sum(T_resid ** 2)
se_dml = np.sqrt(np.mean((Y_resid - theta_dml * T_resid)**2 * T_resid**2) /
                 (np.mean(T_resid**2)**2 * n))
print(f"DML 추정: {theta_dml:.3f} (SE: {se_dml:.3f})")
print(f"진정한 효과: {theta_true}")
```

---

## 2. 교차 적합 (Cross-Fitting)의 필요성

DML에서 교차 적합은 단순한 기술적 디테일이 아니라 이론적으로 필수적인 요소입니다.

$$
\text{Overfitting bias}: \quad \hat{g}(X_i) \text{가 } Y_i \text{에 과적합} \Rightarrow \tilde{Y}_i \text{가 편향}
$$

| 방법 | 설명 | 문제점 |
|------|------|--------|
| 전체 데이터 사용 | $\hat{g}$를 모든 데이터로 학습 | 과적합 편향 발생 |
| Sample splitting | 데이터를 반으로 나눔 | 표본 효율 50% 손실 |
| Cross-fitting | K-fold 교차 학습 | 편향 제거 + 효율 유지 |

> **핵심 직관**: ML 모델이 훈련 데이터에 과적합하면 잔차가 인위적으로 작아져 인과 효과 추정이 편향됩니다. Cross-fitting은 "자기 자신을 예측에 사용하지 않는" 원칙으로 이를 방지합니다.

---

## 3. Neyman Orthogonality

DML이 ML의 느린 수렴 속도($n^{-1/4}$)에도 불구하고 인과 효과에 대해 $\sqrt{n}$-일치성을 달성하는 이론적 핵심은 **Neyman 직교성**입니다.

$$
\psi(\theta_0, \eta_0) = 0 \quad \text{(모멘트 조건)}
$$

$$
\frac{\partial}{\partial \eta} E[\psi(\theta_0, \eta)] \bigg|_{\eta=\eta_0} = 0 \quad \text{(직교성)}
$$

| 성질 | 의미 |
|------|------|
| 직교성 | 뉘앙스 파라미터 $\eta$의 추정 오차가 $\theta$에 1차적으로 영향 없음 |
| $\sqrt{n}$-일치성 | $\hat{\theta} - \theta_0 = O_p(n^{-1/2})$ |
| 점근 정규성 | $\sqrt{n}(\hat{\theta} - \theta_0) \to N(0, V)$ |
| 이중 강건성 | ci-06의 DR 추정과 동일한 보호 |

> **핵심 직관**: 직교성 덕분에 ML 모델의 추정 오차가 $n^{-1/4}$ 속도로만 수렴해도, 인과 효과 $\theta$는 $n^{-1/2}$ 속도로 수렴합니다. 이것이 "느린 ML + 빠른 인과 추론"을 가능하게 합니다.

---

## 4. 인과 표현 학습 (Causal Representation Learning)

인과 표현 학습은 고차원 비정형 데이터(이미지, 텍스트)에서 인과적으로 의미 있는 잠재 변수를 학습하는 최신 연구 분야입니다.

$$
\text{관측}: \mathbf{x} \in \mathbb{R}^d \quad \xrightarrow{\text{encoder}} \quad \text{잠재}: \mathbf{z} \in \mathbb{R}^k \quad \text{s.t. } \mathbf{z} \text{가 인과 구조를 가짐}
$$

| 접근법 | 핵심 아이디어 | 대표 모델 |
|--------|------------|----------|
| iVAE | 보조 변수로 식별 가능한 VAE | Khemakhem et al. (2020) |
| CausalVAE | DAG 제약이 있는 VAE | Yang et al. (2021) |
| CITRIS | 시간적 변화로 식별 | Lippe et al. (2022) |
| BISCUIT | 다중 환경 활용 | Lachapelle et al. (2023) |

> **핵심 직관**: 이미지에서 "조명"과 "객체 형상"이라는 독립적 인과 변수를 분리할 수 있다면, 조명 변화에 강건한 모델을 만들 수 있습니다. 이는 ci-02의 SCM을 잠재 공간으로 확장한 것입니다.

```python
import numpy as np

# 인과 표현 학습의 핵심 아이디어 (개념적)
np.random.seed(42)
n = 1000

# 잠재 인과 변수 (관측 불가)
z1 = np.random.normal(0, 1, n)           # 원인 1 (예: 조명)
z2 = 0.5 * z1 + np.random.normal(0, 1, n)  # 원인 2 (예: z1에 의존)

# 관측 데이터 = 비선형 혼합 (예: 이미지 픽셀)
x1 = np.sin(z1) + 0.3 * z2 + np.random.normal(0, 0.1, n)
x2 = np.cos(z2) + 0.2 * z1 + np.random.normal(0, 0.1, n)
x3 = z1 * z2 + np.random.normal(0, 0.1, n)

print("관측 공간 (3차원)에서 잠재 인과 변수 (2차원)를 복원하는 것이 목표")
print(f"관측 변수 상관: x1-x2={np.corrcoef(x1,x2)[0,1]:.3f}")
print("인과 표현 학습은 비선형 ICA + DAG 학습을 결합합니다")
```

---

## 5. 반사실 추론과 ML

반사실(counterfactual) 추론은 ci-01 인과 사다리의 최상위 단계로, "실제로 일어난 것과 다른 시나리오에서 결과가 어떠했을까"를 추론합니다. ML과의 결합이 활발히 연구되고 있습니다.

$$
\text{반사실}: \quad Y_{x'}(u) = f_Y(x', \text{Pa}_Y \setminus X, u_Y)
$$

| 방법 | 접근 | 장점 |
|------|------|------|
| CEVAE | 잠재 변수로 반사실 추론 | 비선형 교란 처리 |
| GANITE | GAN 기반 잠재 결과 생성 | 복잡한 분포 모델링 |
| DeepMatch | 딥러닝 기반 매칭 | 고차원 공변량 처리 |
| Counterfactual Regression | 표현 학습 + 인과 추론 | ci-06의 매칭을 잠재 공간에서 수행 |

> **핵심 직관**: "이 환자가 다른 약을 복용했더라면 결과가 어떠했을까?"라는 질문에 답하려면, 관측된 특성에서 반사실 결과를 생성하는 모델이 필요합니다. ci-05의 잠재 결과 $Y(0), Y(1)$을 신경망으로 추정하는 것입니다.

```python
import numpy as np

# 반사실 추론의 핵심: 잠재 결과 추정
np.random.seed(42)
n = 3000
X = np.random.normal(0, 1, (n, 5))

# 진정한 잠재 결과
mu0 = np.sin(X[:, 0]) + X[:, 1]**2
mu1 = mu0 + 2.0 + 1.5 * X[:, 0]  # 이질적 효과
Y0 = mu0 + np.random.normal(0, 0.5, n)
Y1 = mu1 + np.random.normal(0, 0.5, n)

T = np.random.binomial(1, 0.5, n)
Y_obs = T * Y1 + (1 - T) * Y0

# 반사실 결과 추정 (T-Learner 사용)
from sklearn.ensemble import GradientBoostingRegressor
model1 = GradientBoostingRegressor(n_estimators=100).fit(X[T==1], Y_obs[T==1])
model0 = GradientBoostingRegressor(n_estimators=100).fit(X[T==0], Y_obs[T==0])

# 치료군의 반사실: "치료 안 받았더라면?"
cf_treated = model0.predict(X[T==1])  # Y(0) 추정
actual_treated = Y_obs[T==1]
ite_treated = actual_treated - cf_treated

print(f"치료군의 추정 ITE 평균: {ite_treated.mean():.3f}")
print(f"진정한 ATT: {(Y1[T==1] - Y0[T==1]).mean():.3f}")
```

---

## 6. 인과적 공정성과 설명 가능성

인과 추론은 ML 모델의 공정성(fairness)과 설명 가능성(explainability)에도 핵심적인 역할을 합니다.

$$
\text{인과적 공정성}: \quad Y_{A=a}(u) = Y_{A=a'}(u) \quad \forall a, a', u
$$

| 공정성 개념 | 정의 | 인과적 관점 |
|-----------|------|-----------|
| 인구통계학적 동등 | $P(\hat{Y}=1 \mid A=a) = P(\hat{Y}=1 \mid A=a')$ | 상관 기반 (약함) |
| 균등 기회 | $P(\hat{Y}=1 \mid Y=1, A=a) = P(\hat{Y}=1 \mid Y=1, A=a')$ | 조건부 상관 |
| 인과적 공정성 | 보호 속성의 인과 효과 = 0 | 반사실 기반 (강함) |
| 경로 특정 공정성 | 특정 인과 경로의 효과 = 0 | 직접/간접 효과 분리 |

> **핵심 직관**: "성별이 채용에 영향을 미치는가"는 상관관계가 아니라 인과적 질문입니다. ci-02의 DAG를 활용하면 성별이 직접 영향을 미치는 경로와 교육·경험을 통해 간접적으로 영향을 미치는 경로를 분리할 수 있습니다.

---

## 7. 실무 도구와 라이브러리

인과 추론과 ML의 결합을 위한 주요 라이브러리입니다.

| 라이브러리 | 핵심 기능 | 언어 |
|----------|----------|------|
| econml | DML, 인과 포레스트, 메타러너 | Python |
| DoWhy | 인과 효과 식별 + 추정 + 반박 | Python |
| CausalML | 업리프트 모델링, 메타러너 | Python |
| grf | 인과 포레스트 (원저자) | R |
| CausalNex | DAG 학습 + 인과 추론 | Python |

> **핵심 직관**: 실무에서는 DoWhy로 인과 구조를 정의하고 식별 전략을 도출한 뒤, econml로 DML/인과 포레스트를 적용하는 워크플로우가 효과적입니다. ci-12에서 구체적 활용 사례를 다룹니다.

```python
# econml을 활용한 DML 예시 (개념적)
# pip install econml

# from econml.dml import DML, CausalForestDML
# from sklearn.ensemble import GradientBoostingRegressor

# DML 추정
# dml = DML(
#     model_y=GradientBoostingRegressor(),
#     model_t=GradientBoostingRegressor(),
#     model_final=LinearRegression()
# )
# dml.fit(Y, T, X=X, W=W)
# ate = dml.ate(X)
# cate = dml.effect(X)
# ci = dml.effect_interval(X, alpha=0.05)

print("econml DML 워크플로우:")
print("1. 뉘앙스 모델 지정 (model_y, model_t)")
print("2. .fit(Y, T, X, W)으로 학습")
print("3. .effect(X)로 CATE 추정")
print("4. .effect_interval(X)로 신뢰 구간")
```

---

## 핵심 정리

- **이중 머신러닝(DML)은 ML로 교란을 유연하게 통제하면서도 $\sqrt{n}$-일치성을 보장합니다**: 교차 적합(cross-fitting)이 과적합 편향을 제거하는 핵심 요소입니다
- **Neyman 직교성은 뉘앙스 파라미터의 느린 수렴 속도가 인과 효과 추정에 1차적 영향을 미치지 않음을 보장합니다**
- **인과 표현 학습은 고차원 비정형 데이터에서 인과적으로 의미 있는 잠재 변수를 식별합니다**: ci-02의 SCM을 잠재 공간으로 확장한 최신 연구 분야입니다
- **반사실 추론을 ML과 결합하면 개인 수준의 "만약 ~했더라면" 질문에 답할 수 있습니다**: CEVAE, GANITE 등이 대표적입니다
- **인과적 공정성은 보호 속성의 인과 효과를 기반으로 ML 모델의 공정성을 정의하며, 경로 특정 분석으로 직접/간접 효과를 분리합니다**
