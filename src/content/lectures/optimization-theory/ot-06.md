# Sharpness-Aware Minimization (SAM)

## 왜 평탄도를 명시적으로 최적화해야 하는가

ot-05에서 flat minima가 일반화에 유리하다는 것을 확인했습니다. 그러나 SGD의 암묵적 정규화(ot-01)만으로는 항상 flat minima에 도달하는 것이 보장되지 않습니다. SAM은 손실 값뿐 아니라 손실 경관의 "날카로움"까지 명시적으로 최소화하여 일반화 성능을 체계적으로 개선합니다.

---

## 1. SAM의 목적 함수

SAM은 최악의 섭동에서의 손실을 최소화합니다.

$$\min_\theta \; L^{\text{SAM}}(\theta) = \max_{\|\epsilon\| \leq \rho} L(\theta + \epsilon)$$

이는 $\rho$-구 내에서 가장 큰 손실을 최소화하는 minimax 문제입니다.

**1차 근사를 통한 풀이:**

$$\epsilon^*(\theta) \approx \rho \frac{\nabla L(\theta)}{\|\nabla L(\theta)\|}$$

따라서 SAM의 업데이트는 다음과 같습니다.

$$\theta_{t+1} = \theta_t - \eta \nabla L(\theta_t + \epsilon^*(\theta_t))$$

| 기호 | 의미 | 일반적 값 |
|------|------|----------|
| $\rho$ | 섭동 반경 | 0.05 ~ 0.2 |
| $\epsilon^*$ | 최악 방향 섭동 | 그래디언트 방향 |
| $\eta$ | 학습률 | 기존과 동일 |

> **핵심 직관**: SAM은 "현재 위치에서 가장 취약한 방향으로 흔들어 본 후, 그 상태에서의 그래디언트를 따라 이동"합니다 — 이는 flat region을 명시적으로 선호하게 만듭니다.

---

## 2. SAM 알고리즘 구현

SAM은 매 스텝마다 두 번의 forward-backward pass가 필요합니다.

```python
class SAM(torch.optim.Optimizer):
    def __init__(self, params, base_optimizer, rho=0.05):
        defaults = dict(rho=rho)
        super().__init__(params, defaults)
        self.base_optimizer = base_optimizer(self.param_groups)

    @torch.no_grad()
    def first_step(self):
        grad_norm = self._grad_norm()
        for group in self.param_groups:
            scale = group['rho'] / (grad_norm + 1e-12)
            for p in group['params']:
                if p.grad is None:
                    continue
                e_w = p.grad * scale
                p.add_(e_w)            # theta + epsilon 이동
                self.state[p]['e_w'] = e_w

    @torch.no_grad()
    def second_step(self):
        for group in self.param_groups:
            for p in group['params']:
                if p.grad is None:
                    continue
                p.sub_(self.state[p]['e_w'])  # 원래 theta로 복귀
        self.base_optimizer.step()

    def _grad_norm(self):
        norm = torch.norm(torch.stack([
            p.grad.norm() for group in self.param_groups
            for p in group['params'] if p.grad is not None
        ]))
        return norm
```

**학습 루프:**

```python
optimizer = SAM(model.parameters(), torch.optim.SGD, rho=0.05)

for x, y in loader:
    # 첫 번째 forward-backward: epsilon 계산
    loss = criterion(model(x), y)
    loss.backward()
    optimizer.first_step()
    optimizer.zero_grad()

    # 두 번째 forward-backward: 실제 업데이트
    criterion(model(x), y).backward()
    optimizer.second_step()
    optimizer.zero_grad()
```

> **핵심 직관**: SAM의 2배 계산 비용이 부담스러울 수 있으나, 동일한 에폭에서 1~2%p의 정확도 향상은 많은 경우 이 비용을 정당화합니다.

---

## 3. SAM과 일반화 이론

SAM이 일반화를 개선하는 이유를 이론적으로 분석합니다.

**PAC-Bayes Bound와의 연결:**

$$L_{\text{test}}(\theta) \leq L_{\text{train}}(\theta + \epsilon) + O\left(\sqrt{\frac{\|\theta\|^2 / \sigma^2 + \ln(N)}{N}}\right)$$

SAM은 $L_{\text{train}}(\theta + \epsilon)$을 직접 최소화하므로, PAC-Bayes bound를 타이트하게 만듭니다.

| 관점 | SAM의 효과 | 이론적 근거 |
|------|-----------|-----------|
| Sharpness 감소 | 헤시안 최대 고유값 감소 | 곡률 제어 |
| PAC-Bayes bound | 일반화 상한 축소 | 분포 안정성 |
| 평탄 영역 탐색 | SGD 노이즈와 상보적 | ot-05 mode connectivity |
| 암묵적 정규화 | weight norm 감소 효과 | AdamW와 유사 |

> **핵심 직관**: SAM은 "학습 데이터에서 잘 맞추는 것"이 아니라 "학습 데이터 근처에서 안정적으로 잘 맞추는 것"을 목표로 합니다.

---

## 4. ASAM: 적응적 SAM

SAM의 고정 $\rho$는 파라미터 스케일에 따라 비효율적일 수 있습니다. ASAM은 파라미터별로 적응적 섭동 반경을 사용합니다.

$$\epsilon^*(\theta) = \rho \frac{T_\theta^2 \nabla L(\theta)}{\|T_\theta \nabla L(\theta)\|}$$

여기서 $T_\theta = \text{diag}(|\theta|)$는 파라미터 크기에 비례하는 스케일링 행렬입니다.

```python
class ASAM:
    def __init__(self, optimizer, model, rho=0.5):
        self.optimizer = optimizer
        self.model = model
        self.rho = rho

    @torch.no_grad()
    def ascent_step(self):
        wgrads = []
        for n, p in self.model.named_parameters():
            if p.grad is None:
                continue
            # 파라미터 크기에 비례하는 적응적 섭동
            t_w = torch.abs(p) if 'weight' in n else torch.ones_like(p)
            wgrads.append((t_w * p.grad).norm())
        wgrad_norm = torch.norm(torch.stack(wgrads))

        for n, p in self.model.named_parameters():
            if p.grad is None:
                continue
            t_w = torch.abs(p) if 'weight' in n else torch.ones_like(p)
            e_w = t_w * t_w * p.grad * self.rho / (wgrad_norm + 1e-12)
            p.add_(e_w)
            self.state[n] = e_w
```

| 방법 | 섭동 공간 | $\rho$ 범위 | 강점 |
|------|---------|-----------|------|
| SAM | $\ell_2$ 구 | 0.05~0.2 | 단순, 안정적 |
| ASAM | 적응적 타원체 | 0.5~2.0 | 스케일 불변 |
| mSAM | 미니배치 분할 | 0.05~0.2 | 분산 효과 |

ViT-Base/16에서 ASAM은 SAM 대비 0.3~0.5%p 추가 정확도 향상을 보입니다.

> **핵심 직관**: ASAM은 "큰 파라미터는 크게, 작은 파라미터는 작게" 섭동하여, 파라미터 스케일에 무관한 평탄도 최적화를 달성합니다.

---

## 5. SAM의 효율적 변형들

SAM의 2배 계산 비용을 줄이기 위한 다양한 변형이 제안되었습니다.

**LookSAM:**

$$\text{매 } k \text{스텝마다 SAM, 나머지는 일반 SGD}$$

**Efficient SAM (ESAM):**

미니배치의 일부만 사용하여 $\epsilon^*$를 계산합니다.

```python
# LookSAM 개념: k스텝 주기로 SAM 적용
for step, (x, y) in enumerate(loader):
    loss = criterion(model(x), y)
    loss.backward()

    if step % k == 0:
        # SAM 스텝: 섭동 방향 계산 및 저장
        sam_direction = compute_epsilon(model)
        apply_perturbation(model, sam_direction)
        criterion(model(x), y).backward()
        remove_perturbation(model, sam_direction)

    optimizer.step()
    optimizer.zero_grad()
```

| 변형 | 추가 비용 | SAM 대비 성능 | 핵심 아이디어 |
|------|---------|-------------|------------|
| SAM | 2x | 기준 | 매 스텝 2회 pass |
| LookSAM | 1.1~1.5x | 95~99% | 주기적 SAM |
| ESAM | 1.3~1.5x | 95~98% | 부분 배치 SAM |
| Random SAM | 1x | 90~95% | 무작위 섭동 |

> **핵심 직관**: SAM의 핵심 이점은 "정확한 최악 방향"이 아니라 "섭동 후 그래디언트 계산"이라는 원칙에서 오며, 근사 방법으로도 상당한 효과를 유지합니다.

---

## 6. SAM과 다른 정규화 기법의 관계

SAM은 기존 정규화 기법들과 상보적으로 작동합니다.

| 정규화 기법 | 메커니즘 | SAM과의 관계 |
|-----------|---------|------------|
| Weight Decay (ot-03) | 파라미터 크기 제한 | 상보적 — 동시 사용 권장 |
| Dropout | 뉴런 무작위 비활성 | 약간 중복 효과 |
| Data Augmentation | 학습 데이터 확장 | 상보적 — 효과 증폭 |
| Label Smoothing | 타겟 분포 완화 | 상보적 — 공동 사용 효과적 |
| SWA (ot-05) | 궤적 평균화 | 대체 가능하나 결합도 가능 |

**SAM + AdamW 결합 (Transformer 학습):**

```python
base_optimizer = torch.optim.AdamW
optimizer = SAM(model.parameters(), base_optimizer,
                rho=0.05, lr=1e-4, weight_decay=0.05)
```

ViT 학습에서 SAM + AdamW + strong augmentation 조합이 ImageNet top-1 정확도 88% 이상을 달성합니다.

> **핵심 직관**: SAM은 기존 정규화 기법을 대체하는 것이 아니라, "손실 경관의 기하학"이라는 직교하는 차원에서 추가적인 정규화를 제공합니다.

---

## 7. SAM의 한계와 미래 방향

SAM은 강력하지만 완벽하지 않으며, 활발한 연구가 진행 중입니다.

| 한계 | 설명 | 연구 방향 |
|------|------|----------|
| 계산 비용 2배 | 대규모 모델에서 부담 | 효율적 변형 (LookSAM 등) |
| $\rho$ 튜닝 | 데이터/모델에 따라 최적값 다름 | 적응적 $\rho$ 스케줄링 |
| 이론적 이해 부족 | 비볼록에서의 수렴 보장 불완전 | ot-10 미해결 문제 |
| 분산 학습 호환성 | gradient sync 2회 필요 | ot-09 분산 SAM |

> **핵심 직관**: SAM은 "왜 flat minima가 좋은가"라는 ot-05의 관찰을 "어떻게 flat minima를 찾을 것인가"로 전환한 실용적 돌파구이며, 최적화-일반화의 연결 고리를 강화합니다.

---

## 핵심 정리

- **SAM은 $\max_{\|\epsilon\| \leq \rho} L(\theta + \epsilon)$을 최소화하여 손실 경관의 평탄도를 명시적으로 개선합니다**
- **알고리즘적으로 매 스텝 2회의 forward-backward pass가 필요하며, 첫 번째로 최악 섭동 방향을, 두 번째로 실제 업데이트를 계산합니다**
- **ASAM은 파라미터 크기에 비례하는 적응적 섭동을 사용하여 스케일 불변 평탄도 최적화를 달성합니다**
- **LookSAM, ESAM 등의 효율적 변형은 계산 비용을 크게 줄이면서도 SAM 효과의 90~99%를 유지합니다**
- **SAM은 weight decay, data augmentation 등 기존 정규화와 상보적이며, ViT + AdamW + SAM 조합이 현대 비전 모델의 강력한 레시피입니다**
