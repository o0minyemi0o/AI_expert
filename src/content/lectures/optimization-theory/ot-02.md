# 모멘텀과 가속 방법 (Momentum and Acceleration Methods)

## 왜 모멘텀이 필요한가

순수 SGD는 좁고 긴 골짜기(ill-conditioned landscape)에서 지그재그 진동하며 수렴이 느립니다. 모멘텀은 과거 그래디언트의 이동 평균을 축적하여 일관된 방향의 움직임을 가속하고 진동을 억제합니다. ot-01에서 다룬 SGD의 한계를 극복하는 첫 번째 핵심 기법입니다.

---

## 1. 물리적 직관: 공이 굴러가는 비유

모멘텀은 물리학의 관성에서 영감을 받았습니다. 경사면을 내려가는 공은 속도를 축적하며, 작은 언덕은 관성으로 넘어갑니다.

$$v_{t+1} = \beta v_t + \nabla L(\theta_t)$$
$$\theta_{t+1} = \theta_t - \eta \, v_{t+1}$$

여기서 $\beta \in [0, 1)$는 모멘텀 계수이고, 일반적으로 $\beta = 0.9$를 사용합니다.

| 파라미터 | 물리적 의미 | 일반적 값 |
|---------|-----------|----------|
| $\eta$ | 힘의 크기 | 0.01 ~ 0.1 |
| $\beta$ | 마찰 계수 (1에 가까울수록 마찰 적음) | 0.9, 0.99 |
| $v_t$ | 속도 (velocity) | 누적된 그래디언트 |

> **핵심 직관**: 모멘텀은 그래디언트가 일관된 방향으로 가리키면 가속하고, 반대 방향으로 진동하면 상쇄시킵니다.

---

## 2. 모멘텀 SGD의 수렴 분석

조건수(condition number) $\kappa = L/\mu$인 강볼록 함수에서 수렴 속도를 비교합니다.

| 방법 | 수렴 속도 | 조건수 의존성 |
|------|----------|-------------|
| GD | $O\left(\left(\frac{\kappa - 1}{\kappa + 1}\right)^t\right)$ | $\kappa$에 선형 |
| GD + Momentum | $O\left(\left(\frac{\sqrt{\kappa} - 1}{\sqrt{\kappa} + 1}\right)^t\right)$ | $\sqrt{\kappa}$에 의존 |

모멘텀은 조건수의 제곱근으로 의존성을 줄여줍니다.

**유효 학습률 분석:**

축적된 모멘텀의 정상 상태에서 유효 학습률은 다음과 같습니다.

$$\eta_{\text{eff}} = \frac{\eta}{1 - \beta}$$

$\beta = 0.9$일 때 유효 학습률은 $10\eta$가 됩니다. 따라서 모멘텀을 추가할 때 학습률을 낮춰야 할 수 있습니다.

> **핵심 직관**: 모멘텀은 $\kappa$를 $\sqrt{\kappa}$로 바꾸는 효과가 있어, 조건이 나쁜 문제에서 특히 유리합니다.

---

## 3. Nesterov 가속 경사법 (NAG)

Nesterov는 "미리 한 발 내딛고 그 위치에서 그래디언트를 계산하라"는 아이디어를 제안했습니다.

$$v_{t+1} = \beta v_t + \nabla L(\theta_t - \eta \beta v_t)$$
$$\theta_{t+1} = \theta_t - \eta \, v_{t+1}$$

PyTorch에서는 동등한 재구성(reformulation)을 사용합니다.

```python
optimizer = torch.optim.SGD(
    model.parameters(),
    lr=0.1,
    momentum=0.9,
    nesterov=True   # Nesterov 가속 활성화
)
```

**Nesterov vs. 일반 모멘텀의 차이:**

| 특성 | 일반 모멘텀 | Nesterov 모멘텀 |
|------|-----------|----------------|
| 그래디언트 평가 위치 | $\theta_t$ | $\theta_t - \eta\beta v_t$ (lookahead) |
| 볼록 함수 수렴 | $O(1/T)$ | $O(1/T^2)$ — 최적 |
| 오버슈팅 | 흔함 | 교정 효과 |
| 실전 효과 | 좋음 | 약간 더 좋음 |

ResNet-50 학습에서 Nesterov 모멘텀은 일반 모멘텀 대비 0.2~0.3%p 더 나은 top-1 정확도를 보입니다.

> **핵심 직관**: Nesterov는 "미래 위치"에서의 그래디언트를 사용하므로, 오버슈팅을 사전에 교정하는 "예측 능력"을 갖습니다.

---

## 4. Polyak 평균화 (Iterate Averaging)

SGD의 궤적은 노이즈 때문에 최적해 주변을 진동합니다. Polyak은 반복(iterate)의 평균을 취하면 더 나은 수렴이 가능하다는 것을 보였습니다.

$$\bar{\theta}_T = \frac{1}{T} \sum_{t=1}^{T} \theta_t$$

**지수 이동 평균 (EMA)** 변형이 실전에서 더 많이 사용됩니다.

$$\bar{\theta}_{t+1} = \alpha \, \bar{\theta}_t + (1 - \alpha) \, \theta_{t+1}, \quad \alpha = 0.999$$

```python
# PyTorch에서 EMA 구현
class EMA:
    def __init__(self, model, decay=0.999):
        self.decay = decay
        self.shadow = {name: p.clone().detach()
                       for name, p in model.named_parameters()}

    @torch.no_grad()
    def update(self, model):
        for name, p in model.named_parameters():
            self.shadow[name].mul_(self.decay).add_(
                p.data, alpha=1 - self.decay
            )

    def apply(self, model):
        for name, p in model.named_parameters():
            p.data.copy_(self.shadow[name])
```

| 전략 | 수렴 이론 | 실전 적용 |
|------|----------|----------|
| 단순 평균 | $O(1/T)$ 보장 | 학습 후반부만 평균 |
| EMA ($\alpha=0.999$) | 최근 iterate 강조 | 대부분의 프레임워크 기본 |
| SWA (후반부 평균) | flat minima 탐색 | ot-05와 연계 |

GPT 학습에서 EMA는 체크포인트 평균화를 통해 평가 perplexity를 1~2 포인트 개선합니다.

> **핵심 직관**: Polyak 평균화는 SGD 궤적의 노이즈를 "평균 내어" 제거하며, 현대 딥러닝에서는 EMA 형태로 거의 보편적으로 사용됩니다.

---

## 5. Lookahead 옵티마이저

Lookahead는 모멘텀과 Polyak 평균화의 아이디어를 결합한 방법입니다.

**알고리즘:**

1. 빠른 가중치(fast weights) $\phi$를 내부 옵티마이저로 $k$스텝 업데이트
2. 느린 가중치(slow weights) $\theta$를 빠른 가중치 방향으로 보간

$$\theta_{t+1} = \theta_t + \alpha (\phi_{t,k} - \theta_t)$$

```python
# Lookahead 개념적 구현
class Lookahead:
    def __init__(self, base_optimizer, k=5, alpha=0.5):
        self.base_optimizer = base_optimizer
        self.k = k
        self.alpha = alpha
        self.step_count = 0

    def step(self):
        self.base_optimizer.step()
        self.step_count += 1
        if self.step_count % self.k == 0:
            for slow_p, fast_p in zip(self.slow_params, self.fast_params):
                slow_p.data.add_(fast_p.data - slow_p.data, alpha=self.alpha)
                fast_p.data.copy_(slow_p.data)
```

| 하이퍼파라미터 | 의미 | 기본값 |
|--------------|------|-------|
| $k$ | 내부 루프 스텝 수 | 5~10 |
| $\alpha$ | 느린 가중치 보간 비율 | 0.5~0.8 |
| 내부 옵티마이저 | SGD, Adam 등 | Adam |

> **핵심 직관**: Lookahead는 "탐험(exploration)"과 "안정화(stabilization)"를 분리하여 학습의 안정성과 수렴 속도를 동시에 확보합니다.

---

## 6. 모멘텀 방법들의 통합적 관점

이 장에서 다룬 방법들은 모두 "과거 정보의 활용"이라는 공통 프레임워크로 이해할 수 있습니다.

$$\theta_{t+1} = \theta_t - \eta \cdot \mathcal{A}(\{g_s\}_{s \leq t}, \{\theta_s\}_{s \leq t})$$

여기서 $\mathcal{A}$는 과거 그래디언트와 파라미터의 집계 함수입니다.

| 방법 | 과거 정보 활용 | 핵심 이점 |
|------|-------------|----------|
| SGD | 없음 | 단순성 |
| Momentum | 그래디언트 EMA | 가속 |
| Nesterov | Lookahead 그래디언트 | 교정 |
| Polyak/EMA | 파라미터 평균 | 안정화 |
| Lookahead | 파라미터 보간 | 탐험+안정 |

ot-03에서는 여기에 "파라미터별 적응적 학습률"이라는 새로운 차원을 추가합니다.

> **핵심 직관**: 모멘텀 계열의 방법들은 1차 정보(그래디언트)만으로 2차 곡률 정보를 간접적으로 활용하는 전략이며, ot-08에서 다루는 명시적 2차 방법의 저비용 근사로 볼 수 있습니다.

---

## 핵심 정리

- **모멘텀 SGD는 과거 그래디언트의 지수 이동 평균을 축적하여 조건수 의존성을 $\kappa$에서 $\sqrt{\kappa}$로 개선합니다**
- **Nesterov 가속은 "미래 위치"에서 그래디언트를 계산하여 볼록 함수에서 최적 수렴 속도 $O(1/T^2)$를 달성합니다**
- **Polyak/EMA 평균화는 SGD 궤적의 노이즈를 제거하며, 현대 딥러닝에서 체크포인트 평균으로 보편적으로 사용됩니다**
- **Lookahead 옵티마이저는 빠른/느린 가중치의 이중 구조로 탐험과 안정화를 동시에 달성합니다**
- **모든 모멘텀 방법은 "과거 정보의 활용" 프레임워크로 통합되며, ot-03의 적응적 학습률과 결합하여 Adam 등을 구성합니다**
