# 그래디언트 클리핑과 혼합 정밀도 (Gradient Clipping and Mixed Precision)

## 왜 그래디언트 안정화와 수치 효율이 중요한가

대규모 모델의 학습에서 그래디언트 폭발(exploding gradients)은 학습을 즉각 파괴하는 치명적 문제입니다. 동시에, 수십억 파라미터 모델을 실용적 시간과 메모리 내에서 학습하려면 저정밀도 연산이 필수입니다. 이 두 기법은 ot-01~ot-04의 모든 최적화 알고리즘이 실제 동작하기 위한 인프라적 기반입니다.

---

## 1. 그래디언트 폭발 문제

RNN이나 깊은 네트워크에서 역전파 시 그래디언트가 레이어를 거치며 지수적으로 증가할 수 있습니다.

$$\frac{\partial L}{\partial \theta_1} = \prod_{k=1}^{T} W_k \cdot \frac{\partial L}{\partial h_T}$$

가중치 행렬의 최대 특이값 $\sigma_{\max}(W) > 1$이면 $T$가 커질수록 그래디언트가 폭발합니다.

$$\left\|\frac{\partial L}{\partial \theta_1}\right\| \leq \sigma_{\max}(W)^T \cdot \left\|\frac{\partial L}{\partial h_T}\right\|$$

| 문제 | 증상 | 원인 |
|------|------|------|
| 그래디언트 폭발 | loss NaN, 파라미터 발산 | $\sigma_{\max}(W) > 1$, 긴 시퀀스 |
| 그래디언트 소실 | 학습 정체, 초기 레이어 미학습 | $\sigma_{\max}(W) < 1$, 포화 활성함수 |

> **핵심 직관**: 그래디언트 폭발은 "한 번의 나쁜 배치"만으로도 전체 학습을 망칠 수 있으므로, 사전 방어 메커니즘이 필수입니다.

---

## 2. 그래디언트 클리핑 (Gradient Clipping)

두 가지 주요 클리핑 방법이 존재합니다.

**Clip by Norm (가장 보편적):**

$$g_t' = \begin{cases} g_t & \text{if } \|g_t\| \leq c \\ c \cdot \frac{g_t}{\|g_t\|} & \text{if } \|g_t\| > c \end{cases}$$

방향은 유지하고 크기만 제한합니다.

**Clip by Value:**

$$g_t'[i] = \text{clip}(g_t[i], -c, c)$$

각 원소를 독립적으로 클리핑합니다 (방향이 변할 수 있음).

```python
# Clip by Norm (권장)
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

# Clip by Value
torch.nn.utils.clip_grad_value_(model.parameters(), clip_value=0.5)

# 학습 루프에서의 사용
for x, y in loader:
    optimizer.zero_grad()
    loss = criterion(model(x), y)
    loss.backward()
    # 반드시 backward() 후, step() 전에 클리핑
    grad_norm = torch.nn.utils.clip_grad_norm_(
        model.parameters(), max_norm=1.0
    )
    optimizer.step()
```

| 방법 | 방향 보존 | 일반적 $c$ | 사용 사례 |
|------|---------|-----------|----------|
| Clip by Norm | 보존 | 1.0 ~ 5.0 | Transformer, LSTM |
| Clip by Value | 변경 가능 | 0.5 ~ 1.0 | 특수한 경우 |
| Adaptive Clipping | 보존 (적응적) | $\mu + k\sigma$ | 실험적 |

GPT-3 학습에서 `max_norm=1.0`의 gradient clipping은 표준 설정이며, 이를 제거하면 학습 중 loss spike가 빈번해집니다.

> **핵심 직관**: Clip by norm은 "방향은 유지하되 걸음 크기만 제한"하므로, 대부분의 경우 clip by value보다 안전합니다.

---

## 3. 그래디언트 노름 모니터링

클리핑 임계값 $c$의 선택은 그래디언트 노름의 통계에 기반해야 합니다.

```python
# 그래디언트 노름 추적
grad_norms = []
for x, y in loader:
    optimizer.zero_grad()
    loss = criterion(model(x), y)
    loss.backward()

    total_norm = 0
    for p in model.parameters():
        if p.grad is not None:
            total_norm += p.grad.data.norm(2).item() ** 2
    total_norm = total_norm ** 0.5
    grad_norms.append(total_norm)

    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
    optimizer.step()

# 통계 분석
print(f"Mean: {np.mean(grad_norms):.2f}")
print(f"Max: {np.max(grad_norms):.2f}")
print(f"99th percentile: {np.percentile(grad_norms, 99):.2f}")
```

| 진단 지표 | 건강한 학습 | 문제 징후 |
|----------|-----------|----------|
| 평균 노름 | 안정적 또는 점진 감소 | 급격한 증가 |
| 최대 노름 | 평균의 2~5배 | 100배 이상 |
| 클리핑 빈도 | 5~20% | 80% 이상 |

> **핵심 직관**: 그래디언트 노름은 학습 건강도의 핵심 진단 지표이며, 99번째 백분위수를 기준으로 클리핑 임계값을 설정하는 것이 실용적입니다.

---

## 4. 부동소수점 정밀도와 딥러닝

컴퓨터에서 실수는 유한 비트로 근사됩니다.

| 형식 | 비트 | 지수 | 가수 | 범위 | $\epsilon$ (기계 정밀도) |
|------|------|------|------|------|---------------------|
| FP32 | 32 | 8 | 23 | $\pm 3.4 \times 10^{38}$ | $\approx 1.2 \times 10^{-7}$ |
| FP16 | 16 | 5 | 10 | $\pm 6.5 \times 10^{4}$ | $\approx 9.8 \times 10^{-4}$ |
| BF16 | 16 | 8 | 7 | $\pm 3.4 \times 10^{38}$ | $\approx 3.9 \times 10^{-3}$ |
| TF32 | 19 | 8 | 10 | $\pm 3.4 \times 10^{38}$ | $\approx 9.8 \times 10^{-4}$ |

**FP16의 문제:** 표현 범위가 $6.5 \times 10^4$까지이므로, 큰 그래디언트나 loss 값이 오버플로우될 수 있습니다.

**BF16의 장점:** FP32와 동일한 지수 범위를 가지므로 오버플로우 위험이 적습니다.

> **핵심 직관**: BF16은 FP16보다 정밀도는 낮지만, 범위가 넓어서 loss scaling 없이도 안정적으로 학습할 수 있어 현대 GPU/TPU의 표준이 되고 있습니다.

---

## 5. 혼합 정밀도 학습 (Mixed Precision Training)

혼합 정밀도는 연산은 FP16/BF16으로, 마스터 가중치는 FP32로 유지하는 전략입니다.

**3가지 핵심 기법:**

1. **FP32 마스터 가중치**: 파라미터의 정확한 사본을 FP32로 유지
2. **FP16 연산**: Forward/backward pass를 FP16으로 수행
3. **Loss Scaling**: FP16 그래디언트의 언더플로우 방지

```python
from torch.cuda.amp import autocast, GradScaler

scaler = GradScaler()  # Loss scaling 관리

for x, y in loader:
    optimizer.zero_grad()

    # Forward pass in FP16
    with autocast():
        output = model(x)
        loss = criterion(output, y)

    # Backward pass with loss scaling
    scaler.scale(loss).backward()

    # Unscale → clip → step
    scaler.unscale_(optimizer)
    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
    scaler.step(optimizer)
    scaler.update()
```

| 단계 | 정밀도 | 이유 |
|------|--------|------|
| 가중치 저장 | FP32 | 작은 업데이트 보존 |
| Forward pass | FP16/BF16 | 속도 및 메모리 |
| Loss 계산 | FP32 | 정확도 |
| Backward pass | FP16/BF16 | 속도 및 메모리 |
| 가중치 업데이트 | FP32 | 정밀도 |

> **핵심 직관**: 혼합 정밀도는 "중요한 곳은 FP32, 나머지는 FP16"이라는 원칙으로 속도 2배, 메모리 절반을 달성하면서 FP32 수준의 정확도를 유지합니다.

---

## 6. Loss Scaling: 언더플로우 방지

FP16의 최소 양수값은 약 $6 \times 10^{-8}$이므로, 작은 그래디언트가 0으로 반올림(언더플로우)됩니다.

**정적 Loss Scaling:**

$$\tilde{L} = S \cdot L, \quad \tilde{g} = S \cdot g, \quad g = \tilde{g} / S$$

스케일 $S$를 고정값(예: 1024)으로 사용합니다.

**동적 Loss Scaling (권장):**

```python
scaler = GradScaler(
    init_scale=2**16,       # 초기 스케일
    growth_factor=2.0,       # 스케일 증가 비율
    backoff_factor=0.5,      # 스케일 감소 비율
    growth_interval=2000     # 증가 주기 (스텝)
)
```

| 이벤트 | 동작 | 이유 |
|--------|------|------|
| 정상 학습 | 2000스텝마다 $S \times 2$ | 언더플로우 최소화 |
| Inf/NaN 감지 | $S \times 0.5$, 스텝 건너뜀 | 오버플로우 복구 |

> **핵심 직관**: 동적 loss scaling은 "가능한 한 큰 스케일을 유지하되, 오버플로우 시 즉시 줄이는" 적응적 전략입니다.

---

## 7. 실전 레시피: 안정적 대규모 학습

그래디언트 클리핑과 혼합 정밀도를 결합한 완전한 학습 루프입니다.

```python
model = TransformerLM(config).cuda()
optimizer = torch.optim.AdamW(
    model.parameters(), lr=3e-4,
    betas=(0.9, 0.95), weight_decay=0.1
)
scaler = GradScaler()

for step, (x, y) in enumerate(loader):
    optimizer.zero_grad()

    with autocast(dtype=torch.bfloat16):  # BF16 사용 시
        loss = model(x, y)

    scaler.scale(loss).backward()
    scaler.unscale_(optimizer)

    # 그래디언트 클리핑
    grad_norm = torch.nn.utils.clip_grad_norm_(
        model.parameters(), max_norm=1.0
    )

    # 학습률 스케줄링 (ot-04)
    lr = get_lr(step)
    for pg in optimizer.param_groups:
        pg['lr'] = lr

    scaler.step(optimizer)
    scaler.update()

    # 모니터링
    if step % 100 == 0:
        print(f"Step {step}, Loss: {loss.item():.4f}, "
              f"Grad Norm: {grad_norm:.4f}, LR: {lr:.6f}")
```

| 체크리스트 | 권장 설정 | 주의사항 |
|-----------|---------|---------|
| 정밀도 | BF16 (GPU지원 시) | A100, H100 이상 |
| 클리핑 | max_norm=1.0 | backward 후, step 전 |
| Loss scaling | 동적 (GradScaler) | FP16 사용 시 필수 |
| 노름 모니터링 | 매 스텝 기록 | 이상 탐지용 |

LLaMA, GPT 계열 학습에서 BF16 + gradient clipping(1.0) + AdamW는 사실상 표준 인프라입니다.

> **핵심 직관**: 그래디언트 클리핑과 혼합 정밀도는 "최적화 알고리즘"이 아니라 "최적화 인프라"이며, 이것이 없으면 ot-01~ot-06의 어떤 기법도 대규모 모델에서 제대로 작동하지 않습니다.

---

## 핵심 정리

- **그래디언트 폭발은 깊은 네트워크와 긴 시퀀스에서 역전파 시 그래디언트가 지수적으로 증가하는 현상이며, 한 번의 발생으로 전체 학습이 붕괴될 수 있습니다**
- **Clip by norm은 그래디언트 방향을 보존하면서 크기만 제한하며, max_norm=1.0이 Transformer 학습의 표준 설정입니다**
- **혼합 정밀도 학습은 연산을 FP16/BF16으로 수행하고 마스터 가중치를 FP32로 유지하여 속도 2배, 메모리 절반을 달성합니다**
- **Loss scaling은 FP16의 좁은 표현 범위에서 발생하는 그래디언트 언더플로우를 방지하며, 동적 스케일링이 권장됩니다**
- **BF16은 FP32와 동일한 지수 범위로 loss scaling 없이도 안정적이며, 최신 GPU에서 혼합 정밀도의 새로운 표준이 되고 있습니다**
