# 대규모 학습의 최적화 (Optimization for Large-Scale Training)

## 왜 대규모 학습에 특별한 최적화가 필요한가

단일 GPU로 GPT나 ViT 같은 대규모 모델을 학습하면 수개월이 걸립니다. 학습 시간을 줄이려면 수백~수천 개의 GPU를 사용하여 배치 크기를 대폭 키워야 합니다. 그러나 ot-01에서 다루었듯 큰 배치는 일반화를 해치며, 단순한 학습률 스케일링만으로는 부족합니다. LARS, LAMB 등의 대규모 학습 전용 기법이 이 문제를 해결합니다.

---

## 1. 데이터 병렬 분산 SGD

가장 기본적인 분산 학습 전략은 데이터 병렬(data parallelism)입니다.

$$g_t^{\text{sync}} = \frac{1}{K}\sum_{k=1}^{K} g_t^{(k)}$$

$K$개의 워커가 각각 미니배치 $B/K$를 처리하고, 그래디언트를 AllReduce로 동기화합니다.

```python
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP

# 분산 초기화
dist.init_process_group(backend='nccl')
model = DDP(model.cuda(), device_ids=[local_rank])

# 학습 루프는 단일 GPU와 동일
for x, y in distributed_loader:
    optimizer.zero_grad()
    loss = criterion(model(x), y)
    loss.backward()       # AllReduce 자동 수행
    optimizer.step()
```

| 병렬 전략 | 분할 대상 | 통신 | 배치 크기 효과 |
|----------|---------|------|-------------|
| 데이터 병렬 | 데이터 | AllReduce(그래디언트) | $B_{\text{eff}} = K \cdot B$ |
| 모델 병렬 | 파라미터 | 활성화 전달 | 변화 없음 |
| 파이프라인 병렬 | 레이어 | 활성화/그래디언트 | 마이크로배치 |

> **핵심 직관**: 데이터 병렬은 유효 배치 크기를 $K$배 증가시키며, 이는 ot-01에서 다룬 그래디언트 분산을 $K$배 줄이는 효과와 동시에 일반화 위험을 수반합니다.

---

## 2. Linear Scaling Rule

배치 크기 증가에 따른 학습률 조정의 기본 원칙입니다.

$$\eta_{K \cdot B} = K \cdot \eta_B$$

**이론적 근거:** $K$배 큰 배치로 1스텝은 원래 배치로 $K$스텝과 유사한 기대 업데이트를 가집니다.

$$K \cdot \eta_B \cdot \frac{1}{KB}\sum_{i=1}^{KB} g_i \approx \eta_B \cdot \frac{1}{B}\sum_{j=1}^{B} g_j \text{ (기대값 동일)}$$

| 기준 배치 | 스케일 배치 | 기준 LR | 스케일 LR | Warmup |
|----------|-----------|---------|----------|--------|
| 256 | 8,192 (32x) | 0.1 | 3.2 | 5 에폭 |
| 256 | 32,768 (128x) | 0.1 | 12.8 | 5~10 에폭 |

**한계:** 배치 크기가 매우 크면(>8K) linear scaling rule만으로는 성능이 급격히 하락합니다.

```python
# Linear scaling rule 적용
base_lr = 0.1
base_batch_size = 256
actual_batch_size = 8192  # 32 GPU x 256

scaled_lr = base_lr * (actual_batch_size / base_batch_size)
optimizer = torch.optim.SGD(model.parameters(), lr=scaled_lr,
                            momentum=0.9, weight_decay=1e-4)

# 필수: warmup 추가
scheduler = torch.optim.lr_scheduler.LinearLR(
    optimizer, start_factor=1/32, total_iters=5 * steps_per_epoch
)
```

> **핵심 직관**: Linear scaling rule은 "배치가 커지면 그래디언트 노이즈가 줄므로, 더 큰 보폭을 취해도 안전하다"는 직관에 기반하지만, 매우 큰 배치에서는 이 근사가 무너집니다.

---

## 3. LARS: 레이어별 적응적 학습률

LARS(Layer-wise Adaptive Rate Scaling)는 각 레이어의 가중치 노름과 그래디언트 노름의 비율로 학습률을 조정합니다.

$$\lambda_l = \xi \cdot \frac{\|w_l\|}{\|g_l\| + \beta \|w_l\|}$$

$$w_l \leftarrow w_l - \eta \cdot \lambda_l \cdot (g_l + \beta w_l)$$

여기서 $\xi$는 trust coefficient (보통 0.001~0.02), $\beta$는 weight decay입니다.

**LARS의 직관:** 레이어마다 $\|w_l\| / \|g_l\|$ 비율이 크게 다릅니다.

| 레이어 | $\|w_l\|$ | $\|g_l\|$ | $\|w\|/\|g\|$ |
|--------|-----------|-----------|---------------|
| Conv1 (초기) | 0.1 | 0.01 | 10 |
| Conv5 (중간) | 1.0 | 0.001 | 1000 |
| FC (최종) | 5.0 | 0.1 | 50 |

```python
# LARS 개념적 구현
class LARS(torch.optim.Optimizer):
    def __init__(self, params, lr, momentum=0.9,
                 weight_decay=1e-4, trust_coeff=0.02):
        defaults = dict(lr=lr, momentum=momentum,
                       weight_decay=weight_decay,
                       trust_coeff=trust_coeff)
        super().__init__(params, defaults)

    @torch.no_grad()
    def step(self):
        for group in self.param_groups:
            for p in group['params']:
                if p.grad is None:
                    continue
                w_norm = p.data.norm()
                g_norm = p.grad.data.norm()

                # LARS 스케일링
                local_lr = group['trust_coeff'] * w_norm / (
                    g_norm + group['weight_decay'] * w_norm + 1e-9
                )
                # 모멘텀 업데이트
                d = p.grad.data + group['weight_decay'] * p.data
                update = group['lr'] * local_lr * d
                p.data.sub_(update)
```

ResNet-50을 배치 크기 32K로 학습 시, LARS는 linear scaling rule만 사용한 경우 대비 정확도 하락을 1%p 이내로 억제합니다.

> **핵심 직관**: LARS는 "각 레이어의 업데이트 크기가 가중치 크기에 비례하도록" 자동 조절하여, 레이어 간 학습 속도 불균형을 해소합니다.

---

## 4. LAMB: Adam + LARS

LAMB(Layer-wise Adaptive Moments optimizer for Batch training)는 LARS의 레이어별 스케일링을 Adam에 적용합니다.

$$m_t, v_t = \text{Adam update} \quad (\text{ot-03 참조})$$
$$r_t = \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon} + \beta w_t$$
$$w_{t+1} = w_t - \eta \cdot \frac{\|w_t\|}{\|r_t\|} \cdot r_t$$

```python
# LAMB 핵심 로직
def lamb_step(param, grad, m, v, step, lr, beta1, beta2, wd, eps):
    # Adam 모멘트 업데이트
    m = beta1 * m + (1 - beta1) * grad
    v = beta2 * v + (1 - beta2) * grad ** 2
    m_hat = m / (1 - beta1 ** step)
    v_hat = v / (1 - beta2 ** step)

    # Adam 방향 + weight decay
    update = m_hat / (torch.sqrt(v_hat) + eps) + wd * param

    # LARS-style trust ratio
    w_norm = param.norm()
    u_norm = update.norm()
    trust_ratio = w_norm / (u_norm + 1e-9) if w_norm > 0 else 1.0

    param -= lr * trust_ratio * update
    return m, v
```

| 방법 | 기반 옵티마이저 | 최대 배치 크기 (ImageNet) | 정확도 유지 |
|------|-------------|----------------------|-----------|
| Linear Scaling | SGD | ~8K | 76.3% → 73.2% |
| LARS | SGD + Momentum | ~32K | 76.3% → 75.4% |
| LAMB | Adam | ~32K | 76.3% → 76.0% |

BERT 사전학습에서 LAMB은 배치 크기를 16K까지 증가시키면서 3일 만에 학습을 완료합니다 (기존 대비 ~4배 가속).

> **핵심 직관**: LAMB = Adam의 적응적 학습률 + LARS의 레이어별 스케일링으로, Transformer와 CNN 모두에서 대규모 배치 학습의 표준 옵티마이저입니다.

---

## 5. 통신 효율화 기법

분산 학습에서 통신 비용은 핵심 병목입니다.

**그래디언트 압축:**

$$\tilde{g} = \text{TopK}(g) \quad \text{또는} \quad \tilde{g} = \text{Sign}(g)$$

**Gradient Accumulation:**

```python
# 통신 빈도를 줄이는 gradient accumulation
accumulation_steps = 4  # 4스텝마다 1번 통신
for step, (x, y) in enumerate(loader):
    with model.no_sync() if (step + 1) % accumulation_steps != 0 else nullcontext():
        loss = criterion(model(x), y)
        loss = loss / accumulation_steps
        loss.backward()

    if (step + 1) % accumulation_steps == 0:
        optimizer.step()
        optimizer.zero_grad()
```

| 기법 | 통신량 감소 | 정확도 영향 | 구현 복잡도 |
|------|-----------|-----------|-----------|
| Gradient accumulation | 선형 | 없음 | 낮음 |
| TopK sparsification | 90~99% | 미미 (error feedback) | 중간 |
| 1-bit SGD | ~32배 | 약간 하락 | 중간 |
| PowerSGD (low-rank) | 90~99% | 미미 | 높음 |

> **핵심 직관**: Gradient accumulation은 "실제로 배치를 크게 만들지 않고도 큰 유효 배치 크기를 시뮬레이션"하는 가장 간단한 방법입니다.

---

## 6. 대규모 학습의 실전 레시피

현대 대규모 모델 학습의 완전한 레시피입니다.

```python
# GPT 스타일 대규모 학습 설정
config = {
    'optimizer': 'AdamW',
    'lr': 6e-4,
    'beta1': 0.9,
    'beta2': 0.95,
    'weight_decay': 0.1,
    'grad_clip': 1.0,            # ot-07
    'warmup_steps': 2000,         # ot-04
    'lr_schedule': 'cosine',      # ot-04
    'batch_size': 4_000_000,      # tokens per batch
    'precision': 'bf16',          # ot-07
    'data_parallel': True,
    'model_parallel': True,       # 모델 크기에 따라
}
```

| 스케일 | GPU 수 | 유효 배치 | 옵티마이저 | 학습 시간 |
|--------|--------|---------|----------|---------|
| BERT-Base | 16 | 256 | AdamW | 4일 |
| GPT-2 (1.5B) | 64 | ~512K tokens | AdamW | 1주 |
| GPT-3 (175B) | ~1000 | ~3.2M tokens | AdamW | 수주 |
| LLaMA-2 (70B) | 2000 | ~4M tokens | AdamW | ~1.7M GPU-hours |

> **핵심 직관**: 대규모 학습은 "최적화 알고리즘의 선택"만큼 "인프라 설정(분산, 정밀도, 통신)"이 중요하며, 이 두 차원이 결합되어야 실용적 학습이 가능합니다.

---

## 7. 학습 안정성: Loss Spike와 복구

대규모 학습에서 간헐적으로 발생하는 loss spike는 심각한 문제입니다.

| 원인 | 증상 | 대응 |
|------|------|------|
| 데이터 이상치 | 갑작스런 loss 증가 | 데이터 필터링, grad clip |
| 수치 불안정 | NaN/Inf | loss scaling, BF16 전환 |
| 학습률 과대 | 반복적 spike | LR 감소, warmup 연장 |
| 배치 구성 불균형 | 특정 스텝 spike | 데이터 셔플링 개선 |

```python
# Loss spike 감지 및 자동 복구
class TrainingMonitor:
    def __init__(self, spike_threshold=5.0):
        self.ema_loss = None
        self.spike_threshold = spike_threshold

    def check(self, loss, step):
        if self.ema_loss is None:
            self.ema_loss = loss
            return False
        self.ema_loss = 0.99 * self.ema_loss + 0.01 * loss
        is_spike = loss > self.spike_threshold * self.ema_loss
        if is_spike:
            print(f"[Step {step}] Loss spike detected: {loss:.4f} "
                  f"vs EMA {self.ema_loss:.4f}")
        return is_spike
```

PaLM (540B) 학습에서 약 20회의 loss spike가 관찰되었으며, 해당 데이터 배치를 건너뛰는 방식으로 복구했습니다.

> **핵심 직관**: 대규모 학습에서 loss spike는 피할 수 없으며, 자동 감지와 복구 메커니즘(체크포인트 롤백, 배치 스킵)을 사전에 구축해야 합니다.

---

## 핵심 정리

- **데이터 병렬 분산 SGD는 유효 배치 크기를 GPU 수에 비례하여 증가시키며, AllReduce로 그래디언트를 동기화합니다**
- **Linear scaling rule은 배치 크기에 비례하여 학습률을 증가시키지만, 매우 큰 배치(>8K)에서는 일반화 성능이 하락합니다**
- **LARS는 레이어별 가중치/그래디언트 노름 비율로 학습률을 자동 조정하여 대규모 배치 SGD의 성능 하락을 방지합니다**
- **LAMB은 Adam에 LARS의 레이어별 스케일링을 결합하여 Transformer의 대규모 배치 학습에서 표준 옵티마이저로 사용됩니다**
- **Gradient accumulation, 그래디언트 압축 등의 통신 효율화 기법과 loss spike 모니터링은 실전 대규모 학습의 필수 인프라입니다**
