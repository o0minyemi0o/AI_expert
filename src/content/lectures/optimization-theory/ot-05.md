# 손실 경관의 기하학 (Geometry of Loss Landscapes)

## 왜 손실 경관의 구조를 이해해야 하는가

딥러닝의 손실 함수 $L(\theta)$는 수백만~수십억 차원의 비볼록 함수입니다. 이 함수의 기하학적 구조는 최적화의 난이도, 일반화 성능, 그리고 ot-01~ot-04에서 다룬 모든 기법의 실질적 작동 방식을 결정합니다. 손실 경관을 이해하면 "왜 딥러닝이 작동하는가"라는 근본 질문에 한 걸음 더 다가갑니다.

---

## 1. 비볼록 최적화의 도전

딥러닝의 손실 함수는 비볼록(non-convex)이며, co-07에서 다룬 볼록 최적화 이론을 직접 적용할 수 없습니다.

$$L(\theta) = \frac{1}{N}\sum_{i=1}^{N} \ell(f_\theta(x_i), y_i)$$

**비볼록 함수의 정상점(stationary points) 분류:**

$$\nabla L(\theta^*) = 0 \text{일 때:}$$

| 유형 | 헤시안 $H = \nabla^2 L(\theta^*)$ | 특성 |
|------|------------------------------|------|
| 극솟값 (local min) | $H \succ 0$ (양정치) | 모든 고유값 > 0 |
| 극댓값 (local max) | $H \prec 0$ (음정치) | 모든 고유값 < 0 |
| 안장점 (saddle) | 부정치 (indefinite) | 양/음 고유값 혼재 |

$$\text{안장점 지수(saddle index)} = \frac{\text{음의 고유값 수}}{\text{전체 고유값 수}}$$

> **핵심 직관**: 고차원에서 무작위 정상점이 극솟값일 확률은 지수적으로 작습니다 — 대부분의 정상점은 안장점입니다.

---

## 2. 안장점 문제와 탈출

$d$차원 공간에서 무작위 정상점의 각 고유값이 양/음이 될 확률이 각각 1/2이라면, 극솟값일 확률은 $(1/2)^d$입니다.

**안장점 탈출 메커니즘:**

1. **SGD 노이즈** (ot-01): 확률적 그래디언트의 노이즈가 음의 곡률 방향으로 자연스럽게 탈출
2. **모멘텀** (ot-02): 관성이 안장점 근처의 느린 수렴을 극복
3. **적응적 학습률** (ot-03): 파라미터별로 스케일링하여 평탄한 방향의 학습률 증가

```python
# 안장점 근처에서의 헤시안 분석
from torch.autograd.functional import hessian

def compute_loss(params_flat):
    # 모델 파라미터를 flat 벡터로 변환
    return loss_fn(model(x), y)

H = hessian(compute_loss, params_flat)
eigenvalues = torch.linalg.eigvalsh(H)
saddle_index = (eigenvalues < 0).float().mean()
```

| 탈출 메커니즘 | 효과적인 경우 | 관련 강의 |
|-------------|------------|----------|
| SGD 노이즈 | 낮은 에너지 안장점 | ot-01 |
| 모멘텀 | 관성으로 느린 영역 통과 | ot-02 |
| 큰 학습률 | 불안정점 회피 | ot-04 |
| 노이즈 주입 | 명시적 탈출 보조 | ot-06 |

> **핵심 직관**: 현대 딥러닝에서 안장점은 이론적 우려보다 실전적 문제가 크지 않습니다 — SGD의 확률적 특성이 자연스럽게 안장점을 탈출하기 때문입니다.

---

## 3. Flat Minima vs. Sharp Minima

같은 학습 손실을 가진 극솟값이라도, 주변의 곡률에 따라 일반화 성능이 크게 다릅니다.

**Flat minimum:** 손실 곡면이 넓고 평탄한 영역의 극솟값

$$L(\theta^* + \delta) \approx L(\theta^*) \quad \text{for } \|\delta\| < r \text{ (큰 } r\text{)}$$

**Sharp minimum:** 손실 곡면이 좁고 가파른 영역의 극솟값

$$L(\theta^* + \delta) \gg L(\theta^*) \quad \text{for small } \|\delta\|$$

**수학적 정의 — 헤시안 기반:**

$$\text{Sharpness}(\theta) = \max_{\|\epsilon\| \leq \rho} L(\theta + \epsilon) - L(\theta)$$

| 특성 | Flat Minimum | Sharp Minimum |
|------|-------------|---------------|
| 헤시안 고유값 | 작음 | 큼 |
| 일반화 | 우수 | 불량 |
| 섭동 민감도 | 낮음 | 높음 |
| PAC-Bayes bound | 타이트 | 느슨 |

> **핵심 직관**: Flat minima는 파라미터의 작은 섭동(테스트 데이터의 분포 변화와 유사)에 강건하므로 일반화가 우수합니다 — ot-06의 SAM은 이 직관을 명시적으로 최적화합니다.

---

## 4. 모드 연결성 (Mode Connectivity)

독립적으로 학습된 두 극솟값 사이를 손실이 증가하지 않는 경로로 연결할 수 있다는 발견입니다.

**Linear Mode Connectivity:**

$$L(\alpha \theta_A + (1-\alpha)\theta_B) \leq \max(L(\theta_A), L(\theta_B))$$

이 조건이 성립하면 두 해는 linearly mode connected입니다.

**비선형 경로 (Bezier curve):**

$$\theta(\alpha) = (1-\alpha)^2 \theta_A + 2\alpha(1-\alpha)\theta_M + \alpha^2 \theta_B$$

```python
# 두 체크포인트 사이의 손실 경관 탐색
def interpolate_and_evaluate(model_A, model_B, alpha, loader):
    for p_A, p_B, p in zip(model_A.parameters(),
                            model_B.parameters(),
                            model.parameters()):
        p.data = alpha * p_A.data + (1 - alpha) * p_B.data
    return evaluate(model, loader)

# alpha를 0에서 1로 변화시키며 loss 측정
losses = [interpolate_and_evaluate(model_A, model_B, a, loader)
          for a in torch.linspace(0, 1, 21)]
```

| 발견 | 의미 | 응용 |
|------|------|------|
| 같은 초기화에서 분기 | 대부분 선형 연결 | 모델 수프, EMA |
| 다른 초기화 | 비선형 경로 존재 | SWA의 이론적 근거 |
| 같은 사전학습에서 파인튜닝 | 선형 연결 가능 | 모델 병합 (model merging) |

> **핵심 직관**: 손실 경관의 극솟값들은 고립된 "점"이 아니라, 연결된 "골짜기"를 형성하며, 이는 모델 앙상블과 평균화(ot-02 Polyak 평균화)의 이론적 근거입니다.

---

## 5. 손실 경관의 시각화

고차원 손실 경관을 2차원으로 시각화하는 기법들이 있습니다.

**Filter-Normalized Visualization (Li et al., 2018):**

$$L(\alpha, \beta) = L(\theta^* + \alpha \hat{d}_1 + \beta \hat{d}_2)$$

여기서 $\hat{d}_1, \hat{d}_2$는 무작위 방향을 필터 단위로 정규화한 벡터입니다.

```python
# 손실 경관 시각화 (개념적 구현)
import numpy as np

d1 = normalize_directions(random_direction_like(model))
d2 = normalize_directions(random_direction_like(model))

grid = np.zeros((41, 41))
for i, alpha in enumerate(np.linspace(-1, 1, 41)):
    for j, beta in enumerate(np.linspace(-1, 1, 41)):
        perturb_model(model, theta_star, alpha * d1 + beta * d2)
        grid[i, j] = evaluate_loss(model, loader)

# grid를 contour plot으로 시각화
```

| 모델/설정 | 경관 특성 | 일반화 |
|----------|---------|--------|
| ResNet + skip connection | 넓고 평탄 | 우수 |
| VGG (no skip) | 좁고 날카로움 | 보통 |
| 큰 배치 학습 | sharp minima 경향 | 주의 필요 |
| 작은 배치 학습 | flat minima 경향 | 우수 |

> **핵심 직관**: Skip connection은 손실 경관을 "매끄럽게" 만들어 최적화를 용이하게 하며, 이것이 ResNet이 VGG보다 깊게 학습 가능한 핵심 이유입니다.

---

## 6. Stochastic Weight Averaging (SWA)

SWA는 학습 후반부의 SGD 궤적을 평균하여 flat minima를 찾는 방법입니다 (ot-02의 Polyak 평균화 확장).

$$\bar{\theta}_{\text{SWA}} = \frac{1}{K}\sum_{k=1}^{K} \theta_{t_k}$$

여기서 $\theta_{t_k}$는 주기적으로 수집된 체크포인트입니다.

```python
from torch.optim.swa_utils import AveragedModel, SWALR

swa_model = AveragedModel(model)
swa_scheduler = SWALR(optimizer, swa_lr=0.05)

for epoch in range(swa_start, total_epochs):
    train_one_epoch(model, optimizer, loader)
    swa_model.update_parameters(model)
    swa_scheduler.step()

# BatchNorm 통계 업데이트 필수
torch.optim.swa_utils.update_bn(loader, swa_model)
```

| SWA 설정 | 값 | 비고 |
|---------|------|------|
| swa_start | 총 에폭의 75% | 수렴 근처에서 시작 |
| swa_lr | 고정 또는 cyclic | 0.01~0.05 |
| 수집 주기 | 매 에폭 | 너무 빈번하면 상관 높음 |

> **핵심 직관**: SWA는 SGD 궤적의 여러 지점을 평균하여 flat region의 중심으로 이동하며, ot-06의 SAM과 상보적인 flat minima 탐색 전략입니다.

---

## 7. 손실 경관과 아키텍처의 관계

모델 아키텍처가 손실 경관의 구조를 근본적으로 결정합니다.

| 아키텍처 요소 | 경관 효과 | 최적화 영향 |
|-------------|---------|-----------|
| Skip connection | 경관 평활화 | 깊은 네트워크 학습 가능 |
| BatchNorm | 곡률 안정화 | 큰 학습률 허용 |
| Dropout | 경관 정규화 | flat minima 유도 |
| 과파라미터화 | 전역 최솟값 연결 | 극솟값 다수, 대부분 유사 성능 |

**과파라미터화의 역설:**

파라미터 수가 데이터 수를 크게 초과하면 오히려 최적화가 쉬워집니다. 모든 극솟값이 전역 최솟값에 가까워지며, ot-10에서 이 현상을 더 깊이 다룹니다.

> **핵심 직관**: 딥러닝의 최적화 성공은 알고리즘뿐 아니라 아키텍처 설계에 의해 좌우되며, skip connection과 normalization은 최적화를 위한 아키텍처 혁신입니다.

---

## 핵심 정리

- **고차원 비볼록 손실 함수에서 대부분의 정상점은 안장점이며, SGD의 확률적 특성이 자연스럽게 안장점을 탈출합니다**
- **Flat minima는 sharp minima보다 일반화 성능이 우수하며, 학습률/배치 크기의 비율이 도달하는 극솟값의 평탄도를 결정합니다**
- **독립적으로 학습된 극솟값들은 손실이 증가하지 않는 경로로 연결될 수 있으며(mode connectivity), 이는 모델 평균화의 이론적 근거입니다**
- **SWA는 학습 후반부 궤적의 평균을 통해 flat minima의 중심으로 이동하여 일반화를 개선합니다**
- **Skip connection, BatchNorm 등의 아키텍처 요소는 손실 경관을 평활화하여 최적화를 근본적으로 용이하게 만듭니다**
