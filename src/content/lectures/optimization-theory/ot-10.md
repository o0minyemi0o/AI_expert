# 최적화의 미해결 문제들 (Open Problems in Optimization)

## 왜 미해결 문제들을 알아야 하는가

ot-01~ot-09에서 다룬 기법들은 실전에서 강력하게 작동하지만, 그 이론적 토대에는 여전히 상당한 공백이 존재합니다. Adam의 수렴성, 일반화와 최적화의 관계, 스케일링 법칙의 기원 등은 현재 활발히 연구되는 최전선의 문제입니다. 이 문제들을 이해하면 현재 방법의 한계를 인식하고, 차세대 최적화 기법의 방향을 예측할 수 있습니다.

---

## 1. Adam의 수렴성 논쟁

ot-03에서 다루었듯, Reddi et al. (2018)은 Adam이 수렴하지 않는 반례를 제시했습니다.

**반례의 구조:**

$$f_t(\theta) = \begin{cases} C\theta & t \mod 3 = 1 \\ -\theta & \text{otherwise} \end{cases}$$

큰 $C$에서 Adam은 $\theta \to \infty$로 발산합니다. 2차 모멘트의 지수 이동 평균이 과거 큰 그래디언트를 "잊어버리기" 때문입니다.

| 주장 | 근거 | 반론 |
|------|------|------|
| Adam은 비수렴 | 이론적 반례 존재 | 반례가 비현실적 |
| AMSGrad가 해결 | $\hat{v}_t = \max(\hat{v}_{t-1}, v_t)$ | 실전에서 개선 미미 |
| 실전에서는 수렴 | 수천 개 실험에서 성공적 | 이론과 실전의 괴리 |

```python
# Adam 비수렴 반례 시연
def adam_counterexample():
    theta = torch.tensor([0.0], requires_grad=True)
    optimizer = torch.optim.Adam([theta], lr=0.01, betas=(0.9, 0.99))

    C = 1010.0
    for t in range(10000):
        optimizer.zero_grad()
        if t % 3 == 0:
            loss = C * theta
        else:
            loss = -theta
        loss.backward()
        optimizer.step()
    return theta.item()  # 발산할 수 있음
```

**현재 상태:** $\beta_2 < 1$인 Adam의 일반적 비볼록 수렴 증명은 최근 발표되었으나 (Défossez et al., 2022), 조건이 제한적이며 실전과의 간극이 남아있습니다.

> **핵심 직관**: Adam의 이론적 수렴성은 "해결된 문제"가 아니라 "조건부로 증명된 문제"이며, 실전에서 왜 잘 작동하는지에 대한 완전한 이해는 여전히 부족합니다.

---

## 2. 일반화-최적화 관계: The Generalization Puzzle

전통적 학습 이론에 따르면, 과파라미터화된 모델은 과적합해야 합니다. 그러나 실제로는 우수한 일반화를 보입니다.

**Double Descent 현상:**

$$\text{테스트 에러} = \begin{cases} \text{감소} & d < N \text{ (전통적 영역)} \\ \text{발산} & d \approx N \text{ (보간 임계점)} \\ \text{다시 감소} & d \gg N \text{ (과파라미터화 영역)} \end{cases}$$

| 전통적 관점 | 현대적 관점 |
|-----------|-----------|
| 복잡도 증가 → 과적합 | 복잡도 증가 → 이중 하강 |
| 정규화 필수 | 암묵적 정규화로 충분 |
| 전역 최솟값은 과적합 | 모든 전역 최솟값이 유사 성능 |

**SGD의 암묵적 편향 (Implicit Bias):**

$$\text{SGD가 수렴하는 해} \neq \text{임의의 전역 최솟값}$$

SGD는 가능한 해 중에서 "가장 단순한" 해를 암묵적으로 선호합니다. 선형 모델에서는 최소 노름 해로 수렴함이 증명되었습니다.

$$\theta_{\text{SGD}} \to \theta^* = \arg\min_\theta \|\theta\| \quad \text{s.t.} \quad L(\theta) = 0$$

> **핵심 직관**: 딥러닝의 일반화는 모델 자체의 복잡도보다 "최적화 알고리즘이 선택하는 해의 특성"에 더 크게 의존하며, 이는 전통적 학습 이론의 근본적 확장을 요구합니다.

---

## 3. Edge of Stability 현상

Cohen et al. (2021)은 경사 하강법이 안정성의 경계에서 학습한다는 놀라운 현상을 발견했습니다.

**관찰:**

$$\lambda_{\max}(H_t) \to \frac{2}{\eta} \quad \text{(sharpness가 } 2/\eta \text{로 수렴)}$$

헤시안의 최대 고유값이 $2/\eta$에 도달한 후, 이 값 주위를 진동합니다.

| 단계 | $\lambda_{\max}$ 거동 | loss 거동 |
|------|---------------------|----------|
| Progressive sharpening | 단조 증가 | 단조 감소 |
| Edge of Stability | $\approx 2/\eta$에서 진동 | 비단조 감소 |

```python
# Edge of Stability 관찰 실험
from torch.autograd.functional import hessian

eigenvalues_log = []
for step, (x, y) in enumerate(loader):
    optimizer.zero_grad()
    loss = criterion(model(x), y)
    loss.backward()
    optimizer.step()

    if step % 100 == 0:
        # 헤시안 최대 고유값 추정 (power iteration)
        max_eig = estimate_max_eigenvalue(model, x, y)
        eigenvalues_log.append(max_eig)
        print(f"Step {step}: max_eig = {max_eig:.2f}, "
              f"2/lr = {2/lr:.2f}")
```

**의미:** 기존 수렴 이론은 $\eta < 2/\lambda_{\max}$를 요구하지만, 실제 학습은 이 경계를 위반하면서도 수렴합니다.

> **핵심 직관**: Edge of Stability는 "GD가 스스로 곡률을 제어하며 학습한다"는 것을 보여주며, 기존의 부드러운(smooth) 최적화 이론으로는 설명되지 않는 새로운 현상입니다.

---

## 4. 스케일링 법칙과 최적 학습 설정

Kaplan et al. (2020), Hoffmann et al. (2022, Chinchilla)은 모델 크기, 데이터, 계산량 사이의 거듭제곱 법칙을 발견했습니다.

$$L(N, D) = \frac{A}{N^\alpha} + \frac{B}{D^\beta} + L_\infty$$

여기서 $N$은 파라미터 수, $D$는 학습 토큰 수입니다.

| 스케일링 | Kaplan (2020) | Chinchilla (2022) |
|---------|-------------|-------------------|
| 최적 비율 $N:D$ | $N$ 우선 | $N \propto D$ (균등) |
| $\alpha$ | ~0.076 | ~0.050 |
| $\beta$ | ~0.095 | ~0.095 |
| 실전 영향 | 큰 모델, 적은 데이터 | 적절한 모델, 많은 데이터 |

**최적화와의 연결:**

$$\text{최적 학습률} \propto N^{-\gamma}, \quad \text{최적 배치 크기} \propto L^{-\delta}$$

| 미해결 질문 | 현재 상태 |
|-----------|----------|
| 스케일링 법칙의 이론적 근거는? | 경험적 관찰, 부분적 설명만 |
| 최적화 하이퍼파라미터도 스케일링 법칙을 따르는가? | 예 (Yang et al., muP) |
| 스케일링 법칙은 영원히 지속되는가? | 불확실 — 데이터/작업 한계 |

> **핵심 직관**: 스케일링 법칙은 "얼마나 큰 모델을 얼마나 많은 데이터로 학습해야 최적인가"라는 질문에 정량적 답을 제공하지만, 왜 이 법칙이 성립하는지는 미해결입니다.

---

## 5. muP: 전이 가능한 하이퍼파라미터

Yang et al. (2022)의 Maximal Update Parametrization(muP)은 작은 모델에서 찾은 최적 하이퍼파라미터를 큰 모델에 직접 전이하는 방법입니다.

$$\text{표준 파라미터화: } \eta_{\text{opt}} \text{이 모델 폭 } d \text{에 의존}$$
$$\text{muP: } \eta_{\text{opt}} \text{이 모델 폭 } d \text{에 무관}$$

**핵심 스케일링 규칙:**

| 파라미터 | 표준 (SP) | muP |
|---------|----------|-----|
| 초기화 스케일 | $O(1/\sqrt{d})$ | $O(1/\sqrt{d})$ |
| 학습률 | 폭에 따라 재튜닝 | 폭 무관 (전이 가능) |
| 출력 레이어 LR | 동일 | $O(1/d)$ |
| Attention logit 스케일 | $1/\sqrt{d_k}$ | $1/d_k$ |

```python
# muP 개념: 레이어별 학습률 스케일링
for name, param in model.named_parameters():
    if 'output_proj' in name:
        param_group['lr'] = base_lr / d_model  # 출력 레이어
    elif 'attention' in name:
        param_group['lr'] = base_lr             # 어텐션
    else:
        param_group['lr'] = base_lr             # 기본
```

> **핵심 직관**: muP는 "작은 모델에서의 하이퍼파라미터 탐색 결과를 큰 모델에 무비용으로 전이"할 수 있게 하여, 대규모 모델의 하이퍼파라미터 탐색 비용을 획기적으로 절감합니다.

---

## 6. 차세대 옵티마이저 후보들

Adam을 넘어서려는 다양한 시도들이 진행 중입니다.

**Lion (Evolved Sign Descent):**

$$m_t = \beta_1 m_{t-1} + (1 - \beta_1) g_t$$
$$\theta_{t+1} = \theta_t - \eta \cdot \text{sign}(\beta_2 m_{t-1} + (1-\beta_2) g_t)$$

메모리를 절반으로 줄이면서 Adam과 동등한 성능을 달성합니다.

**Shampoo (Structured Preconditioner):**

ot-08의 K-FAC을 일반화한 구조화된 2차 방법으로, Google에서 대규모 실전에 적용하고 있습니다.

```python
# Lion 옵티마이저 (Google Brain, AutoML로 발견)
class Lion(torch.optim.Optimizer):
    def __init__(self, params, lr=1e-4, betas=(0.9, 0.99),
                 weight_decay=0.0):
        defaults = dict(lr=lr, betas=betas, weight_decay=weight_decay)
        super().__init__(params, defaults)

    @torch.no_grad()
    def step(self):
        for group in self.param_groups:
            for p in group['params']:
                if p.grad is None:
                    continue
                # Weight decay
                p.mul_(1 - group['lr'] * group['weight_decay'])
                # 모멘텀 방향의 부호만 사용
                grad = p.grad
                state = self.state[p]
                if 'exp_avg' not in state:
                    state['exp_avg'] = torch.zeros_like(p)
                m = state['exp_avg']
                beta1, beta2 = group['betas']
                update = m * beta1 + grad * (1 - beta1)
                p.add_(torch.sign(update), alpha=-group['lr'])
                m.mul_(beta2).add_(grad, alpha=1 - beta2)
```

| 옵티마이저 | 메모리 | 연산 | 성능 (LM) | 성숙도 |
|----------|--------|------|----------|-------|
| AdamW | $2d$ (m, v) | $O(d)$ | 기준 | 매우 높음 |
| Lion | $d$ (m만) | $O(d)$ | 동등~우수 | 중간 |
| Shampoo | $O(\sum d_l^{4/3})$ | $O(\sum d_l^2)$ | 우수 | 낮음 |
| Sophia | $2d$ + 주기적 헤시안 | $O(d)$ + 주기 비용 | 우수 | 낮음 |

> **핵심 직관**: 차세대 옵티마이저들은 "적은 메모리로 더 나은 곡률 정보 활용" 또는 "AutoML로 발견된 새로운 업데이트 규칙"이라는 두 방향에서 Adam에 도전하고 있습니다.

---

## 7. 열린 질문들과 연구 방향

딥러닝 최적화의 핵심 미해결 문제들을 정리합니다.

| 질문 | 현재 이해 수준 | 중요성 |
|------|-------------|-------|
| SGD의 암묵적 편향은 무엇인가? | 선형 모델만 완전 이해 | 극히 높음 |
| Adam이 SGD보다 나은 이유는? | 경험적 관찰, 부분적 이론 | 높음 |
| 배치 크기의 최적값은? | 경험적 규칙, 스케일링 법칙 | 높음 |
| 최적화와 일반화는 분리 가능한가? | 아니오 (암묵적 정규화 등) | 극히 높음 |
| Edge of Stability은 왜 발생하는가? | 현상 관찰, 불완전 이론 | 중간 |
| 스케일링 법칙의 이론적 기원은? | 미해결 | 극히 높음 |
| Transformer에 최적인 옵티마이저는? | AdamW가 최선이라는 보장 없음 | 높음 |

**연구 프런티어:**

1. **SDE 근사**: SGD를 확률 미분 방정식으로 모델링하여 연속 시간 분석
2. **신경 접선 커널 (NTK)**: 과파라미터화 극한에서의 학습 역학 분석
3. **특징 학습 이론**: NTK를 넘어서는 비선형 학습 역학
4. **토폴로지적 접근**: 손실 경관의 위상적 구조 분석

> **핵심 직관**: 딥러닝 최적화의 "이론이 실전을 따라잡지 못하는" 현 상황은 위기가 아니라 기회입니다 — 이 간극을 메우는 연구가 차세대 학습 패러다임을 열 것입니다.

---

## 핵심 정리

- **Adam의 수렴성은 특정 조건에서만 증명되었으며, 이론적 반례와 실전 성공 사이의 간극은 여전히 완전히 설명되지 않습니다**
- **과파라미터화된 딥러닝의 일반화는 SGD의 암묵적 편향(최소 노름 해 선호)에 크게 의존하며, 전통적 학습 이론의 근본적 확장을 요구합니다**
- **Edge of Stability 현상은 GD가 안정성 경계 $\lambda_{\max} \approx 2/\eta$에서 스스로 곡률을 조절하며 학습한다는 새로운 역학을 보여줍니다**
- **스케일링 법칙과 muP는 모델 크기에 따른 최적 학습 설정을 예측하지만, 그 이론적 기원은 미해결입니다**
- **Lion, Shampoo, Sophia 등 차세대 옵티마이저들이 Adam에 도전하고 있으며, 더 적은 메모리와 더 나은 곡률 활용이 핵심 방향입니다**
