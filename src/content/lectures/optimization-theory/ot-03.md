# 적응적 학습률: Adam 계열 (Adaptive Learning Rates: The Adam Family)

## 왜 적응적 학습률이 필요한가

ot-01에서 다룬 SGD는 모든 파라미터에 동일한 학습률을 적용합니다. 그러나 임베딩 레이어의 희소 그래디언트와 출력 레이어의 밀집 그래디언트는 전혀 다른 스케일을 가지므로, 파라미터별로 학습률을 자동 조절하는 적응적 방법이 필요합니다. Adam 계열은 현대 딥러닝, 특히 NLP와 생성 모델 학습의 사실상 표준입니다.

---

## 1. AdaGrad: 적응적 학습률의 시작

AdaGrad는 각 파라미터의 과거 그래디언트 제곱의 누적합으로 학습률을 스케일링합니다.

$$s_t = s_{t-1} + g_t \odot g_t$$
$$\theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{s_t} + \epsilon} \odot g_t$$

여기서 $\odot$은 원소별 곱이고, $\epsilon \approx 10^{-8}$은 수치 안정성을 위한 상수입니다.

| 장점 | 단점 |
|------|------|
| 희소 특징에 큰 학습률 자동 부여 | $s_t$가 단조 증가하여 학습률이 0으로 수렴 |
| 하이퍼파라미터 튜닝 적음 | 딥러닝의 비볼록 문제에서 조기 정지 |
| 볼록 문제에서 최적 regret bound | 장기 학습에 부적합 |

> **핵심 직관**: AdaGrad는 "자주 업데이트된 파라미터는 천천히, 드물게 업데이트된 파라미터는 빠르게"라는 원칙을 구현하지만, 누적 특성 때문에 딥러닝에서는 직접 사용되지 않습니다.

---

## 2. RMSProp: 누적 문제의 해결

Hinton이 제안한 RMSProp는 AdaGrad의 단조 증가 문제를 지수 이동 평균으로 해결합니다.

$$s_t = \rho \, s_{t-1} + (1 - \rho) \, g_t \odot g_t$$
$$\theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{s_t} + \epsilon} \odot g_t$$

$\rho = 0.99$가 일반적이며, 이는 약 100스텝의 윈도우로 과거 그래디언트 크기를 추적합니다.

```python
optimizer = torch.optim.RMSprop(
    model.parameters(),
    lr=0.001,
    alpha=0.99,     # rho에 해당
    eps=1e-8
)
```

| 하이퍼파라미터 | 역할 | 기본값 |
|--------------|------|-------|
| $\eta$ | 글로벌 학습률 | 0.001 |
| $\rho$ (alpha) | 2차 모멘트 감쇠율 | 0.99 |
| $\epsilon$ | 수치 안정성 | $10^{-8}$ |

> **핵심 직관**: RMSProp은 "최근의 그래디언트 크기"에 기반하여 학습률을 조절하므로, 비정상(non-stationary) 환경에서도 효과적으로 적응합니다.

---

## 3. Adam: 모멘텀 + 적응적 학습률의 결합

Adam(Adaptive Moment Estimation)은 ot-02의 모멘텀(1차 모멘트)과 RMSProp의 2차 모멘트를 결합합니다.

$$m_t = \beta_1 m_{t-1} + (1 - \beta_1) g_t \quad \text{(1차 모멘트)}$$
$$v_t = \beta_2 v_{t-1} + (1 - \beta_2) g_t^2 \quad \text{(2차 모멘트)}$$

**편향 보정 (Bias Correction):**

초기화 $m_0 = v_0 = 0$으로 인한 편향을 보정합니다.

$$\hat{m}_t = \frac{m_t}{1 - \beta_1^t}, \quad \hat{v}_t = \frac{v_t}{1 - \beta_2^t}$$

**업데이트 규칙:**

$$\theta_{t+1} = \theta_t - \eta \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}$$

```python
optimizer = torch.optim.Adam(
    model.parameters(),
    lr=1e-3,         # 기본 학습률
    betas=(0.9, 0.999),  # (beta1, beta2)
    eps=1e-8,
    weight_decay=0     # Adam에서 weight_decay 주의!
)
```

| 하이퍼파라미터 | 역할 | 기본값 | 민감도 |
|--------------|------|-------|-------|
| $\eta$ | 글로벌 스텝 크기 | $10^{-3}$ | 높음 |
| $\beta_1$ | 1차 모멘트 감쇠 | 0.9 | 낮음 |
| $\beta_2$ | 2차 모멘트 감쇠 | 0.999 | 중간 |
| $\epsilon$ | 수치 안정성 | $10^{-8}$ | 낮음 |

> **핵심 직관**: Adam의 업데이트 크기는 대략 $\eta$ 수준으로 "정규화"됩니다 — $\hat{m}_t / \sqrt{\hat{v}_t}$는 대략 $\pm 1$ 범위의 부호 정보에 가깝습니다.

---

## 4. AdamW: 올바른 Weight Decay

원래 Adam에서 L2 정규화(weight decay)를 적용하면 그래디언트에 $\lambda\theta$를 더합니다.

$$g_t' = g_t + \lambda \theta_t \quad \text{(L2 regularization)}$$

이 방식은 적응적 학습률과 상호작용하여 weight decay의 효과가 왜곡됩니다.

**AdamW의 해결책: Decoupled Weight Decay**

$$\theta_{t+1} = (1 - \eta\lambda)\theta_t - \eta \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}$$

Weight decay를 그래디언트가 아닌 파라미터에 직접 적용합니다.

```python
# 올바른 방법: AdamW 사용
optimizer = torch.optim.AdamW(
    model.parameters(),
    lr=1e-3,
    weight_decay=0.01   # decoupled weight decay
)

# 잘못된 방법: Adam + L2
optimizer = torch.optim.Adam(
    model.parameters(),
    lr=1e-3,
    weight_decay=0.01   # 적응적 학습률과 간섭!
)
```

| 방법 | Weight Decay 적용 | 적응적 학습률과의 상호작용 |
|------|-------------------|----------------------|
| Adam + L2 | $g_t + \lambda\theta$ | 왜곡됨 |
| AdamW | $(1-\eta\lambda)\theta$ | 독립적 |

GPT, BERT, ViT 등 거의 모든 Transformer 학습에서 AdamW가 표준으로 사용됩니다.

> **핵심 직관**: AdamW는 "학습"과 "정규화"를 분리하여 각각이 의도대로 작동하게 만듭니다 — Transformer 학습의 사실상 필수 옵티마이저입니다.

---

## 5. Adam의 문제점과 변형들

Adam은 특정 조건에서 수렴 실패가 이론적으로 증명되었습니다(Reddi et al., 2018).

**AMSGrad:**

$v_t$의 최대값을 유지하여 수렴을 보장합니다.

$$\hat{v}_t = \max(\hat{v}_{t-1}, v_t)$$

**RAdam (Rectified Adam):**

학습 초기에 2차 모멘트 추정이 불안정한 문제를 분산 보정으로 해결합니다.

```python
# RAdam: 초기 학습의 불안정성 해결
optimizer = torch.optim.RAdam(
    model.parameters(),
    lr=1e-3,
    betas=(0.9, 0.999)
)
```

| 변형 | 해결하는 문제 | 추가 비용 |
|------|-------------|----------|
| AMSGrad | 비수렴 반례 | $v_t$ 최대값 저장 |
| RAdam | 초기 학습 불안정 | 분산 추정 계산 |
| NAdam | Nesterov 모멘텀 결합 | 없음 |
| AdaBound | SGD로 점진적 전환 | 학습률 클리핑 |

> **핵심 직관**: Adam의 이론적 문제점들이 실전에서 얼마나 심각한지는 여전히 논쟁 중이며, ot-10에서 이 미해결 문제를 상세히 다룹니다.

---

## 6. 실전 옵티마이저 선택 가이드

```python
# 컴퓨터 비전 (CNN): SGD + Momentum이 여전히 강력
optimizer = torch.optim.SGD(model.parameters(), lr=0.1,
                            momentum=0.9, weight_decay=1e-4)

# NLP / Transformer: AdamW가 표준
optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4,
                              betas=(0.9, 0.95), weight_decay=0.1)

# 생성 모델 (GAN): 불안정한 학습에는 보수적 설정
optimizer_G = torch.optim.Adam(G.parameters(), lr=2e-4,
                               betas=(0.5, 0.999))
optimizer_D = torch.optim.Adam(D.parameters(), lr=2e-4,
                               betas=(0.5, 0.999))
```

| 도메인 | 권장 옵티마이저 | $\eta$ 범위 | $\beta$ 설정 |
|--------|--------------|------------|-------------|
| CNN (ResNet 등) | SGD + Momentum | 0.01~0.1 | $\beta=0.9$ |
| Transformer (BERT, GPT) | AdamW | $10^{-5}$~$10^{-3}$ | $(0.9, 0.95)$~$(0.9, 0.999)$ |
| GAN | Adam | $10^{-4}$~$3\times10^{-4}$ | $(0.0, 0.9)$~$(0.5, 0.999)$ |
| 파인튜닝 | AdamW (낮은 lr) | $10^{-6}$~$10^{-4}$ | $(0.9, 0.999)$ |

> **핵심 직관**: 옵티마이저 선택은 문제 유형에 따라 달라지며, "하나의 최고 옵티마이저"는 존재하지 않습니다 — ot-04의 학습률 스케줄링과 결합해야 완성됩니다.

---

## 핵심 정리

- **AdaGrad는 파라미터별 적응적 학습률의 선구자이지만, 누적 특성으로 학습률이 조기에 소멸하는 문제가 있습니다**
- **RMSProp는 지수 이동 평균으로 AdaGrad의 단조 감소 문제를 해결하여 비정상 환경에서도 적응적으로 동작합니다**
- **Adam은 1차 모멘트(모멘텀)와 2차 모멘트(RMSProp)를 결합하고, 편향 보정으로 초기 단계의 편향을 제거합니다**
- **AdamW는 weight decay를 그래디언트에서 분리(decouple)하여 Transformer 학습의 표준 옵티마이저로 자리잡았습니다**
- **Adam의 이론적 수렴 문제를 해결하려는 AMSGrad, RAdam 등의 변형이 존재하지만, 실전에서의 영향은 제한적입니다**
