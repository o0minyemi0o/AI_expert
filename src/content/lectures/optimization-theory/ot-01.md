# 확률적 경사 하강법 (Stochastic Gradient Descent)

## 왜 확률적 경사 하강법을 이해해야 하는가

딥러닝의 모든 학습은 결국 SGD의 변형입니다. 수백억 파라미터 모델도 근본적으로 SGD의 수렴 성질에 의존하며, 미니배치 크기와 학습률의 선택이 학습 속도와 일반화 성능을 결정합니다. SGD의 이론적 토대를 이해하면 ot-02~ot-10에서 다룰 모든 고급 최적화 기법의 근거를 파악할 수 있습니다.

---

## 1. 경험적 위험 최소화와 경사 하강법

딥러닝의 학습 목표는 경험적 위험(empirical risk)을 최소화하는 것입니다.

$$\min_{\theta} \; L(\theta) = \frac{1}{N} \sum_{i=1}^{N} \ell(f_\theta(x_i), y_i)$$

전체 데이터셋에 대한 그래디언트를 매번 계산하면 정확하지만, $N$이 수백만 이상일 때 비현실적입니다.

| 방법 | 배치 크기 | 그래디언트 분산 | 계산 비용 (per step) |
|------|-----------|----------------|---------------------|
| GD (Full Batch) | $N$ | 0 | $O(N)$ |
| SGD | 1 | 최대 | $O(1)$ |
| Mini-batch SGD | $B$ | 중간 | $O(B)$ |

> **핵심 직관**: SGD는 정확한 그래디언트 대신 노이즈가 섞인 추정치를 사용하지만, 기대값이 올바른 방향을 가리키므로 수렴합니다.

---

## 2. SGD의 수렴 분석

SGD의 업데이트 규칙은 다음과 같습니다.

$$\theta_{t+1} = \theta_t - \eta_t \, g_t, \quad g_t = \nabla \ell(f_{\theta_t}(x_{i_t}), y_{i_t})$$

여기서 $g_t$는 확률적 그래디언트이며, 비편향 추정치입니다: $\mathbb{E}[g_t] = \nabla L(\theta_t)$.

**볼록 함수에서의 수렴 속도:**

- $L$-smooth, 강볼록 ($\mu$-strongly convex): $O(1/T)$ 수렴
- $L$-smooth, 볼록: $O(1/\sqrt{T})$ 수렴

**비볼록 함수에서의 수렴** (딥러닝의 실제 경우):

$$\frac{1}{T} \sum_{t=0}^{T-1} \mathbb{E}\left[\|\nabla L(\theta_t)\|^2\right] \leq O\left(\frac{1}{\sqrt{T}}\right)$$

이는 평균적으로 그래디언트 노름이 0에 수렴함을 의미합니다.

> **핵심 직관**: 비볼록 환경에서 SGD는 전역 최솟값이 아닌 정상점(stationary point) 수렴만 보장하지만, ot-05에서 다루듯 딥러닝에서는 이것만으로도 충분합니다.

---

## 3. 미니배치와 분산 감소

미니배치 크기 $B$는 그래디언트 추정의 분산을 제어합니다.

$$\text{Var}(g_t^{(B)}) = \frac{\sigma^2}{B}$$

여기서 $\sigma^2$는 개별 샘플 그래디언트의 분산입니다.

```python
import torch
import torch.nn as nn

model = nn.Linear(784, 10)
optimizer = torch.optim.SGD(model.parameters(), lr=0.01)
loader = torch.utils.data.DataLoader(dataset, batch_size=64, shuffle=True)

for epoch in range(100):
    for x_batch, y_batch in loader:
        optimizer.zero_grad()
        loss = nn.functional.cross_entropy(model(x_batch), y_batch)
        loss.backward()
        optimizer.step()
```

| 배치 크기 $B$ | 분산 감소 | GPU 활용도 | 일반화 성능 |
|--------------|----------|-----------|-----------|
| 32 | 낮음 | 낮음 | 우수 |
| 256 | 중간 | 중간 | 양호 |
| 8192 | 높음 | 높음 | 주의 필요 |

ResNet-50 학습에서 배치 크기를 256에서 8192로 늘리면 학습 속도는 빨라지지만, ot-09에서 다루는 linear scaling rule 없이는 일반화 성능이 급격히 하락합니다.

> **핵심 직관**: 배치 크기를 $k$배 늘리면 분산이 $k$배 줄지만, 일반화에 도움이 되는 "건강한 노이즈"도 함께 줄어듭니다.

---

## 4. 학습률의 역할과 선택

학습률 $\eta$는 SGD에서 가장 중요한 하이퍼파라미터입니다.

**이론적 조건:**

수렴을 보장하려면 Robbins-Monro 조건을 만족해야 합니다.

$$\sum_{t=1}^{\infty} \eta_t = \infty, \quad \sum_{t=1}^{\infty} \eta_t^2 < \infty$$

예를 들어 $\eta_t = \frac{c}{\sqrt{t}}$는 이 조건을 만족합니다.

**실전에서의 학습률 선택:**

```python
# 학습률 탐색 (Learning Rate Range Test)
lrs = torch.logspace(-5, 0, steps=100)
losses = []
for lr in lrs:
    optimizer = torch.optim.SGD(model.parameters(), lr=lr.item())
    # 짧은 학습 후 loss 기록
    loss = train_one_epoch(model, optimizer, loader)
    losses.append(loss)
# loss가 급감하기 시작하는 지점의 lr 선택
```

> **핵심 직관**: 학습률이 너무 크면 발산하고, 너무 작으면 수렴이 느립니다. ot-04에서 다루는 스케줄링은 이 딜레마를 해소합니다.

---

## 5. SGD의 암묵적 정규화

SGD의 노이즈가 단순한 약점이 아니라 일반화를 돕는 정규화 효과를 제공한다는 것이 현대 최적화 이론의 핵심 발견입니다.

$$\theta_{t+1} = \theta_t - \eta \nabla L(\theta_t) - \eta (\underbrace{g_t - \nabla L(\theta_t)}_{\text{gradient noise}})$$

이 노이즈의 공분산은 다음과 같습니다.

$$C(\theta) = \frac{1}{B} \left(\frac{1}{N}\sum_{i=1}^{N} \nabla \ell_i \nabla \ell_i^\top - \nabla L \nabla L^\top\right)$$

| 관점 | 효과 | 관련 강의 |
|------|------|----------|
| 탈출 능력 | sharp minima에서 탈출 용이 | ot-05 |
| 암묵적 편향 | flat minima 선호 | ot-06 |
| SDE 근사 | 연속 시간 확산 과정으로 모델링 | ot-10 |

GPT 학습에서 SGD 노이즈는 loss landscape의 날카로운 골짜기를 피하고 평탄한 영역으로 이동하도록 유도합니다.

> **핵심 직관**: SGD의 노이즈는 버그가 아니라 피처입니다 — 학습률과 배치 크기의 비율 $\eta/B$가 노이즈 강도를 결정합니다.

---

## 6. 분산 감소 기법

순수 SGD의 분산 문제를 해결하는 고전적 방법들이 존재합니다.

**SVRG (Stochastic Variance Reduced Gradient):**

$$g_t^{\text{SVRG}} = \nabla \ell_{i_t}(\theta_t) - \nabla \ell_{i_t}(\tilde{\theta}) + \nabla L(\tilde{\theta})$$

주기적으로 전체 그래디언트 $\nabla L(\tilde{\theta})$를 계산하여 분산을 제어합니다.

**SARAH / STORM:**

최근의 분산 감소 기법들은 전체 그래디언트 계산 없이도 분산을 줄입니다.

```python
# SVRG 개념 구현
theta_tilde = model_snapshot.parameters()   # 주기적 스냅샷
full_grad = compute_full_gradient(theta_tilde, dataset)

for x_i, y_i in sampler:
    grad_current = compute_grad(theta, x_i, y_i)
    grad_snapshot = compute_grad(theta_tilde, x_i, y_i)
    # 분산 감소된 그래디언트
    g_svrg = grad_current - grad_snapshot + full_grad
    theta = theta - lr * g_svrg
```

> **핵심 직관**: 분산 감소 기법은 이론적으로 우수하지만, 딥러닝 실전에서는 Adam 계열(ot-03)이 더 널리 사용됩니다.

---

## 7. 실전 SGD 레시피

현대 딥러닝에서 SGD는 여전히 강력한 옵티마이저입니다.

```python
optimizer = torch.optim.SGD(
    model.parameters(),
    lr=0.1,           # 초기 학습률
    momentum=0.9,     # ot-02에서 상세 설명
    weight_decay=1e-4, # L2 정규화
    nesterov=True      # Nesterov 모멘텀
)

scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
    optimizer, T_max=200  # ot-04에서 상세 설명
)
```

| 모델 | 옵티마이저 | 학습률 | 배치 크기 |
|------|----------|--------|----------|
| ResNet-50 | SGD + Momentum | 0.1 | 256 |
| ViT-Base | AdamW (ot-03) | 1e-3 | 4096 |
| GPT-3 | Adam (ot-03) | 6e-4 | ~3.2M tokens |

> **핵심 직관**: 컴퓨터 비전에서 SGD + Momentum은 여전히 Adam과 동등하거나 우수한 최종 성능을 달성하며, NLP에서는 Adam 계열이 표준입니다.

---

## 핵심 정리

- **SGD는 전체 그래디언트의 비편향 추정치를 사용하여 $O(1/\sqrt{T})$의 속도로 정상점에 수렴합니다**
- **미니배치 크기 $B$는 그래디언트 분산을 $\sigma^2/B$로 줄이지만, 너무 큰 배치는 일반화를 해칠 수 있습니다**
- **학습률은 Robbins-Monro 조건을 만족해야 수렴이 보장되며, 실전에서는 스케줄링(ot-04)과 결합합니다**
- **SGD의 그래디언트 노이즈는 암묵적 정규화로 작용하여 flat minima를 선호하게 만듭니다**
- **분산 감소 기법(SVRG 등)은 이론적으로 우수하지만, 딥러닝 실전에서는 적응적 방법(ot-03)이 더 보편적입니다**
