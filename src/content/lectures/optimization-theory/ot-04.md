# 학습률 스케줄링 (Learning Rate Scheduling)

## 왜 학습률을 스케줄링해야 하는가

ot-01에서 Robbins-Monro 조건이 요구하듯, 고정 학습률로는 SGD의 수렴 성질이 최적이 아닙니다. 학습 초기에는 큰 학습률로 빠르게 탐색하고, 후반에는 작은 학습률로 정밀하게 수렴해야 합니다. 학습률 스케줄링은 ot-03의 옵티마이저와 결합하여 학습 성능을 극적으로 향상시킵니다.

---

## 1. Step Decay (계단식 감쇠)

가장 고전적이고 직관적인 방법으로, 일정 에폭마다 학습률을 고정 비율로 줄입니다.

$$\eta_t = \eta_0 \cdot \gamma^{\lfloor t / S \rfloor}$$

여기서 $\gamma$는 감쇠율(보통 0.1), $S$는 감쇠 주기(에폭 단위)입니다.

```python
scheduler = torch.optim.lr_scheduler.StepLR(
    optimizer, step_size=30, gamma=0.1
)

# 또는 특정 에폭에서 감쇠
scheduler = torch.optim.lr_scheduler.MultiStepLR(
    optimizer, milestones=[60, 120, 160], gamma=0.2
)
```

| 하이퍼파라미터 | 의미 | 일반적 값 |
|--------------|------|----------|
| $\eta_0$ | 초기 학습률 | 0.1 (SGD) |
| $\gamma$ | 감쇠 비율 | 0.1 ~ 0.2 |
| milestones | 감쇠 시점 | [60, 120, 160] / 200 에폭 |

ResNet 원 논문에서 ImageNet 학습 시 에폭 30, 60, 90에서 학습률을 10분의 1로 감소시키는 것이 표준 레시피였습니다.

> **핵심 직관**: Step decay는 구현이 간단하지만, 최적의 감쇠 시점과 비율을 찾기 위한 추가 탐색 비용이 발생합니다.

---

## 2. Cosine Annealing

학습률을 코사인 함수를 따라 부드럽게 감쇠시킵니다.

$$\eta_t = \eta_{\min} + \frac{1}{2}(\eta_{\max} - \eta_{\min})\left(1 + \cos\left(\frac{t}{T}\pi\right)\right)$$

```python
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
    optimizer, T_max=200, eta_min=0
)

# Warm restart 변형 (SGDR)
scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(
    optimizer, T_0=10, T_mult=2, eta_min=1e-6
)
```

**Cosine vs. Step Decay 비교:**

| 특성 | Step Decay | Cosine Annealing |
|------|-----------|-----------------|
| 전환 | 급격한 점프 | 부드러운 전이 |
| 하이퍼파라미터 | milestone 선택 필요 | $T_{\max}$만 결정 |
| 학습 후반 | 플래토 | 지속적 감쇠 |
| 성능 | 좋음 | 0.3~0.5%p 더 나음 (일반적) |

ResNet-50에서 cosine annealing은 step decay 대비 top-1 정확도 약 0.5%p 개선을 보여줍니다.

> **핵심 직관**: Cosine annealing은 하이퍼파라미터가 적으면서도 학습 전체에 걸쳐 부드러운 탐색-수렴 전이를 제공합니다.

---

## 3. Linear Warmup

학습 초기 몇 스텝 동안 학습률을 0에서 목표값까지 선형으로 증가시킵니다.

$$\eta_t = \eta_{\max} \cdot \frac{t}{T_{\text{warmup}}}, \quad t < T_{\text{warmup}}$$

**왜 warmup이 필요한가:**

1. Adam의 2차 모멘트 $v_t$가 초기에 부정확 (ot-03 참조)
2. BatchNorm 통계가 초기에 불안정
3. 큰 초기 그래디언트로 인한 파라미터 발산 방지

```python
# warmup + cosine decay 결합 (Transformer 표준)
def lr_lambda(step):
    warmup_steps = 4000
    if step < warmup_steps:
        return step / warmup_steps
    # cosine decay
    progress = (step - warmup_steps) / (total_steps - warmup_steps)
    return 0.5 * (1 + math.cos(math.pi * progress))

scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)
```

| 모델 | Warmup 스텝 | 총 학습 스텝 | Warmup 비율 |
|------|-----------|-----------|-----------|
| BERT-Base | 10,000 | 1,000,000 | 1% |
| GPT-3 | 375 | 300B tokens | ~0.1% |
| ViT-Large | 10,000 | 300,000 | 3.3% |

GPT 학습에서 warmup 없이 시작하면 초기 loss spike로 학습이 불안정해지거나 발산합니다.

> **핵심 직관**: Warmup은 옵티마이저 상태와 모델 통계가 안정화될 시간을 확보하며, 대규모 모델일수록 중요성이 커집니다.

---

## 4. OneCycleLR (Super-Convergence)

Leslie Smith이 제안한 정책으로, 학습률을 한 번 올렸다가 내리는 사이클을 사용합니다.

**3단계 구조:**

1. **Warmup**: $\eta_{\min} \to \eta_{\max}$ (전체의 30~45%)
2. **Annealing**: $\eta_{\max} \to \eta_{\min}$ (나머지 대부분)
3. **Annihilation**: $\eta_{\min} \to \eta_{\min}/100$ (마지막 5%)

```python
scheduler = torch.optim.lr_scheduler.OneCycleLR(
    optimizer,
    max_lr=0.01,
    total_steps=total_steps,
    pct_start=0.3,       # warmup 비율
    anneal_strategy='cos', # 감쇠 전략
    div_factor=25,        # 초기 lr = max_lr / 25
    final_div_factor=1e4  # 최종 lr = max_lr / 10000
)
```

| 하이퍼파라미터 | 역할 | 기본값 |
|--------------|------|-------|
| max_lr | 최대 학습률 | LR range test로 결정 |
| pct_start | warmup 비율 | 0.3 |
| div_factor | 초기 감쇠 비율 | 25 |
| final_div_factor | 최종 감쇠 비율 | $10^4$ |

**Super-convergence** 현상: OneCycleLR은 일반 스케줄 대비 같은 정확도를 5~10배 적은 에폭으로 달성할 수 있습니다.

> **핵심 직관**: OneCycleLR은 높은 학습률 구간에서 정규화 효과를 얻고(ot-01의 암묵적 정규화와 연관), 후반부에서 정밀 수렴합니다.

---

## 5. Warmup + Cosine: Transformer의 표준 레시피

현대 대규모 모델 학습의 사실상 표준 스케줄입니다.

$$\eta_t = \begin{cases} \eta_{\max} \cdot \frac{t}{T_w} & t < T_w \\[6pt] \eta_{\min} + \frac{1}{2}(\eta_{\max} - \eta_{\min})\left(1 + \cos\left(\frac{t - T_w}{T - T_w}\pi\right)\right) & t \geq T_w \end{cases}$$

```python
# 완전한 Transformer 학습 스케줄러
from torch.optim.lr_scheduler import SequentialLR, LinearLR, CosineAnnealingLR

warmup = LinearLR(optimizer, start_factor=1e-7/1e-3,
                  end_factor=1.0, total_iters=warmup_steps)
cosine = CosineAnnealingLR(optimizer, T_max=total_steps - warmup_steps,
                           eta_min=1e-6)
scheduler = SequentialLR(optimizer, [warmup, cosine],
                         milestones=[warmup_steps])
```

| 모델 | $\eta_{\max}$ | $\eta_{\min}$ | Warmup | 총 스텝 |
|------|-------------|-------------|--------|---------|
| BERT-Base | $10^{-4}$ | $10^{-7}$ | 10K | 1M |
| GPT-2 | $2.5\times10^{-4}$ | $10^{-6}$ | 2K | 250K |
| LLaMA-2 70B | $1.5\times10^{-4}$ | $1.5\times10^{-5}$ | 2K | ~2T tokens |

> **핵심 직관**: Warmup + cosine은 학습 초기 안정성(warmup)과 후반부 부드러운 수렴(cosine)을 모두 확보하여, Transformer 학습의 "기본 레시피"로 자리잡았습니다.

---

## 6. 학습률 탐색과 진단

최적 학습률을 찾는 체계적 방법들이 존재합니다.

**LR Range Test (Smith, 2017):**

```python
# 학습률을 지수적으로 증가시키며 loss 관찰
from torch_lr_finder import LRFinder

finder = LRFinder(model, optimizer, criterion)
finder.range_test(loader, start_lr=1e-7, end_lr=10, num_iter=100)
finder.plot()  # loss가 급감하기 시작하는 지점의 lr 선택
```

**진단 신호:**

| 증상 | 원인 | 해결책 |
|------|------|-------|
| Loss 진동/발산 | 학습률 과대 | $\eta$ 감소 또는 warmup 추가 |
| Loss 감소 매우 느림 | 학습률 과소 | $\eta$ 증가 |
| Loss plateau 후 정체 | 감쇠 시점 부적절 | 스케줄 조정 |
| 초기 loss spike | warmup 부재 | warmup 추가 |

> **핵심 직관**: LR range test는 최적 학습률 범위를 체계적으로 탐색하는 저비용 방법이며, 모든 스케줄링 전략의 출발점입니다.

---

## 7. 스케줄링과 배치 크기의 관계

학습률과 배치 크기는 독립적이지 않습니다.

**Linear Scaling Rule** (ot-09에서 상세 설명):

$$\eta_B = \eta_0 \cdot \frac{B}{B_0}$$

배치 크기를 $k$배 늘리면 학습률도 $k$배 늘려야 같은 학습 역학을 유지합니다.

| 배치 크기 | 학습률 | Warmup 에폭 | 참조 |
|----------|--------|-----------|------|
| 256 | 0.1 | 0 | 기준 |
| 8192 | 3.2 | 5 | Linear scaling |
| 32768 | LARS/LAMB | 5~10 | ot-09 참조 |

> **핵심 직관**: 배치 크기 변경 시 학습률과 warmup도 함께 조정해야 하며, ot-09의 LARS/LAMB은 이 문제를 파라미터별로 자동 처리합니다.

---

## 핵심 정리

- **Step decay는 직관적이지만 최적 감쇠 시점 탐색이 필요하며, cosine annealing은 하이퍼파라미터가 적으면서 일관되게 우수한 성능을 제공합니다**
- **Linear warmup은 옵티마이저 상태와 모델 통계의 초기 불안정성을 해소하며, Transformer 학습에서 필수적입니다**
- **OneCycleLR은 super-convergence를 통해 학습 에폭을 대폭 줄이면서 동등한 성능을 달성할 수 있습니다**
- **Warmup + cosine decay는 현대 대규모 모델 학습의 사실상 표준 스케줄이며, LLaMA, GPT 등에서 공통으로 사용됩니다**
- **LR range test를 통해 최적 학습률 범위를 체계적으로 탐색하고, 배치 크기 변경 시 linear scaling rule을 적용해야 합니다**
