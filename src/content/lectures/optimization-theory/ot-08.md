# 이차 최적화와 자연 경사법 (Second-Order Optimization and Natural Gradient)

## 왜 이차 정보를 활용해야 하는가

ot-01~ot-03의 모든 방법은 1차 정보(그래디언트)만 사용합니다. 그러나 손실 함수의 곡률(curvature) 정보를 활용하면 조건수에 무관한 수렴이 가능합니다. 뉴턴 방법은 이론적으로 가장 빠르지만, 헤시안 계산 비용이 문제입니다. 자연 경사법과 K-FAC은 이 비용을 실용적 수준으로 줄이는 핵심 아이디어를 제공합니다.

---

## 1. 뉴턴 방법 (Newton's Method)

손실 함수를 2차 테일러 전개로 근사하고, 이 근사를 정확히 최소화합니다.

$$L(\theta + \Delta\theta) \approx L(\theta) + \nabla L^\top \Delta\theta + \frac{1}{2} \Delta\theta^\top H \Delta\theta$$

최적 업데이트:

$$\Delta\theta^* = -H^{-1} \nabla L(\theta)$$

여기서 $H = \nabla^2 L(\theta)$는 헤시안 행렬입니다.

| 특성 | 경사 하강법 | 뉴턴 방법 |
|------|-----------|----------|
| 수렴 속도 | 선형 $O(\kappa \log(1/\epsilon))$ | 이차 $O(\log\log(1/\epsilon))$ |
| 스텝당 비용 | $O(d)$ | $O(d^3)$ 또는 $O(d^2)$ |
| 조건수 민감도 | 높음 | 없음 (이상적) |
| 메모리 | $O(d)$ | $O(d^2)$ |

**딥러닝에서의 문제:** 파라미터 수 $d$가 수억일 때, $d \times d$ 헤시안은 저장조차 불가능합니다.

> **핵심 직관**: 뉴턴 방법은 "그래디언트뿐 아니라 곡률까지 고려하여 최적의 스텝 크기와 방향을 결정"하지만, 헤시안의 $O(d^2)$ 비용이 딥러닝에서의 직접 적용을 차단합니다.

---

## 2. 준뉴턴 방법: L-BFGS

전체 헤시안 대신 그래디언트 변화량으로 역헤시안을 근사합니다.

$$H_t^{-1} \approx \text{L-BFGS}(\{s_k, y_k\}_{k=t-m}^{t-1})$$

여기서 $s_k = \theta_{k+1} - \theta_k$, $y_k = g_{k+1} - g_k$이고, $m$은 메모리 크기입니다.

```python
optimizer = torch.optim.LBFGS(
    model.parameters(),
    lr=1.0,
    max_iter=20,       # 라인 서치 최대 반복
    history_size=10,   # 메모리 크기 m
    line_search_fn='strong_wolfe'  # nm-09에서 다룬 line search
)

def closure():
    optimizer.zero_grad()
    loss = criterion(model(x), y)
    loss.backward()
    return loss

optimizer.step(closure)
```

| 하이퍼파라미터 | 역할 | 기본값 |
|--------------|------|-------|
| history_size | 역헤시안 근사에 사용할 과거 정보 수 | 10~20 |
| max_iter | 라인 서치 반복 | 20 |
| line_search_fn | 라인 서치 전략 | strong_wolfe |

**한계:** 미니배치에서의 노이즈에 취약하며, 분산 학습에 비적합합니다.

> **핵심 직관**: L-BFGS는 소규모 문제나 파인튜닝에서 매우 빠르게 수렴하지만, 대규모 확률적 학습에는 적합하지 않습니다.

---

## 3. 자연 경사법 (Natural Gradient)

파라미터 공간이 아닌 **함수(분포) 공간**에서의 가장 가파른 방향을 따릅니다.

**KL 발산 기반 정의:**

$$\tilde{\nabla} L(\theta) = F^{-1} \nabla L(\theta)$$

여기서 $F$는 피셔 정보 행렬(Fisher Information Matrix)입니다.

$$F = \mathbb{E}_{x \sim p_\text{data}}\left[\nabla \log p(y|x;\theta) \, \nabla \log p(y|x;\theta)^\top\right]$$

**자연 경사의 의미:**

$$\theta_{t+1} = \arg\min_{\theta'} \left\{ L(\theta') \;\middle|\; D_{\text{KL}}(p_{\theta_t} \| p_{\theta'}) \leq \epsilon \right\}$$

이는 모델 출력 분포의 변화를 $\epsilon$ 이하로 제한하면서 손실을 최대한 줄이는 업데이트입니다.

| 비교 | 경사 하강법 | 자연 경사법 |
|------|-----------|-----------|
| 거리 측도 | 유클리드 $\|\Delta\theta\|^2$ | KL 발산 $D_{\text{KL}}$ |
| 파라미터화 불변 | 아니오 | 예 |
| 전곡행렬 | $I$ (항등) | $F$ (피셔) |
| ot-03 Adam과의 관계 | 대각 근사 | 이론적 기반 |

> **핵심 직관**: 자연 경사법은 "파라미터가 어떻게 바뀌었느냐"가 아니라 "모델이 어떻게 바뀌었느냐"를 기준으로 최적화하므로, 파라미터화 방식에 무관한 본질적 업데이트를 제공합니다.

---

## 4. 피셔 정보 행렬과 Adam의 관계

ot-03에서 다룬 Adam의 2차 모멘트는 피셔 정보 행렬의 대각 근사로 해석할 수 있습니다.

$$v_t = \beta_2 v_{t-1} + (1-\beta_2)g_t^2 \approx \text{diag}(\hat{F})$$

| 근사 수준 | 방법 | 비용 | 정확도 |
|----------|------|------|-------|
| 스칼라 | SGD (글로벌 학습률) | $O(1)$ | 낮음 |
| 대각 | Adam, AdaGrad | $O(d)$ | 중간 |
| 블록 대각 | K-FAC | $O(d \cdot b)$ | 높음 |
| 전체 | 자연 경사 | $O(d^2)$ | 최고 |

```python
# Adam이 자연 경사의 대각 근사임을 보여주는 해석
# Adam 업데이트: theta -= lr * m_hat / sqrt(v_hat)
# 자연 경사: theta -= lr * F_inv @ grad
# F ≈ diag(v_hat) 일 때: F_inv @ grad ≈ grad / sqrt(v_hat)
```

> **핵심 직관**: Adam은 자연 경사법의 대각 근사로 이해할 수 있으며, 이것이 Adam이 파라미터 스케일에 강건한 이유를 설명합니다.

---

## 5. K-FAC: 크로네커 인수분해 근사

K-FAC(Kronecker-Factored Approximate Curvature)은 피셔 행렬을 레이어별로 크로네커 곱으로 근사합니다.

**완전 연결 레이어 $y = Wx + b$의 경우:**

$$F_W \approx (aa^\top) \otimes (gg^\top) = A \otimes G$$

여기서 $a$는 입력 활성화, $g$는 출력 그래디언트입니다.

**역행렬 계산 효율:**

$$(A \otimes G)^{-1} = A^{-1} \otimes G^{-1}$$

$d_\text{in} \times d_\text{in}$과 $d_\text{out} \times d_\text{out}$ 행렬의 역행렬만 필요합니다.

```python
# K-FAC 개념적 구현 (단일 레이어)
class KFACLayer:
    def __init__(self, damping=1e-3):
        self.damping = damping
        self.A = None  # 입력 공분산
        self.G = None  # 그래디언트 공분산

    def update_stats(self, a_input, g_output):
        # 지수 이동 평균으로 통계 업데이트
        A_new = a_input.T @ a_input / a_input.shape[0]
        G_new = g_output.T @ g_output / g_output.shape[0]
        if self.A is None:
            self.A, self.G = A_new, G_new
        else:
            self.A = 0.95 * self.A + 0.05 * A_new
            self.G = 0.95 * self.G + 0.05 * G_new

    def precondition(self, grad):
        # (A + lambda*I)^{-1} @ grad @ (G + lambda*I)^{-1}
        A_inv = torch.linalg.inv(self.A + self.damping * torch.eye(self.A.shape[0]))
        G_inv = torch.linalg.inv(self.G + self.damping * torch.eye(self.G.shape[0]))
        return A_inv @ grad @ G_inv
```

| 특성 | Adam | K-FAC |
|------|------|-------|
| 곡률 근사 | 대각 | 블록 크로네커 |
| 파라미터 상관 | 무시 | 레이어 내 포착 |
| 메모리 | $O(d)$ | $O(\sum d_l^2)$ |
| 스텝당 비용 | $O(d)$ | $O(\sum d_l^3)$ |
| 수렴 스텝 수 | 보통 | 적음 |

> **핵심 직관**: K-FAC은 레이어 내의 파라미터 상관관계를 포착하여 Adam보다 적은 스텝으로 수렴하지만, 스텝당 비용이 더 높아 총 시간에서의 이점은 상황에 따라 다릅니다.

---

## 6. 감쇠와 Trust Region

2차 방법은 곡률 추정이 부정확할 때 불안정할 수 있으므로, 안전 장치가 필요합니다.

**Tikhonov 감쇠 (Damping):**

$$\Delta\theta = -(H + \lambda I)^{-1} \nabla L$$

$\lambda > 0$은 헤시안의 음의 고유값을 양으로 이동시켜 안정성을 보장합니다.

**Trust Region:**

$$\min_{\Delta\theta} \; m(\Delta\theta) \quad \text{s.t.} \quad \|\Delta\theta\| \leq \Delta$$

2차 근사 $m(\Delta\theta)$를 신뢰 영역 $\Delta$ 내에서만 최소화합니다.

| 전략 | 방법 | 효과 |
|------|------|------|
| 감쇠 ($\lambda$ 큼) | 경사 하강에 가까움 | 안전하지만 느림 |
| 감쇠 ($\lambda$ 작음) | 뉴턴에 가까움 | 빠르지만 불안정 |
| 적응적 감쇠 | Levenberg-Marquardt | 균형 |
| Trust region | 영역 크기 적응 | 수렴 보장 |

> **핵심 직관**: 감쇠 파라미터 $\lambda$는 "1차 방법(SGD)과 2차 방법(뉴턴) 사이의 연속적 보간"을 제공하며, 큰 $\lambda$는 SGD로, $\lambda \to 0$은 뉴턴으로 수렴합니다.

---

## 7. 실전에서의 2차 방법의 위치

2차 방법은 이론적으로 우수하지만 실전 채택은 제한적입니다.

| 시나리오 | 권장 방법 | 이유 |
|---------|---------|------|
| 대규모 사전학습 (GPT 등) | AdamW (ot-03) | 단순성, 분산 학습 호환 |
| 소규모 파인튜닝 | L-BFGS 또는 Adam | 데이터 적어 빠른 수렴 필요 |
| 강화학습 정책 최적화 | TRPO/PPO (자연 경사 기반) | KL 제약 자연스러움 |
| 과학 계산 / PDE | L-BFGS | 결정론적, 매끄러운 목적함수 |

```python
# 강화학습에서의 자연 경사 (PPO의 핵심 아이디어)
# KL divergence 제약 하에서 정책 업데이트
ratio = new_policy / old_policy
surrogate = ratio * advantage
clipped = torch.clamp(ratio, 1-eps, 1+eps) * advantage
loss = -torch.min(surrogate, clipped).mean()
```

**미래 전망:** Shampoo(Google)와 같은 구조화된 2차 방법이 Adam과 비슷한 메모리로 더 빠른 수렴을 보여주며, ot-10에서 이 방향을 더 다룹니다.

> **핵심 직관**: 2차 최적화의 가치는 "이론적 수렴 속도"보다 "실질적 벽시계 시간"으로 평가해야 하며, 1차 방법의 단순성과 확장성은 여전히 강력한 장점입니다.

---

## 핵심 정리

- **뉴턴 방법은 헤시안의 역행렬을 사용하여 이차 수렴을 달성하지만, $O(d^2)$ 메모리와 $O(d^3)$ 계산 비용으로 딥러닝에서 직접 사용이 불가합니다**
- **자연 경사법은 파라미터 공간이 아닌 분포 공간에서 최적화하여 파라미터화 불변 업데이트를 제공하며, Adam은 이의 대각 근사로 해석됩니다**
- **K-FAC은 피셔 행렬을 크로네커 곱으로 근사하여 레이어 내 파라미터 상관관계를 효율적으로 포착합니다**
- **감쇠(damping)와 trust region은 2차 근사의 부정확성을 보완하며, $\lambda$는 SGD와 뉴턴 사이의 보간을 제어합니다**
- **실전에서 2차 방법은 소규모 파인튜닝과 강화학습(TRPO/PPO)에서 주로 사용되며, 대규모 학습에서는 AdamW가 여전히 표준입니다**
