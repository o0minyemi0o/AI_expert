# 그래프 신경망 기초

## 왜 그래프 신경망이 중요한가

이미지는 CNN, 시퀀스는 RNN/Transformer가 처리하지만, **그래프 구조 데이터**에는 전용 아키텍처가 필요합니다. 소셜 네트워크의 노드 분류, 분자 구조의 성질 예측, 추천 시스템의 사용자-아이템 그래프 등에서 **GNN(Graph Neural Network)**이 핵심 도구가 되었습니다. ga-08의 스펙트럴 이론에서 출발하여, 공간 도메인의 **메시지 패싱**으로 발전한 과정을 다룹니다.

> **핵심 직관**: GNN의 핵심은 **"이웃의 정보를 모아서 자신을 갱신"**하는 메시지 패싱입니다. 이는 ga-11의 Pregel("Think Like a Vertex")과 정확히 같은 패러다임이며, 학습 가능한 가중치를 추가한 것이 GNN입니다.

## 1. 메시지 패싱 프레임워크

```
Message Passing Neural Network (MPNN):

  각 노드 v의 표현 h_v를 반복적으로 갱신

  h_v^{(k)} = UPDATE(h_v^{(k-1)}, AGGREGATE({h_u^{(k-1)} : u ∈ N(v)}))

  k: 레이어 (메시지 패싱 라운드)
  N(v): v의 이웃 노드 집합
  h_v^{(0)}: 노드 v의 초기 특성 벡터

  과정:
  1. MESSAGE: 이웃 노드의 표현을 수집
  2. AGGREGATE: 수집된 표현을 합산/평균/최대
  3. UPDATE: 현재 표현과 결합하여 갱신

  K번 반복 후 h_v^{(K)}는 K-hop 이웃 정보를 반영

       ○─○─○─○─●─○─○─○─○
               ↑
          K=1: 직접 이웃
       K=2: 2-hop 이웃까지
    K=3: 3-hop 이웃까지의 정보 반영

  수용 영역(Receptive Field) = K-hop 이웃
  → K가 클수록 넓은 범위 고려
  → 하지만 과도한 K → over-smoothing 문제
```

## 2. GCN (Graph Convolutional Network)

```
GCN (Kipf & Welling, 2017):

  스펙트럴 그래프 컨볼루션의 1차 근사

  H^{(l+1)} = σ(D̃^{-1/2} Ã D̃^{-1/2} H^{(l)} W^{(l)})

  Ã = A + I (자기 루프 추가)
  D̃ = D + I (차수 행렬 갱신)
  W^{(l)}: 학습 가능한 가중치 행렬
  σ: 활성화 함수 (ReLU 등)

  노드 v에 대해 풀어쓰면:
  h_v^{(l+1)} = σ(W^{(l)} · Σ_{u∈N(v)∪{v}} h_u^{(l)} / √(d̃_v · d̃_u))

  직관:
  1. 이웃 + 자신의 특성을 수집
  2. 차수로 정규화 (고차수 노드의 영향 조절)
  3. 가중치 곱 + 비선형 활성화
  → ga-08의 정규화 라플라시안 D^{-1/2}AD^{-1/2}과 연결!
```

```python
import torch
import torch.nn.functional as F

class GCNLayer(torch.nn.Module):
    def __init__(self, in_dim, out_dim):
        super().__init__()
        self.W = torch.nn.Linear(in_dim, out_dim, bias=False)

    def forward(self, A_hat, H):
        """
        A_hat: 정규화된 인접 행렬 D̃^{-1/2} Ã D̃^{-1/2}
        H: 노드 특성 행렬 [N, in_dim]
        """
        return F.relu(A_hat @ self.W(H))

class GCN(torch.nn.Module):
    def __init__(self, in_dim, hidden_dim, out_dim):
        super().__init__()
        self.layer1 = GCNLayer(in_dim, hidden_dim)
        self.layer2 = GCNLayer(hidden_dim, out_dim)

    def forward(self, A_hat, X):
        H = self.layer1(A_hat, X)
        H = self.layer2(A_hat, H)
        return F.log_softmax(H, dim=1)
```

> **핵심 직관**: GCN은 ga-08의 스펙트럴 필터를 1차로 근사한 것입니다. **"이웃 평균 → 선형 변환 → 비선형 활성화"**라는 단순한 연산이 스펙트럴 컨볼루션의 효과를 내며, 행렬 곱으로 전체 그래프를 한 번에 처리할 수 있습니다.

## 3. GraphSAGE

```
GraphSAGE (Hamilton et al., 2017):

  GCN의 한계: 전체 그래프가 필요 (transductive)
  GraphSAGE: 이웃 샘플링 → 새 노드에도 적용 (inductive)

  핵심 차이:
  1. 이웃 샘플링: 전체 이웃 대신 K개만 랜덤 샘플
  2. 다양한 AGGREGATE 함수
  3. 학습된 표현으로 새 노드 임베딩 가능

  AGGREGATE 선택지:
  ├─ Mean: h_N = mean({h_u : u ∈ S(v)})
  ├─ LSTM: 이웃을 시퀀스로 처리 (순서 랜덤)
  ├─ Pool: h_N = max(σ(W_pool · h_u + b))
  └─ GCN: 정규화된 합

  갱신:
  h_v^{(k)} = σ(W^{(k)} · CONCAT(h_v^{(k-1)}, h_N^{(k)}))

  미니배치 학습:
  1. 타겟 노드 집합 선택
  2. 각 노드의 K-hop 이웃 샘플링
  3. 안쪽(K-hop)부터 바깥쪽(1-hop)으로 집계
  → 전체 그래프를 메모리에 올리지 않아도 됨!
```

```
GraphSAGE의 미니배치:

  Layer 2: 타겟 [v]
           ↑ 집계
  Layer 1: [v] + 1-hop 이웃 (샘플 S₁개)
           ↑ 집계
  Layer 0: [v] + 1-hop + 2-hop (샘플 S₁×S₂개)

  계산 그래프 크기: O(S₁ × S₂ × ... × S_K)
  → K=2, S=10이면 ~100 노드만 필요

  GCN vs GraphSAGE:
  GCN: 전체 그래프 행렬 곱 → 메모리 O(N)
  GraphSAGE: 미니배치 → 메모리 O(S^K) ≪ N
  → 대규모 그래프(ga-11)에서 필수
```

## 4. GAT (Graph Attention Network)

```
GAT (Veličković et al., 2018):

  모든 이웃이 동등하게 중요한가? → NO
  → 어텐션으로 이웃의 중요도를 학습

  어텐션 계수:
  e_{vu} = LeakyReLU(a^T · [Wh_v || Wh_u])
  α_{vu} = softmax_u(e_{vu}) = exp(e_{vu}) / Σ_{j∈N(v)} exp(e_{vj})

  갱신:
  h_v' = σ(Σ_{u∈N(v)} α_{vu} · Wh_u)

  Multi-head Attention (dl-10의 Transformer와 유사):
  h_v' = CONCAT(head_1, ..., head_K)
  head_i = σ(Σ_{u∈N(v)} α^i_{vu} · W^i h_u)

  GCN vs GAT:
  GCN: 가중치 = 1/√(d_v · d_u) (구조적, 고정)
  GAT: 가중치 = α_{vu} (학습된, 적응적)
  → 이질적 이웃에서 GAT가 더 효과적
```

```
GAT의 장점과 한계:

  장점:
  ├─ 이웃별 다른 가중치 → 중요한 이웃에 집중
  ├─ 어텐션 가중치 해석 가능 (일부)
  └─ inductive: 새 그래프에도 적용 가능

  한계:
  ├─ 이웃 쌍별 어텐션 계산 → O(E × d) 비용
  ├─ Over-smoothing: 깊은 레이어에서 표현 수렴
  └─ Over-squashing: 먼 노드 정보가 병목

  GATv2 (Brody et al., 2021):
  원래 GAT의 어텐션은 제한적 (정적 순위)
  GATv2: e = a^T · LeakyReLU([Wh_v || Wh_u])
  → 더 표현력 있는 동적 어텐션
```

> **핵심 직관**: GAT는 dl-10의 Transformer 어텐션을 그래프에 적용한 것입니다. Transformer가 "모든 토큰 쌍의 관계"를 학습하듯, GAT는 "모든 이웃 쌍의 중요도"를 학습합니다. 차이점은 GAT가 그래프 구조(간선)로 어텐션 범위를 제한한다는 것입니다.

## 5. GNN의 표현력과 한계

```
Weisfeiler-Leman (WL) 테스트와 GNN:

  WL 테스트: 그래프 동형 판별의 휴리스틱
  1. 각 노드에 색상(해시) 부여
  2. 이웃의 색상 집합 + 자신의 색상 → 새 색상
  3. 수렴까지 반복
  4. 최종 색상 분포가 같으면 "동형일 수 있음"

  핵심 정리 (Xu et al., 2019):
  MPNN의 표현력 ≤ 1-WL 테스트

  → 1-WL로 구분 못하는 그래프는 어떤 MPNN도 구분 못함!
  예: 정규 그래프에서 일부 구조적 차이

  GIN (Graph Isomorphism Network):
  WL 테스트와 동등한 표현력을 가진 GNN
  h_v^{(k)} = MLP((1 + ε) · h_v^{(k-1)} + Σ h_u^{(k-1)})
  → sum aggregation + MLP = 최대 표현력

Over-smoothing:
  레이어를 깊게 쌓으면 모든 노드의 표현이 수렴
  → 깊은 GNN이 항상 좋지 않음
  해결: Residual 연결, JKNet (Jump Knowledge), DropEdge
```

## 6. GNN 태스크와 실전 응용

```
태스크 유형:

  노드 레벨: 각 노드의 라벨 예측
  ├─ 논문 분류 (Cora, Citeseer)
  ├─ 사용자 분류 (사기 탐지)
  └─ 단백질 기능 예측

  간선 레벨: 간선 존재/유형 예측
  ├─ 링크 예측 (추천, 지식 그래프 완성)
  └─ h_edge = DECODE(h_u, h_v) → 내적, MLP 등

  그래프 레벨: 전체 그래프의 성질 예측
  ├─ 분자 성질 예측 (약물 발견)
  ├─ READOUT: 노드 표현 → 그래프 표현
  └─ READOUT = mean/sum/attention pooling

응용:
  ├─ 추천 시스템: PinSage (Pinterest), LightGCN
  ├─ 약물 발견: 분자 그래프 → 독성/활성 예측
  ├─ 교통 예측: 도로 그래프 → 시공간 GNN
  ├─ 사기 탐지: 거래 그래프의 이상 노드 탐지
  └─ 코드 분석: AST/CFG 그래프 → 버그 예측

PyG (PyTorch Geometric) 예시:
  from torch_geometric.nn import GCNConv
  class Net(torch.nn.Module):
      def __init__(self):
          super().__init__()
          self.conv1 = GCNConv(dataset.num_features, 16)
          self.conv2 = GCNConv(16, dataset.num_classes)
      def forward(self, data):
          x = F.relu(self.conv1(data.x, data.edge_index))
          x = F.dropout(x, training=self.training)
          return self.conv2(x, data.edge_index)
```

GNN은 ga-08의 스펙트럴 이론에서 출발하여, ga-11의 대규모 처리(GraphSAGE 미니배치)와 결합되며, dl-10의 어텐션 메커니즘(GAT)을 그래프 도메인에 확장한 것입니다.

## 핵심 정리

- GNN의 핵심은 **메시지 패싱**: "이웃 정보를 모아(AGGREGATE) 자신을 갱신(UPDATE)"하는 반복으로, K-hop 이웃 정보를 반영합니다
- **GCN**은 스펙트럴 필터의 1차 근사로, $\tilde{D}^{-1/2}\tilde{A}\tilde{D}^{-1/2}HW$라는 간결한 행렬 곱으로 구현됩니다
- **GraphSAGE**는 이웃 샘플링 + 미니배치로 대규모 그래프에서도 학습 가능하며, 새 노드에도 적용 가능(inductive)합니다
- **GAT**는 어텐션으로 이웃별 중요도를 학습하여, 이질적 이웃 구조에서 GCN보다 효과적입니다
- MPNN의 표현력은 **1-WL 테스트에 의해 상한**이 있으며, over-smoothing으로 깊은 GNN에는 주의가 필요합니다
