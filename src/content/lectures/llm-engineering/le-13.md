# 분산 학습과 서빙

## 왜 분산 처리가 중요한가

현대 LLM은 단일 GPU의 메모리와 연산 능력을 초과합니다. 70B 모델의 FP16 가중치만 140GB이고, 학습 시에는 옵티마이저 상태와 그래디언트까지 포함하여 수백 GB가 필요합니다. 분산 학습과 서빙 기술 없이는 대규모 모델을 학습하거나 효율적으로 배포할 수 없습니다. 이 강의에서는 데이터 병렬, 모델 병렬, 파이프라인 병렬, 그리고 DeepSpeed와 FSDP 같은 실무 프레임워크를 다룹니다 ([dl-08](/lectures/deep-learning/dl-08)의 분산 학습 기초 참조).

## 1. 분산 학습의 필요성

### 메모리 요구량 분석

학습 시 GPU 메모리는 네 가지 요소로 구성됩니다.

$$\text{Total Memory} = M_\text{params} + M_\text{gradients} + M_\text{optimizer} + M_\text{activations}$$

| 구성 요소 | FP32 | FP16/BF16 | 비율 (Adam) |
|-----------|------|----------|------------|
| 파라미터 | $4N$ bytes | $2N$ bytes | 1x |
| 그래디언트 | $4N$ bytes | $2N$ bytes | 1x |
| 옵티마이저 (Adam) | $8N$ bytes | $8N$ bytes | 4x |
| 활성화 | 가변 | 가변 | 가변 |

여기서 $N$은 파라미터 수입니다. 예를 들어 70B 모델의 Mixed Precision 학습에는:

$$M_\text{total} \approx 2 \times 70 + 2 \times 70 + 8 \times 70 = 840 \text{ GB}$$

> **핵심 직관**: Adam 옵티마이저의 상태(모멘텀 + 분산)가 파라미터 자체보다 4배 많은 메모리를 차지합니다. 이것이 학습이 추론보다 훨씬 많은 GPU 메모리를 요구하는 핵심 이유이며, ZeRO 같은 최적화의 주요 타겟입니다.

## 2. 병렬화 전략

### 3가지 병렬화 방식

```
[데이터 병렬 (DP)]               [텐서 병렬 (TP)]

GPU 0: 전체 모델 + 데이터 1/N    GPU 0: 레이어의 왼쪽 절반
GPU 1: 전체 모델 + 데이터 2/N    GPU 1: 레이어의 오른쪽 절반
GPU 2: 전체 모델 + 데이터 3/N    (레이어 내부를 분할)
(동일 모델, 다른 데이터)

[파이프라인 병렬 (PP)]            [3D 병렬 (DP + TP + PP)]

GPU 0: 레이어 1-10               DP x TP x PP = 총 GPU 수
GPU 1: 레이어 11-20              예: 8 x 4 x 4 = 128 GPUs
GPU 2: 레이어 21-30
(레이어 단위로 분할)
```

| 전략 | 통신 패턴 | 통신량 | 메모리 절감 | 적합한 규모 |
|------|----------|--------|-----------|-----------|
| 데이터 병렬 (DP) | All-Reduce | 그래디언트 | 없음 | < 10B |
| 텐서 병렬 (TP) | All-Reduce | 활성화 | 선형 | 노드 내 (NVLink) |
| 파이프라인 병렬 (PP) | Point-to-Point | 활성화 | 선형 | 노드 간 |
| ZeRO (Stage 3) | All-Gather | 파라미터 | 선형 | 범용 |

## 3. DeepSpeed ZeRO

DeepSpeed의 ZeRO(Zero Redundancy Optimizer)는 데이터 병렬의 메모리 중복을 제거합니다.

| ZeRO Stage | 분할 대상 | 메모리 절감 | 통신 오버헤드 |
|------------|----------|-----------|-------------|
| Stage 1 | 옵티마이저 상태 | ~4x | 1x |
| Stage 2 | + 그래디언트 | ~8x | 1x |
| Stage 3 | + 파라미터 | ~N (GPU 수) | 1.5x |

```python
# DeepSpeed ZeRO Stage 3 설정
# ds_config.json
ds_config = {
    "bf16": {"enabled": True},
    "zero_optimization": {
        "stage": 3,
        "offload_optimizer": {"device": "cpu", "pin_memory": True},
        "offload_param": {"device": "cpu", "pin_memory": True},
        "overlap_comm": True,
        "contiguous_gradients": True,
        "reduce_bucket_size": 5e8,
        "stage3_prefetch_bucket_size": 5e8,
        "stage3_param_persistence_threshold": 1e6,
    },
    "gradient_accumulation_steps": 4,
    "train_micro_batch_size_per_gpu": 2,
}
```

```python
# HuggingFace + DeepSpeed 학습
from transformers import AutoModelForCausalLM, TrainingArguments
from trl import SFTTrainer

training_args = TrainingArguments(
    output_dir="./output",
    deepspeed="ds_config.json",    # DeepSpeed 설정 파일
    per_device_train_batch_size=2,
    gradient_accumulation_steps=4,
    num_train_epochs=3,
    bf16=True,
    logging_steps=10,
)
# le-05의 SFTTrainer와 동일한 방식으로 학습
```

> **핵심 직관**: ZeRO Stage 3는 "모든 GPU가 전체 모델을 갖고 있을 필요 없다"는 발상입니다. 필요한 파라미터를 필요한 시점에 통신으로 가져오고, 사용 후 버립니다. 메모리와 통신의 트레이드오프를 시스템이 자동으로 관리합니다.

## 4. FSDP (Fully Sharded Data Parallel)

PyTorch 네이티브 분산 학습 프레임워크로, DeepSpeed ZeRO와 유사한 원리입니다.

```python
# PyTorch FSDP 설정
from torch.distributed.fsdp import FullyShardedDataParallel as FSDP
from torch.distributed.fsdp import ShardingStrategy, MixedPrecision
import torch

# FSDP 래핑
mp_policy = MixedPrecision(
    param_dtype=torch.bfloat16,
    reduce_dtype=torch.bfloat16,
    buffer_dtype=torch.bfloat16,
)

model = FSDP(
    model,
    sharding_strategy=ShardingStrategy.FULL_SHARD,  # ZeRO Stage 3 동등
    mixed_precision=mp_policy,
    device_id=torch.cuda.current_device(),
    auto_wrap_policy=transformer_auto_wrap_policy,
)
```

### DeepSpeed vs FSDP 비교

| 특성 | DeepSpeed | FSDP |
|------|-----------|------|
| 소속 | Microsoft | PyTorch (Meta) |
| CPU 오프로드 | ZeRO-Offload | 지원 |
| NVMe 오프로드 | ZeRO-Infinity | 미지원 |
| HuggingFace 통합 | 우수 | 우수 |
| 커스텀 커널 | 풍부 | 제한적 |
| 디버깅 용이성 | 보통 | 우수 (PyTorch 네이티브) |

## 5. 분산 추론

### 텐서 병렬 추론

```
[분산 추론 전략 선택]

[모델 크기]
     │
┌────▼──────────┐
│ 단일 GPU에     │
│ 적재 가능?     │
└────┬──────────┘
 Yes │         No
┌────▼────┐  ┌──▼────────────┐
│단일 GPU │  │ 단일 노드      │
│서빙     │  │ 내 GPU 수로   │
│(vLLM)   │  │ 충분?         │
└─────────┘  └──┬────────────┘
             Yes│         No
            ┌───▼──────┐ ┌──▼────────────┐
            │텐서 병렬  │ │파이프라인 병렬  │
            │(NVLink)  │ │+ 텐서 병렬     │
            │(vLLM TP) │ │(멀티노드)      │
            └──────────┘ └───────────────┘
```

```python
# vLLM 텐서 병렬 서빙 (le-10의 vLLM 확장)
from vllm import LLM, SamplingParams

# 70B 모델을 4 GPU에 텐서 병렬로 서빙
llm = LLM(
    model="meta-llama/Llama-3.1-70B-Instruct",
    tensor_parallel_size=4,              # 4 GPU 텐서 병렬
    gpu_memory_utilization=0.9,
    max_model_len=8192,
    enforce_eager=False,                 # CUDA Graph 활성화
)

# OpenAI 호환 API 서버로 시작
# python -m vllm.entrypoints.openai.api_server \
#   --model meta-llama/Llama-3.1-70B-Instruct \
#   --tensor-parallel-size 4
```

### 시나리오 1: 70B 모델 파인튜닝 인프라

70B 모델을 QLoRA([le-05](/lectures/llm-engineering/le-05))로 파인튜닝합니다. DeepSpeed ZeRO Stage 3 + CPU Offload를 사용하여 A100 80GB 4장으로 학습합니다. 4비트 양자화된 기본 모델(35GB) + LoRA 파라미터 + 옵티마이저 상태를 GPU와 CPU에 분산하여 메모리를 관리합니다 ([mo-04](/lectures/mlops/mo-04)의 GPU 인프라 관리 참조).

### 시나리오 2: 글로벌 추론 서비스 배포

다국적 서비스에서 지연 시간을 최소화하기 위해 여러 리전에 추론 서버를 배포합니다. 각 리전에 Llama 3.1 70B를 4xA100으로 텐서 병렬 서빙하고, 트래픽에 따라 자동 스케일링을 구성합니다. 피크 시간에는 추가 노드를 파이프라인 병렬로 연결하여 처리량을 늘립니다 ([le-12](/lectures/llm-engineering/le-12)의 아키텍처 패턴 참조).

> **핵심 직관**: 분산 처리의 핵심 원칙은 "통신을 최소화하면서 메모리와 연산을 균등하게 분배"하는 것입니다. 텐서 병렬은 고속 연결(NVLink)이 필요하고, 파이프라인 병렬은 느린 연결에서도 동작합니다. 인프라 토폴로지에 맞는 전략 선택이 성능을 좌우합니다.

## 핵심 정리

- LLM 학습 시 Adam 옵티마이저 상태가 파라미터의 4배 메모리를 차지하며, 이것이 분산 학습의 핵심 동기입니다
- 데이터 병렬은 같은 모델을 복제하고, 텐서 병렬은 레이어 내부를, 파이프라인 병렬은 레이어 단위로 분할합니다
- DeepSpeed ZeRO Stage 3은 파라미터, 그래디언트, 옵티마이저를 모두 분할하여 GPU당 메모리를 N분의 1로 줄입니다
- FSDP는 PyTorch 네이티브로 ZeRO와 유사한 기능을 제공하며, 디버깅과 생태계 통합에서 강점을 가집니다
- 분산 추론에서 텐서 병렬은 NVLink 같은 고속 연결이 필요하고, 파이프라인 병렬은 느린 연결에서도 동작합니다
