# 파인튜닝: RLHF와 정렬

## 왜 정렬(Alignment)이 중요한가

[le-05](/lectures/llm-engineering/le-05)에서 다룬 SFT는 모델이 지시를 따르도록 가르치지만, "어떤 응답이 더 좋은가"를 명시적으로 학습시키지는 않습니다. 인간의 선호도에 맞게 모델을 정렬(Align)하는 것이 RLHF(Reinforcement Learning from Human Feedback)와 그 대안 기법들의 핵심 목표입니다. 정렬 없는 모델은 유해하거나 편향된 출력을 생성할 수 있으며, 이는 프로덕션 배포에서 치명적인 문제를 야기합니다.

## 1. RLHF 파이프라인

RLHF는 세 단계로 구성됩니다. 사전학습과 SFT 이후의 최종 정렬 단계입니다.

```
[SFT 모델] ──────────────────────────────────────────────┐
     │                                                    │
     ▼                                                    ▼
┌──────────────┐    ┌──────────────┐    ┌──────────────────────┐
│ 1. 선호 데이터 │    │ 2. 보상 모델  │    │ 3. PPO로 정책 최적화   │
│    수집       │───▶│    학습      │───▶│    (RL 단계)          │
│ (인간 비교)   │    │ (점수 예측)   │    │                      │
└──────────────┘    └──────────────┘    └──────────────────────┘
```

### 1단계: 선호 데이터 수집

동일한 프롬프트에 대해 두 개의 응답을 생성하고, 인간 평가자가 더 나은 응답을 선택합니다.

$$\mathcal{D} = \{(x, y_w, y_l)\}_{i=1}^{N}$$

여기서 $x$는 프롬프트, $y_w$는 선호된(winning) 응답, $y_l$은 비선호된(losing) 응답입니다.

### 2단계: 보상 모델 (Reward Model) 학습

보상 모델 $r_\theta$는 Bradley-Terry 모델을 기반으로 학습됩니다.

$$\mathcal{L}_{RM}(\theta) = -\mathbb{E}_{(x, y_w, y_l) \sim \mathcal{D}} \left[\log \sigma\left(r_\theta(x, y_w) - r_\theta(x, y_l)\right)\right]$$

> **핵심 직관**: 보상 모델은 "절대적 점수"가 아닌 "상대적 선호"를 학습합니다. 이는 인간이 절대 평가보다 비교 판단에 훨씬 일관적이라는 심리학적 사실에 기반합니다.

### 3단계: PPO 최적화

SFT 모델을 초기 정책 $\pi_\text{ref}$로 두고, 보상을 최대화하면서 원래 모델에서 너무 벗어나지 않도록 KL 페널티를 부여합니다.

$$\max_{\pi} \mathbb{E}_{x \sim \mathcal{D}, y \sim \pi(\cdot|x)} \left[r_\theta(x, y) - \beta \cdot D_{KL}\left(\pi(\cdot|x) \| \pi_\text{ref}(\cdot|x)\right)\right]$$

$\beta$는 KL 페널티 강도로, 너무 작으면 보상 해킹(reward hacking)이 발생하고, 너무 크면 학습이 진행되지 않습니다.

## 2. DPO: 보상 모델 없는 정렬

Direct Preference Optimization(DPO)은 보상 모델 학습 단계를 생략하고, 선호 데이터로부터 직접 정책을 최적화합니다.

$$\mathcal{L}_{DPO}(\theta) = -\mathbb{E}_{(x, y_w, y_l)} \left[\log \sigma\left(\beta \log \frac{\pi_\theta(y_w|x)}{\pi_\text{ref}(y_w|x)} - \beta \log \frac{\pi_\theta(y_l|x)}{\pi_\text{ref}(y_l|x)}\right)\right]$$

| 특성 | RLHF (PPO) | DPO | KTO |
|------|-----------|-----|-----|
| 보상 모델 필요 | O | X | X |
| 데이터 형식 | 쌍별 비교 | 쌍별 비교 | 단일 응답 + 좋음/나쁨 |
| 학습 안정성 | 낮음 (RL 특유) | 높음 | 높음 |
| 메모리 사용 | 높음 (4 모델) | 중간 (2 모델) | 중간 (2 모델) |
| 성능 | 기준 | 동등~우수 | 약간 낮음 |
| 구현 복잡도 | 높음 | 낮음 | 낮음 |

> **핵심 직관**: DPO의 핵심 통찰은 "최적 보상 함수는 최적 정책으로부터 분석적으로(closed-form) 유도할 수 있다"는 것입니다. 따라서 보상 모델을 별도로 학습할 필요가 없습니다.

## 3. 실습: TRL로 DPO 학습

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
from trl import DPOTrainer, DPOConfig
from datasets import load_dataset

# 모델과 레퍼런스 모델 로드
model_name = "meta-llama/Llama-3.1-8B-Instruct"
model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype="auto", device_map="auto")
tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token

# 선호 데이터 로드 (chosen / rejected 쌍)
dataset = load_dataset("argilla/ultrafeedback-binarized-preferences")

# DPO 학습 설정
dpo_config = DPOConfig(
    output_dir="./dpo-llama3-output",
    beta=0.1,                    # KL 페널티 강도
    num_train_epochs=1,
    per_device_train_batch_size=2,
    gradient_accumulation_steps=8,
    learning_rate=5e-7,          # SFT보다 훨씬 작은 학습률
    bf16=True,
    logging_steps=10,
)

# DPO Trainer 실행
trainer = DPOTrainer(
    model=model,
    args=dpo_config,
    train_dataset=dataset["train"],
    tokenizer=tokenizer,
)
trainer.train()
```

## 4. 헌법적 AI (Constitutional AI)

Anthropic이 제안한 Constitutional AI(CAI)는 인간 평가자 대신 AI 자체가 원칙(헌법)에 따라 응답을 평가하고 개선합니다.

```
[프롬프트] ──▶ [초기 응답 생성] ──▶ [헌법 원칙으로 자기 비판]
                                          │
                                          ▼
                                   [개선된 응답 생성]
                                          │
                                          ▼
                                   [AI 피드백으로 RLHF]
                                   (RLAIF)
```

```python
# Constitutional AI 자기 비판 프로세스 예시
from openai import OpenAI

client = OpenAI()
principles = [
    "응답은 편견 없이 공정해야 합니다.",
    "응답은 유해한 활동을 조장하지 않아야 합니다.",
    "응답은 정확한 정보만 포함해야 합니다.",
]

def constitutional_critique(prompt: str, response: str) -> str:
    critique_prompt = f"""다음 원칙에 따라 응답을 평가하고 개선하세요.

원칙: {principles}
질문: {prompt}
응답: {response}

위 원칙 중 위반하는 것이 있다면 지적하고, 개선된 응답을 작성하세요."""

    result = client.chat.completions.create(
        model="gpt-4o",
        messages=[{"role": "user", "content": critique_prompt}],
    )
    return result.choices[0].message.content
```

## 5. 정렬 기법 비교와 선택

```
[정렬 필요]
     │
     ▼
┌──────────────┐
│ 쌍별 비교     │
│ 데이터 있음?  │
└──────┬───────┘
   Yes │       No
  ┌────▼────┐ ┌──▼──────────┐
  │데이터    │ │ KTO 또는     │
  │규모?    │ │ CAI 적용     │
  └────┬────┘ └─────────────┘
  <10K │  >10K
  ┌────▼────┐ ┌──▼──────────┐
  │DPO 권장  │ │RLHF(PPO)도  │
  │(간단/안정)│ │고려 가능     │
  └─────────┘ └─────────────┘
```

### 시나리오 1: 안전한 의료 상담 AI

의료 AI에서 유해한 조언을 절대 생성하지 않도록 정렬해야 합니다. 의료 전문가 50명이 5,000개의 응답 쌍을 비교 평가하고, DPO로 학습합니다. 추가로 "의학적 근거 없는 조언 금지", "응급 상황 시 119 안내" 등의 헌법 원칙을 적용하여 CAI 기반 추가 정렬을 수행합니다 ([le-11](/lectures/llm-engineering/le-11)의 가드레일 참조).

### 시나리오 2: 코딩 어시스턴트 선호도 학습

코딩 어시스턴트의 응답 품질을 개선합니다. 개발자들이 "코드 정확성", "설명 명확성", "보안 고려" 기준으로 응답 쌍을 비교합니다. DPO를 적용하되, $\beta=0.05$로 낮게 설정하여 더 공격적으로 선호 응답 방향으로 학습합니다. SFT([le-05](/lectures/llm-engineering/le-05))와 결합하면 성능이 극대화됩니다.

> **핵심 직관**: 정렬은 "모델이 할 수 있는 것"에서 "모델이 해야 하는 것"으로의 전환입니다. 기술적으로는 확률 분포의 재형성이며, 윤리적으로는 AI 시스템의 가치관을 설계하는 과정입니다.

## 핵심 정리

- RLHF는 선호 데이터 수집 → 보상 모델 학습 → PPO 최적화의 3단계로 구성되며, KL 페널티로 보상 해킹을 방지합니다
- DPO는 보상 모델 없이 선호 데이터로 직접 정책을 최적화하며, 구현이 간단하고 학습이 안정적이어서 실무에서 선호됩니다
- 헌법적 AI(CAI)는 인간 평가자 대신 AI 자기 비판을 활용하여 확장 가능한 정렬을 구현합니다
- 정렬 기법 선택은 데이터 형태(쌍별 비교 vs 단일 평가), 데이터 규모, 구현 복잡도를 기준으로 결정합니다
- 정렬의 학습률은 SFT보다 10-100배 작아야 하며, 과도한 정렬은 모델의 유용성을 훼손할 수 있습니다
