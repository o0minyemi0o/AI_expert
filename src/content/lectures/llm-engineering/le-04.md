# RAG 심화

## 왜 RAG 심화 기법이 중요한가

기초 RAG 파이프라인([le-03](/lectures/llm-engineering/le-03))은 단순한 질의에는 효과적이지만, 실제 프로덕션 환경에서는 복잡한 질의, 다중 문서 추론, 검색 정확도 부족 등 다양한 한계에 직면합니다. 이 강의에서는 하이브리드 검색, 리랭킹, 쿼리 변환, 멀티홉 RAG 등 고급 기법과 RAG 시스템을 체계적으로 평가하는 방법을 다룹니다.

## 1. 하이브리드 검색 (Hybrid Search)

벡터 검색(의미 기반)과 키워드 검색(어휘 기반)을 결합하여 검색 품질을 향상시킵니다.

| 검색 방식 | 장점 | 단점 | 적합한 경우 |
|-----------|------|------|-----------|
| Dense (벡터) | 의미적 유사성 포착 | 정확한 키워드 매칭 약함 | 자연어 질의 |
| Sparse (BM25) | 정확한 키워드 매칭 | 의미적 관계 파악 불가 | 고유명사, 코드 |
| Hybrid | 양쪽 장점 결합 | 가중치 튜닝 필요 | 프로덕션 시스템 |

$$\text{score}_{\text{hybrid}} = \alpha \cdot \text{score}_{\text{dense}} + (1 - \alpha) \cdot \text{score}_{\text{sparse}}$$

```python
from langchain_community.retrievers import BM25Retriever
from langchain.retrievers import EnsembleRetriever
from langchain_community.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings

# Dense 검색기
vectorstore = Chroma.from_documents(docs, OpenAIEmbeddings())
dense_retriever = vectorstore.as_retriever(search_kwargs={"k": 5})

# Sparse 검색기 (BM25)
bm25_retriever = BM25Retriever.from_documents(docs, k=5)

# 하이브리드 검색기 (가중치 0.6:0.4)
hybrid_retriever = EnsembleRetriever(
    retrievers=[dense_retriever, bm25_retriever],
    weights=[0.6, 0.4],
)
results = hybrid_retriever.invoke("Transformer 어텐션 메커니즘")
```

> **핵심 직관**: 하이브리드 검색은 "의미 검색의 재현율"과 "키워드 검색의 정밀도"를 결합합니다. 특히 기술 문서, 법률 문서처럼 정확한 용어가 중요한 도메인에서 단독 벡터 검색 대비 15-25% 성능 향상을 보입니다.

## 2. 리랭킹 (Re-ranking)

초기 검색 결과를 Cross-Encoder 모델로 재순위화하여 정밀도를 높입니다.

```
[사용자 쿼리]
      │
      ▼
┌──────────────┐
│ 1차 검색 (Bi-│    Top-K (K=20)
│ Encoder)     │──────────────┐
└──────────────┘              │
                              ▼
                    ┌──────────────┐
                    │ 리랭킹 (Cross-│   Top-N (N=5)
                    │ Encoder)      │──────────┐
                    └──────────────┘           │
                                               ▼
                                     ┌──────────────┐
                                     │ LLM 생성      │
                                     └──────────────┘
```

```python
from sentence_transformers import CrossEncoder

# Cross-Encoder 리랭킹
reranker = CrossEncoder("BAAI/bge-reranker-v2-m3")

query = "LLM 파인튜닝 방법"
passages = ["LoRA는 저랭크 적응 기법입니다...", "GPT-4는 OpenAI의 모델입니다...", ...]

# (쿼리, 문서) 쌍으로 점수 계산
pairs = [[query, p] for p in passages]
scores = reranker.predict(pairs)

# 점수 기준 재정렬
ranked = sorted(zip(passages, scores), key=lambda x: x[1], reverse=True)
top_passages = [p for p, s in ranked[:5]]
```

## 3. 쿼리 변환 (Query Transformation)

사용자의 원래 쿼리를 검색에 최적화된 형태로 변환합니다.

### 주요 쿼리 변환 기법

| 기법 | 설명 | 활용 사례 |
|------|------|----------|
| Query Rewriting | 질문을 검색 친화적으로 재작성 | 구어체 → 핵심 키워드 |
| HyDE | 가상 답변 생성 후 그것으로 검색 | 추상적 질문 |
| Sub-question | 복합 질문을 하위 질문으로 분해 | 멀티홉 질의 |
| Step-back | 더 일반적인 질문으로 확장 | 구체적 질문 |

```python
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate

llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)

# HyDE: Hypothetical Document Embeddings
hyde_prompt = ChatPromptTemplate.from_template("""
다음 질문에 대한 가상의 답변 문서를 작성하세요.
실제 정확성보다 관련 용어와 개념을 포함하는 것이 중요합니다.

질문: {question}
가상 답변:""")

chain = hyde_prompt | llm
hypothetical_doc = chain.invoke({"question": "LoRA의 작동 원리는?"})
# 이 가상 답변을 임베딩하여 실제 문서 검색에 사용
```

> **핵심 직관**: HyDE의 핵심 아이디어는 "질문의 임베딩"보다 "답변의 임베딩"이 실제 관련 문서와 더 가까운 벡터 공간에 위치한다는 것입니다. 질문과 답변의 의미적 거리를 줄여 검색 정확도를 높입니다.

## 4. 멀티홉 RAG (Multi-hop RAG)

하나의 질문에 답하기 위해 여러 문서를 순차적으로 검색하고 추론하는 기법입니다.

```python
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate

llm = ChatOpenAI(model="gpt-4o", temperature=0)

def multi_hop_rag(question: str, retriever, max_hops: int = 3):
    context = []
    current_query = question

    for hop in range(max_hops):
        # 현재 쿼리로 검색
        docs = retriever.invoke(current_query)
        context.extend([d.page_content for d in docs])

        # 답변 가능 여부 판단
        check_prompt = ChatPromptTemplate.from_template("""
        컨텍스트: {context}
        질문: {question}
        현재 정보로 질문에 완전히 답할 수 있습니까? (예/아니오)
        답할 수 없다면 추가로 필요한 정보를 검색 쿼리로 작성하세요.""")

        result = (check_prompt | llm).invoke({
            "context": "\n".join(context), "question": question
        })

        if "예" in result.content:
            break
        current_query = result.content  # 후속 쿼리로 다음 검색

    return context
```

### 시나리오 1: 기술 지원 챗봇 고도화

기존 RAG 기반 기술 지원 챗봇의 정확도가 60%에 머물러 있습니다. 분석 결과, 고유 제품명 검색 실패(30%)와 복합 질문 처리 실패(10%)가 주요 원인입니다. 하이브리드 검색으로 제품명 검색을 개선하고, Sub-question 분해로 복합 질문을 처리하여 정확도를 85%까지 향상시킵니다.

## 5. RAG 평가 (Evaluation)

RAG 시스템은 검색과 생성 두 단계를 각각 평가해야 합니다 ([le-07](/lectures/llm-engineering/le-07)에서 LLM 평가 전반을 다룹니다).

### RAG 평가 프레임워크: RAGAS

| 메트릭 | 측정 대상 | 설명 |
|--------|----------|------|
| Faithfulness | 생성 품질 | 답변이 검색된 컨텍스트에 충실한가 |
| Answer Relevancy | 생성 품질 | 답변이 질문에 관련 있는가 |
| Context Precision | 검색 품질 | 검색된 문서 중 관련 문서 비율 |
| Context Recall | 검색 품질 | 관련 문서 중 검색된 비율 |

```python
from ragas import evaluate
from ragas.metrics import faithfulness, answer_relevancy, context_precision
from datasets import Dataset

eval_data = {
    "question": ["RAG란 무엇인가?"],
    "answer": ["RAG는 검색 증강 생성으로..."],
    "contexts": [["RAG(Retrieval-Augmented Generation)은..."]],
    "ground_truth": ["RAG는 외부 지식을 검색하여 LLM 생성을 보강하는 기법"]
}

dataset = Dataset.from_dict(eval_data)
results = evaluate(dataset, metrics=[faithfulness, answer_relevancy, context_precision])
print(results)
```

### 시나리오 2: RAG 시스템 A/B 테스트

프로덕션 RAG 시스템의 청킹 전략을 변경하려 합니다. 기존(512 토큰 고정)과 새 전략(의미 기반 청킹)을 100개의 골드 셋 질의로 평가합니다. RAGAS의 Context Precision이 0.72에서 0.81로 향상되었으나, 인덱싱 시간이 3배 증가하여 비용-성능 트레이드오프를 분석합니다 ([le-12](/lectures/llm-engineering/le-12)의 아키텍처 설계 참조).

```
[RAG 개선 의사결정]
        │
   ┌────▼────────┐
   │ 문제 진단    │
   └────┬────────┘
        │
   ┌────▼────┐
   │검색 품질? │
   └────┬────┘
  낮음  │    정상
   ┌────▼────────┐  ┌──▼──────────┐
   │ 검색 개선    │  │ 생성 품질?   │
   │- 하이브리드  │  └──┬──────────┘
   │- 리랭킹     │   낮음│    정상
   │- 쿼리 변환  │  ┌────▼────────┐
   └────────────┘  │ 생성 개선    │
                   │- 프롬프트    │
                   │- 모델 변경   │
                   └─────────────┘
```

> **핵심 직관**: RAG 시스템 디버깅의 첫 단계는 항상 "검색이 잘못된 것인지, 생성이 잘못된 것인지"를 구분하는 것입니다. 검색된 컨텍스트를 먼저 확인하면 80%의 문제 원인을 파악할 수 있습니다.

## 핵심 정리

- 하이브리드 검색은 Dense와 Sparse 검색을 결합하여 키워드 매칭과 의미 검색의 장점을 동시에 활용합니다
- 리랭킹(Cross-Encoder)은 초기 검색 결과를 정교하게 재순위화하여 Top-K 정밀도를 크게 향상시킵니다
- 쿼리 변환(HyDE, Sub-question 등)은 사용자 질의를 검색에 최적화된 형태로 변환하여 재현율을 높입니다
- 멀티홉 RAG는 복합 질문에 대해 여러 번의 검색-추론 루프를 통해 점진적으로 답변을 구성합니다
- RAG 평가는 RAGAS 프레임워크를 사용하여 검색 품질(Context Precision/Recall)과 생성 품질(Faithfulness)을 분리하여 측정합니다
