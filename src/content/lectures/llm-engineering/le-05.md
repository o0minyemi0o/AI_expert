# 파인튜닝: SFT

## 왜 지도 미세조정(SFT)이 중요한가

프롬프트 엔지니어링([le-02](/lectures/llm-engineering/le-02))과 RAG([le-03](/lectures/llm-engineering/le-03))만으로는 해결하기 어려운 문제가 있습니다. 도메인 특화 어투, 일관된 출력 형식, 특수 태스크에 대한 높은 정확도가 필요할 때 지도 미세조정(Supervised Fine-Tuning, SFT)이 필요합니다. 이 강의에서는 효율적 파인튜닝 기법인 LoRA/QLoRA와 데이터 준비, 과적합 방지 전략을 다룹니다.

## 1. 파인튜닝이 필요한 경우

파인튜닝은 비용이 수반되므로, 반드시 필요한 경우에만 적용해야 합니다.

```
[현재 모델 성능 불충분]
         │
    ┌────▼─────┐
    │프롬프트로 │
    │개선 가능? │
    └────┬─────┘
     Yes │    No
    ┌────▼──┐ ┌──▼─────────┐
    │프롬프트│ │ RAG로      │
    │개선   │ │ 해결 가능?  │
    │(le-02)│ └──┬─────────┘
    └──────┘  Yes│    No
         ┌──────▼──┐ ┌──▼────────┐
         │RAG 적용 │ │파인튜닝    │
         │(le-03)  │ │필요       │
         └─────────┘ └───────────┘
```

| 접근법 | 비용 | 시간 | 적합한 경우 |
|--------|------|------|-----------|
| 프롬프트 엔지니어링 | 최저 | 시간 | 형식, 톤 조정 |
| RAG | 낮음 | 일~주 | 외부 지식 필요 |
| SFT | 중간 | 주~월 | 행동 패턴 학습 |
| 사전학습부터 | 매우 높음 | 월~년 | 새로운 언어/도메인 |

## 2. LoRA와 QLoRA

전체 파라미터를 업데이트하는 Full Fine-tuning은 메모리와 비용 부담이 큽니다. LoRA(Low-Rank Adaptation)는 기존 가중치를 동결하고 저랭크 행렬만 학습합니다.

### LoRA의 수학적 원리

기존 가중치 $W_0 \in \mathbb{R}^{d \times k}$에 대해:

$$W = W_0 + \Delta W = W_0 + BA$$

여기서 $B \in \mathbb{R}^{d \times r}$, $A \in \mathbb{R}^{r \times k}$이고, 랭크 $r \ll \min(d, k)$입니다.

> **핵심 직관**: LoRA의 핵심 가정은 "파인튜닝에서 실제로 필요한 가중치 변화는 저차원 부분공간에 집중된다"는 것입니다. 랭크 $r=16$이면 7B 모델의 학습 파라미터를 0.1%로 줄이면서도 Full Fine-tuning의 97% 성능을 달성할 수 있습니다.

### LoRA vs QLoRA 비교

| 특성 | Full FT | LoRA | QLoRA |
|------|---------|------|-------|
| 학습 파라미터 | 100% | ~0.1% | ~0.1% |
| 7B 모델 VRAM | ~60GB | ~16GB | ~6GB |
| 70B 모델 VRAM | ~500GB+ | ~160GB | ~48GB |
| 성능 | 기준 | ~97% | ~95% |
| 학습 속도 | 느림 | 빠름 | 중간 |

QLoRA는 기본 모델을 4비트로 양자화하고 그 위에 LoRA를 적용합니다 ([le-10](/lectures/llm-engineering/le-10)의 양자화 기법 참조).

## 3. 실습: PEFT로 LoRA 파인튜닝

```python
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments
from peft import LoraConfig, get_peft_model, TaskType
from trl import SFTTrainer
from datasets import load_dataset

# 모델 로드
model_name = "meta-llama/Llama-3.1-8B-Instruct"
model = AutoModelForCausalLM.from_pretrained(
    model_name, torch_dtype="auto", device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token

# LoRA 설정
lora_config = LoraConfig(
    task_type=TaskType.CAUSAL_LM,
    r=16,                          # 랭크
    lora_alpha=32,                 # 스케일링 팩터
    lora_dropout=0.05,
    target_modules=["q_proj", "v_proj", "k_proj", "o_proj"],
)

model = get_peft_model(model, lora_config)
model.print_trainable_parameters()
# 출력: trainable params: 6,553,600 || all params: 8,030,261,248 || 0.08%
```

```python
# 학습 설정
training_args = TrainingArguments(
    output_dir="./lora-llama3-output",
    num_train_epochs=3,
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,
    learning_rate=2e-4,
    warmup_ratio=0.1,
    logging_steps=10,
    save_strategy="epoch",
    bf16=True,
)

# SFTTrainer로 학습
trainer = SFTTrainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    tokenizer=tokenizer,
)
trainer.train()
```

## 4. 데이터 준비

SFT의 성능은 모델보다 데이터 품질에 의해 결정됩니다.

### 데이터 형식

```python
# Chat 형식 학습 데이터 예시
training_example = {
    "messages": [
        {"role": "system", "content": "당신은 의료 상담 AI입니다."},
        {"role": "user", "content": "두통이 3일째 지속됩니다."},
        {"role": "assistant", "content": "3일 이상 지속되는 두통은 전문의 상담이 필요합니다. 두통의 위치, 강도, 동반 증상을 말씀해 주시겠습니까?"},
    ]
}
```

### 데이터 품질 체크리스트

| 기준 | 설명 | 확인 방법 |
|------|------|----------|
| 정확성 | 답변 내용이 사실적인가 | 도메인 전문가 검수 |
| 일관성 | 동일 질문에 일관된 답변인가 | 중복 질의 교차 확인 |
| 다양성 | 다양한 유형의 질의를 포함하는가 | 카테고리별 분포 분석 |
| 적절한 길이 | 너무 짧거나 길지 않은가 | 토큰 수 분포 확인 |
| 형식 준수 | 의도한 출력 형식을 따르는가 | 자동 형식 검증 |

> **핵심 직관**: "1,000개의 고품질 데이터가 100,000개의 저품질 데이터보다 낫습니다." LIMA 논문(2023)은 단 1,000개의 정교하게 큐레이션된 예제로도 강력한 파인튜닝이 가능함을 보여주었습니다.

### 데이터 생성 전략

```python
# LLM을 활용한 학습 데이터 생성 (Self-Instruct 방식)
from openai import OpenAI

client = OpenAI()

def generate_training_data(seed_examples: list[dict], n: int = 100):
    generated = []
    for i in range(n):
        prompt = f"""다음 예시와 유사하지만 다른 질문-답변 쌍을 생성하세요.

예시: {seed_examples[i % len(seed_examples)]}

새로운 질문-답변 쌍을 JSON 형식으로 생성하세요."""

        response = client.chat.completions.create(
            model="gpt-4o", messages=[{"role": "user", "content": prompt}],
            temperature=0.8,
        )
        generated.append(response.choices[0].message.content)
    return generated
```

## 5. 과적합 방지 전략

소규모 데이터셋으로 파인튜닝할 때 과적합은 주요 위험입니다 ([dl-05](/lectures/deep-learning/dl-05)의 정규화 기법 참조).

| 전략 | 설명 | 권장 설정 |
|------|------|----------|
| 조기 종료 | 검증 손실 모니터링 | patience=3 |
| 드롭아웃 | LoRA 드롭아웃 적용 | 0.05~0.1 |
| 학습률 스케줄링 | Cosine 스케줄러 | warmup 10% |
| 데이터 증강 | 패러프레이징, 백번역 | 2-3배 증강 |
| 랭크 제한 | LoRA 랭크 낮추기 | r=8~32 |

```python
# QLoRA로 70B 모델 파인튜닝 (단일 A100 80GB)
from transformers import BitsAndBytesConfig
import torch

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_use_double_quant=True,
)

model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-3.1-70B-Instruct",
    quantization_config=bnb_config,
    device_map="auto",
)
```

### 시나리오 1: 고객 응대 톤 통일

대형 이커머스 기업에서 고객 응대 톤을 브랜드 가이드에 맞게 통일하고자 합니다. 기존 우수 상담원의 응대 로그 2,000건을 큐레이션하여 Llama 3.1 8B를 LoRA로 파인튜닝합니다. 도메인 전문가가 답변 품질을 5점 척도로 평가한 점수를 기준으로 상위 30%만 학습 데이터로 사용합니다.

### 시나리오 2: 코드 리뷰 자동화

개발팀의 코드 리뷰 패턴을 학습시킵니다. GitHub PR 리뷰 이력 5,000건을 수집하고, (코드 변경, 리뷰 코멘트) 쌍으로 학습 데이터를 구성합니다. QLoRA로 Code Llama 34B를 파인튜닝하여 팀 특화 코드 리뷰 봇을 구축합니다 ([mo-06](/lectures/mlops/mo-06)의 모델 배포 참조).

> **핵심 직관**: SFT는 "모델에 새로운 지식을 주입"하기보다 "이미 가진 지식을 특정 방식으로 표현하도록 가르치는 것"에 가깝습니다. 따라서 데이터의 품질과 형식이 양보다 훨씬 중요합니다.

## 핵심 정리

- SFT는 프롬프트 엔지니어링과 RAG로 해결할 수 없는 행동 패턴 학습에 사용하며, 항상 비용-편익을 먼저 분석합니다
- LoRA는 전체 파라미터의 0.1%만 학습하면서 Full Fine-tuning의 97% 성능을 달성하는 효율적 기법입니다
- QLoRA는 4비트 양자화와 LoRA를 결합하여 70B 모델도 단일 GPU에서 파인튜닝할 수 있게 합니다
- SFT 데이터의 품질이 양보다 중요하며, 1,000개의 정교한 예제로도 유의미한 성능 향상이 가능합니다
- 과적합 방지를 위해 조기 종료, LoRA 드롭아웃, 검증 데이터 분리를 반드시 적용해야 합니다
