# LLM 엔지니어링의 현재와 미래

## 왜 트렌드 분석이 중요한가

LLM 분야는 6개월마다 패러다임이 변할 만큼 빠르게 진화합니다. 현재의 최적 전략이 내년에는 구식이 될 수 있으며, 반대로 초기 단계의 기술이 곧 실무 표준이 될 수 있습니다. LLM 엔지니어는 단순히 현재 도구를 잘 사용하는 것을 넘어, 기술 흐름을 읽고 적응하는 능력이 필요합니다. 이 강의에서는 Mixture of Experts, 긴 컨텍스트, 추론 시간 컴퓨트, 소형 언어 모델, 그리고 앞으로의 트렌드를 분석합니다.

## 1. Mixture of Experts (MoE)

### MoE 아키텍처

MoE는 전체 파라미터 중 일부 전문가(Expert)만 활성화하여 연산 효율을 높이는 아키텍처입니다.

```
[입력 토큰]
     │
     ▼
┌──────────┐
│ 게이팅   │
│ 네트워크 │
└────┬─────┘
     │ (상위 K개 선택)
┌────▼────┬────────┬────────┬────────┐
│Expert 1 │Expert 2│Expert 3│  ...   │
│(활성화) │(비활성)│(활성화) │        │
└────┬────┘        └────┬───┘        │
     │                  │            │
     └──────┬───────────┘            │
            ▼                        │
     [가중 합산 출력]                  │
```

$$y = \sum_{i=1}^{K} g_i(x) \cdot E_i(x)$$

여기서 $g_i(x)$는 게이팅 네트워크의 가중치이고, $K$는 활성화되는 전문가 수(보통 2)입니다.

| 모델 | 전체 파라미터 | 활성 파라미터 | 전문가 수 | 활성 전문가 |
|------|-------------|-------------|----------|-----------|
| Mixtral 8x7B | 46.7B | ~13B | 8 | 2 |
| Mixtral 8x22B | 141B | ~39B | 8 | 2 |
| GPT-4 (추정) | ~1.8T | ~280B | ~16 | 2 |
| DeepSeek-V2 | 236B | ~21B | 160 | 6 |

> **핵심 직관**: MoE의 핵심 가치는 "모델 용량(지식)은 전체 파라미터에 비례하지만, 추론 비용은 활성 파라미터에만 비례한다"는 것입니다. 즉, 큰 모델의 지능과 작은 모델의 속도를 동시에 얻을 수 있습니다.

## 2. 긴 컨텍스트 (Long Context)

### 컨텍스트 길이의 진화

| 시기 | 모델 | 컨텍스트 길이 | 혁신 |
|------|------|-------------|------|
| 2022 | GPT-3.5 | 4K | 기본 |
| 2023 | Claude 2 | 100K | 위치 보간 |
| 2024 | Gemini 1.5 Pro | 1M~10M | Ring Attention |
| 2024 | Claude 3.5 | 200K | 효율적 어텐션 |
| 2025+ | 차세대 모델 | 10M+ | 무한 컨텍스트 지향 |

### 긴 컨텍스트의 과제

$$\text{KV Cache Memory} \propto \text{seq\_len} \times \text{layers} \times \text{hidden\_dim}$$

[le-10](/lectures/llm-engineering/le-10)에서 다룬 것처럼, 128K 토큰의 KV 캐시만으로 70B 모델에서 320GB를 차지합니다.

```
[긴 컨텍스트 vs RAG 선택]

[외부 지식 통합 필요]
        │
   ┌────▼──────────┐
   │ 대상 문서      │
   │ 크기?          │
   └────┬──────────┘
   <100K │   >100K
   ┌─────▼────┐  ┌──▼──────────┐
   │긴 컨텍스트│  │ RAG 필요     │
   │에 직접    │  │ (le-03)     │
   │삽입      │  │             │
   └──────────┘  └──┬──────────┘
                    │
              ┌─────▼──────────┐
              │ 정밀 검색      │
              │ 필요한가?      │
              └────┬───────────┘
              Yes  │        No
              ┌────▼────┐ ┌──▼────────────┐
              │RAG +    │ │긴 컨텍스트     │
              │청크 검색 │ │+ 요약 전략     │
              └─────────┘ └───────────────┘
```

> **핵심 직관**: 컨텍스트 길이가 길어져도 "Lost in the Middle" 문제가 존재합니다. 모델은 입력의 시작과 끝 부분에 있는 정보를 잘 활용하지만, 중간부의 정보는 놓치는 경향이 있습니다. 중요한 정보는 컨텍스트의 처음이나 끝에 배치하는 전략이 필요합니다.

## 3. 추론 시간 컴퓨트 (Inference-Time Compute)

### 테스트 타임 컴퓨트 스케일링

학습 시간의 컴퓨트를 늘리는 대신, 추론 시 더 많은 연산을 투입하여 성능을 향상시키는 패러다임입니다.

| 기법 | 원리 | 연산 증가 | 성능 향상 |
|------|------|----------|----------|
| Chain-of-Thought | 단계별 추론 생성 | ~2-5x 토큰 | 수학/추론 대폭 향상 |
| Self-Consistency | 다수 경로 샘플링 후 다수결 | ~10-40x | 5-15% |
| Tree-of-Thought | 탐색 트리 구성 | ~10-50x | 창의적 문제 |
| Best-of-N | N개 생성 후 최선 선택 | ~Nx | 보상 모델 의존 |
| Verifier 기반 | 검증기로 단계별 확인 | ~3-10x | 수학 크게 향상 |

```python
# Self-Consistency: 다수 경로 샘플링
from openai import OpenAI
from collections import Counter

client = OpenAI()

def self_consistency(question: str, n_samples: int = 10) -> str:
    responses = []
    for _ in range(n_samples):
        response = client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": "단계별로 추론하고 최종 답을 [답: X] 형식으로 제시하세요."},
                {"role": "user", "content": question},
            ],
            temperature=0.7,  # 다양한 추론 경로를 위해 높은 temperature
        )
        answer = extract_answer(response.choices[0].message.content)
        responses.append(answer)

    # 다수결 투표
    counter = Counter(responses)
    return counter.most_common(1)[0][0]
```

## 4. 소형 언어 모델 (SLM)

### SLM의 부상

| 모델 | 파라미터 | 성능 (MMLU) | 특징 |
|------|---------|-----------|------|
| Phi-3 Mini | 3.8B | 69% | 데이터 품질 최적화 |
| Gemma 2 | 2B/9B | 56%/72% | 지식 증류 |
| Llama 3.2 | 1B/3B | 49%/63% | 모바일 최적화 |
| Qwen 2.5 | 0.5B/1.5B/3B | 가변 | 다국어 |
| Mistral Small | 8B | 70% | 효율적 아키텍처 |

```python
# 소형 모델의 지식 증류 (Knowledge Distillation)
from transformers import AutoModelForCausalLM, AutoTokenizer
from openai import OpenAI

client = OpenAI()

def generate_distillation_data(prompts: list[str], teacher_model: str = "gpt-4o") -> list[dict]:
    """교사 모델의 응답으로 학습 데이터 생성"""
    training_data = []
    for prompt in prompts:
        response = client.chat.completions.create(
            model=teacher_model,
            messages=[{"role": "user", "content": prompt}],
            temperature=0.3,
        )
        training_data.append({
            "messages": [
                {"role": "user", "content": prompt},
                {"role": "assistant", "content": response.choices[0].message.content},
            ]
        })
    return training_data
# 생성된 데이터로 소형 모델을 SFT (le-05 참조)
```

### SLM 활용 전략

```
[모델 크기 선택]

[배포 환경]
     │
┌────▼─────────┐
│ 엣지/모바일? │
└────┬─────────┘
 Yes │         No
┌────▼────┐  ┌──▼──────────┐
│SLM      │  │ 서버 환경    │
│1B~3B    │  │             │
│(le-10)  │  └──┬──────────┘
└─────────┘     │
           ┌────▼──────────┐
           │ 복잡도 높은    │
           │ 태스크?        │
           └────┬──────────┘
            Yes │        No
           ┌────▼────┐ ┌──▼────────────┐
           │대형 모델 │ │SLM 파인튜닝    │
           │7B~70B+  │ │3B~8B          │
           └─────────┘ │비용 효율적     │
                       └───────────────┘
```

## 5. 미래 트렌드 분석

### 향후 1-3년 핵심 트렌드

| 트렌드 | 현재 상태 | 예상 영향 | 엔지니어 대응 |
|--------|----------|----------|-------------|
| MoE 보편화 | 확산 중 | 비용 50% 절감 | MoE 서빙 인프라 학습 |
| 10M+ 컨텍스트 | 초기 단계 | RAG 역할 재정의 | 긴 컨텍스트 활용 패턴 |
| 추론 시간 스케일링 | 연구 활발 | 소형 모델 성능 향상 | 비용-성능 트레이드오프 |
| 멀티모달 네이티브 | 확산 중 | 새로운 응용 폭발 | 멀티모달 파이프라인 |
| 에이전트 프레임워크 | 성숙 중 | 자동화 범위 확대 | 도구 설계 역량 강화 |
| 온디바이스 AI | 초기 상용화 | 프라이버시 해결 | 양자화/경량화 기법 |

### 시나리오 1: MoE 모델 서빙 최적화

Mixtral 8x22B (141B 파라미터, 39B 활성)를 비용 효율적으로 서빙합니다. 전체 파라미터를 메모리에 올려야 하지만 연산은 활성 파라미터에만 발생하므로, 고메모리-저연산 GPU 구성이 유리합니다. vLLM([le-10](/lectures/llm-engineering/le-10))의 Expert Parallelism을 활용하여 전문가를 GPU별로 분산합니다.

### 시나리오 2: 소형 모델 + 추론 시간 컴퓨트

비용 민감한 환경에서 3B 소형 모델에 Self-Consistency와 Chain-of-Thought를 결합합니다. 단순 질문에는 직접 응답(1x 비용)하고, 복잡한 추론이 필요한 질문에는 10개 경로를 샘플링하여 다수결(10x 비용)합니다. 태스크 분류기([le-12](/lectures/llm-engineering/le-12)의 모델 계층화)로 비용을 동적 관리하면, 70B 모델의 90% 성능을 20% 비용으로 달성할 수 있습니다.

> **핵심 직관**: LLM 엔지니어링의 미래는 "큰 모델 하나에 의존"에서 "작은 모델 + 스마트한 시스템 설계"로 이동하고 있습니다. 모델 크기보다 시스템 아키텍처, 추론 전략, 도구 설계가 최종 성능을 결정하는 시대가 오고 있으며, 이는 이 과정 전체([le-01](/lectures/llm-engineering/le-01)~[le-13](/lectures/llm-engineering/le-13))에서 다룬 엔지니어링 역량의 중요성이 더욱 커짐을 의미합니다.

## 핵심 정리

- MoE 아키텍처는 전체 파라미터의 지식 용량은 유지하면서 활성 파라미터만으로 추론하여 효율성을 극대화합니다
- 긴 컨텍스트(1M+ 토큰)는 RAG의 필요성을 줄이지만, KV 캐시 메모리와 Lost-in-the-Middle 문제를 해결해야 합니다
- 추론 시간 컴퓨트 스케일링(Self-Consistency, CoT)은 소형 모델의 성능을 대형 모델 수준으로 끌어올릴 수 있습니다
- 소형 언어 모델(1B~8B)은 지식 증류와 데이터 품질 최적화로 빠르게 성능이 향상되고 있으며, 엣지 배포의 핵심입니다
- LLM 엔지니어링의 미래는 모델 크기보다 시스템 설계(에이전트, 도구, 캐싱, 라우팅)가 최종 성능을 결정하는 방향으로 진화하고 있습니다
