# LLM 추론 최적화

## 왜 추론 최적화가 중요한가

LLM 서비스의 비용 대부분은 추론(inference) 단계에서 발생합니다. 학습은 한 번이지만 추론은 매 요청마다 수행되기 때문입니다. 7B 모델도 최적화 없이 서빙하면 GPU 비용이 급격히 증가하고, 응답 지연이 사용자 경험을 해칩니다. 이 강의에서는 KV 캐시, 양자화, 고성능 서빙 엔진, 스펙큘레이티브 디코딩, 배치 전략 등 프로덕션 수준의 추론 최적화 기법을 다룹니다 ([le-05](/lectures/llm-engineering/le-05)에서 언급한 QLoRA의 양자화 기반 기술을 심층적으로 살펴봅니다).

## 1. KV 캐시

자기회귀(autoregressive) 생성에서 이전 토큰의 Key, Value를 재계산하지 않고 캐시에 저장하여 재사용합니다.

### KV 캐시 메모리 계산

$$\text{KV Cache Memory} = 2 \times n_\text{layers} \times n_\text{heads} \times d_\text{head} \times \text{seq\_len} \times \text{batch} \times \text{bytes}$$

| 모델 | 레이어 | 헤드 | 시퀀스 1K | 시퀀스 8K | 시퀀스 128K |
|------|--------|------|----------|----------|------------|
| Llama 3.1 8B | 32 | 32 | 0.5 GB | 4 GB | 64 GB |
| Llama 3.1 70B | 80 | 64 | 2.5 GB | 20 GB | 320 GB |

> **핵심 직관**: KV 캐시는 "시간을 공간으로 교환"하는 전형적 최적화입니다. 긴 컨텍스트에서는 모델 가중치보다 KV 캐시가 더 많은 메모리를 차지할 수 있으며, 이것이 긴 문맥 처리의 실질적 병목입니다.

### KV 캐시 최적화 기법

| 기법 | 원리 | 메모리 절감 | 품질 영향 |
|------|------|-----------|----------|
| MQA (Multi-Query Attention) | K,V 헤드를 1개로 공유 | ~8x | 미미 |
| GQA (Grouped-Query Attention) | K,V 헤드를 그룹으로 공유 | ~4x | 거의 없음 |
| PagedAttention | OS 페이징 기법 적용 | ~2x (단편화 제거) | 없음 |
| KV Cache Quantization | KV를 INT8로 양자화 | ~2x | 약간 |

## 2. 양자화 (Quantization)

모델 가중치의 정밀도를 낮춰 메모리와 연산 비용을 줄입니다.

### 양자화 방식 비교

| 방식 | 비트 | 방법 | 캘리브레이션 | 대표 구현 |
|------|------|------|------------|----------|
| FP16 | 16 | 기본 | 불필요 | PyTorch 기본 |
| INT8 | 8 | 동적/정적 | 선택적 | bitsandbytes |
| GPTQ | 4 | 가중치 전용, 레이어별 | 필요 (128샘플) | AutoGPTQ |
| AWQ | 4 | 활성화 인지 | 필요 | AutoAWQ |
| GGUF | 2-8 | CPU 친화 | 필요 | llama.cpp |

$$\text{메모리} \approx \frac{\text{파라미터 수} \times \text{비트}}{8} \text{ bytes}$$

| 모델 | FP16 | INT8 | INT4 (GPTQ/AWQ) |
|------|------|------|-----------------|
| 7B | 14 GB | 7 GB | 3.5 GB |
| 13B | 26 GB | 13 GB | 6.5 GB |
| 70B | 140 GB | 70 GB | 35 GB |

```python
# AWQ 양자화 적용
from awq import AutoAWQForCausalLM
from transformers import AutoTokenizer

model_path = "meta-llama/Llama-3.1-8B-Instruct"
quant_config = {"zero_point": True, "q_group_size": 128, "w_bit": 4}

# 양자화 실행 (캘리브레이션 데이터 필요)
model = AutoAWQForCausalLM.from_pretrained(model_path)
tokenizer = AutoTokenizer.from_pretrained(model_path)
model.quantize(tokenizer, quant_config=quant_config)

# 양자화 모델 저장
model.save_quantized("llama3-8b-awq")
```

> **핵심 직관**: GPTQ는 "각 레이어의 양자화 오차를 다음 레이어에서 보정"하고, AWQ는 "중요한 가중치(활성화 크기 기준)의 정밀도를 우선 보존"합니다. 실무에서 AWQ가 GPTQ보다 약간 더 나은 품질-속도 트레이드오프를 보이는 경향이 있습니다.

## 3. vLLM을 이용한 고성능 서빙

vLLM은 PagedAttention과 연속 배칭(Continuous Batching)을 활용한 고성능 LLM 서빙 엔진입니다.

```python
# vLLM 서버 시작
from vllm import LLM, SamplingParams

# 모델 로드 (AWQ 양자화 모델 지원)
llm = LLM(
    model="meta-llama/Llama-3.1-8B-Instruct",
    quantization="awq",
    tensor_parallel_size=2,       # 2 GPU 텐서 병렬
    gpu_memory_utilization=0.9,
    max_model_len=8192,
)

# 배치 추론
prompts = ["AI의 미래는", "양자 컴퓨팅이란", "기후 변화 대응 방법"]
sampling_params = SamplingParams(temperature=0.7, max_tokens=256, top_p=0.9)
outputs = llm.generate(prompts, sampling_params)

for output in outputs:
    print(f"Prompt: {output.prompt[:30]}...")
    print(f"Output: {output.outputs[0].text[:100]}...")
```

```
[vLLM 서빙 아키텍처]

요청 1 ──▶ ┌──────────────────────────────┐
요청 2 ──▶ │   Continuous Batching        │
요청 3 ──▶ │   Scheduler                  │ ──▶ GPU
            │                              │
요청 4 ──▶ │   PagedAttention             │
            │   (KV Cache 동적 할당)        │
            └──────────────────────────────┘
```

## 4. 스펙큘레이티브 디코딩

작은 드래프트 모델이 먼저 여러 토큰을 생성하고, 큰 타겟 모델이 한 번에 검증하는 방식으로 지연 시간을 줄입니다.

```
[스펙큘레이티브 디코딩 흐름]

Draft Model (소형):  토큰1 → 토큰2 → 토큰3 → 토큰4  (빠른 생성)
                          │        │        │        │
Target Model (대형):      ✓        ✓        ✗        -  (한 번에 검증)
                                            │
최종 출력:            토큰1 → 토큰2 → 수정토큰3  (수정 후 재시작)
```

| 설정 | 타겟 모델 | 드래프트 모델 | 속도 향상 | 품질 |
|------|----------|-------------|----------|------|
| 설정 A | Llama 3.1 70B | Llama 3.1 8B | ~2-3x | 동일 |
| 설정 B | GPT-4급 | GPT-3.5급 | ~2x | 동일 |
| Self-Draft | 동일 모델 | 일부 레이어 | ~1.5x | 동일 |

## 5. 배치 전략과 프로덕션 최적화

### 배치 전략 비교

```
[배치 전략 선택]
       │
  ┌────▼────────┐
  │ 실시간      │
  │ 응답 필요?  │
  └────┬────────┘
   Yes │       No
  ┌────▼─────────┐ ┌──▼──────────────┐
  │Continuous    │ │Offline Batch    │
  │Batching     │ │Processing       │
  │(vLLM/TGI)   │ │(높은 처리량)     │
  └──────────────┘ └─────────────────┘
```

| 전략 | 지연 시간 | 처리량 | 적합 시나리오 |
|------|----------|--------|-------------|
| 정적 배치 | 높음 | 중간 | 오프라인 처리 |
| 연속 배치 | 낮음 | 높음 | 실시간 API |
| 우선순위 배치 | 가변 | 높음 | 유료/무료 계층 |

### 시나리오 1: 비용 효율적 API 서빙

스타트업에서 Llama 3.1 70B를 서빙합니다. AWQ 4비트 양자화로 A100 40GB 2장에 배포하고, vLLM의 연속 배칭으로 처리량을 최대화합니다. 비용 비교: 폐쇄형 API 대비 일 10만 요청 기준 약 60-70% 비용 절감이 가능합니다 ([le-12](/lectures/llm-engineering/le-12)의 비용 최적화 참조).

### 시나리오 2: 엣지 디바이스 배포

모바일 앱에 3B 모델을 탑재합니다. GGUF 포맷으로 4비트 양자화하여 모델 크기를 1.5GB로 줄이고, llama.cpp 기반으로 CPU에서 추론합니다. 스펙큘레이티브 디코딩 대신 단순한 KV 캐시 최적화에 집중합니다 ([le-14](/lectures/llm-engineering/le-14)의 소형 언어 모델 참조).

> **핵심 직관**: 추론 최적화는 "모델 품질 vs 속도 vs 비용"의 3차원 트레이드오프입니다. 양자화는 품질을 약간 희생하고, 배칭은 지연 시간을 약간 증가시키며, 텐서 병렬은 비용을 증가시킵니다. 프로덕션에서는 이 세 축의 균형점을 찾는 것이 핵심입니다.

## 핵심 정리

- KV 캐시는 자기회귀 생성에서 이전 토큰의 K,V를 재사용하며, GQA와 PagedAttention으로 메모리를 절감합니다
- 양자화(GPTQ/AWQ)는 4비트로 모델 크기를 4배 줄이면서 원본 성능의 95% 이상을 유지합니다
- vLLM은 PagedAttention과 연속 배칭으로 정적 배칭 대비 2-4배 처리량 향상을 달성합니다
- 스펙큘레이티브 디코딩은 소형 드래프트 모델의 빠른 생성과 대형 모델의 검증을 결합하여 품질 손실 없이 2-3배 속도를 개선합니다
- 프로덕션 추론 최적화는 모델 품질, 응답 속도, GPU 비용의 3차원 트레이드오프에서 최적 균형점을 찾는 과정입니다
