# RAG 기초

## 왜 RAG가 중요한가

LLM은 학습 데이터에 포함되지 않은 정보에 대해 부정확한 답변(할루시네이션)을 생성하는 근본적 한계가 있습니다. 검색 증강 생성(Retrieval-Augmented Generation, RAG)은 외부 지식을 실시간으로 검색하여 LLM에 제공함으로써 이 문제를 해결합니다. 파인튜닝 없이도 도메인 특화된 정확한 답변을 생성할 수 있어, 실무에서 가장 널리 사용되는 LLM 활용 패턴입니다.

## 1. RAG 파이프라인 개요

RAG는 크게 인덱싱(오프라인)과 검색-생성(온라인) 두 단계로 구성됩니다.

```
[오프라인: 인덱싱]
문서 수집 → 청킹 → 임베딩 → 벡터 DB 저장

[온라인: 검색-생성]
사용자 질문 → 쿼리 임베딩 → 유사도 검색 → 컨텍스트 구성 → LLM 생성 → 응답
```

> **핵심 직관**: RAG는 "오픈북 시험"과 같습니다. 모델이 모든 것을 외우지 않아도, 관련 자료를 찾아 참고하면서 답변할 수 있게 해줍니다. 핵심은 "적절한 자료를 빠르게 찾는 것"입니다.

## 2. 임베딩 모델

임베딩 모델은 텍스트를 고차원 벡터 공간에 매핑합니다. 의미적으로 유사한 텍스트는 가까운 벡터로 변환됩니다 ([dl-03](/lectures/deep-learning/dl-03)의 벡터 표현 참조).

### 주요 임베딩 모델 비교

| 모델 | 차원 | 최대 토큰 | MTEB 점수 | 특징 |
|------|------|----------|----------|------|
| text-embedding-3-large | 3072 | 8191 | 64.6 | OpenAI, 차원 축소 지원 |
| text-embedding-3-small | 1536 | 8191 | 62.3 | 비용 효율적 |
| BGE-M3 | 1024 | 8192 | 68.2 | 다국어, 오픈소스 |
| E5-mistral-7B | 4096 | 32768 | 66.6 | 긴 문서 지원 |
| Cohere embed-v3 | 1024 | 512 | 64.5 | 검색 최적화 |

```python
from openai import OpenAI

client = OpenAI()

def get_embedding(text: str, model: str = "text-embedding-3-small") -> list[float]:
    response = client.embeddings.create(input=text, model=model)
    return response.data[0].embedding

# 유사도 계산
import numpy as np

def cosine_similarity(a: list[float], b: list[float]) -> float:
    a, b = np.array(a), np.array(b)
    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))

q_emb = get_embedding("파이썬 리스트 정렬 방법")
d_emb = get_embedding("Python에서 sort()와 sorted()로 리스트를 정렬합니다")
print(f"유사도: {cosine_similarity(q_emb, d_emb):.4f}")
```

## 3. 청킹 전략 (Chunking)

문서를 적절한 크기로 분할하는 것은 RAG 성능에 직접적 영향을 미칩니다.

| 전략 | 설명 | 장점 | 단점 |
|------|------|------|------|
| 고정 크기 | N 토큰 단위 분할 | 구현 간단 | 문맥 단절 |
| 재귀적 분할 | 구분자 기반 계층적 분할 | 문맥 보존 | 불균일 크기 |
| 의미 기반 | 임베딩 유사도 기반 분할 | 의미 단위 보존 | 계산 비용 |
| 문서 구조 기반 | 헤딩/섹션 기반 분할 | 논리적 단위 | 문서 형식 의존 |

```python
from langchain.text_splitter import RecursiveCharacterTextSplitter

splitter = RecursiveCharacterTextSplitter(
    chunk_size=512,
    chunk_overlap=50,
    separators=["\n\n", "\n", ". ", " ", ""],
    length_function=len,
)

documents = splitter.split_text(long_document)
```

### 청킹 크기 결정 흐름

```
[문서 유형 확인]
       │
  ┌────▼────┐
  │ 구조화?  │
  └────┬────┘
   Yes │    No
  ┌────▼────┐ ┌──▼──────────┐
  │구조 기반 │ │ 문서 길이?   │
  │분할     │ └──┬──────────┘
  └────────┘    │
          짧음(<2K)  긴 문서(>2K)
          ┌────▼──┐ ┌──▼────────┐
          │전체를  │ │재귀적 분할  │
          │하나의  │ │chunk_size  │
          │청크로  │ │= 256~1024  │
          └───────┘ └───────────┘
```

> **핵심 직관**: 청크 크기는 "정밀도와 재현율의 트레이드오프"입니다. 작은 청크는 검색 정밀도를 높이지만 문맥을 잃고, 큰 청크는 문맥을 보존하지만 노이즈를 포함합니다. 일반적으로 256-512 토큰이 좋은 출발점입니다.

## 4. 벡터 데이터베이스

벡터 DB는 임베딩 벡터를 저장하고, 유사도 기반 검색을 효율적으로 수행합니다.

| 벡터 DB | 유형 | ANN 알고리즘 | 특징 |
|---------|------|-------------|------|
| Pinecone | 관리형 SaaS | 독자 알고리즘 | 완전 관리형, 쉬운 시작 |
| Weaviate | 오픈소스/클라우드 | HNSW | 하이브리드 검색 지원 |
| Qdrant | 오픈소스/클라우드 | HNSW | Rust 기반, 고성능 |
| ChromaDB | 오픈소스 | HNSW | 경량, 프로토타이핑 |
| pgvector | PostgreSQL 확장 | IVFFlat/HNSW | 기존 DB와 통합 |

```python
import chromadb

client = chromadb.PersistentClient(path="./chroma_db")
collection = client.get_or_create_collection(
    name="documents",
    metadata={"hnsw:space": "cosine"},
)

# 문서 저장
collection.add(
    documents=["Python은 프로그래밍 언어입니다", "RAG는 검색 증강 생성입니다"],
    ids=["doc1", "doc2"],
    metadatas=[{"source": "wiki"}, {"source": "lecture"}],
)

# 검색
results = collection.query(query_texts=["프로그래밍 언어란?"], n_results=3)
```

## 5. 검색 파이프라인 구성

전체 RAG 파이프라인을 LangChain으로 구성하는 예시입니다.

```python
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate

# 벡터 스토어 생성
embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
vectorstore = Chroma.from_documents(documents, embeddings)

# 검색기 설정
retriever = vectorstore.as_retriever(search_kwargs={"k": 4})

# 프롬프트 템플릿
prompt = PromptTemplate.from_template("""
다음 컨텍스트를 참고하여 질문에 답변하세요.
컨텍스트에 없는 내용은 "정보가 없습니다"라고 답하세요.

컨텍스트: {context}
질문: {question}
답변:""")

# QA 체인
qa_chain = RetrievalQA.from_chain_type(
    llm=ChatOpenAI(model="gpt-4o-mini", temperature=0),
    retriever=retriever,
    chain_type_kwargs={"prompt": prompt},
)
answer = qa_chain.invoke("RAG의 주요 구성 요소는?")
```

### 시나리오 1: 사내 문서 검색 시스템

기업 내부 위키, 매뉴얼, 정책 문서(약 10,000페이지)에 대한 질의응답 시스템을 구축합니다. 문서는 Markdown/PDF 혼합이며, 구조 기반 청킹으로 섹션 단위로 분할합니다. pgvector를 사용하여 기존 PostgreSQL 인프라와 통합하고, 메타데이터(부서, 문서 유형, 최종 수정일)를 필터링에 활용합니다 ([mo-04](/lectures/mlops/mo-04) 참조).

### 시나리오 2: 학술 논문 Q&A

연구팀에서 특정 분야의 논문 1,000편에 대해 질문할 수 있는 시스템을 구축합니다. 논문의 Abstract, Introduction, Method, Result 섹션별로 청킹하고, 섹션 유형을 메타데이터로 저장합니다. 검색 시 "방법론 관련 질문"에는 Method 섹션을 우선 검색하는 전략을 적용합니다 ([le-04](/lectures/llm-engineering/le-04)에서 심화 기법을 다룹니다).

> **핵심 직관**: RAG의 성능은 "생성 모델의 능력"보다 "검색의 품질"에 더 크게 좌우됩니다. 아무리 뛰어난 LLM도 관련 없는 문서가 제공되면 올바른 답변을 생성할 수 없습니다.

## 핵심 정리

- RAG는 인덱싱(오프라인)과 검색-생성(온라인) 두 단계로 구성되며, 파인튜닝 없이 도메인 지식을 활용할 수 있습니다
- 임베딩 모델 선택 시 다국어 지원, 최대 토큰 길이, MTEB 벤치마크 점수를 종합적으로 고려해야 합니다
- 청킹 전략은 문서 유형과 태스크에 따라 달라지며, 일반적으로 256-512 토큰이 좋은 출발점입니다
- 벡터 DB는 프로토타이핑(ChromaDB)부터 프로덕션(Pinecone, Qdrant)까지 용도에 맞게 선택합니다
- RAG 시스템의 성능 병목은 대부분 검색 단계에 있으며, 검색 품질 개선이 최우선 과제입니다
