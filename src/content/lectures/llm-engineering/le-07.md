# LLM 평가

## 왜 LLM 평가가 중요한가

LLM의 출력은 확률적이고 다양하기 때문에, 전통적인 머신러닝 평가 지표(정확도, F1)만으로는 품질을 측정하기 어렵습니다. 파인튜닝([le-05](/lectures/llm-engineering/le-05)), 정렬([le-06](/lectures/llm-engineering/le-06)), 프롬프트 변경([le-02](/lectures/llm-engineering/le-02)) 이후 "정말 나아졌는가"를 체계적으로 판단하지 못하면, 프로덕션에서 예기치 못한 성능 저하가 발생합니다. 이 강의에서는 벤치마크, 자동 평가, LLM-as-Judge, 인간 평가를 포괄하는 평가 프레임워크를 다룹니다.

## 1. 평가의 차원

LLM 평가는 단일 지표로 환원할 수 없으며, 다차원적으로 접근해야 합니다.

| 평가 차원 | 측정 대상 | 대표 지표 |
|-----------|----------|----------|
| 정확성 | 사실에 부합하는가 | 팩트 체크, Hallucination Rate |
| 유용성 | 사용자 요구를 충족하는가 | Task Completion Rate |
| 안전성 | 유해하지 않은가 | Toxicity Score, Refusal Rate |
| 일관성 | 동일 입력에 안정적인가 | Response Variance |
| 형식 준수 | 지정 형식을 따르는가 | Format Compliance Rate |
| 지연 시간 | 충분히 빠른가 | TTFT, TPS |

> **핵심 직관**: "좋은 LLM"의 정의는 도메인과 용도에 따라 완전히 달라집니다. 의료 AI에서는 안전성이 최우선이고, 코딩 어시스턴트에서는 정확성이, 챗봇에서는 유용성이 핵심입니다. 평가 설계는 곧 제품 요구사항의 정량화입니다.

## 2. 벤치마크 평가

### 주요 벤치마크

| 벤치마크 | 영역 | 형식 | 한계 |
|---------|------|------|------|
| MMLU | 다분야 지식 | 4지선다 | 암기 측정에 가까움 |
| HumanEval | 코딩 | 코드 생성 | Python 편향 |
| GSM8K | 수학 추론 | 서술형 | 초등 수준 |
| TruthfulQA | 사실성 | 생성/선택 | 정적 데이터셋 |
| MT-Bench | 대화 품질 | 멀티턴 | GPT-4 기반 평가 의존 |
| KMMLU | 한국어 지식 | 4지선다 | 범위 제한 |

```python
# lm-evaluation-harness로 벤치마크 실행
# pip install lm-eval
import subprocess

# MMLU 벤치마크 실행
subprocess.run([
    "lm_eval", "--model", "hf",
    "--model_args", "pretrained=meta-llama/Llama-3.1-8B-Instruct",
    "--tasks", "mmlu",
    "--batch_size", "8",
    "--output_path", "./eval_results",
])
```

> **핵심 직관**: 벤치마크 점수는 "모델의 일반적 능력"을 보여주지만, "내 프로덕트에서의 성능"을 보장하지 않습니다. 벤치마크는 모델 후보군 선별(shortlisting)에 사용하고, 최종 판단은 도메인 특화 평가로 해야 합니다.

## 3. 자동 평가 지표

### 전통적 NLP 지표

| 지표 | 수식 | 적용 | 한계 |
|------|------|------|------|
| BLEU | n-gram precision | 번역 | 의미 무시 |
| ROUGE | n-gram recall | 요약 | 동의어 미반영 |
| BERTScore | 임베딩 코사인 유사도 | 범용 | 계산 비용 |
| Exact Match | 정확 일치 | QA | 너무 엄격 |

```python
# BERTScore를 활용한 자동 평가
from bert_score import score

references = ["서울은 대한민국의 수도입니다."]
candidates = ["대한민국의 수도는 서울입니다."]

P, R, F1 = score(candidates, references, lang="ko", model_type="bert-base-multilingual-cased")
print(f"BERTScore F1: {F1.mean():.4f}")  # ~0.95 (의미 보존 반영)
```

### 태스크별 평가 전략

```
[평가 대상 태스크]
       │
  ┌────▼────────┐
  │ 정답이      │
  │ 존재하는가?  │
  └────┬────────┘
   Yes │       No
  ┌────▼────┐ ┌──▼──────────────┐
  │EM/F1   │ │ 생성 품질 평가    │
  │BLEU    │ │ 필요             │
  │ROUGE   │ └──┬──────────────┘
  └────────┘    │
           ┌────▼────────┐
           │ 인간 평가    │
           │ 가능한가?    │
           └────┬────────┘
            Yes │       No
           ┌────▼────┐ ┌──▼──────────┐
           │ 인간 평가│ │LLM-as-Judge │
           │ + 샘플링 │ │ 적용        │
           └─────────┘ └─────────────┘
```

## 4. LLM-as-Judge

대규모 평가에서 인간 평가의 비용을 줄이기 위해, 강력한 LLM을 평가자로 활용합니다.

```python
from openai import OpenAI

client = OpenAI()

def llm_judge(question: str, answer: str, criteria: list[str]) -> dict:
    criteria_text = "\n".join(f"- {c}" for c in criteria)
    prompt = f"""다음 질문-답변 쌍을 아래 기준으로 1-5점 척도로 평가하세요.

질문: {question}
답변: {answer}

평가 기준:
{criteria_text}

각 기준별 점수와 근거를 JSON으로 출력하세요.
{{"scores": {{"기준명": {{"score": 점수, "reason": "근거"}}}}, "overall": 종합점수}}"""

    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[{"role": "user", "content": prompt}],
        response_format={"type": "json_object"},
    )
    return response.choices[0].message.content

# 사용 예시
result = llm_judge(
    question="RAG의 장점을 설명하세요.",
    answer="RAG는 외부 지식을 활용하여 환각을 줄입니다.",
    criteria=["정확성", "완전성", "명확성"],
)
```

### LLM-as-Judge의 편향

| 편향 유형 | 설명 | 완화 방법 |
|-----------|------|----------|
| 위치 편향 | 첫 번째 응답을 선호 | 순서 랜덤화 |
| 장문 편향 | 긴 응답을 선호 | 길이 정규화 |
| 자기 선호 | 동일 모델 출력 선호 | 다른 모델로 평가 |
| 형식 편향 | 정돈된 형식 선호 | 내용 중심 평가 지시 |

## 5. 평가 프레임워크 구축

### 시나리오 1: RAG 시스템 평가

[le-03](/lectures/llm-engineering/le-03)에서 구축한 RAG 시스템의 품질을 체계적으로 평가합니다. 검색 품질(Retrieval)과 생성 품질(Generation)을 분리하여 측정합니다.

```python
# RAG 평가 프레임워크 (RAGAS 방식)
def evaluate_rag(question: str, retrieved_contexts: list[str],
                 generated_answer: str, ground_truth: str) -> dict:
    return {
        "context_relevancy": score_relevancy(question, retrieved_contexts),
        "faithfulness": score_faithfulness(generated_answer, retrieved_contexts),
        "answer_relevancy": score_relevancy(question, [generated_answer]),
        "answer_correctness": score_correctness(generated_answer, ground_truth),
    }
```

### 시나리오 2: 파인튜닝 전후 비교

SFT([le-05](/lectures/llm-engineering/le-05)) 전후 성능을 비교합니다. 100개의 도메인 테스트 질문을 고정하고, 기본 모델과 파인튜닝 모델의 응답을 blind A/B 테스트합니다. LLM-as-Judge로 1차 필터링 후, 상위/하위 20%에 대해 인간 평가를 수행하는 하이브리드 전략이 비용 효율적입니다.

```python
# A/B 테스트 자동화
import random

def ab_test(questions: list[str], model_a, model_b, judge_fn) -> dict:
    results = {"a_wins": 0, "b_wins": 0, "tie": 0}
    for q in questions:
        ans_a, ans_b = model_a(q), model_b(q)
        # 순서 편향 방지를 위한 랜덤화
        if random.random() > 0.5:
            ans_a, ans_b = ans_b, ans_a
            swapped = True
        else:
            swapped = False
        winner = judge_fn(q, ans_a, ans_b)
        if swapped:
            winner = "b" if winner == "a" else ("a" if winner == "b" else "tie")
        results[f"{winner}_wins"] += 1
    return results
```

> **핵심 직관**: 평가 시스템 구축에 전체 개발 시간의 20-30%를 투자하는 것이 적절합니다. "측정하지 않으면 개선할 수 없다"는 원칙은 LLM 엔지니어링에서 특히 중요합니다. 평가가 곧 개발 속도를 결정합니다.

## 핵심 정리

- LLM 평가는 정확성, 유용성, 안전성, 일관성, 형식 준수, 지연 시간의 다차원적 접근이 필요합니다
- 벤치마크(MMLU, HumanEval 등)는 모델 후보군 선별에 유용하지만, 프로덕션 성능을 보장하지 않습니다
- LLM-as-Judge는 대규모 평가의 비용을 줄여주지만, 위치 편향과 장문 편향 등을 반드시 완화해야 합니다
- RAG 시스템은 검색 품질과 생성 품질을 분리하여 평가해야 병목 지점을 정확히 진단할 수 있습니다
- 최적의 평가 전략은 자동 평가로 1차 필터링 후 인간 평가로 최종 검증하는 하이브리드 방식입니다
