# LLM 애플리케이션 설계

## 왜 설계 패턴이 중요한가

LLM을 활용한 프로토타입은 빠르게 만들 수 있지만, 프로덕션 수준의 서비스로 전환하려면 체계적인 아키텍처 설계가 필수입니다. 비용 폭주, 장애 전파, 성능 저하 등의 문제는 대부분 설계 단계에서 예방할 수 있습니다. 이 강의에서는 프로덕션 LLM 애플리케이션의 아키텍처 패턴, 비용 최적화, 캐싱, 폴백 전략, 그리고 배포 전 체크리스트를 다룹니다.

## 1. 아키텍처 패턴

### LLM 게이트웨이 패턴

모든 LLM 호출을 게이트웨이를 통해 중앙 관리합니다.

```
[클라이언트]
     │
     ▼
┌──────────────────────────────────────┐
│          LLM 게이트웨이               │
│  ┌──────┐ ┌──────┐ ┌──────┐         │
│  │캐싱  │ │로깅  │ │레이트│         │
│  │      │ │      │ │리밋  │         │
│  └──┬───┘ └──┬───┘ └──┬───┘         │
│     └────────┴────────┘              │
│              │                       │
│  ┌───────────▼──────────────┐        │
│  │    라우터 (모델 선택)      │        │
│  └───┬──────┬──────┬────────┘        │
│      │      │      │                 │
│   ┌──▼──┐┌──▼──┐┌──▼──┐             │
│   │GPT-4││Claude││Local│             │
│   │  o  ││ 3.5 ││ LLM │             │
│   └─────┘└─────┘└─────┘             │
└──────────────────────────────────────┘
```

```python
# LLM 게이트웨이 구현 (LiteLLM 활용)
import litellm

class LLMGateway:
    def __init__(self):
        self.cache = {}
        self.request_count = 0

    def route(self, prompt: str, complexity: str = "auto") -> str:
        """복잡도에 따라 적절한 모델로 라우팅"""
        model_map = {
            "low": "gpt-4o-mini",      # 간단한 분류, 추출
            "medium": "gpt-4o",         # 일반 생성
            "high": "claude-sonnet-4-20250514",  # 복잡한 추론
        }
        if complexity == "auto":
            complexity = self._estimate_complexity(prompt)
        model = model_map.get(complexity, "gpt-4o")
        return litellm.completion(model=model, messages=[{"role": "user", "content": prompt}])

    def _estimate_complexity(self, prompt: str) -> str:
        """프롬프트 복잡도 자동 추정"""
        if len(prompt) < 100:
            return "low"
        elif len(prompt) < 500:
            return "medium"
        return "high"
```

> **핵심 직관**: LLM 게이트웨이는 마이크로서비스 아키텍처의 API 게이트웨이와 같은 역할을 합니다. 로깅, 캐싱, 라우팅, 폴백을 한 곳에서 관리함으로써 개별 서비스는 LLM 호출의 복잡성에서 자유로워집니다.

## 2. 비용 최적화

### 비용 구조 분석

| 비용 요소 | 비중 | 최적화 방법 |
|-----------|------|-----------|
| 입력 토큰 | 20-40% | 프롬프트 압축, 불필요한 컨텍스트 제거 |
| 출력 토큰 | 40-60% | max_tokens 제한, 간결한 응답 유도 |
| 모델 선택 | 10-30% | 태스크별 적정 모델 선택 |
| 재시도 | 5-10% | 캐싱, 에러 핸들링 |

$$\text{월간 비용} = \text{일 요청 수} \times 30 \times (\text{입력 토큰} \times P_\text{in} + \text{출력 토큰} \times P_\text{out})$$

```python
# 비용 추적 및 최적화
from openai import OpenAI

client = OpenAI()

class CostTracker:
    PRICING = {  # USD per 1M tokens
        "gpt-4o": {"input": 2.50, "output": 10.00},
        "gpt-4o-mini": {"input": 0.15, "output": 0.60},
        "claude-sonnet-4-20250514": {"input": 3.00, "output": 15.00},
    }

    def __init__(self):
        self.total_cost = 0.0
        self.call_log = []

    def track(self, model: str, usage: dict) -> float:
        pricing = self.PRICING.get(model, {"input": 0, "output": 0})
        cost = (usage["prompt_tokens"] * pricing["input"]
                + usage["completion_tokens"] * pricing["output"]) / 1_000_000
        self.total_cost += cost
        self.call_log.append({"model": model, "cost": cost, "tokens": usage})
        return cost
```

### 모델 계층화 전략

```
[요청 처리 흐름]

[사용자 요청] ──▶ [분류기 (gpt-4o-mini)]
                         │
              ┌──────────┼──────────┐
              ▼          ▼          ▼
         [단순 FAQ]  [일반 질문]  [복잡한 분석]
              │          │          │
              ▼          ▼          ▼
         [캐시/규칙] [gpt-4o-mini] [gpt-4o]
         비용: $0    비용: $0.001   비용: $0.01
```

## 3. 캐싱 전략

### 시맨틱 캐싱

동일하거나 유사한 질문에 대해 이전 응답을 재사용합니다.

```python
import hashlib
import numpy as np
from openai import OpenAI

client = OpenAI()

class SemanticCache:
    def __init__(self, similarity_threshold: float = 0.95):
        self.cache = {}          # hash -> response
        self.embeddings = {}     # hash -> embedding
        self.threshold = similarity_threshold

    def _get_embedding(self, text: str) -> list[float]:
        response = client.embeddings.create(model="text-embedding-3-small", input=text)
        return response.data[0].embedding

    def get(self, query: str) -> str | None:
        query_emb = self._get_embedding(query)
        for key, cached_emb in self.embeddings.items():
            similarity = np.dot(query_emb, cached_emb) / (
                np.linalg.norm(query_emb) * np.linalg.norm(cached_emb)
            )
            if similarity >= self.threshold:
                return self.cache[key]
        return None

    def set(self, query: str, response: str) -> None:
        key = hashlib.md5(query.encode()).hexdigest()
        self.cache[key] = response
        self.embeddings[key] = self._get_embedding(query)
```

| 캐싱 전략 | 히트율 | 구현 복잡도 | 적합 시나리오 |
|-----------|--------|-----------|-------------|
| 정확 매칭 | 낮음 | 매우 간단 | 정형화된 쿼리 |
| 시맨틱 캐싱 | 높음 | 중간 | FAQ, 고객 질문 |
| 프롬프트 + 파라미터 캐싱 | 중간 | 간단 | 동일 설정 반복 |

> **핵심 직관**: 시맨틱 캐싱은 "서울 날씨"와 "서울 오늘 기온"을 같은 질문으로 인식하여 캐시 히트율을 크게 높입니다. 임베딩 비용이 LLM 호출 비용의 1/100 수준이므로, 캐시 히트율이 5%만 되어도 투자 대비 효과가 있습니다.

## 4. 폴백 전략

```
[LLM 호출 실패 시 폴백 전략]

[Primary Model: GPT-4o]
         │
    실패 (타임아웃/에러/레이트리밋)
         │
         ▼
[Fallback 1: Claude 3.5 Sonnet]
         │
    실패
         │
         ▼
[Fallback 2: 로컬 모델 (Llama 3.1)]
         │
    실패
         │
         ▼
[Fallback 3: 캐시된 유사 응답]
         │
    없음
         │
         ▼
[사용자에게 오류 메시지 + 재시도 안내]
```

```python
import asyncio
from openai import OpenAI, APIError, RateLimitError

class ResilientLLM:
    def __init__(self):
        self.models = [
            {"provider": "openai", "model": "gpt-4o"},
            {"provider": "anthropic", "model": "claude-sonnet-4-20250514"},
            {"provider": "local", "model": "llama-3.1-8b"},
        ]
        self.cache = SemanticCache()

    def generate(self, prompt: str, max_retries: int = 3) -> str:
        # 캐시 확인
        cached = self.cache.get(prompt)
        if cached:
            return cached

        # 모델 폴백 체인
        for model_config in self.models:
            for attempt in range(max_retries):
                try:
                    response = self._call_model(model_config, prompt)
                    self.cache.set(prompt, response)
                    return response
                except RateLimitError:
                    wait = 2 ** attempt
                    asyncio.sleep(wait)
                except APIError:
                    break  # 다음 모델로 폴백

        return "죄송합니다. 일시적인 오류가 발생했습니다. 잠시 후 다시 시도해주세요."
```

## 5. 프로덕션 체크리스트

### 배포 전 필수 확인 항목

| 카테고리 | 항목 | 확인 |
|---------|------|------|
| **성능** | 응답 지연 시간 P95 < 목표값 | [ ] |
| | 동시 요청 처리 능력 확인 | [ ] |
| | 토큰 사용량 모니터링 구축 | [ ] |
| **안전성** | 프롬프트 인젝션 방어 ([le-11](/lectures/llm-engineering/le-11)) | [ ] |
| | 출력 필터링 구현 | [ ] |
| | PII 마스킹 적용 | [ ] |
| **비용** | 비용 상한선 설정 | [ ] |
| | 모델 계층화 적용 | [ ] |
| | 캐싱 구현 | [ ] |
| **운영** | 로그 수집 및 분석 ([mo-05](/lectures/mlops/mo-05)) | [ ] |
| | 폴백 체인 구성 | [ ] |
| | A/B 테스트 인프라 ([le-07](/lectures/llm-engineering/le-07)) | [ ] |
| **평가** | 자동 평가 파이프라인 구축 | [ ] |
| | 레드팀 테스트 완료 | [ ] |
| | 사용자 피드백 수집 경로 확보 | [ ] |

### 시나리오 1: B2B SaaS AI 어시스턴트

기업 고객 대상 AI 어시스턴트 서비스를 구축합니다. 유료 플랜 사용자에게는 GPT-4o로, 무료 플랜에게는 GPT-4o-mini로 라우팅합니다. 시맨틱 캐싱으로 반복 질문 비용을 70% 절감하고, 월 API 비용 상한을 고객별로 설정합니다. 폴백 체인에 자체 파인튜닝 모델([le-05](/lectures/llm-engineering/le-05))을 포함하여 API 장애에도 서비스를 유지합니다.

### 시나리오 2: 실시간 고객 채팅 서비스

이커머스 실시간 채팅에서 응답 지연 P95를 3초 이내로 유지해야 합니다. 스트리밍 응답(SSE)으로 체감 지연을 줄이고, 첫 토큰 지연(TTFT)을 최적화합니다. RAG([le-03](/lectures/llm-engineering/le-03))와 에이전트([le-08](/lectures/llm-engineering/le-08))를 결합하여 주문 조회, 환불 등을 처리하되, 추론 최적화([le-10](/lectures/llm-engineering/le-10))로 서빙 비용을 관리합니다.

> **핵심 직관**: 프로덕션 LLM 서비스는 "LLM 호출"이 전체의 30%이고, 나머지 70%는 캐싱, 폴백, 모니터링, 비용 관리 등의 엔지니어링입니다. 이 70%가 서비스의 안정성과 수익성을 결정합니다.

## 핵심 정리

- LLM 게이트웨이 패턴은 캐싱, 로깅, 라우팅, 폴백을 중앙에서 관리하여 시스템 복잡도를 줄입니다
- 모델 계층화(복잡도별 모델 분배)로 품질 손실 없이 LLM API 비용을 50-70% 절감할 수 있습니다
- 시맨틱 캐싱은 유사 질문을 인식하여 캐시 히트율을 높이며, 임베딩 비용 대비 절감 효과가 큽니다
- 폴백 체인은 주 모델 장애 시 대체 모델과 캐시로 서비스 연속성을 보장합니다
- 프로덕션 배포 전 성능, 안전성, 비용, 운영, 평가의 5대 카테고리를 모두 점검해야 합니다
