# 정규화 방법

## 왜 정규화를 이해해야 하는가

신경망은 학습 데이터를 **완벽하게 외울 수 있을 만큼** 표현력이 큽니다. 랜덤 레이블도 외울 수 있습니다. **정규화(Regularization)**는 이 과도한 표현력을 제한하여 일반화를 유도합니다. Dropout, Weight Decay, Data Augmentation, Label Smoothing 등 각 기법이 **어떤 메커니즘으로** 과적합을 방지하는지 이해합니다.

---

## 1. 과적합의 본질

$$
\text{일반화 오차} = \text{학습 오차} + \text{일반화 갭}
$$

정규화는 학습 오차를 약간 올리더라도 **일반화 갭을 크게 줄여** 총 오차를 낮춥니다.

### Bias-Variance 트레이드오프

| 용어 | 의미 | 정규화 효과 |
|------|------|-----------|
| Bias | 모델의 체계적 오차 | 약간 증가 |
| Variance | 데이터 변화에 대한 민감도 | **크게 감소** |

> **핵심 직관**: 정규화는 모델에 "좀 더 단순해져라"라고 말하는 것입니다. 복잡한 패턴보다 단순한 패턴을 선호하도록 유도합니다.

---

## 2. Weight Decay (L2 정규화)

### 수식

손실에 가중치 크기의 벌칙을 추가합니다:

$$
L_{total} = L_{data} + \frac{\lambda}{2}\sum_i w_i^2
$$

그래디언트 업데이트:

$$
w \leftarrow w - \eta \frac{\partial L_{data}}{\partial w} - \eta\lambda w = (1 - \eta\lambda)w - \eta \frac{\partial L_{data}}{\partial w}
$$

$(1 - \eta\lambda)$가 가중치를 조금씩 줄입니다 → "weight decay".

### L2 vs Decoupled Weight Decay

Adam 옵티마이저에서 L2 정규화와 weight decay는 다릅니다:

```python
# L2 정규화: 그래디언트에 포함
# AdamW: weight decay를 그래디언트와 분리 (더 나음)
optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=0.01)
```

AdamW가 표준인 이유: Adam의 적응적 학습률이 L2 벌칙을 왜곡하기 때문입니다. Decoupled weight decay는 이 문제를 해결합니다.

---

## 3. Dropout

### 메커니즘

학습 시 각 뉴런을 확률 $p$로 **랜덤하게 비활성화**합니다.

$$
\hat{y}_i = \frac{m_i}{1-p} \cdot y_i, \quad m_i \sim \text{Bernoulli}(1-p)
$$

$1/(1-p)$ 스케일링은 추론 시 기대값을 맞추기 위함입니다.

```python
dropout = nn.Dropout(p=0.1)

# 학습 시: 10%의 뉴런을 랜덤하게 0으로 + 나머지를 1/0.9배
model.train()
output = dropout(hidden)

# 추론 시: 모든 뉴런 사용 (dropout 비활성화)
model.eval()
output = dropout(hidden)  # 아무 일도 안 함
```

### 왜 작동하는가

1. **앙상블 효과**: 매 학습 스텝마다 다른 서브네트워크를 학습 → 지수적으로 많은 모델의 암묵적 앙상블
2. **공동 적응 방지**: 뉴런이 특정 다른 뉴런에 의존하지 못함 → 각 뉴런이 독립적으로 유용한 특성 학습
3. **잡음 주입**: 학습에 랜덤성 추가 → 더 강건한 표현

### Transformer에서의 Dropout

```python
# 어텐션 가중치에 dropout
attn_weights = F.softmax(scores, dim=-1)
attn_weights = dropout(attn_weights)  # 일부 어텐션 연결 차단

# 잔차 연결 전에 dropout
x = x + dropout(attention_output)
x = x + dropout(ffn_output)
```

---

## 4. Data Augmentation

학습 데이터를 **변형하여** 가상의 새 데이터를 만듭니다.

### 이미지

```python
from torchvision import transforms

augmentation = transforms.Compose([
    transforms.RandomHorizontalFlip(p=0.5),
    transforms.RandomRotation(degrees=15),
    transforms.ColorJitter(brightness=0.2, contrast=0.2),
    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),
    transforms.RandomErasing(p=0.1),
])
```

### 텍스트 (LLM)

- **토큰 마스킹**: 일부 토큰을 [MASK]로 대체 (BERT)
- **백번역**: 다른 언어로 번역 후 다시 번역
- **패러프레이즈**: 같은 의미의 다른 표현으로 변환

### Mixup과 CutMix

```python
# Mixup: 두 이미지와 레이블을 선형 보간
lam = np.random.beta(alpha, alpha)
x_mixed = lam * x1 + (1 - lam) * x2
y_mixed = lam * y1 + (1 - lam) * y2

# CutMix: 이미지의 일부 영역을 다른 이미지로 대체
# 레이블은 면적 비율로 혼합
```

---

## 5. Stochastic Depth

dl-09에서 간략히 다뤘던 내용입니다. 학습 시 잔차 블록을 확률적으로 건너뜁니다.

$$
x_{l+1} = \begin{cases} F_l(x_l) + x_l & \text{확률 } p_l \\ x_l & \text{확률 } 1 - p_l \end{cases}
$$

깊은 층일수록 건너뛸 확률을 높입니다 (선형 스케줄). EfficientNet, DeiT 등에서 사용합니다.

---

## 6. Early Stopping

검증 손실이 더 이상 개선되지 않으면 학습을 중단합니다.

```python
best_val_loss = float('inf')
patience = 5
counter = 0

for epoch in range(max_epochs):
    train_loss = train_one_epoch(model)
    val_loss = evaluate(model)

    if val_loss < best_val_loss:
        best_val_loss = val_loss
        counter = 0
        save_checkpoint(model)
    else:
        counter += 1
        if counter >= patience:
            print("Early stopping!")
            break
```

> **핵심 직관**: Early stopping은 "암묵적 정규화"입니다. 학습 시간 자체가 모델 복잡도를 제한합니다. 오래 학습할수록 더 복잡한 패턴을 외우므로.

---

## 7. 정규화 기법 조합

현대 모델은 여러 정규화를 동시에 사용합니다:

| 기법 | Transformer (LLM) | CNN (ResNet) |
|------|-------------------|-------------|
| Weight Decay | AdamW (0.01~0.1) | SGD + WD |
| Dropout | 0.0~0.1 | 거의 안 씀 |
| Data Aug | 토큰 마스킹 | 강한 증강 |
| Label Smooth | 0.1 | 선택적 |
| Stochastic Depth | 선택적 | EfficientNet |
| Early Stopping | 보통 안 씀 (고정 스텝) | 종종 사용 |

### 주의: 과도한 정규화

정규화를 너무 많이 하면 **과소적합(underfitting)**이 발생합니다. 학습 데이터에서조차 성능이 나쁩니다.

---

## 핵심 정리

1. **Weight Decay**는 가중치 크기에 벌칙을 주어 단순한 해를 선호하게 하며, Adam에서는 **AdamW**(decoupled)를 사용해야 한다.
2. **Dropout**은 뉴런을 랜덤하게 비활성화하여 **암묵적 앙상블** 효과를 내고 공동 적응을 방지한다.
3. **Data Augmentation**은 학습 데이터를 변형하여 가상의 새 데이터를 만들며, 일반화에 가장 직접적인 효과가 있다.
4. **Label Smoothing**은 과도한 확신을 방지하고, **Stochastic Depth**는 깊은 네트워크에서 블록을 랜덤하게 건너뛴다.
5. 정규화 기법들은 **조합하여** 사용하되, 과도하면 과소적합이 발생할 수 있다.
