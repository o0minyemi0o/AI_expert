# 효율적 어텐션

## 왜 효율적 어텐션이 필요한가

표준 어텐션의 계산과 메모리 복잡도는 시퀀스 길이 $T$에 대해 $O(T^2)$입니다. GPT-4의 128K 토큰, Claude의 200K 토큰처럼 긴 컨텍스트를 지원하려면 이 이차 비용을 해결해야 합니다. **Flash Attention**, **Multi-Query Attention**, **Grouped-Query Attention**, **KV Cache**는 현대 LLM의 실전 배포에 필수적인 기술입니다.

---

## 1. 표준 어텐션의 병목

### 메모리 문제

```python
# 표준 어텐션
S = Q @ K.T        # (T, T) — T²개의 float 저장
A = softmax(S)     # (T, T) — 또 T²개 저장
O = A @ V          # (T, d)
```

$T = 128{,}000$이면 어텐션 행렬만 $128K \times 128K \times 4$ bytes $\approx$ **64 GB**입니다. 단일 GPU에 올릴 수 없습니다.

### 계산 vs 메모리

현대 GPU는 계산보다 **메모리 대역폭**이 병목입니다. GPU의 SRAM(빠른 메모리)은 수십 MB뿐이고, HBM(느린 메모리)은 수십 GB입니다. 표준 어텐션은 HBM에 반복적으로 읽고 써야 합니다.

```
HBM (느림, 큼)  ←→  SRAM (빠름, 작음)  ←→  계산 유닛
80 GB              20 MB                 312 TFLOPS
2 TB/s             19 TB/s
```

---

## 2. Flash Attention

### 핵심 아이디어: 타일링 + 온라인 softmax

Flash Attention은 어텐션을 **블록 단위**로 계산하여, $T \times T$ 행렬을 **한 번도 전체 생성하지 않습니다**.

### 알고리즘 개요

1. Q, K, V를 블록(타일)으로 분할
2. 각 블록 쌍에 대해 SRAM에서 부분 어텐션 계산
3. **온라인 softmax** 알고리즘으로 블록 간 결과를 점진적으로 합산
4. 최종 결과만 HBM에 기록

```
표준 어텐션:  Q, K → HBM에 S 저장 → softmax → HBM에 A 저장 → A @ V
Flash Attention: Q, K, V 블록을 SRAM에서 직접 계산 → 결과만 HBM에 기록
```

### 온라인 Softmax

softmax를 한 번에 계산하지 않고, 블록씩 누적합니다:

$$
m_{\text{new}} = \max(m_{\text{old}}, m_{\text{block}})
$$
$$
\ell_{\text{new}} = e^{m_{\text{old}} - m_{\text{new}}} \cdot \ell_{\text{old}} + e^{m_{\text{block}} - m_{\text{new}}} \cdot \ell_{\text{block}}
$$

이전 블록의 결과를 새 최대값으로 보정하면서 누적합니다.

### 효과

| | 표준 어텐션 | Flash Attention |
|------|-----------|----------------|
| HBM 읽기/쓰기 | $O(T^2)$ | $O(T^2 d / M)$ ($M$: SRAM 크기) |
| 메모리 사용 | $O(T^2)$ | **$O(T)$** |
| 실제 속도 | 1x | **2-4x 빠름** |

> **핵심 직관**: Flash Attention은 수학적으로 표준 어텐션과 **정확히 같은 결과**를 냅니다. 근사가 아닙니다. 메모리 접근 패턴만 최적화합니다.

---

## 3. Multi-Query Attention (MQA)

### 문제: 추론 시 KV Cache 메모리

자기회귀 생성에서 이전 토큰의 K, V를 저장하는 **KV Cache**가 필요합니다. Multi-Head Attention에서는 헤드 수만큼의 K, V를 저장해야 합니다.

$$
\text{KV Cache 크기} = 2 \times n_{\text{layers}} \times n_{\text{heads}} \times T \times d_k \times \text{bytes}
$$

### MQA의 해결

**모든 헤드가 하나의 K, V를 공유**합니다. Q만 헤드별로 다릅니다.

```python
class MultiQueryAttention(nn.Module):
    def __init__(self, d_model, n_heads):
        super().__init__()
        self.d_k = d_model // n_heads
        self.n_heads = n_heads

        self.W_q = nn.Linear(d_model, d_model)      # 헤드별 Q
        self.W_k = nn.Linear(d_model, self.d_k)      # 공유 K (1개)
        self.W_v = nn.Linear(d_model, self.d_k)      # 공유 V (1개)
        self.W_o = nn.Linear(d_model, d_model)

    def forward(self, x):
        B, T, D = x.shape
        Q = self.W_q(x).view(B, T, self.n_heads, self.d_k).transpose(1, 2)
        K = self.W_k(x).unsqueeze(1).expand(-1, self.n_heads, -1, -1)
        V = self.W_v(x).unsqueeze(1).expand(-1, self.n_heads, -1, -1)

        scores = Q @ K.transpose(-2, -1) / (self.d_k ** 0.5)
        attn = F.softmax(scores, dim=-1)
        out = (attn @ V).transpose(1, 2).contiguous().view(B, T, D)
        return self.W_o(out)
```

### 효과

| | Multi-Head | Multi-Query |
|------|-----------|-------------|
| K, V 파라미터 | $h \times d_k \times d$ | $d_k \times d$ |
| KV Cache 크기 | $h$배 | **1배** |
| 품질 | 기준 | 약간 하락 |

---

## 4. Grouped-Query Attention (GQA)

MQA와 MHA의 **절충안**입니다. K, V를 여러 헤드가 아닌 **여러 그룹**이 공유합니다.

```
MHA:  Q₁K₁V₁  Q₂K₂V₂  Q₃K₃V₃  Q₄K₄V₄  (각 헤드마다 고유 KV)
GQA:  Q₁K₁V₁  Q₂K₁V₁  Q₃K₂V₂  Q₄K₂V₂  (2그룹, 그룹당 2헤드)
MQA:  Q₁K₁V₁  Q₂K₁V₁  Q₃K₁V₁  Q₄K₁V₁  (1그룹)
```

LLaMA 2 (70B)에서 도입되어 현대 LLM의 표준이 되었습니다.

| | MHA | GQA | MQA |
|------|-----|-----|-----|
| KV 헤드 수 | $h$ | $g$ (보통 $h/4$ ~ $h/8$) | 1 |
| KV Cache | 1x | $g/h$배 | $1/h$배 |
| 품질 | 최고 | MHA에 근접 | 약간 하락 |

---

## 5. KV Cache

### 자기회귀 생성의 비효율

```python
# 매 토큰 생성마다 전체 시퀀스를 다시 계산?
for i in range(max_new_tokens):
    logits = model(all_tokens[:i+1])  # 이전 토큰도 다시 계산!
    next_token = sample(logits[:, -1])
```

### KV Cache로 해결

```python
# 이전 토큰의 K, V를 캐시하여 재사용
kv_cache = {}

for i in range(max_new_tokens):
    if i == 0:
        # 첫 토큰: 전체 프롬프트 처리
        logits, kv_cache = model(prompt, kv_cache=None)
    else:
        # 이후: 새 토큰 하나만 처리
        logits, kv_cache = model(new_token, kv_cache=kv_cache)
    next_token = sample(logits[:, -1])
```

### 내부 동작

```python
def attention_with_cache(q, k, v, kv_cache):
    if kv_cache is not None:
        # 캐시된 K, V에 새 토큰의 K, V를 추가
        k = torch.cat([kv_cache["k"], k], dim=-2)
        v = torch.cat([kv_cache["v"], v], dim=-2)

    # 새 토큰의 Q만으로 전체 시퀀스에 어텐션
    scores = q @ k.transpose(-2, -1) / scale
    attn = softmax(scores)
    output = attn @ v

    return output, {"k": k, "v": v}
```

### Prefill vs Decode

| 단계 | 입력 | 연산 | 병목 |
|------|------|------|------|
| **Prefill** | 전체 프롬프트 | 모든 토큰의 KV 계산 | 계산(compute-bound) |
| **Decode** | 토큰 1개씩 | 캐시 조회 + 새 토큰 처리 | 메모리(memory-bound) |

---

## 6. 기타 효율적 어텐션

### Sliding Window Attention

각 토큰이 고정 크기 윈도우 내의 토큰만 참조합니다. Mistral에서 사용합니다.

$$
\text{Attention}(q_i) = \text{softmax}\left(\frac{q_i K_{[i-w:i]}^T}{\sqrt{d_k}}\right) V_{[i-w:i]}
$$

복잡도: $O(T \cdot w)$ (선형)

### PagedAttention (vLLM)

KV Cache를 OS의 가상 메모리처럼 **페이지 단위**로 관리합니다. 메모리 단편화를 줄여 배치 처리 효율을 높입니다.

### Ring Attention

시퀀스를 여러 GPU에 분산하고, KV 블록을 **링 형태**로 전달하며 어텐션을 계산합니다. 매우 긴 시퀀스(1M+ 토큰)를 처리할 수 있습니다.

---

## 7. ML에서의 의미

### 실전 배포

Flash Attention 2/3은 PyTorch에 통합되어 있으며, `torch.nn.functional.scaled_dot_product_attention`을 통해 자동으로 활용됩니다.

```python
# PyTorch 2.0+
output = F.scaled_dot_product_attention(Q, K, V, is_causal=True)
# 자동으로 Flash Attention 또는 Memory Efficient Attention 선택
```

### 추론 최적화 스택

```
모델 → GQA (KV Cache 축소)
     → KV Cache (중복 계산 제거)
     → Flash Attention (메모리 최적화)
     → PagedAttention (배치 효율)
     → Quantization (메모리 추가 절약)
```

---

## 핵심 정리

1. 표준 어텐션의 $O(T^2)$ 메모리/계산 복잡도가 긴 시퀀스의 최대 병목이다.
2. **Flash Attention**은 타일링과 온라인 softmax로 HBM 접근을 최소화하며, 정확한 결과를 $O(T)$ 메모리로 계산한다.
3. **GQA**는 K, V 헤드를 그룹으로 공유하여 KV Cache 메모리를 줄이며, MHA에 근접한 품질을 유지한다.
4. **KV Cache**는 자기회귀 생성에서 이전 토큰의 K, V를 재사용하여 중복 계산을 제거한다.
5. 실전 LLM 배포는 Flash Attention + GQA + KV Cache + PagedAttention의 **조합**으로 효율을 극대화한다.
