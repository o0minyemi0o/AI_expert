# 활성화 함수

## 왜 활성화 함수를 깊이 이해해야 하는가

활성화 함수 없이는 아무리 많은 층을 쌓아도 **선형 변환의 합성은 선형 변환**입니다. 활성화 함수가 비선형성을 도입하여 신경망이 복잡한 함수를 표현할 수 있게 합니다. ReLU에서 GELU, SiLU/Swish로의 진화 과정을 이해하면, "왜 이 함수가 저 함수보다 나은가"를 수학적으로 설명할 수 있습니다.

---

## 1. 활성화 함수의 역할

선형 변환만 있다면:

$$
W_2(W_1 x + b_1) + b_2 = (W_2 W_1)x + (W_2 b_1 + b_2) = W'x + b'
$$

아무리 쌓아도 단 하나의 선형 변환과 동일합니다. **비선형 활성화가 표현력의 원천**입니다.

---

## 2. Sigmoid와 Tanh: 역사적 함수

### Sigmoid

$$
\sigma(x) = \frac{1}{1 + e^{-x}}, \quad \sigma'(x) = \sigma(x)(1-\sigma(x))
$$

- 출력 범위: $(0, 1)$
- 최대 미분값: $\sigma'(0) = 0.25$

### Tanh

$$
\tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}, \quad \tanh'(x) = 1 - \tanh^2(x)
$$

- 출력 범위: $(-1, 1)$
- 최대 미분값: $\tanh'(0) = 1$

### 포화 문제

두 함수 모두 $|x|$가 크면 미분이 0에 가까워집니다(**포화, saturation**). 10층만 거쳐도:
- Sigmoid: $0.25^{10} \approx 10^{-6}$
- 그래디언트가 사실상 0

이것이 **기울기 소실**의 직접적 원인이며, ReLU의 등장 배경입니다.

---

## 3. ReLU (Rectified Linear Unit)

$$
\text{ReLU}(x) = \max(0, x), \quad \text{ReLU}'(x) = \begin{cases} 1 & x > 0 \\ 0 & x \leq 0 \end{cases}
$$

### 장점

1. **포화 없음**: 양수 영역에서 미분이 상수 1 → 기울기 소실 완화
2. **계산 효율**: 지수 함수 없이 비교 연산만
3. **희소 활성화**: ~50%의 뉴런이 0 출력 → 효율적 표현

### 한계: Dead Neuron

$x < 0$이면 미분이 0이므로, 한 번 비활성화되면 영구히 학습되지 않을 수 있습니다.

```python
# Dead neuron 확인
with torch.no_grad():
    activations = model.relu_layer(test_input)
    dead_fraction = (activations == 0).float().mean()
    print(f"Dead neurons: {dead_fraction:.1%}")
```

---

## 4. ReLU 변형들

### Leaky ReLU

$$
\text{LeakyReLU}(x) = \begin{cases} x & x > 0 \\ \alpha x & x \leq 0 \end{cases}
$$

$\alpha = 0.01$ (보통). 음수 영역에서도 작은 기울기를 유지하여 dead neuron 방지.

### PReLU (Parametric ReLU)

$\alpha$를 학습 가능한 파라미터로 만듭니다.

### ELU (Exponential Linear Unit)

$$
\text{ELU}(x) = \begin{cases} x & x > 0 \\ \alpha(e^x - 1) & x \leq 0 \end{cases}
$$

음수 영역이 $-\alpha$로 포화되어 평균이 0에 가까워지는 self-normalizing 효과.

---

## 5. GELU (Gaussian Error Linear Unit)

$$
\text{GELU}(x) = x \cdot \Phi(x) = x \cdot \frac{1}{2}\left[1 + \text{erf}\left(\frac{x}{\sqrt{2}}\right)\right]
$$

$\Phi(x)$는 표준 정규분포의 CDF입니다.

### 근사

$$
\text{GELU}(x) \approx 0.5x\left(1 + \tanh\left[\sqrt{2/\pi}(x + 0.044715x^3)\right]\right)
$$

```python
# PyTorch에서
F.gelu(x)
# 근사 버전
F.gelu(x, approximate='tanh')
```

### 왜 GELU인가

- **부드러운 전환**: ReLU의 $x=0$에서의 불연속 미분 대신, 매끄럽게 0과 1 사이를 전이
- **확률적 해석**: "입력 $x$가 다른 뉴런보다 얼마나 큰지"에 따라 통과 비율 결정
- BERT, GPT, ViT 등 Transformer 계열의 표준 활성화

### ReLU vs GELU

| 특성 | ReLU | GELU |
|------|------|------|
| $x < 0$ | 정확히 0 | **거의 0 (부드럽게)** |
| $x = 0$ | 미분 불연속 | 미분 연속 |
| 음수 작은 값 | 무시 | **약간 통과** |
| 계산 비용 | 매우 낮음 | ReLU보다 높음 |

---

## 6. SiLU / Swish

$$
\text{SiLU}(x) = x \cdot \sigma(x) = \frac{x}{1 + e^{-x}}
$$

GELU와 형태가 매우 유사하지만, CDF 대신 sigmoid를 사용합니다.

### Swish (파라미터 버전)

$$
\text{Swish}_\beta(x) = x \cdot \sigma(\beta x)
$$

$\beta = 1$이면 SiLU와 동일합니다. $\beta \to \infty$이면 ReLU에 수렴합니다.

```python
F.silu(x)  # PyTorch에서
```

LLaMA, Mistral 등 현대 LLM이 SiLU를 사용합니다.

---

## 7. GLU 변형 (Gated Linear Units)

$$
\text{GLU}(x) = (xW_1) \odot \sigma(xW_2)
$$

입력의 한 변환이 **게이트** 역할을 하여 다른 변환을 통과시키거나 차단합니다.

### SwiGLU

$$
\text{SwiGLU}(x) = (\text{SiLU}(xW_1)) \odot (xW_3)
$$

LLaMA의 FFN에서 사용됩니다. GLU + SiLU의 조합입니다.

### GEGLU

$$
\text{GEGLU}(x) = (\text{GELU}(xW_1)) \odot (xW_3)
$$

게이트 메커니즘이 추가 파라미터를 사용하지만, 같은 총 파라미터에서 더 나은 성능을 보입니다.

---

## 8. 활성화 함수 선택 가이드

| 상황 | 추천 | 이유 |
|------|------|------|
| CNN (일반) | ReLU | 검증됨, 빠름 |
| Transformer (NLP) | GELU | BERT/GPT 표준 |
| 최신 LLM | SiLU + SwiGLU | LLaMA/Mistral 표준 |
| GAN | LeakyReLU | Dead neuron 방지 |
| 출력층 (분류) | Softmax | 확률 분포 |
| 출력층 (이진) | Sigmoid | $[0, 1]$ 범위 |
| 출력층 (회귀) | 없음 (Linear) | 범위 제한 불필요 |

---

## 9. ML에서의 의미

### 활성화 함수와 초기화의 관계

활성화 함수의 성질이 적절한 초기화를 결정합니다:
- ReLU 계열: **Kaiming 초기화** ($\text{Var} = 2/n_{\text{in}}$)
- Sigmoid/Tanh: **Xavier 초기화** ($\text{Var} = 2/(n_{\text{in}} + n_{\text{out}})$)

### Universal Approximation Theorem

단일 은닉층 + 비선형 활성화로 임의의 연속 함수를 근사할 수 있다는 정리입니다. 활성화 함수의 구체적 선택보다, 비선형성의 **존재** 자체가 핵심입니다.

---

## 핵심 정리

1. 활성화 함수는 **비선형성을 도입**하여 신경망이 선형 변환 이상을 표현할 수 있게 한다.
2. **Sigmoid/Tanh**는 포화로 인해 기울기 소실이 발생하며, **ReLU**는 양수 영역의 상수 기울기로 이를 해결했다.
3. **GELU**는 ReLU의 불연속성을 부드럽게 하며, Transformer의 표준 활성화이다.
4. **SiLU/Swish**는 GELU와 유사하며, **SwiGLU**와 함께 현대 LLM의 표준이다.
5. **GLU 변형**(SwiGLU, GEGLU)은 게이트 메커니즘으로 정보 흐름을 제어하며, 같은 파라미터 대비 더 나은 성능을 보인다.
