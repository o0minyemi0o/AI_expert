# Mixture of Experts

## 왜 MoE를 이해해야 하는가

모델을 키우면 성능이 좋아지지만, 계산 비용도 비례하여 증가합니다. **Mixture of Experts (MoE)**는 이 비례 관계를 깨뜨립니다. 파라미터는 크게 늘리되, 각 입력에 대해 **일부 전문가만 활성화**하여 계산량을 제한합니다. Mixtral, Switch Transformer, GPT-4(추정) 등 최신 대규모 모델의 핵심 기술입니다.

---

## 1. 핵심 아이디어

일반 Transformer의 FFN을 **여러 전문가(Expert) FFN**으로 대체하고, **게이팅 네트워크(Router)**가 각 토큰을 어떤 전문가에게 보낼지 결정합니다.

```
일반 Transformer:
토큰 → [Attention] → [FFN] → 출력

MoE Transformer:
토큰 → [Attention] → [Router → Expert₁ or Expert₂ or ... Expert_N] → 출력
```

### 예시

8개의 전문가가 있고, 각 토큰은 상위 2개의 전문가만 사용한다면:
- **총 파라미터**: 8배 (FFN이 8개)
- **활성 파라미터**: 2배 (2개만 활성)
- 결과: 파라미터는 많지만 **계산량은 적음**

> **핵심 직관**: MoE는 "모든 문제를 하나의 큰 네트워크로 풀지 말고, 여러 전문가가 나눠서 풀자"는 아이디어입니다. 각 토큰이 자신에게 적합한 전문가를 선택합니다.

---

## 2. 게이팅 (Routing) 메커니즘

### Top-K 라우팅

```python
class Router(nn.Module):
    def __init__(self, d_model, num_experts, top_k=2):
        super().__init__()
        self.gate = nn.Linear(d_model, num_experts, bias=False)
        self.top_k = top_k

    def forward(self, x):
        # x: (batch, seq, d_model)
        logits = self.gate(x)                    # (batch, seq, num_experts)
        top_k_logits, top_k_indices = logits.topk(self.top_k, dim=-1)
        top_k_weights = F.softmax(top_k_logits, dim=-1)
        return top_k_weights, top_k_indices
```

### 전체 MoE 레이어

```python
class MoELayer(nn.Module):
    def __init__(self, d_model, d_ff, num_experts, top_k=2):
        super().__init__()
        self.experts = nn.ModuleList([
            FeedForward(d_model, d_ff) for _ in range(num_experts)
        ])
        self.router = Router(d_model, num_experts, top_k)

    def forward(self, x):
        weights, indices = self.router(x)  # 각 토큰의 전문가 선택

        output = torch.zeros_like(x)
        for k in range(self.router.top_k):
            expert_idx = indices[:, :, k]      # 선택된 전문가 인덱스
            expert_weight = weights[:, :, k:k+1]  # 가중치

            for e in range(len(self.experts)):
                mask = (expert_idx == e)
                if mask.any():
                    expert_input = x[mask]
                    expert_output = self.experts[e](expert_input)
                    output[mask] += expert_weight[mask] * expert_output

        return output
```

---

## 3. 부하 분산 (Load Balancing)

### 문제: 라우터 붕괴

라우팅을 학습에 맡기면, 일부 전문가에게만 토큰이 몰리는 **붕괴(collapse)** 현상이 발생합니다. 한 전문가가 많이 사용되면 더 잘 학습되고, 더 많이 선택되는 양성 피드백 루프입니다.

### 해결: 보조 손실

각 전문가가 **균등하게** 사용되도록 보조 손실을 추가합니다.

$$
L_{\text{balance}} = N \sum_{i=1}^N f_i \cdot p_i
$$

- $f_i$: 전문가 $i$에 할당된 토큰 비율
- $p_i$: 전문가 $i$의 평균 라우팅 확률
- $N$: 전문가 수

이상적으로 $f_i = p_i = 1/N$이면 $L_{\text{balance}} = 1$이고, 불균형할수록 커집니다.

```python
def load_balance_loss(router_logits, num_experts):
    # router_logits: (batch * seq, num_experts)
    probs = F.softmax(router_logits, dim=-1)

    # 각 전문가에 할당된 토큰 비율
    top1 = router_logits.argmax(dim=-1)
    freq = torch.bincount(top1, minlength=num_experts).float()
    freq = freq / freq.sum()

    # 각 전문가의 평균 확률
    avg_prob = probs.mean(dim=0)

    return num_experts * (freq * avg_prob).sum()
```

---

## 4. Expert Parallelism

MoE의 전문가들을 **여러 GPU에 분산**합니다.

```
GPU 0: Expert 0, 1
GPU 1: Expert 2, 3
GPU 2: Expert 4, 5
GPU 3: Expert 6, 7
```

### All-to-All 통신

1. 각 GPU에서 라우팅 결정
2. **All-to-All**: 토큰을 해당 전문가가 있는 GPU로 전송
3. 각 GPU에서 자신의 전문가로 처리
4. **All-to-All**: 결과를 원래 GPU로 반환

이 통신 비용이 MoE의 실전 병목입니다.

---

## 5. 대표 모델들

### Switch Transformer (2021)

- Top-1 라우팅 (토큰당 전문가 1개만)
- 계산량은 동일하면서 파라미터 4~64배
- 단순함이 핵심

### Mixtral 8x7B (2023)

| 특성 | 값 |
|------|-----|
| 전문가 수 | 8 |
| 활성 전문가 | 2 (Top-2) |
| 전문가당 파라미터 | ~7B |
| 총 파라미터 | ~47B |
| 활성 파라미터 | ~13B |

47B 파라미터이지만, 추론 시 13B 모델의 계산량으로 LLaMA 2 70B에 근접한 성능을 냅니다.

### DeepSeek-MoE

더 세밀한 전문가(fine-grained experts)를 사용합니다. 전문가 수를 늘리고 크기를 줄여, 더 정밀한 라우팅을 가능하게 합니다.

---

## 6. MoE의 트레이드오프

| 장점 | 단점 |
|------|------|
| 적은 계산으로 큰 모델 | **총 메모리**는 여전히 큼 (모든 전문가 저장) |
| 학습 효율적 | **All-to-All 통신** 오버헤드 |
| 확장성 좋음 | **부하 분산** 어려움 |
| 전문화 가능 | 학습 불안정할 수 있음 |

### Dense vs MoE 비교

같은 **학습 비용(FLOP)**으로:

| 모델 | 파라미터 | 활성 파라미터 | 학습 FLOP | 성능 |
|------|---------|------------|-----------|------|
| Dense 7B | 7B | 7B | 1x | 기준 |
| MoE 8x7B | 47B | 13B | ~2x | Dense 30B급 |

MoE는 같은 계산 예산으로 더 좋은 성능을 달성하지만, 메모리와 통신 비용이 증가합니다.

---

## 7. ML에서의 의미

### 추론 최적화

MoE 모델은 활성 파라미터가 적어 추론이 빠르지만, 모든 전문가를 메모리에 올려야 합니다. 전문가를 디스크에서 동적 로딩하는 **offloading** 기법이 연구되고 있습니다.

### 전문가 분석

학습된 전문가가 무엇에 특화되었는지 분석할 수 있습니다. 일부 전문가는 수학에, 일부는 코드에, 일부는 자연어에 특화되는 경향이 있습니다.

---

## 핵심 정리

1. **MoE**는 FFN을 여러 전문가로 대체하고, 게이팅 네트워크가 각 토큰을 적합한 전문가에게 라우팅한다.
2. **Top-K 라우팅**으로 각 토큰은 $K$개의 전문가만 사용하여, 총 파라미터는 크지만 **활성 파라미터는 적다**.
3. **부하 분산 손실**은 토큰이 특정 전문가에 몰리는 라우터 붕괴를 방지한다.
4. **Expert Parallelism**으로 전문가를 여러 GPU에 분산하며, All-to-All 통신이 실전 병목이다.
5. Mixtral 8x7B은 47B 파라미터이지만 **13B의 계산량**으로 70B급 성능을 달성하며, MoE의 실전 효과를 입증했다.
