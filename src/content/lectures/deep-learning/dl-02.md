# 어텐션 메커니즘

## 왜 어텐션을 깊이 이해해야 하는가

Transformer 이전의 시퀀스 모델(RNN, LSTM)은 긴 시퀀스에서 정보를 잃었습니다. **어텐션(Attention)**은 시퀀스의 어떤 위치든 직접 참조할 수 있게 하여 이 문제를 해결합니다. 현대 AI의 거의 모든 것—GPT, BERT, Vision Transformer, Diffusion Model—이 어텐션 위에 구축되어 있습니다. 행렬 연산 수준에서 정확히 이해하는 것이 필수입니다.

---

## 1. 어텐션의 직관

### 핵심 비유: 검색 엔진

어텐션은 **검색**입니다:
- **Query (Q)**: "내가 찾고 싶은 것"
- **Key (K)**: "각 항목의 제목/태그"
- **Value (V)**: "각 항목의 내용"

검색 과정:
1. Query와 각 Key의 유사도를 계산
2. 유사도를 확률로 변환 (softmax)
3. 확률에 따라 Value를 가중 평균

> **핵심 직관**: 어텐션은 "어디를 볼 것인가"를 학습하는 메커니즘입니다. 모든 위치를 보되, 관련 있는 위치에 더 많은 가중치를 둡니다.

---

## 2. Scaled Dot-Product Attention

### 수식

$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
$$

- $Q \in \mathbb{R}^{T_q \times d_k}$: 쿼리 행렬
- $K \in \mathbb{R}^{T_k \times d_k}$: 키 행렬
- $V \in \mathbb{R}^{T_k \times d_v}$: 값 행렬
- $T_q$: 쿼리 시퀀스 길이, $T_k$: 키/값 시퀀스 길이
- $d_k$: 키 차원, $d_v$: 값 차원

### 단계별 분해

| 단계 | 연산 | 크기 | 의미 |
|------|------|------|------|
| 1 | $S = QK^T$ | $T_q \times T_k$ | 유사도 점수 행렬 |
| 2 | $S' = S / \sqrt{d_k}$ | $T_q \times T_k$ | 스케일링 |
| 3 | $A = \text{softmax}(S')$ | $T_q \times T_k$ | 어텐션 가중치 (각 행의 합 = 1) |
| 4 | $O = AV$ | $T_q \times d_v$ | 가중 평균 출력 |

### $\sqrt{d_k}$ 스케일링의 이유

$Q$와 $K$의 원소가 $\mathcal{N}(0, 1)$이면, 내적 $\mathbf{q}^T\mathbf{k} = \sum_{i=1}^{d_k} q_i k_i$의 분산은 $d_k$입니다.

$$
\text{Var}(\mathbf{q}^T\mathbf{k}) = d_k
$$

$d_k$가 크면 내적 값이 커져서 softmax가 극단적으로 뾰족해집니다 (하나가 1에 가깝고 나머지는 0에 가까움). 이러면 그래디언트가 소실됩니다. $\sqrt{d_k}$로 나누면 분산이 1이 되어 softmax가 적절한 분포를 유지합니다.

---

## 3. Self-Attention

Self-attention에서는 Q, K, V가 **같은 시퀀스**에서 옵니다.

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class SelfAttention(nn.Module):
    def __init__(self, d_model, d_k):
        super().__init__()
        self.W_q = nn.Linear(d_model, d_k, bias=False)
        self.W_k = nn.Linear(d_model, d_k, bias=False)
        self.W_v = nn.Linear(d_model, d_k, bias=False)
        self.scale = d_k ** 0.5

    def forward(self, x):
        # x: (batch, seq_len, d_model)
        Q = self.W_q(x)  # (batch, seq_len, d_k)
        K = self.W_k(x)  # (batch, seq_len, d_k)
        V = self.W_v(x)  # (batch, seq_len, d_k)

        scores = Q @ K.transpose(-2, -1) / self.scale  # (batch, seq, seq)
        attn = F.softmax(scores, dim=-1)
        output = attn @ V  # (batch, seq_len, d_k)
        return output, attn
```

### 투영 관점

$Q = XW^Q$, $K = XW^K$이므로:

$$
QK^T = XW^Q(W^K)^TX^T
$$

$M = W^Q(W^K)^T$는 $d \times d$ 행렬로, 입력 공간에서의 **유사도 함수를 학습**합니다. 이는 Mahalanobis 거리 $\mathbf{x}^TM\mathbf{y}$를 학습하는 것과 같습니다.

---

## 4. Multi-Head Attention

단일 어텐션으로는 하나의 "관점"만 포착합니다. **Multi-Head Attention**은 여러 관점을 동시에 학습합니다.

$$
\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, \ldots, \text{head}_h)W^O
$$

$$
\text{head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)
$$

```python
class MultiHeadAttention(nn.Module):
    def __init__(self, d_model, n_heads):
        super().__init__()
        assert d_model % n_heads == 0
        self.d_k = d_model // n_heads
        self.n_heads = n_heads

        self.W_q = nn.Linear(d_model, d_model)
        self.W_k = nn.Linear(d_model, d_model)
        self.W_v = nn.Linear(d_model, d_model)
        self.W_o = nn.Linear(d_model, d_model)

    def forward(self, x, mask=None):
        B, T, D = x.shape

        # 투영 후 헤드로 분할
        Q = self.W_q(x).view(B, T, self.n_heads, self.d_k).transpose(1, 2)
        K = self.W_k(x).view(B, T, self.n_heads, self.d_k).transpose(1, 2)
        V = self.W_v(x).view(B, T, self.n_heads, self.d_k).transpose(1, 2)
        # (B, n_heads, T, d_k)

        scores = Q @ K.transpose(-2, -1) / (self.d_k ** 0.5)

        if mask is not None:
            scores = scores.masked_fill(mask == 0, float("-inf"))

        attn = F.softmax(scores, dim=-1)
        out = attn @ V  # (B, n_heads, T, d_k)

        # 헤드 합치기
        out = out.transpose(1, 2).contiguous().view(B, T, D)
        return self.W_o(out)
```

### 왜 Multi-Head인가

| 헤드 수 | 각 헤드의 d_k | 효과 |
|---------|-------------|------|
| 1 | $d_{model}$ | 하나의 유사도 함수만 학습 |
| $h$ | $d_{model}/h$ | $h$개의 서로 다른 유사도 함수 |

실제로 학습된 어텐션 헤드를 분석하면, 어떤 헤드는 **문법적 관계**(주어-동사)를, 어떤 헤드는 **의미적 관계**(동의어)를, 어떤 헤드는 **위치적 관계**(인접 토큰)를 포착합니다.

---

## 5. 어텐션 마스크

### Causal Mask (자기회귀 모델)

미래 토큰을 보지 못하게 합니다. GPT 계열의 핵심입니다.

$$
\text{mask}_{ij} = \begin{cases} 0 & \text{if } j \leq i \\ -\infty & \text{if } j > i \end{cases}
$$

```python
def causal_mask(seq_len):
    mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1)
    return mask == 0  # True = 허용, False = 차단

# seq_len=4일 때:
# [[True, False, False, False],
#  [True, True,  False, False],
#  [True, True,  True,  False],
#  [True, True,  True,  True ]]
```

### Padding Mask

패딩 토큰에 대한 어텐션을 차단합니다.

```python
def padding_mask(seq, pad_idx=0):
    return (seq != pad_idx).unsqueeze(1).unsqueeze(2)
    # (batch, 1, 1, seq_len) — 브로드캐스팅 가능
```

---

## 6. Cross-Attention

Self-attention과 달리, Q와 K/V가 **다른 소스**에서 옵니다.

```python
# 인코더-디코더 어텐션
# Q: 디코더 출력에서
# K, V: 인코더 출력에서
Q = decoder_output @ W_q  # "내가 찾고 싶은 것"
K = encoder_output @ W_k  # "소스의 각 위치 태그"
V = encoder_output @ W_v  # "소스의 각 위치 내용"
```

번역에서: 디코더가 "다음 단어를 생성하려면 소스 문장의 어디를 봐야 하는가?"를 결정합니다.

---

## 7. 어텐션의 계산 복잡도

$$
O(T^2 \cdot d_k)
$$

시퀀스 길이 $T$에 대해 **이차(quadratic)** 복잡도입니다. 이것이 긴 시퀀스를 다루는 데 가장 큰 병목이며, dl-06에서 다루는 효율적 어텐션의 동기가 됩니다.

| 시퀀스 길이 | 어텐션 행렬 크기 | 메모리 (FP32) |
|------------|----------------|--------------|
| 512 | 512 × 512 | ~1 MB |
| 2,048 | 2K × 2K | ~16 MB |
| 8,192 | 8K × 8K | ~256 MB |
| 128,000 | 128K × 128K | ~64 GB |

---

## 8. ML에서의 의미

### Vision Transformer (ViT)

이미지를 패치로 나누어 시퀀스로 취급하고 self-attention을 적용합니다.

### 어텐션 시각화

어텐션 가중치를 시각화하면 모델이 어떤 토큰에 주목하는지 해석할 수 있습니다. 다만 이것이 항상 모델의 "추론 과정"을 반영하는 것은 아닙니다.

### 어텐션을 넘어서

최근 Mamba 등 State Space Model(SSM)이 어텐션 없이 선형 복잡도로 시퀀스를 처리하는 접근을 보여주고 있습니다.

---

## 핵심 정리

1. 어텐션은 **Query-Key 유사도로 Value의 가중 평균**을 구하는 연산이며, "어디를 볼 것인가"를 학습한다.
2. **$\sqrt{d_k}$ 스케일링**은 내적의 분산을 1로 맞춰 softmax가 너무 뾰족해지는 것을 방지한다.
3. **Multi-Head Attention**은 $h$개의 서로 다른 유사도 함수를 병렬로 학습하여 다양한 관계를 포착한다.
4. **Causal Mask**는 미래 토큰을 차단하여 자기회귀 생성을 가능하게 한다.
5. 어텐션의 계산 복잡도는 $O(T^2 d_k)$로, 시퀀스 길이에 대해 **이차적**이다. 이것이 효율적 어텐션 연구의 동기이다.
