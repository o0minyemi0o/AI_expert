# 신경 아키텍처 탐색

## 왜 NAS를 이해해야 하는가

신경망의 아키텍처—층 수, 너비, 연결 구조, 활성화 함수—는 지금까지 사람이 설계했습니다. **Neural Architecture Search (NAS)**는 이 과정을 자동화합니다. EfficientNet, MnasNet 등 NAS로 발견된 아키텍처가 수동 설계를 능가한 사례가 많습니다. NAS의 핵심 개념과 효율적 방법을 이해합니다.

---

## 1. NAS의 세 요소

모든 NAS 방법은 세 가지를 정의해야 합니다:

| 요소 | 의미 | 예시 |
|------|------|------|
| **탐색 공간** | 가능한 아키텍처의 집합 | 층 수, 커널 크기, 연결 |
| **탐색 전략** | 공간을 어떻게 탐색하는가 | RL, 진화, 미분 |
| **평가 전략** | 후보를 어떻게 평가하는가 | 학습 후 정확도 |

---

## 2. 탐색 공간

### 셀 기반 탐색 공간

전체 네트워크가 아닌, 반복되는 **셀(cell)**의 구조를 탐색합니다.

```
네트워크 = [Normal Cell × N] → [Reduction Cell] → [Normal Cell × N] → ...
```

셀 내부에서 탐색할 것:
- 어떤 연산을 사용할지 (3×3 conv, 5×5 conv, max pool, identity 등)
- 어떤 입력을 사용할지 (이전 셀, 이전 이전 셀 등)
- 어떻게 결합할지 (덧셈, 연결)

### NASNet 탐색 공간 예시

```python
# 가능한 연산들
operations = [
    'identity',        # skip connection
    'conv_3x3',        # 3×3 convolution
    'conv_5x5',        # 5×5 convolution
    'sep_conv_3x3',    # separable 3×3
    'sep_conv_5x5',    # separable 5×5
    'avg_pool_3x3',    # 3×3 average pooling
    'max_pool_3x3',    # 3×3 max pooling
    'dil_conv_3x3',    # dilated 3×3
]
```

---

## 3. 탐색 전략

### 강화 학습 기반 (NASNet, 2017)

**컨트롤러 RNN**이 아키텍처를 생성하고, 검증 정확도를 보상으로 받아 REINFORCE로 학습합니다.

```
컨트롤러 → 아키텍처 생성 → 학습 → 검증 정확도 → 보상 → 컨트롤러 업데이트
```

문제: 각 후보를 **완전히 학습**해야 하므로 막대한 비용 (500~800 GPU-days).

### 진화 알고리즘 기반

아키텍처 집단을 유지하며, 선택/교차/돌연변이로 진화시킵니다.

```python
# 간소화된 진화 NAS
population = [random_architecture() for _ in range(100)]

for generation in range(num_generations):
    # 평가
    fitness = [evaluate(arch) for arch in population]

    # 선택 (상위 20%)
    parents = select_top(population, fitness, top_k=20)

    # 돌연변이
    children = [mutate(random.choice(parents)) for _ in range(80)]

    population = parents + children
```

### 미분 가능 NAS (DARTS, 2019)

아키텍처 선택을 **연속적 완화**하여, 그래디언트로 최적화합니다.

모든 가능한 연산을 가중합으로 표현:

$$
\bar{o}(x) = \sum_{o \in \mathcal{O}} \frac{\exp(\alpha_o)}{\sum_{o'} \exp(\alpha_{o'})} \cdot o(x)
$$

$\alpha$가 아키텍처 파라미터이고, 가중치와 $\alpha$를 번갈아 최적화합니다.

```python
class MixedOp(nn.Module):
    def __init__(self, C):
        super().__init__()
        self.ops = nn.ModuleList([
            SepConv3x3(C, C),
            SepConv5x5(C, C),
            nn.AvgPool2d(3, 1, 1),
            nn.Identity(),
        ])
        self.alpha = nn.Parameter(torch.zeros(len(self.ops)))

    def forward(self, x):
        weights = F.softmax(self.alpha, dim=0)
        return sum(w * op(x) for w, op in zip(weights, self.ops))
```

| 방법 | 비용 | 장점 | 단점 |
|------|------|------|------|
| RL | 매우 높음 | 일반적 | 느림 |
| 진화 | 높음 | 단순 | 느림 |
| **DARTS** | **낮음** (~1 GPU-day) | 효율적 | 불안정할 수 있음 |

---

## 4. 효율적 평가

### Weight Sharing (One-Shot NAS)

모든 후보 아키텍처가 **가중치를 공유**하는 하나의 거대한 **슈퍼넷(Supernet)**을 학습합니다.

```
Supernet: 모든 가능한 연산과 연결을 포함
          ↓
서브넷 샘플링: 특정 경로만 활성화
          ↓
평가: 학습 없이 슈퍼넷의 공유 가중치로 평가
```

비용이 개별 학습의 수천 분의 일로 줄어듭니다.

### 조기 중단 예측

학습 초기(예: 10 에폭)의 성능으로 최종 성능을 **예측**하여, 유망하지 않은 후보를 빠르게 제거합니다.

---

## 5. NAS로 발견된 대표 모델

### EfficientNet

**복합 스케일링**: 너비, 깊이, 해상도를 동시에 최적화합니다.

$$
\text{depth}: d = \alpha^\phi, \quad \text{width}: w = \beta^\phi, \quad \text{resolution}: r = \gamma^\phi
$$

$$
\alpha \cdot \beta^2 \cdot \gamma^2 \approx 2 \quad \text{(FLOP 제약)}
$$

NAS로 기본 모델(B0)을 찾고, 복합 스케일링으로 B1~B7을 생성합니다.

### MnasNet

모바일 기기를 타겟으로, **정확도와 지연 시간**을 동시에 최적화합니다.

$$
\text{reward} = \text{accuracy} \times \left(\frac{\text{latency}}{\text{target}}\right)^{-0.07}
$$

---

## 6. NAS의 한계와 비판

1. **재현성**: 탐색 결과가 시드에 민감할 수 있음
2. **탐색 공간 편향**: 탐색 공간 자체가 사람이 설계한 것
3. **비용**: 효율적 방법도 여전히 상당한 계산 필요
4. **전이성**: 작은 데이터셋에서 찾은 아키텍처가 큰 데이터셋에서도 좋은가?

최근에는 Transformer의 하이퍼파라미터(층 수, 헤드 수, FFN 크기)를 NAS로 최적화하는 연구가 진행 중입니다.

---

## 핵심 정리

1. NAS는 **탐색 공간, 탐색 전략, 평가 전략** 세 요소로 구성되며, 아키텍처 설계를 자동화한다.
2. **DARTS**는 아키텍처 선택을 미분 가능하게 완화하여 ~1 GPU-day로 탐색하며, 가장 효율적인 방법 중 하나이다.
3. **One-Shot NAS**는 슈퍼넷의 가중치 공유로 개별 학습 없이 후보를 평가한다.
4. **EfficientNet**은 NAS + 복합 스케일링으로, 같은 계산량에서 최고 성능을 달성했다.
5. NAS는 강력하지만, 탐색 공간 자체의 설계와 결과의 재현성에 주의가 필요하다.
