# 가중치 초기화

## 왜 초기화가 중요한가

모든 가중치를 0으로 초기화하면? 모든 뉴런이 동일한 그래디언트를 받아 영원히 같은 값을 유지합니다. 너무 크게 초기화하면 활성화가 폭발하고, 너무 작으면 소실됩니다. **올바른 초기화는 학습의 출발점**이며, 잘못된 초기화는 아예 학습이 안 되게 합니다. la-13에서 선형대수 관점으로 다뤘던 내용을 더 깊이 확장합니다.

---

## 1. 대칭 깨기 (Symmetry Breaking)

### 0 초기화의 문제

```python
# 모든 가중치가 0이면
W = torch.zeros(128, 128)

# 모든 뉴런의 출력이 동일 → 그래디언트도 동일 → 업데이트도 동일
# 128개의 뉴런이 사실상 1개와 같음
```

### 랜덤 초기화의 필요성

가중치를 랜덤으로 초기화하여 각 뉴런이 다른 특성을 학습하도록 **대칭을 깨야** 합니다. 문제는 "얼마나 크게" 랜덤하게 할 것인가입니다.

---

## 2. 분산 보존 원리

핵심 아이디어: **각 층의 출력 분산이 입력 분산과 같아야 한다.**

$y = Wx$에서 $W \in \mathbb{R}^{n_{out} \times n_{in}}$이고, $x_j$가 독립이고 $E[x_j] = 0$, $\text{Var}(x_j) = v$이면:

$$
\text{Var}(y_i) = \text{Var}\left(\sum_{j=1}^{n_{in}} w_{ij} x_j\right) = n_{in} \cdot \text{Var}(w) \cdot v
$$

$\text{Var}(y_i) = v$가 되려면:

$$
\text{Var}(w) = \frac{1}{n_{in}}
$$

이것이 **fan-in** 초기화의 기본 원리입니다.

> **핵심 직관**: 입력이 100차원이면, 가중치의 분산을 $1/100$으로 설정해야 합력(100개의 합)이 적절한 크기를 유지합니다.

---

## 3. Xavier/Glorot 초기화

전방과 역방향 모두에서 분산을 보존합니다.

### 유도

- 전방: $\text{Var}(w) = 1/n_{in}$
- 역방향: $\text{Var}(w) = 1/n_{out}$
- 절충: $\text{Var}(w) = \frac{2}{n_{in} + n_{out}}$

### 구현

$$
W \sim \mathcal{N}\left(0, \frac{2}{n_{in} + n_{out}}\right) \quad \text{또는} \quad W \sim \text{Uniform}\left(-\sqrt{\frac{6}{n_{in}+n_{out}}}, \sqrt{\frac{6}{n_{in}+n_{out}}}\right)
$$

```python
nn.init.xavier_uniform_(layer.weight)  # 균등 분포
nn.init.xavier_normal_(layer.weight)   # 정규 분포
```

### 적용 대상

Sigmoid, Tanh 등 **선형에 가까운 영역**에서 동작하는 활성화 함수에 적합합니다.

---

## 4. Kaiming/He 초기화

ReLU의 특성을 반영합니다.

### 유도

ReLU는 음수를 0으로 만들어 **분산을 절반**으로 줄입니다:

$$
\text{Var}(\text{ReLU}(x)) = \frac{1}{2}\text{Var}(x) \quad \text{(x가 대칭 분포일 때)}
$$

따라서:

$$
\text{Var}(y_i) = \frac{n_{in}}{2} \cdot \text{Var}(w) \cdot v
$$

$\text{Var}(y_i) = v$가 되려면:

$$
\text{Var}(w) = \frac{2}{n_{in}}
$$

```python
nn.init.kaiming_uniform_(layer.weight, mode='fan_in', nonlinearity='relu')
nn.init.kaiming_normal_(layer.weight, mode='fan_in', nonlinearity='relu')
```

### fan_in vs fan_out

| 모드 | 분산 | 보존 대상 |
|------|------|----------|
| `fan_in` | $2/n_{in}$ | 전방 패스의 분산 |
| `fan_out` | $2/n_{out}$ | 역방향 패스의 분산 |

일반적으로 `fan_in`이 기본이고, 이것이 대부분의 경우에 잘 작동합니다.

---

## 5. 직교 초기화 (Orthogonal Initialization)

가중치 행렬을 **직교 행렬**로 초기화합니다.

$$
W^TW = I \quad \implies \quad \|Wx\| = \|x\|
$$

직교 행렬은 벡터의 노름을 보존하므로, 깊은 네트워크에서 신호 크기가 변하지 않습니다.

```python
nn.init.orthogonal_(layer.weight)
```

### 장점

- 노름이 정확히 보존됨 (통계적이 아닌 결정적)
- RNN/LSTM에서 특히 효과적 (시간 축을 따라 반복 곱셈)
- 스펙트럼(특이값)이 균일하여 조건수가 좋음

---

## 6. 실험으로 확인하기

```python
import torch
import torch.nn as nn
import matplotlib.pyplot as plt

def check_activations(init_fn, n_layers=50, width=256):
    """각 층의 활성화 분산을 측정"""
    layers = []
    for _ in range(n_layers):
        layer = nn.Linear(width, width, bias=False)
        init_fn(layer.weight)
        layers.append(layer)

    x = torch.randn(32, width)
    variances = []

    for layer in layers:
        x = torch.relu(layer(x))
        variances.append(x.var().item())

    return variances

# 비교
too_small = check_activations(lambda w: nn.init.normal_(w, std=0.01))
too_large = check_activations(lambda w: nn.init.normal_(w, std=1.0))
kaiming = check_activations(
    lambda w: nn.init.kaiming_normal_(w, nonlinearity='relu')
)

# too_small: 분산이 0으로 급감 (소실)
# too_large: 분산이 무한으로 발산 (폭발)
# kaiming: 분산이 대략 유지됨
```

---

## 7. 특수한 초기화 전략

### 잔차 블록의 초기화

GPT-2는 잔차 블록의 마지막 투영 가중치를 $1/\sqrt{N}$으로 스케일합니다 ($N$: 잔차 블록 수).

$$
W_{proj} \leftarrow \frac{W_{proj}}{\sqrt{2N}}
$$

이는 $N$개의 잔차 기여가 합산되어 분산이 $N$배 커지는 것을 보상합니다.

### LoRA의 초기화

LoRA에서 $\Delta W = BA$의 초기화:
- $A$: Kaiming 초기화 (또는 정규 분포)
- $B$: **0으로 초기화**

학습 시작 시 $\Delta W = 0$이므로 원래 모델과 동일하게 시작합니다. 점진적으로 수정을 학습합니다.

### 임베딩 레이어

```python
# 일반적: 표준 정규 분포로 초기화
nn.init.normal_(embedding.weight, std=0.02)
```

Transformer에서 임베딩과 최종 분류 층의 가중치를 공유(weight tying)할 때, 초기화의 영향이 양쪽에 미칩니다.

---

## 8. 초기화 선택 가이드

| 활성화 함수 | 추천 초기화 |
|------------|-----------|
| Sigmoid / Tanh | Xavier |
| ReLU | **Kaiming** |
| GELU / SiLU | Kaiming (약간의 조정) |
| 없음 (Linear) | Xavier |
| RNN/LSTM | 직교 |
| 잔차 블록 마지막 | 스케일링된 초기화 |

---

## 핵심 정리

1. **0 초기화**는 대칭을 깨지 못하여 모든 뉴런이 동일하게 학습되므로, **랜덤 초기화**가 필수이다.
2. **Xavier 초기화**는 $\text{Var}(w) = 2/(n_{in}+n_{out})$으로 Sigmoid/Tanh에 적합하다.
3. **Kaiming 초기화**는 ReLU의 음수 절반 제거를 보상하여 $\text{Var}(w) = 2/n_{in}$으로 설정한다.
4. **직교 초기화**는 노름을 정확히 보존하며, RNN과 매우 깊은 네트워크에 효과적이다.
5. 잔차 블록과 LoRA 등 **구조에 따른 특수 초기화**가 학습 안정성에 결정적이다.
