# Transformer 아키텍처

## 왜 Transformer를 밑바닥부터 이해해야 하는가

2017년 "Attention Is All You Need" 이후 Transformer는 NLP, 컴퓨터 비전, 음성, 생물학 등 거의 모든 AI 분야의 기반 아키텍처가 되었습니다. `nn.Transformer`를 호출하는 것을 넘어, 각 구성 요소가 **왜 그렇게 설계되었는지**, **어떻게 상호작용하는지**를 이해하면 변형과 개선이 가능합니다.

---

## 1. 전체 구조

원래 Transformer는 **인코더-디코더** 구조입니다.

```
입력 시퀀스 → [인코더] → 인코딩된 표현
                                ↓
출력 시퀀스 → [디코더] → 다음 토큰 예측
```

### 인코더 블록 (N개 스택)

```
입력 임베딩 + 위치 인코딩
    ↓
[Multi-Head Self-Attention]
    ↓ (+ Residual + LayerNorm)
[Feed-Forward Network]
    ↓ (+ Residual + LayerNorm)
```

### 디코더 블록 (N개 스택)

```
출력 임베딩 + 위치 인코딩
    ↓
[Masked Multi-Head Self-Attention]  ← 미래 토큰 차단
    ↓ (+ Residual + LayerNorm)
[Multi-Head Cross-Attention]        ← 인코더 출력 참조
    ↓ (+ Residual + LayerNorm)
[Feed-Forward Network]
    ↓ (+ Residual + LayerNorm)
```

---

## 2. 위치 인코딩 (Positional Encoding)

어텐션은 순서를 모릅니다. 집합(set) 연산이기 때문입니다. 위치 정보를 주입해야 합니다.

### 사인파 위치 인코딩

$$
PE_{(pos, 2i)} = \sin\left(\frac{pos}{10000^{2i/d_{model}}}\right)
$$
$$
PE_{(pos, 2i+1)} = \cos\left(\frac{pos}{10000^{2i/d_{model}}}\right)
$$

- $pos$: 시퀀스에서의 위치 (0, 1, 2, ...)
- $i$: 차원 인덱스
- $d_{model}$: 모델 차원

```python
import torch
import math

def sinusoidal_encoding(max_len, d_model):
    pe = torch.zeros(max_len, d_model)
    position = torch.arange(0, max_len).unsqueeze(1).float()
    div_term = torch.exp(
        torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)
    )
    pe[:, 0::2] = torch.sin(position * div_term)
    pe[:, 1::2] = torch.cos(position * div_term)
    return pe
```

### 왜 사인/코사인인가

1. **상대적 위치를 선형 변환으로 표현 가능**: $PE_{pos+k}$를 $PE_{pos}$의 선형 함수로 표현할 수 있습니다.
2. **외삽 가능**: 학습 시 보지 못한 더 긴 시퀀스에도 적용 가능합니다.
3. **유한한 값 범위**: $[-1, 1]$ 사이의 값으로, 임베딩과 스케일이 맞습니다.

### 학습 가능한 위치 임베딩

```python
# BERT, GPT 등에서 사용
self.pos_embedding = nn.Embedding(max_len, d_model)
```

학습 가능한 임베딩이 더 표현력이 높지만, 학습 시 본 최대 길이를 넘길 수 없습니다.

### RoPE (Rotary Position Embedding)

LLaMA 등 최신 모델에서 사용합니다. 위치 정보를 벡터의 **회전**으로 인코딩하여, 상대적 위치에만 의존하는 어텐션을 만듭니다.

---

## 3. Feed-Forward Network (FFN)

각 위치에 독립적으로 적용되는 2층 MLP입니다.

$$
\text{FFN}(x) = \text{GELU}(xW_1 + b_1)W_2 + b_2
$$

```python
class FeedForward(nn.Module):
    def __init__(self, d_model, d_ff, dropout=0.1):
        super().__init__()
        self.w1 = nn.Linear(d_model, d_ff)
        self.w2 = nn.Linear(d_ff, d_model)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        return self.w2(self.dropout(F.gelu(self.w1(x))))
```

일반적으로 $d_{ff} = 4 \times d_{model}$입니다. FFN은 **각 토큰의 표현을 비선형적으로 변환**하는 역할을 합니다. Attention이 "토큰 간 혼합"이라면, FFN은 "토큰 내 변환"입니다.

---

## 4. Residual Connection과 Layer Normalization

### Residual Connection

$$
\text{output} = \text{Layer}(x) + x
$$

입력을 출력에 더합니다. dl-09에서 자세히 다루지만, 핵심 효과는:
- 그래디언트가 잔차 경로(identity)를 통해 직접 흐를 수 있어 **소실 방지**
- 깊은 네트워크의 학습을 안정화

### Layer Normalization

$$
\text{LayerNorm}(x) = \gamma \odot \frac{x - \mu}{\sqrt{\sigma^2 + \epsilon}} + \beta
$$

각 토큰 벡터를 정규화합니다. BatchNorm과 달리 배치 차원이 아닌 **특성 차원**에서 정규화하므로, 시퀀스 길이나 배치 크기에 독립적입니다.

### Pre-Norm vs Post-Norm

```python
# Post-Norm (원래 논문)
x = LayerNorm(x + Attention(x))
x = LayerNorm(x + FFN(x))

# Pre-Norm (GPT-2 이후 표준)
x = x + Attention(LayerNorm(x))
x = x + FFN(LayerNorm(x))
```

Pre-Norm이 학습이 더 안정적입니다. 그래디언트가 LayerNorm을 거치지 않고 잔차 경로로 직접 흐를 수 있기 때문입니다.

---

## 5. 밑바닥부터 구현

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class TransformerBlock(nn.Module):
    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):
        super().__init__()
        self.attn = MultiHeadAttention(d_model, n_heads)
        self.ff = FeedForward(d_model, d_ff, dropout)
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, mask=None):
        # Pre-Norm 구조
        x = x + self.dropout(self.attn(self.norm1(x), mask))
        x = x + self.dropout(self.ff(self.norm2(x)))
        return x

class Transformer(nn.Module):
    def __init__(self, vocab_size, d_model, n_heads, d_ff, n_layers,
                 max_len, dropout=0.1):
        super().__init__()
        self.token_emb = nn.Embedding(vocab_size, d_model)
        self.pos_emb = nn.Embedding(max_len, d_model)
        self.blocks = nn.ModuleList([
            TransformerBlock(d_model, n_heads, d_ff, dropout)
            for _ in range(n_layers)
        ])
        self.norm = nn.LayerNorm(d_model)
        self.head = nn.Linear(d_model, vocab_size, bias=False)
        self.d_model = d_model

    def forward(self, x, mask=None):
        B, T = x.shape
        tok = self.token_emb(x)  # (B, T, d_model)
        pos = self.pos_emb(torch.arange(T, device=x.device))
        x = tok + pos

        for block in self.blocks:
            x = block(x, mask)

        x = self.norm(x)
        logits = self.head(x)  # (B, T, vocab_size)
        return logits
```

---

## 6. 세 가지 아키텍처 패밀리

| 유형 | 어텐션 마스크 | 용도 | 대표 모델 |
|------|------------|------|----------|
| **인코더-디코더** | 인코더: 양방향, 디코더: causal + cross | 번역, 요약 | T5, BART |
| **인코더만** | 양방향 (모든 토큰이 서로 참조) | 분류, 토큰 레이블링 | BERT |
| **디코더만** | Causal (왼쪽만 참조) | 텍스트 생성 | GPT, LLaMA |

디코더만(Decoder-only) 구조가 현재 LLM의 주류입니다. 단순하면서도 생성 작업에 자연스럽습니다.

---

## 7. 학습과 추론

### 학습: Teacher Forcing

```python
# 입력: [BOS, I, love, NLP]
# 타겟: [I, love, NLP, EOS]
logits = model(input_ids)
loss = F.cross_entropy(
    logits.view(-1, vocab_size),
    target_ids.view(-1)
)
```

Causal mask 덕분에 모든 위치의 손실을 **한 번의 전방 패스**로 계산할 수 있습니다.

### 추론: Autoregressive Generation

```python
def generate(model, prompt_ids, max_new_tokens):
    for _ in range(max_new_tokens):
        logits = model(prompt_ids)
        next_logit = logits[:, -1, :]  # 마지막 토큰의 logit
        probs = F.softmax(next_logit, dim=-1)
        next_token = torch.multinomial(probs, num_samples=1)
        prompt_ids = torch.cat([prompt_ids, next_token], dim=1)
    return prompt_ids
```

추론 시에는 한 토큰씩 생성하므로, 이전 계산을 **KV Cache**로 재사용하는 것이 중요합니다 (dl-06에서 상세히 다룹니다).

---

## 8. 하이퍼파라미터

| 파라미터 | GPT-2 Small | GPT-3 | LLaMA-7B |
|---------|------------|-------|----------|
| $d_{model}$ | 768 | 12,288 | 4,096 |
| $n_{heads}$ | 12 | 96 | 32 |
| $d_{ff}$ | 3,072 | 49,152 | 11,008 |
| $n_{layers}$ | 12 | 96 | 32 |
| 파라미터 수 | 117M | 175B | 6.7B |

---

## 핵심 정리

1. Transformer는 **Multi-Head Attention + FFN + Residual + LayerNorm**의 블록을 $N$개 쌓은 구조이다.
2. **위치 인코딩**은 어텐션의 순서 불변성을 보완하며, 사인파/학습 가능/RoPE 등의 방식이 있다.
3. Attention은 **토큰 간 정보 혼합**, FFN은 **토큰 내 비선형 변환**을 담당한다.
4. **Pre-Norm** 구조가 Post-Norm보다 학습이 안정적이며, 현대 모델의 표준이다.
5. 세 가지 패밀리(인코더-디코더, 인코더만, 디코더만) 중 **디코더만(Decoder-only)**이 현재 LLM의 주류이다.
