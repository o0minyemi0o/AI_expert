# 잔차 연결과 깊이

## 왜 잔차 연결을 이해해야 하는가

2015년 ResNet이 등장하기 전까지 20층 이상의 네트워크는 오히려 성능이 나빠졌습니다. **잔차 연결(Residual Connection)**은 이 "깊이의 저주"를 해결하여 100층, 1000층의 네트워크를 가능하게 했습니다. Transformer의 모든 블록에도 잔차 연결이 있습니다. 왜 이 간단한 덧셈이 이토록 강력한지, 수학적으로 이해합니다.

---

## 1. 깊은 네트워크의 문제

### 성능 저하 (Degradation)

깊은 네트워크가 얕은 네트워크보다 **학습 오차도 높은** 현상입니다. 과적합이 아닙니다.

| 네트워크 | 학습 오차 | 테스트 오차 |
|---------|----------|-----------|
| 20층 | 5.0% | 7.5% |
| 56층 | 6.5% | 8.3% |

56층이 20층의 해를 포함할 수 있으므로(추가 층이 항등 변환을 학습하면 됨), 이론적으로는 더 나빠질 수 없습니다. 하지만 실제로는 **최적화가 항등 변환을 찾지 못합니다**.

---

## 2. 잔차 연결의 수식

$$
y = F(x) + x
$$

$F(x)$는 **잔차(residual)**—입력과 출력의 차이—를 학습합니다.

```python
class ResidualBlock(nn.Module):
    def __init__(self, d_model):
        super().__init__()
        self.layer = nn.Sequential(
            nn.Linear(d_model, d_model),
            nn.ReLU(),
            nn.Linear(d_model, d_model),
        )

    def forward(self, x):
        return self.layer(x) + x  # 잔차 + 항등
```

### 왜 작동하는가: 세 가지 관점

---

## 3. 관점 1: 최적화 지형

$F(x) + x$에서 **항등 변환이 기본값**입니다.

- $F(x) = 0$이면 $y = x$ (항등 변환)
- 네트워크가 "아무것도 하지 않는 것"이 쉬움
- 가중치를 0에 가깝게 초기화하면 자연스럽게 항등에서 시작

잔차 없이 $y = G(x)$에서 항등 변환을 학습하려면 $G$가 정확히 $x$를 출력해야 하는데, 이는 비선형 함수에서 어렵습니다.

> **핵심 직관**: 잔차 연결은 "기본은 입력을 그대로 통과시키고, 필요한 수정만 학습한다"는 것입니다. 0을 학습하는 것이 항등 변환을 학습하는 것보다 쉽습니다.

---

## 4. 관점 2: 그래디언트 흐름

$L$개의 잔차 블록을 거친 후:

$$
x_L = x_0 + \sum_{l=0}^{L-1} F_l(x_l)
$$

그래디언트:

$$
\frac{\partial L}{\partial x_0} = \frac{\partial L}{\partial x_L} \cdot \left(1 + \frac{\partial}{\partial x_0}\sum_{l=0}^{L-1} F_l(x_l)\right)
$$

**1이라는 항**이 존재합니다. 이것이 그래디언트가 잔차 블록의 가중치와 무관하게 직접 흐를 수 있는 **고속도로(highway)**입니다. 잔차 블록의 그래디언트가 0에 가까워도, 1을 통한 경로가 항상 살아있습니다.

잔차 연결 없이는:

$$
\frac{\partial L}{\partial x_0} = \frac{\partial L}{\partial x_L} \cdot \prod_{l=0}^{L-1} \frac{\partial F_l}{\partial x_l}
$$

곱이 연쇄되므로 기울기 소실/폭발이 발생합니다.

---

## 5. 관점 3: 앙상블 해석

$L$개의 잔차 블록이 있으면, 입력에서 출력까지 $2^L$개의 경로가 존재합니다.

```
x → [Block 1] → [Block 2] → [Block 3] → 출력
  ↘              ↘              ↘
   ────────────→ ────────────→ ──→
```

각 블록을 "건너뛸지 통과할지" 선택하면 $2^L$개의 경로입니다. 이것은 **지수적으로 많은 얕은 네트워크의 앙상블**과 유사합니다.

실험적으로, 학습된 ResNet에서 일부 블록을 제거해도 성능이 급격히 떨어지지 않습니다. 이는 앙상블 해석을 지지합니다.

---

## 6. Pre-Activation ResNet

원래 ResNet:
$$
y = \text{ReLU}(F(x) + x)
$$

Pre-Activation (He et al., 2016):
$$
y = F(\text{ReLU}(\text{BN}(x))) + x
$$

```python
# 원래: Conv → BN → ReLU → Conv → BN → (+x) → ReLU
# Pre-Act: BN → ReLU → Conv → BN → ReLU → Conv → (+x)
```

Pre-Activation이 더 좋은 이유: 잔차 경로에 어떤 비선형 변환도 없어, 그래디언트가 완전히 자유롭게 흐릅니다.

---

## 7. DenseNet: 모든 층을 연결

ResNet의 아이디어를 극단으로 밀어붙인 것입니다.

$$
x_l = F_l([x_0, x_1, \ldots, x_{l-1}])
$$

이전 **모든 층의 출력을 연결(concatenation)**하여 입력으로 사용합니다.

```python
class DenseBlock(nn.Module):
    def __init__(self, n_layers, in_channels, growth_rate):
        super().__init__()
        self.layers = nn.ModuleList()
        for i in range(n_layers):
            self.layers.append(
                nn.Sequential(
                    nn.BatchNorm2d(in_channels + i * growth_rate),
                    nn.ReLU(),
                    nn.Conv2d(in_channels + i * growth_rate, growth_rate,
                              kernel_size=3, padding=1),
                )
            )

    def forward(self, x):
        features = [x]
        for layer in self.layers:
            out = layer(torch.cat(features, dim=1))
            features.append(out)
        return torch.cat(features, dim=1)
```

### ResNet vs DenseNet

| | ResNet | DenseNet |
|------|--------|---------|
| 연결 방식 | 덧셈 ($+$) | 연결 (concat) |
| 파라미터 효율 | 보통 | 높음 (growth rate 작음) |
| 특성 재사용 | 간접적 | 직접적 |
| 메모리 | 보통 | 높음 (중간 특성 저장) |

---

## 8. Transformer에서의 잔차 연결

Transformer 블록의 모든 서브레이어에 잔차 연결이 있습니다:

```python
# Pre-Norm Transformer 블록
x = x + attention(layer_norm(x))  # 어텐션 + 잔차
x = x + ffn(layer_norm(x))        # FFN + 잔차
```

100층 이상의 Transformer가 학습 가능한 이유가 바로 이 잔차 연결입니다. 각 블록의 기여는 점진적(incremental)이며, 기본은 입력을 그대로 통과시킵니다.

### Residual Stream 관점

최근 연구에서는 Transformer를 **잔차 스트림(residual stream)** 위에 어텐션과 FFN이 정보를 읽고 쓰는 것으로 해석합니다. 잔차 스트림은 모든 층을 관통하는 "공유 메모리 버스"입니다.

---

## 9. Stochastic Depth

학습 시 잔차 블록을 **랜덤하게 건너뛰는** 정규화 기법입니다.

$$
x_{l+1} = \begin{cases} F_l(x_l) + x_l & \text{확률 } p_l \\ x_l & \text{확률 } 1-p_l \end{cases}
$$

Dropout이 뉴런을 끄듯, Stochastic Depth는 **전체 블록**을 끕니다. 깊은 네트워크에서 특히 효과적이며, EfficientNet 등에서 사용됩니다.

---

## 핵심 정리

1. **잔차 연결** $y = F(x) + x$는 "기본은 항등 변환, 수정만 학습"하여 깊은 네트워크의 최적화를 가능하게 한다.
2. 그래디언트 관점에서, 잔차 경로의 **상수 1**이 그래디언트가 직접 흐르는 고속도로를 제공하여 소실을 방지한다.
3. 앙상블 관점에서, $L$개의 잔차 블록은 $2^L$개의 경로를 형성하여 **암묵적 앙상블**로 동작한다.
4. **Pre-Activation** 구조가 잔차 경로를 완전히 자유롭게 하여 더 깊은 네트워크를 가능하게 한다.
5. Transformer의 잔차 연결은 **잔차 스트림**으로, 모든 층이 공유하는 정보 통로로 해석된다.
