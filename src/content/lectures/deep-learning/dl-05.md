# 정규화 기법

## 왜 정규화를 깊이 이해해야 하는가

BatchNorm이 등장하기 전, 깊은 네트워크를 학습하는 것은 매우 어려웠습니다. **정규화(Normalization)**는 각 층의 입력 분포를 안정화하여 학습을 극적으로 가속합니다. BatchNorm, LayerNorm, RMSNorm, GroupNorm은 각각 다른 차원에서 정규화하며, 적용 상황이 다릅니다. "언제 무엇을 쓰는가"를 정확히 아는 것이 핵심입니다.

---

## 1. 정규화의 일반 공식

모든 정규화 기법은 같은 패턴을 따릅니다:

$$
\hat{x}_i = \frac{x_i - \mu}{\sqrt{\sigma^2 + \epsilon}} \cdot \gamma + \beta
$$

- $\mu$: 평균 (어떤 차원에서 계산하느냐가 핵심)
- $\sigma^2$: 분산
- $\gamma, \beta$: 학습 가능한 스케일/시프트 파라미터
- $\epsilon$: 수치 안정성을 위한 작은 값

> **핵심 직관**: 차이는 $\mu$와 $\sigma^2$를 **어떤 차원에서 계산하는가**뿐입니다. 이것이 BatchNorm, LayerNorm, GroupNorm, InstanceNorm을 구분합니다.

---

## 2. Batch Normalization

**배치 차원**에서 정규화합니다. 같은 채널/특성의 값을 배치 내에서 모아 통계를 구합니다.

$$
\mu_c = \frac{1}{B} \sum_{i=1}^{B} x_{i,c}, \quad \sigma_c^2 = \frac{1}{B} \sum_{i=1}^{B} (x_{i,c} - \mu_c)^2
$$

```python
# 입력: (Batch, Features) 또는 (Batch, Channels, H, W)
# 정규화 차원: Batch (+ 공간 차원)

# (B, C, H, W)에서: 각 채널마다 B×H×W개의 값으로 μ, σ² 계산
bn = nn.BatchNorm2d(num_channels)
```

### 학습 vs 추론

| 단계 | 평균/분산 | 비고 |
|------|----------|------|
| 학습 | 미니배치에서 계산 | 배치마다 다름 |
| 추론 | **이동 평균(running mean/var)** 사용 | 고정된 통계 |

```python
# 학습 시
bn.train()   # 배치 통계 사용 + 이동 평균 업데이트

# 추론 시
bn.eval()    # 이동 평균 사용 (배치 크기에 무관)
```

### 장점과 한계

| 장점 | 한계 |
|------|------|
| 학습 가속 (높은 학습률 허용) | **배치 크기에 의존** |
| 내부 공변량 이동 완화 | 배치 크기 작으면 불안정 |
| 약한 정규화 효과 | **시퀀스 모델에 부적합** (가변 길이) |

---

## 3. Layer Normalization

**특성 차원**에서 정규화합니다. 각 샘플을 독립적으로 처리합니다.

$$
\mu_i = \frac{1}{d} \sum_{j=1}^{d} x_{i,j}, \quad \sigma_i^2 = \frac{1}{d} \sum_{j=1}^{d} (x_{i,j} - \mu_i)^2
$$

```python
# 입력: (Batch, Seq, Features)
# 정규화 차원: Features (마지막 차원)

ln = nn.LayerNorm(d_model)
# 각 토큰 벡터를 독립적으로 정규화
```

### BatchNorm vs LayerNorm

입력이 `(B, T, D)`일 때:

| | BatchNorm | LayerNorm |
|------|-----------|-----------|
| 정규화 차원 | B (배치) | D (특성) |
| 통계 계산 대상 | 같은 특성의 모든 샘플 | 같은 샘플의 모든 특성 |
| 배치 크기 의존 | **예** | 아니오 |
| 추론 시 | 이동 평균 필요 | 배치 통계 불필요 |
| 시퀀스 모델 | 부적합 | **적합** |

> **핵심 직관**: BatchNorm은 "이 특성이 배치 전체에서 어떤 분포인가"를, LayerNorm은 "이 샘플의 특성들이 어떤 분포인가"를 정규화합니다.

---

## 4. RMSNorm

LayerNorm에서 **평균 빼기(centering)를 제거**한 변형입니다.

$$
\text{RMSNorm}(x) = \frac{x}{\sqrt{\frac{1}{d}\sum_{i=1}^d x_i^2 + \epsilon}} \cdot \gamma
$$

```python
class RMSNorm(nn.Module):
    def __init__(self, d_model, eps=1e-6):
        super().__init__()
        self.weight = nn.Parameter(torch.ones(d_model))
        self.eps = eps

    def forward(self, x):
        rms = torch.sqrt(torch.mean(x ** 2, dim=-1, keepdim=True) + self.eps)
        return x / rms * self.weight
```

### 왜 RMSNorm인가

1. **계산 효율**: 평균 계산과 빼기를 생략 (~10-15% 빠름)
2. **성능 동등**: 실험적으로 LayerNorm과 거의 같은 성능
3. **현대 LLM 표준**: LLaMA, Mistral 등이 사용

---

## 5. Group Normalization

채널을 **그룹으로 나누어** 각 그룹 내에서 정규화합니다.

$$
\text{채널을 G개 그룹으로}: \{C_1, C_2, \ldots, C_G\}
$$

각 그룹 내에서 LayerNorm과 같은 방식으로 정규화합니다.

```python
# 입력: (B, C, H, W)
# 32개 채널을 8개 그룹으로 나눔 (그룹당 4채널)
gn = nn.GroupNorm(num_groups=8, num_channels=32)
```

### 특수한 경우

| 그룹 수 | 동등한 것 |
|---------|----------|
| $G = C$ (채널당 1개) | Instance Normalization |
| $G = 1$ (전체가 1그룹) | Layer Normalization |

### 언제 사용하는가

- **배치 크기가 작은 경우**: 객체 탐지, 세그멘테이션 등 GPU 메모리 제한으로 배치가 작을 때
- **배치 통계가 불안정한 경우**: BatchNorm의 대안

---

## 6. Instance Normalization

각 채널을 각 샘플에서 **개별적으로** 정규화합니다.

```python
# 입력: (B, C, H, W)
# 각 (배치, 채널) 조합마다 H×W 값으로 통계 계산
instance_norm = nn.InstanceNorm2d(num_channels)
```

주로 **스타일 변환(Style Transfer)**에서 사용됩니다. 스타일 정보가 채널 통계에 담겨있기 때문입니다.

---

## 7. 정규화 차원 시각화

입력 텐서가 `(B, C, H, W)`일 때:

```
BatchNorm:    B × H × W를 모아서 통계 → C개의 (μ, σ²)
LayerNorm:    C × H × W를 모아서 통계 → B개의 (μ, σ²)
InstanceNorm: H × W를 모아서 통계     → B × C개의 (μ, σ²)
GroupNorm:    (C/G) × H × W를 모아서  → B × G개의 (μ, σ²)
```

---

## 8. 어떤 정규화를 선택할 것인가

| 상황 | 추천 | 이유 |
|------|------|------|
| CNN + 큰 배치 | BatchNorm | 가장 검증됨 |
| CNN + 작은 배치 | GroupNorm | 배치 크기 무관 |
| Transformer/LLM | LayerNorm 또는 RMSNorm | 시퀀스 모델에 적합 |
| 스타일 변환 | InstanceNorm | 스타일 통계 분리 |
| 최신 LLM | RMSNorm | 효율적, 성능 동등 |

---

## 9. ML에서의 의미

### Transformer에서의 위치

Transformer 블록에서 LayerNorm/RMSNorm은 Attention과 FFN **전(Pre-Norm)** 또는 **후(Post-Norm)**에 배치됩니다. Pre-Norm이 현대 표준입니다.

### BatchNorm과 Dropout의 상호작용

BatchNorm과 Dropout을 함께 쓰면 추론 시 분포 불일치가 발생할 수 있습니다. 현대 아키텍처에서는 둘 중 하나만 사용하는 경향이 있습니다.

### Weight Normalization

파라미터 자체를 정규화하는 방법도 있습니다:

$$
\mathbf{w} = \frac{g}{\|\mathbf{v}\|} \mathbf{v}
$$

활성화가 아닌 가중치의 방향과 크기를 분리합니다.

---

## 핵심 정리

1. 모든 정규화는 **같은 공식**(정규화 + 스케일/시프트)이며, **어떤 차원에서 통계를 계산하느냐**가 차이이다.
2. **BatchNorm**은 배치 차원에서 정규화하여 학습을 가속하지만, 배치 크기에 의존하고 시퀀스 모델에 부적합하다.
3. **LayerNorm**은 특성 차원에서 정규화하여 배치 크기에 무관하며, Transformer의 표준이다.
4. **RMSNorm**은 LayerNorm에서 centering을 제거하여 더 효율적이며, 현대 LLM(LLaMA 등)의 표준이다.
5. **GroupNorm**은 채널을 그룹으로 나누어 정규화하며, 배치가 작은 비전 모델에 적합하다.
