# 지식 증류

## 왜 지식 증류를 이해해야 하는가

거대한 모델(교사)의 지식을 작은 모델(학생)에게 전달하는 **Knowledge Distillation**은 모델 배포의 핵심 기술입니다. GPT-4 수준의 성능을 모바일 기기에서 실행할 수 있는 모델로 압축하는 것이 목표입니다. "왜 soft label이 hard label보다 정보가 많은가"를 이해하면, 증류의 본질을 파악할 수 있습니다.

---

## 1. 기본 프레임워크

### Teacher-Student 구조

```
[대형 교사 모델] → soft predictions
                        ↓
[소형 학생 모델] → 교사의 지식을 학습
```

학생은 두 가지를 동시에 학습합니다:
1. **정답(hard label)**에 대한 Cross-Entropy
2. **교사의 출력(soft label)**을 모방

$$
L = (1-\alpha) \cdot L_{CE}(y, \sigma(z_s)) + \alpha \cdot T^2 \cdot L_{KD}(\sigma(z_t/T), \sigma(z_s/T))
$$

- $z_t$: 교사의 logit
- $z_s$: 학생의 logit
- $T$: 온도 (temperature)
- $\alpha$: 가중치 (보통 0.5~0.9)

---

## 2. 온도(Temperature)의 역할

Softmax에 온도 $T$를 적용합니다:

$$
p_i = \frac{\exp(z_i/T)}{\sum_j \exp(z_j/T)}
$$

| 온도 | 분포 | 효과 |
|------|------|------|
| $T = 1$ | 원래 분포 (뾰족) | 정보 적음 |
| $T > 1$ | **부드러운 분포** | 클래스 간 관계 드러남 |
| $T \to \infty$ | 균등 분포 | 모든 정보 소실 |

### 왜 soft label이 중요한가

Hard label: "이것은 고양이다" → [0, 0, 1, 0, 0]

Soft label (T=3): [0.01, 0.05, **0.80**, 0.10, 0.04]
- 호랑이(0.10)가 자동차(0.01)보다 고양이에 가까움
- 이 **클래스 간 유사도 정보**가 dark knowledge

```python
def distillation_loss(student_logits, teacher_logits, labels,
                      temperature=4.0, alpha=0.7):
    # Soft target loss (KL divergence)
    soft_student = F.log_softmax(student_logits / temperature, dim=-1)
    soft_teacher = F.softmax(teacher_logits / temperature, dim=-1)
    kd_loss = F.kl_div(soft_student, soft_teacher, reduction='batchmean')

    # Hard target loss
    ce_loss = F.cross_entropy(student_logits, labels)

    return alpha * temperature**2 * kd_loss + (1 - alpha) * ce_loss
```

> **핵심 직관**: 교사의 soft label에는 "고양이는 호랑이와 비슷하고, 자동차와는 다르다"는 **구조적 정보**가 담겨있습니다. Hard label의 [0, 0, 1, 0, 0]에서는 이 정보가 모두 사라집니다. $T^2$ 스케일링은 그래디언트 크기를 보정합니다.

---

## 3. Feature Distillation

출력뿐 아니라 **중간 층의 특성(feature)**도 모방합니다.

$$
L_{feature} = \sum_l \|f_l^{teacher}(x) - g_l(f_l^{student}(x))\|^2
$$

$g_l$은 학생의 특성 차원을 교사에 맞추는 투영 레이어입니다.

```python
class FeatureDistillation(nn.Module):
    def __init__(self, student_dims, teacher_dims):
        super().__init__()
        # 차원 매칭을 위한 투영
        self.projectors = nn.ModuleList([
            nn.Linear(s_dim, t_dim)
            for s_dim, t_dim in zip(student_dims, teacher_dims)
        ])

    def forward(self, student_features, teacher_features):
        loss = 0
        for proj, s_feat, t_feat in zip(
            self.projectors, student_features, teacher_features
        ):
            loss += F.mse_loss(proj(s_feat), t_feat.detach())
        return loss
```

### Attention Transfer

어텐션 맵을 전달합니다:

$$
L_{AT} = \sum_l \left\|\frac{A_l^S}{\|A_l^S\|_2} - \frac{A_l^T}{\|A_l^T\|_2}\right\|^2
$$

---

## 4. Self-Distillation

교사와 학생이 **같은 모델**인 경우입니다.

### Born-Again Networks

1. 모델 A를 정상 학습
2. 같은 구조의 모델 B를 A의 soft label로 학습
3. B가 A보다 나은 성능 (!)

### 깊은 층 → 얕은 층

같은 네트워크 내에서 깊은 층의 지식을 얕은 층에 전달합니다.

```python
# 깊은 블록의 출력을 얕은 블록의 학습 신호로
deep_features = model.layer4(x)
shallow_features = model.layer2(x)
self_distill_loss = F.mse_loss(
    projector(shallow_features), deep_features.detach()
)
```

---

## 5. LLM에서의 증류

### 대형 LLM → 소형 LLM

```python
# 교사 (큰 모델)의 토큰별 확률 분포를 학생이 모방
teacher_probs = F.softmax(teacher_logits / T, dim=-1)  # (B, T, V)
student_log_probs = F.log_softmax(student_logits / T, dim=-1)

kd_loss = F.kl_div(student_log_probs, teacher_probs, reduction='batchmean')
```

### 합성 데이터 증류

교사 모델이 **합성 학습 데이터를 생성**하고, 학생이 이를 학습합니다.

1. 대형 LLM (GPT-4 등)에게 질문-답변 데이터 생성 요청
2. 생성된 데이터로 소형 모델 파인튜닝
3. Alpaca, Vicuna 등이 이 방식

### DistilBERT

BERT의 증류 버전:
- 교사: BERT-base (110M 파라미터)
- 학생: DistilBERT (66M, 40% 축소)
- 성능: BERT의 97%를 유지하면서 60% 빠름

---

## 6. 증류의 이론적 이해

### Dark Knowledge

교사의 soft label에 담긴 정보:
1. **클래스 간 유사도**: 비슷한 클래스는 비슷한 확률
2. **입력의 난이도**: 어려운 예시는 확률이 분산
3. **레이블 노이즈 보정**: 교사가 노이즈를 평활화

### Regularization 관점

증류는 학생에게 **암묵적 정규화**를 제공합니다. 교사의 부드러운 분포가 학생의 과적합을 방지합니다. Label smoothing과 유사한 효과입니다.

---

## 7. 실전 팁

| 파라미터 | 일반적 값 | 효과 |
|---------|----------|------|
| Temperature $T$ | 3~20 | 높을수록 soft, 너무 높으면 정보 소실 |
| $\alpha$ | 0.5~0.9 | KD 손실 가중치 |
| 교사 크기 | 학생의 3~10배 | 너무 크면 갭이 커서 어려움 |

### 교사-학생 갭

교사가 너무 크면 학생이 따라가지 못합니다. **점진적 증류**(대형 → 중형 → 소형)가 효과적일 수 있습니다.

---

## 핵심 정리

1. 지식 증류는 **교사의 soft label**을 학생이 모방하는 것이며, soft label에는 **클래스 간 유사도** 등 dark knowledge가 담겨있다.
2. **온도 $T$**는 softmax를 부드럽게 하여 더 많은 정보를 전달하며, $T^2$ 스케일링이 그래디언트를 보정한다.
3. **Feature Distillation**은 중간 층의 표현을 전달하여 출력 증류만으로는 얻기 어려운 구조적 지식을 전이한다.
4. **Self-Distillation**은 같은 구조의 모델을 자신의 soft label로 재학습하여 성능을 향상시킨다.
5. LLM에서는 **합성 데이터 증류**(대형 모델이 데이터를 생성하여 소형 모델 학습)가 주류이다.
