# 현대 Transformer 변형

## 왜 아키텍처 변형을 이해해야 하는가

GPT, BERT, T5, LLaMA는 모두 Transformer이지만, 각각 다른 설계 결정을 했습니다. **왜 그 결정을 했는가**를 이해하면, 새로운 모델을 읽고 평가할 수 있으며, 자신의 문제에 맞는 선택을 할 수 있습니다.

---

## 1. BERT: 양방향 인코더

### 핵심 아이디어: Masked Language Modeling (MLM)

```
입력: The [MASK] sat on the [MASK]
목표: The cat sat on the mat
```

토큰의 15%를 마스킹하고 복원하도록 학습합니다. **양방향 컨텍스트**를 활용할 수 있습니다.

### 아키텍처 특징

| 특성 | BERT |
|------|------|
| 구조 | 인코더만 |
| 어텐션 | 양방향 (모든 토큰이 서로 참조) |
| 사전학습 | MLM + Next Sentence Prediction |
| 용도 | 분류, NER, 질의응답 |
| 생성 | 직접 생성 불가 (양방향이므로) |

### 한계

양방향이라 **텍스트 생성에 부적합**합니다. 분류 등 이해(understanding) 작업에는 강하지만, 생성(generation)에는 디코더 구조가 필요합니다.

---

## 2. GPT 계열: 자기회귀 디코더

### GPT-1 → GPT-2 → GPT-3의 진화

| 모델 | 파라미터 | 핵심 변화 |
|------|---------|----------|
| GPT-1 (2018) | 117M | Transformer 디코더 + 사전학습 |
| GPT-2 (2019) | 1.5B | 스케일 업, Pre-Norm으로 변경 |
| GPT-3 (2020) | 175B | In-context learning 발견 |

### GPT의 설계 결정

```python
# GPT 스타일 차이점
# 1. Pre-Norm (Post-Norm이 아닌)
x = x + attention(layer_norm(x))

# 2. GELU 활성화 (ReLU가 아닌)
ffn = gelu(x @ W1) @ W2

# 3. 학습 가능한 위치 임베딩
pos_emb = nn.Embedding(max_len, d_model)
```

### In-Context Learning

GPT-3의 가장 중요한 발견입니다. 모델이 **프롬프트에 주어진 예시**만으로 새로운 작업을 수행할 수 있습니다.

```
번역해줘:
영어: Hello → 한국어: 안녕하세요
영어: Thank you → 한국어: 감사합니다
영어: Good morning → 한국어:
```

파라미터 업데이트 없이, 어텐션만으로 패턴을 "학습"합니다.

---

## 3. T5: Text-to-Text Transfer

### 핵심 아이디어: 모든 것을 텍스트 변환으로

```
분류:    "sentiment: I love this movie" → "positive"
번역:    "translate to French: Hello"   → "Bonjour"
요약:    "summarize: [긴 텍스트]"       → "[요약]"
질의응답: "question: ... context: ..."   → "[답변]"
```

### 아키텍처 특징

- **인코더-디코더** 구조 (원래 Transformer와 동일)
- Span corruption 사전학습: 연속된 토큰 그룹을 마스킹
- 상대적 위치 바이어스: 절대 위치 인코딩 대신

---

## 4. LLaMA: 효율적 오픈소스 LLM

Meta의 LLaMA는 현대 LLM의 설계 표준을 확립했습니다.

### 핵심 설계 결정

| 선택 | LLaMA의 결정 | 이유 |
|------|-------------|------|
| 정규화 | **RMSNorm** (LayerNorm 대신) | 평균 계산 생략, 더 빠름 |
| 활성화 | **SiLU/Swish** (GELU 대신) | 약간 더 나은 성능 |
| FFN | **SwiGLU** | 게이트 메커니즘 추가 |
| 위치 인코딩 | **RoPE** | 상대적 위치, 외삽 가능 |
| 어텐션 | **GQA** (LLaMA 2+) | KV 캐시 메모리 절약 |

### RMSNorm

$$
\text{RMSNorm}(x) = \frac{x}{\text{RMS}(x)} \cdot \gamma, \quad \text{RMS}(x) = \sqrt{\frac{1}{d}\sum_{i=1}^d x_i^2}
$$

LayerNorm에서 평균 빼기(centering)를 생략합니다. 실험적으로 성능 차이가 거의 없으면서 더 빠릅니다.

### SwiGLU FFN

$$
\text{SwiGLU}(x) = (\text{SiLU}(xW_1) \odot xW_3) \cdot W_2
$$

일반 FFN의 2개 가중치 대신 3개를 사용하지만, 총 파라미터 수는 비슷하게 $d_{ff}$를 조절합니다.

```python
class SwiGLU(nn.Module):
    def __init__(self, d_model, d_ff):
        super().__init__()
        self.w1 = nn.Linear(d_model, d_ff, bias=False)
        self.w2 = nn.Linear(d_ff, d_model, bias=False)
        self.w3 = nn.Linear(d_model, d_ff, bias=False)

    def forward(self, x):
        return self.w2(F.silu(self.w1(x)) * self.w3(x))
```

---

## 5. 아키텍처 비교 총정리

| | BERT | GPT | T5 | LLaMA |
|------|------|-----|----|----|
| 구조 | 인코더 | 디코더 | 인코더-디코더 | 디코더 |
| 어텐션 방향 | 양방향 | Causal | 양방향 + Causal | Causal |
| Norm | Post-Norm | Pre-Norm | Pre-Norm | Pre-RMSNorm |
| FFN 활성화 | GELU | GELU | ReLU | SwiGLU |
| 위치 인코딩 | 학습 가능 | 학습 가능 | 상대적 바이어스 | RoPE |
| 사전학습 | MLM | 자기회귀 LM | Span corruption | 자기회귀 LM |
| 강점 | 이해 작업 | 생성 | 다목적 | 효율적 생성 |

---

## 6. 최신 트렌드

### Mixture of Experts (MoE)

파라미터를 늘리면서도 계산량을 유지합니다. dl-08에서 자세히 다룹니다.

### 긴 컨텍스트

128K~1M 토큰의 컨텍스트 윈도우를 지원하기 위한 위치 인코딩 확장 (YaRN, ALiBi 등).

### Post-Training

RLHF, DPO 등으로 사전학습된 모델을 인간 선호에 맞게 정렬합니다. 이것이 ChatGPT와 GPT-3의 차이입니다.

---

## 핵심 정리

1. **BERT**는 양방향 인코더로 이해 작업에 강하지만, 자기회귀 생성에는 부적합하다.
2. **GPT** 계열은 디코더만 구조로 자기회귀 생성에 최적화되며, 스케일링과 함께 In-Context Learning이 등장했다.
3. **T5**는 인코더-디코더 구조로 모든 작업을 text-to-text로 통일했다.
4. **LLaMA**는 RMSNorm, SwiGLU, RoPE, GQA 등 현대적 설계를 조합하여 효율적 LLM의 표준을 확립했다.
5. 현재 LLM의 주류는 **디코더만(Decoder-only) + Pre-Norm + RoPE + GQA**의 조합이다.
