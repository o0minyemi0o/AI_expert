# 신경망 기초 재정립

## 왜 기초를 다시 다져야 하는가

딥러닝을 "사용"하는 것과 "이해"하는 것은 다릅니다. 역전파(backpropagation)는 단순히 `loss.backward()`를 호출하는 것이 아니라, **계산 그래프 위의 연쇄 법칙 적용**입니다. 자동 미분(autodiff)의 원리를 정확히 이해하면, 커스텀 레이어 설계, 그래디언트 디버깅, 새로운 최적화 기법 구현이 가능해집니다.

---

## 1. 계산 그래프

모든 신경망 연산은 **유향 비순환 그래프(DAG)**로 표현됩니다.

$$
y = \sigma(Wx + b)
$$

이 수식의 계산 그래프:

```
x, W → matmul → z₁
z₁, b → add → z₂
z₂ → sigmoid → y
```

각 노드는 **연산(operation)**이고, 간선은 **텐서(데이터)**의 흐름입니다.

### 전방 패스 (Forward Pass)

입력에서 출력 방향으로 값을 계산합니다.

$$
z_1 = Wx, \quad z_2 = z_1 + b, \quad y = \sigma(z_2)
$$

### 후방 패스 (Backward Pass)

출력에서 입력 방향으로 그래디언트를 전파합니다.

$$
\frac{\partial L}{\partial z_2} = \frac{\partial L}{\partial y} \cdot \sigma'(z_2), \quad \frac{\partial L}{\partial W} = \frac{\partial L}{\partial z_1} \cdot x^T
$$

> **핵심 직관**: 전방 패스는 "값을 계산"하고, 후방 패스는 "각 파라미터가 손실에 얼마나 기여하는지"를 역추적합니다.

---

## 2. 연쇄 법칙(Chain Rule)의 엄밀한 적용

합성 함수 $L = f(g(h(x)))$에 대해:

$$
\frac{dL}{dx} = \frac{dL}{df} \cdot \frac{df}{dg} \cdot \frac{dg}{dh} \cdot \frac{dh}{dx}
$$

다변수로 확장하면 **야코비안(Jacobian)**이 등장합니다.

$\mathbf{y} = f(\mathbf{x})$에서 $\mathbf{x} \in \mathbb{R}^n$, $\mathbf{y} \in \mathbb{R}^m$이면:

$$
J = \frac{\partial \mathbf{y}}{\partial \mathbf{x}} = \begin{pmatrix} \frac{\partial y_1}{\partial x_1} & \cdots & \frac{\partial y_1}{\partial x_n} \\ \vdots & \ddots & \vdots \\ \frac{\partial y_m}{\partial x_1} & \cdots & \frac{\partial y_m}{\partial x_n} \end{pmatrix} \in \mathbb{R}^{m \times n}
$$

역전파에서 실제로 계산하는 것은 야코비안 자체가 아니라 **벡터-야코비안 곱(VJP)**입니다:

$$
\bar{\mathbf{x}} = \bar{\mathbf{y}}^T J
$$

여기서 $\bar{\mathbf{y}} = \frac{\partial L}{\partial \mathbf{y}}$는 상위에서 전파된 그래디언트입니다.

---

## 3. 역전파: 구체적 예시

단순 2층 네트워크를 완전히 풀어봅니다.

$$
\hat{y} = W_2 \cdot \text{ReLU}(W_1 x + b_1) + b_2
$$
$$
L = \frac{1}{2}(\hat{y} - y)^2
$$

### 전방 패스

$$
z_1 = W_1 x + b_1, \quad a_1 = \text{ReLU}(z_1), \quad z_2 = W_2 a_1 + b_2, \quad L = \frac{1}{2}(z_2 - y)^2
$$

### 후방 패스 (출력 → 입력 순서)

$$
\frac{\partial L}{\partial z_2} = z_2 - y
$$

$$
\frac{\partial L}{\partial W_2} = \frac{\partial L}{\partial z_2} \cdot a_1^T, \quad \frac{\partial L}{\partial b_2} = \frac{\partial L}{\partial z_2}
$$

$$
\frac{\partial L}{\partial a_1} = W_2^T \cdot \frac{\partial L}{\partial z_2}
$$

$$
\frac{\partial L}{\partial z_1} = \frac{\partial L}{\partial a_1} \odot \mathbb{1}[z_1 > 0] \quad \text{(ReLU의 미분)}
$$

$$
\frac{\partial L}{\partial W_1} = \frac{\partial L}{\partial z_1} \cdot x^T, \quad \frac{\partial L}{\partial b_1} = \frac{\partial L}{\partial z_1}
$$

각 단계에서 **상위 그래디언트 × 국소 미분**을 곱하는 패턴이 반복됩니다.

---

## 4. 자동 미분 (Automatic Differentiation)

### 수치 미분 vs 기호 미분 vs 자동 미분

| 방법 | 원리 | 장단점 |
|------|------|--------|
| 수치 미분 | $f'(x) \approx \frac{f(x+h) - f(x-h)}{2h}$ | 간단하지만 느리고 부정확 |
| 기호 미분 | 수식을 기호적으로 미분 | 정확하지만 표현식 폭발 |
| **자동 미분** | 계산 그래프 + 연쇄 법칙 | **정확하고 효율적** |

### Forward mode vs Reverse mode

| | Forward mode | Reverse mode (역전파) |
|------|-------------|---------------------|
| 방향 | 입력 → 출력 | 출력 → 입력 |
| 계산 | 야코비안-벡터 곱 (JVP) | **벡터-야코비안 곱 (VJP)** |
| 비용 | 입력 차원에 비례 | **출력 차원에 비례** |
| 적합한 경우 | 입력 < 출력 | **입력 > 출력** (딥러닝) |

딥러닝에서는 파라미터(입력)가 수백만 개이고 손실(출력)은 스칼라 1개이므로, **Reverse mode**가 압도적으로 효율적입니다. 한 번의 후방 패스로 모든 파라미터의 그래디언트를 동시에 얻습니다.

---

## 5. PyTorch의 Autograd 내부

```python
import torch

x = torch.tensor(2.0, requires_grad=True)
w = torch.tensor(3.0, requires_grad=True)
b = torch.tensor(1.0, requires_grad=True)

# 전방 패스 — 계산 그래프 구축
y = w * x + b        # y = 3*2 + 1 = 7
loss = (y - 5) ** 2  # loss = (7-5)^2 = 4

# 후방 패스 — 그래디언트 계산
loss.backward()

print(x.grad)  # dL/dx = 2(y-5) * w = 2*2*3 = 12
print(w.grad)  # dL/dw = 2(y-5) * x = 2*2*2 = 8
print(b.grad)  # dL/db = 2(y-5) * 1 = 2*2   = 4
```

### grad_fn 체인

```python
print(loss.grad_fn)          # PowBackward0
print(loss.grad_fn.next_functions)  # SubBackward0의 참조
# 이 체인이 계산 그래프의 역방향 경로
```

### 그래디언트 누적

```python
x = torch.tensor(1.0, requires_grad=True)

# 첫 번째 backward
y = x ** 2
y.backward()
print(x.grad)  # 2.0

# 두 번째 backward — 그래디언트가 누적됨!
y = x ** 3
y.backward()
print(x.grad)  # 2.0 + 3.0 = 5.0

# 항상 초기화 필요
x.grad.zero_()
```

---

## 6. 그래디언트 소실과 폭발

깊은 네트워크에서 그래디언트가 전파되면서 지수적으로 줄거나 커질 수 있습니다.

$$
\frac{\partial L}{\partial W_1} = \frac{\partial L}{\partial z_L} \cdot \prod_{l=2}^{L} W_l \cdot \text{diag}(\sigma'(z_{l-1}))
$$

### 소실 (Vanishing)

$\sigma' < 1$이면 곱이 지수적으로 줄어듭니다. sigmoid의 $\sigma'_{\max} = 0.25$이므로 10층만 거쳐도 $0.25^{10} \approx 10^{-6}$입니다.

### 폭발 (Exploding)

$\|W_l\| > 1$이면 곱이 지수적으로 커집니다.

### 해결책

| 문제 | 해결 |
|------|------|
| 소실 | ReLU, Residual connection, 적절한 초기화 |
| 폭발 | Gradient clipping, 정규화 |
| 양쪽 모두 | BatchNorm/LayerNorm, LSTM/GRU 게이트 |

```python
# Gradient clipping
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
```

---

## 7. 간단한 Autograd 구현

자동 미분의 핵심을 직접 구현해봅니다.

```python
class Value:
    def __init__(self, data, children=(), op=""):
        self.data = data
        self.grad = 0.0
        self._backward = lambda: None
        self._children = set(children)

    def __add__(self, other):
        other = other if isinstance(other, Value) else Value(other)
        out = Value(self.data + other.data, (self, other), "+")

        def _backward():
            self.grad += out.grad   # dL/da = dL/dout * 1
            other.grad += out.grad  # dL/db = dL/dout * 1
        out._backward = _backward
        return out

    def __mul__(self, other):
        other = other if isinstance(other, Value) else Value(other)
        out = Value(self.data * other.data, (self, other), "*")

        def _backward():
            self.grad += other.data * out.grad  # dL/da = b * dL/dout
            other.grad += self.data * out.grad  # dL/db = a * dL/dout
        out._backward = _backward
        return out

    def backward(self):
        # 위상 정렬 후 역순으로 backward
        topo = []
        visited = set()
        def build(v):
            if v not in visited:
                visited.add(v)
                for child in v._children:
                    build(child)
                topo.append(v)
        build(self)

        self.grad = 1.0
        for v in reversed(topo):
            v._backward()

# 사용
x = Value(2.0)
w = Value(3.0)
b = Value(1.0)
y = w * x + b      # 7.0
loss = (y + Value(-5.0)) * (y + Value(-5.0))  # 4.0
loss.backward()
print(f"x.grad={x.grad}, w.grad={w.grad}, b.grad={b.grad}")
# x.grad=12.0, w.grad=8.0, b.grad=4.0
```

이 구현은 Andrej Karpathy의 micrograd에서 영감을 받은 것으로, PyTorch autograd의 핵심 원리와 동일합니다.

---

## 8. ML에서의 의미

### 커스텀 autograd 함수

```python
class CustomReLU(torch.autograd.Function):
    @staticmethod
    def forward(ctx, input):
        ctx.save_for_backward(input)
        return input.clamp(min=0)

    @staticmethod
    def backward(ctx, grad_output):
        input, = ctx.saved_tensors
        grad_input = grad_output.clone()
        grad_input[input < 0] = 0
        return grad_input
```

### 그래디언트 체크

```python
from torch.autograd import gradcheck
input = torch.randn(3, 4, requires_grad=True, dtype=torch.double)
test = gradcheck(CustomReLU.apply, input)  # 수치 미분과 비교
```

---

## 핵심 정리

1. **계산 그래프**는 연산을 DAG로 표현하며, 전방 패스로 값을, 후방 패스로 그래디언트를 계산한다.
2. **역전파**는 연쇄 법칙을 계산 그래프 위에서 역순으로 적용하는 것이며, 각 노드에서 "상위 그래디언트 × 국소 미분"을 계산한다.
3. **Reverse mode 자동 미분**은 출력이 스칼라일 때 한 번의 패스로 모든 파라미터의 그래디언트를 얻으며, 이것이 딥러닝에 최적인 이유이다.
4. 그래디언트 **소실/폭발**은 깊은 네트워크의 근본 문제이며, ReLU, Residual connection, 적절한 초기화로 완화한다.
5. PyTorch의 autograd는 각 텐서 연산의 `grad_fn`을 통해 계산 그래프를 동적으로 구축하고, `backward()`로 VJP를 역전파한다.
