# 컴퓨터 비전 시스템

## 왜 컴퓨터 비전 시스템 설계가 중요한가

컴퓨터 비전은 자율주행, 의료 영상, 제조 품질 검사, 콘텐츠 모더레이션 등 **실물 세계와 ML이 만나는 접점**입니다. dl-05에서 배운 CNN과 dl-12의 Vision Transformer가 기반 기술이며, 엣지 배포와 실시간 처리라는 시스템 관점의 도전이 추가됩니다.

> **핵심 직관**: CV 시스템의 핵심 트레이드오프는 "정확도 vs 속도"입니다. 자율주행에서 10ms 늦은 판단은 사고를 의미하고, 의료 영상에서 0.1%의 정확도 차이는 오진을 의미합니다.

## 1. CV 시스템 아키텍처

```
CV 시스템 공통 파이프라인:

  [입력 (이미지/비디오)]
       │
       ▼
  [전처리]
  ├─ 리사이즈, 정규화
  ├─ 데이터 증강 (학습 시)
  └─ 포맷 변환 (RGB, 텐서)
       │
       ▼
  [백본 모델 (특징 추출)]
  ├─ ResNet, EfficientNet (CNN 계열)
  └─ ViT, Swin Transformer (트랜스포머 계열)
       │
       ▼
  [태스크 헤드]
  ├─ 분류 → Softmax 레이어
  ├─ 탐지 → RPN + 바운딩 박스 회귀
  ├─ 분할 → 픽셀별 예측
  └─ 생성 → 디코더
       │
       ▼
  [후처리]
  ├─ NMS (탐지)
  ├─ 신뢰도 필터링
  └─ 결과 집계
```

## 2. 이미지 분류 시스템

| 모델 | 파라미터 | Top-1 Acc | 추론 속도 | 적합 환경 |
|------|---------|-----------|----------|----------|
| MobileNetV3 | 5M | 75% | 5ms | 모바일/엣지 |
| EfficientNet-B0 | 5M | 77% | 8ms | 균형 |
| ResNet-50 | 25M | 79% | 15ms | 서버 |
| EfficientNet-B7 | 66M | 84% | 50ms | 정확도 중시 |
| ViT-Large | 307M | 88% | 100ms | 최고 정확도 |

```
모델 선택 기준 (sd-01 프레임워크 적용):

  배포 환경이 어디인가?
  ├─ 모바일/IoT → MobileNet, EfficientNet-Lite
  ├─ 엣지 서버  → EfficientNet, ResNet-50
  └─ 클라우드  → EfficientNet-B4+, ViT
```

## 3. 객체 탐지 시스템

```
객체 탐지 모델 분류:

  Two-Stage (높은 정확도)           One-Stage (빠른 속도)
  ─────────────────────          ─────────────────────
  Faster R-CNN                    YOLO (v5, v8, v11)
  ├─ 1단계: 후보 영역 생성(RPN)      ├─ 이미지를 그리드로 분할
  └─ 2단계: 분류 + 박스 정제         └─ 한 번에 분류 + 위치 예측

  Cascade R-CNN                   SSD
  └─ 다단계 정제 (더 정밀)           └─ 다중 스케일 특징맵

  정확도:  Two-Stage > One-Stage
  속도:    One-Stage > Two-Stage
```

| 시나리오 | 추천 모델 | 이유 |
|---------|----------|------|
| 실시간 CCTV | YOLOv8-nano | FPS 우선 |
| 자율주행 | YOLOv8-large | 속도 + 정확도 |
| 의료 영상 | Faster R-CNN | 정확도 최우선 |
| 소매 재고 | EfficientDet | 균형 |

```python
# YOLO 기반 객체 탐지 파이프라인 (간소화)
from ultralytics import YOLO

class ObjectDetectionPipeline:
    def __init__(self, model_path: str, conf_threshold: float = 0.5):
        self.model = YOLO(model_path)
        self.conf_threshold = conf_threshold

    def detect(self, image) -> list[dict]:
        results = self.model(image, conf=self.conf_threshold)
        detections = []
        for box in results[0].boxes:
            detections.append({
                "class": results[0].names[int(box.cls)],
                "confidence": float(box.conf),
                "bbox": box.xyxy[0].tolist(),  # [x1, y1, x2, y2]
            })
        return detections

    def detect_video(self, video_path, sample_rate: int = 5):
        """비디오에서 N프레임마다 탐지 수행"""
        # 모든 프레임을 처리하면 비용 과다
        # → 프레임 샘플링 + 트래킹으로 보간
        pass
```

> **핵심 직관**: 비디오 객체 탐지에서 모든 프레임을 독립적으로 처리하면 비용이 30fps × 모델 추론 시간만큼 발생합니다. **프레임 샘플링 + 객체 트래킹**으로 대부분의 연산을 절약할 수 있습니다.

## 4. 데이터 증강 전략

CV에서 데이터 증강은 모델 성능에 결정적 영향을 미칩니다.

```
데이터 증강 기법:

  기본 증강                    고급 증강
  ──────────                 ──────────
  - 좌우 반전                  - CutOut: 이미지 일부 제거
  - 랜덤 크롭                  - MixUp: 두 이미지 블렌딩
  - 색상 변환                  - CutMix: 영역 교환
  - 회전 (±15°)               - Mosaic: 4개 이미지 합성
  - 밝기/대비 조절              - AutoAugment: 자동 정책 탐색

  주의: 도메인에 따라 적합한 증강이 다름
  ├─ 의료 영상: 좌우 반전 주의 (심장 위치!)
  ├─ 위성 영상: 회전 자유 (상하좌우 무관)
  └─ 문서: 기울기, 노이즈 추가
```

## 5. 엣지 배포

모바일이나 IoT 디바이스에서 모델을 실행하기 위한 최적화 기법입니다.

| 기법 | 크기 감소 | 속도 향상 | 정확도 영향 |
|------|----------|----------|-----------|
| 양자화 (INT8) | 4× | 2-3× | -0.5~1% |
| 프루닝 (50%) | 2× | 1.5× | -0.5~2% |
| 지식 증류 | 5-10× | 3-5× | -1~3% |
| 아키텍처 검색(NAS) | — | 최적 | 유지 |

```
모델 최적화 파이프라인:

  원본 모델 (ResNet-50, 25M params)
       │
       ├─ 프루닝: 중요도 낮은 가중치 제거 → 12M params
       │
       ├─ 지식 증류: 큰 모델 → 작은 모델로 지식 전달
       │   Teacher: ResNet-50 → Student: MobileNet
       │
       ├─ 양자화: FP32 → INT8 (4× 메모리 절감)
       │
       └─ 런타임 최적화
           ├─ TensorRT (NVIDIA GPU)
           ├─ CoreML (Apple)
           ├─ TFLite (모바일)
           └─ ONNX Runtime (범용)
```

> **핵심 직관**: 엣지 배포에서는 "모델 정확도 1% 하락 vs 추론 속도 3배 향상"의 트레이드오프를 비즈니스 요구사항에 맞게 판단해야 합니다. sd-10에서 다룰 모델 서빙과 연결되는 핵심 주제입니다.

## 6. CV 시스템 평가 메트릭

```
태스크별 평가 메트릭:

  이미지 분류:
  └─ Top-1/5 Accuracy, F1 Score

  객체 탐지:
  ├─ mAP (mean Average Precision): IoU 임계값별
  │   mAP@0.5: IoU 50% 이상이면 정답
  │   mAP@0.5:0.95: 다양한 IoU에서 평균
  └─ FPS (Frames Per Second): 실시간성

  세그멘테이션:
  └─ mIoU (mean Intersection over Union)

  IoU 계산:
  ┌──────────┐
  │  예측 박스  │
  │   ┌──────┼──┐
  │   │교집합 │  │ 실제 박스
  └───┼──────┘  │
      └─────────┘
  IoU = 교집합 면적 / 합집합 면적
```

CV 시스템은 dl-05(CNN)과 dl-12(ViT)의 모델 지식을 프로덕션 환경에 적용하는 과정이며, 엣지 배포와 실시간 처리는 sp 과정의 시스템 프로그래밍 지식과도 연결됩니다.

## 핵심 정리

- CV 파이프라인은 **전처리 → 백본(특징 추출) → 태스크 헤드 → 후처리**로 구성됩니다
- 객체 탐지는 **Two-Stage(정확도)**와 **One-Stage(속도)** 중 시나리오에 맞게 선택합니다
- **데이터 증강**은 CV 성능에 결정적이며, 도메인 특성에 맞는 증강 기법을 선택해야 합니다
- 엣지 배포는 **양자화, 프루닝, 지식 증류**를 조합하여 정확도 손실을 최소화하며 최적화합니다
- 평가 메트릭은 태스크별로 다르며, 객체 탐지의 **mAP**과 실시간성의 **FPS**를 함께 고려합니다
