# 모델 서빙 아키텍처

## 왜 모델 서빙 아키텍처가 중요한가

아무리 정확한 모델도 프로덕션에서 안정적으로 서빙되지 않으면 가치가 없습니다. 모델 서빙은 학습된 모델을 **실시간으로 추론을 수행하며, 수백만 QPS를 안정적으로 처리**하는 인프라입니다. mlops 과정에서 개념적으로 다뤘다면, 여기서는 면접에서 물어보는 수준의 깊은 설계를 다룹니다.

> **핵심 직관**: 모델 서빙에서 가장 흔한 실수는 "오프라인에서 잘 되는 모델을 그대로 프로덕션에 배포하는 것"입니다. 오프라인과 온라인의 성능 격차(serving skew)를 줄이는 것이 핵심 과제입니다.

## 1. 서빙 패턴

```
모델 서빙 3대 패턴:

  온라인 서빙 (Real-time)
  ──────────────────────
  요청 → 피처 조회 → 추론 → 응답
  지연시간: < 100ms
  사용: 추천, 검색, 광고 (sd-02~04)

  배치 서빙 (Batch)
  ──────────────────────
  전체 데이터 → 배치 추론 → 결과 저장
  지연시간: 시간 단위
  사용: 이메일 추천, 일일 리포트

  비동기 서빙 (Async / Near-real-time)
  ──────────────────────
  이벤트 → 큐 → 추론 → 결과 저장
  지연시간: 초~분 단위
  사용: 콘텐츠 모더레이션, 이미지 처리
```

| 패턴 | 지연시간 | 처리량 | 복잡도 | 비용 |
|------|---------|--------|--------|------|
| 온라인 | < 100ms | 중간 | 높음 | 높음 |
| 배치 | 시간 | 매우 높음 | 낮음 | 낮음 |
| 비동기 | 초~분 | 높음 | 중간 | 중간 |

## 2. 지연시간 최적화

```
지연시간 분해:

  전체 지연시간 (End-to-end Latency)
       │
       ├─ 네트워크 지연: ~5ms
       │
       ├─ 피처 조회: ~10ms
       │   ├─ 캐시 히트: 1ms
       │   └─ 피처 스토어: 5-10ms
       │
       ├─ 모델 추론: ~20ms
       │   ├─ 전처리: 2ms
       │   ├─ 추론: 15ms
       │   └─ 후처리: 3ms
       │
       └─ 비즈니스 로직: ~5ms

  총: ~40ms (p50), ~80ms (p99)
```

| 최적화 기법 | 지연시간 감소 | 구현 난이도 |
|-----------|-------------|-----------|
| 모델 양자화 (INT8) | 2-4× | 낮음 |
| 배칭 (Dynamic Batching) | 처리량 ↑ | 중간 |
| 모델 캐싱 (결과 캐시) | 10-100× | 낮음 |
| GPU 추론 | 5-10× | 중간 |
| 모델 경량화 (sd-08) | 2-5× | 높음 |
| 그래프 최적화 (TensorRT) | 2-3× | 중간 |

```python
# 동적 배칭: 개별 요청을 모아서 한 번에 처리
import asyncio
from collections import deque

class DynamicBatcher:
    def __init__(self, model, max_batch: int = 32, max_wait_ms: int = 10):
        self.model = model
        self.max_batch = max_batch
        self.max_wait = max_wait_ms / 1000
        self.queue = deque()  # (input, future) 쌍

    async def predict(self, input_data):
        """개별 요청을 큐에 추가하고 배치 처리 대기"""
        future = asyncio.Future()
        self.queue.append((input_data, future))

        if len(self.queue) >= self.max_batch:
            await self._process_batch()
        else:
            await asyncio.sleep(self.max_wait)
            if self.queue:  # 타임아웃 후 남은 요청 처리
                await self._process_batch()

        return await future

    async def _process_batch(self):
        batch = []
        futures = []
        while self.queue and len(batch) < self.max_batch:
            inp, fut = self.queue.popleft()
            batch.append(inp)
            futures.append(fut)

        # 배치 추론 (GPU 활용 극대화)
        results = self.model.predict_batch(batch)
        for fut, result in zip(futures, results):
            fut.set_result(result)
```

> **핵심 직관**: 동적 배칭은 "개별 요청의 지연시간을 약간 희생하여 전체 처리량을 크게 높이는" 트레이드오프입니다. max_wait를 너무 길게 잡으면 지연시간이 늘고, 짧게 잡으면 배칭 효과가 줄어듭니다.

## 3. A/B 테스트 인프라

```
A/B 테스트 아키텍처:

  [사용자 요청]
       │
       ▼
  [트래픽 분배기]
  ├─ 실험 그룹 A (50%): 기존 모델 v1.2
  └─ 실험 그룹 B (50%): 신규 모델 v1.3
       │
       ▼
  [결과 로깅]
       │
       ▼
  [통계 분석]
  ├─ 가드레일 메트릭 (크래시율, 지연시간)
  ├─ 핵심 메트릭 (CTR, 매출)
  └─ 통계적 유의성 (p-value < 0.05)

  핵심 원칙:
  ├─ 동일 사용자는 항상 같은 그룹 (해시 기반)
  ├─ 충분한 샘플 크기 확보 (검정력 분석)
  └─ 가드레일 위반 시 자동 롤백
```

| 항목 | 설명 |
|------|------|
| 분배 단위 | 사용자 ID 해시 (세션 아님) |
| 실험 기간 | 최소 1-2주 (요일 효과 포함) |
| 샘플 크기 | 검출하려는 효과 크기에 따라 계산 |
| 가드레일 | 지연시간, 에러율, 크래시율 |
| 다중 비교 | Bonferroni 보정 |

## 4. 카나리 배포와 점진적 롤아웃

```
배포 전략 비교:

  빅뱅 배포 (위험)
  └─ 100% 트래픽을 한 번에 전환
     위험: 버그 발생 시 전체 영향

  카나리 배포 (안전)
  └─ 1% → 5% → 25% → 100% 점진적 확대
     각 단계에서 메트릭 확인 후 확대/롤백 결정

  블루-그린 배포
  └─ 두 환경(Blue, Green)을 번갈아 사용
     롤백: 트래픽을 이전 환경으로 전환

  섀도우 배포
  └─ 실제 트래픽으로 추론하되 결과는 버림
     실제 환경 성능 검증, 사용자 영향 없음
```

```
카나리 배포 판단 기준:

  트래픽 1% 배포
       │
       ├─ 에러율 증가? → 즉시 롤백
       │
       ├─ 지연시간 p99 악화? → 즉시 롤백
       │
       ├─ 핵심 메트릭 하락? → 대기 또는 롤백
       │
       └─ 모두 정상? → 5%로 확대
            └─ ... → 25% → 50% → 100%
```

> **핵심 직관**: 프로덕션 ML에서 "롤백 가능성"은 설계의 핵심입니다. 모델 버전 관리, 피처 호환성, 트래픽 전환을 모두 고려한 롤백 계획이 없으면 배포를 시작하면 안 됩니다.

## 5. 모니터링과 알림

```
모델 서빙 모니터링 계층:

  [인프라 모니터링]
  ├─ CPU/GPU 사용률
  ├─ 메모리 사용량
  └─ 네트워크 I/O

  [서비스 모니터링]
  ├─ QPS (초당 요청 수)
  ├─ 지연시간 (p50, p95, p99)
  ├─ 에러율
  └─ 가용성

  [모델 모니터링]
  ├─ 입력 분포 드리프트
  ├─ 예측 분포 변화
  ├─ 온라인 메트릭 (CTR 등)
  └─ 피처 품질 (sd-09)

  알림 우선순위:
  P0 (즉시): 에러율 > 5%, 서비스 다운
  P1 (1시간 내): 지연시간 p99 2× 증가
  P2 (일 단위): 입력 분포 드리프트 감지
```

## 6. 스케일링 전략

| 전략 | 설명 | 적합 상황 |
|------|------|----------|
| 수직 스케일링 | 더 큰 GPU/CPU | 초기 단계 |
| 수평 스케일링 | 서버 수 증가 | 트래픽 증가 |
| 오토스케일링 | 트래픽에 따라 자동 조정 | 트래픽 변동 큼 |
| 멀티 리전 | 지리적 분산 배포 | 글로벌 서비스 |
| 모델 샤딩 | 대형 모델 분산 배치 | LLM 서빙 (sd-11) |

모델 서빙은 sd-01~sd-08의 모든 ML 시스템이 공유하는 프로덕션 인프라이며, sd-11에서 다룰 LLM 서빙의 기초가 됩니다.

## 핵심 정리

- 모델 서빙은 **온라인(실시간), 배치, 비동기** 3가지 패턴이 있으며, 요구사항에 맞게 선택합니다
- 지연시간 최적화는 **양자화, 동적 배칭, 결과 캐싱, GPU 추론**을 조합하여 달성합니다
- **A/B 테스트**는 사용자 ID 해시 기반으로 분배하고, 가드레일 메트릭으로 안전성을 보장합니다
- **카나리 배포**(1% → 5% → 25% → 100%)로 위험을 최소화하며, 롤백 계획은 배포 전 필수입니다
- 모니터링은 **인프라 → 서비스 → 모델** 3계층으로 구성하며, 알림 우선순위를 명확히 정합니다
