# LLM 애플리케이션 설계

## 왜 LLM 애플리케이션 설계가 중요한가

LLM(Large Language Model)은 ML 시스템 설계의 패러다임을 바꾸고 있습니다. 기존에는 태스크별로 모델을 학습했지만, 이제는 **하나의 범용 모델을 프롬프트와 검색으로 특화**시키는 방식이 주류가 되었습니다. llm-engineering 과정에서 기초를 다뤘다면, 여기서는 면접에서 출제되는 수준의 시스템 설계를 다룹니다.

> **핵심 직관**: LLM 애플리케이션의 핵심 도전은 "환각(hallucination)을 줄이면서 비용을 관리하는 것"입니다. RAG, 가드레일, 캐싱이 이 두 가지를 동시에 해결하는 핵심 기법입니다.

## 1. LLM 애플리케이션 아키텍처 패턴

```
LLM 애플리케이션 3대 패턴:

  패턴 1: 직접 프롬프팅
  ─────────────────────
  사용자 입력 → 프롬프트 구성 → LLM → 응답
  적합: 단순 분류, 요약, 번역
  비용: 낮음

  패턴 2: RAG (Retrieval-Augmented Generation)
  ─────────────────────
  사용자 입력 → 관련 문서 검색 → 프롬프트+문서 → LLM → 응답
  적합: 지식 기반 Q&A, 고객 지원
  비용: 중간

  패턴 3: 에이전트 (Agent)
  ─────────────────────
  사용자 입력 → LLM(계획) → 도구 실행 → LLM(종합) → 응답
  적합: 복잡한 태스크, 멀티스텝 추론
  비용: 높음
```

| 패턴 | 정확도 | 비용 | 지연시간 | 복잡도 |
|------|--------|------|---------|--------|
| 직접 프롬프팅 | 낮~중 | 낮음 | 낮음 | 낮음 |
| RAG | 중~높 | 중간 | 중간 | 중간 |
| 에이전트 | 높음 | 높음 | 높음 | 높음 |

## 2. RAG 시스템 설계

```
RAG 파이프라인 상세:

  [오프라인] 문서 인덱싱:
  ┌──────────────────────────────────────────┐
  │ 원본 문서                                   │
  │    │                                      │
  │    ▼                                      │
  │ [청킹 Chunking]                            │
  │ ├─ 고정 크기 (512 토큰)                     │
  │ ├─ 문단/섹션 기반                           │
  │ └─ 시맨틱 기반 (의미 단위)                   │
  │    │                                      │
  │    ▼                                      │
  │ [임베딩 생성] → [벡터 스토어 저장]             │
  │                 (FAISS, Pinecone, Weaviate)  │
  └──────────────────────────────────────────┘

  [온라인] 쿼리 처리:
  쿼리 → 임베딩 → 유사 문서 검색(Top-K) → 프롬프트 구성 → LLM → 응답
```

| 청킹 전략 | 장점 | 단점 | 적합 상황 |
|----------|------|------|----------|
| 고정 크기 | 단순, 일관성 | 의미 단위 파괴 | 일반 텍스트 |
| 문단 기반 | 의미 보존 | 크기 불균일 | 구조화 문서 |
| 시맨틱 | 최적 의미 단위 | 계산 비용 | 높은 품질 필요 |
| 재귀적 | 계층적 구조 반영 | 구현 복잡 | 긴 문서 |

```python
# RAG 파이프라인 (간소화)
class RAGPipeline:
    def __init__(self, vector_store, llm, embed_model):
        self.vector_store = vector_store
        self.llm = llm
        self.embed_model = embed_model

    def query(self, question: str, top_k: int = 5) -> str:
        # 1. 쿼리 임베딩
        query_emb = self.embed_model.encode(question)

        # 2. 유사 문서 검색
        docs = self.vector_store.search(query_emb, top_k=top_k)

        # 3. 프롬프트 구성
        context = "\n\n".join([d.text for d in docs])
        prompt = f"""다음 문서를 참고하여 질문에 답하세요.
문서에 없는 내용은 "알 수 없습니다"라고 답하세요.

문서:
{context}

질문: {question}
답변:"""

        # 4. LLM 호출
        return self.llm.generate(prompt)
```

> **핵심 직관**: RAG의 품질은 **검색 품질**에 크게 의존합니다. 잘못된 문서를 검색하면 LLM도 잘못된 답을 생성합니다. sd-03에서 다룬 검색 시스템 설계가 RAG의 기초입니다.

## 3. 에이전트 아키텍처

```
에이전트 실행 루프:

  [사용자 요청]
       │
       ▼
  [LLM: 계획 수립]
  "이 태스크를 완료하려면 3단계가 필요합니다:
   1. 데이터베이스에서 사용자 정보 조회
   2. 외부 API로 추천 결과 가져오기
   3. 결과를 종합하여 응답 생성"
       │
       ▼
  [도구 실행]
  ├─ tool_call: query_db(user_id=123)
  │   → 결과: {name: "김철수", ...}
  ├─ tool_call: get_recommendations(user_id=123)
  │   → 결과: [item_1, item_2, ...]
       │
       ▼
  [LLM: 결과 종합]
  "김철수님, 추천 상품은 ..."
       │
       ▼
  [응답 반환]

  메모리 관리:
  ├─ 단기 메모리: 현재 대화 히스토리
  ├─ 장기 메모리: 사용자 선호도 DB
  └─ 작업 메모리: 중간 결과 저장
```

## 4. 비용 관리

LLM 비용은 토큰 수에 비례하며, 대규모 서비스에서는 빠르게 증가합니다.

| 최적화 기법 | 비용 절감 | 품질 영향 |
|-----------|----------|----------|
| 프롬프트 최적화 | 20-50% | 최소 |
| 시맨틱 캐싱 | 30-70% | 없음 |
| 모델 라우팅 | 40-60% | 최소 |
| 파인튜닝 대체 | 50-80% | 개선 가능 |

```
모델 라우팅 전략:

  [사용자 요청]
       │
       ▼
  [복잡도 분류기]
  ├─ 단순 질문 → 소형 모델 (GPT-4o-mini, Haiku)
  │               비용: $0.01 / 1K 토큰
  │
  ├─ 보통 질문 → 중형 모델 (Sonnet, GPT-4o)
  │               비용: $0.03 / 1K 토큰
  │
  └─ 복잡한 질문 → 대형 모델 (Opus, o1)
                    비용: $0.15 / 1K 토큰

  전체 비용 = 80% × $0.01 + 15% × $0.03 + 5% × $0.15
            ≈ 원래의 30% 수준
```

> **핵심 직관**: 시맨틱 캐싱은 "동일하거나 유사한 질문에 대해 이전 답변을 재사용하는 것"입니다. 임베딩 유사도로 캐시 히트를 판단하면, LLM 호출 없이 즉시 응답할 수 있습니다.

## 5. 가드레일과 안전성

```
LLM 가드레일 아키텍처:

  [사용자 입력]
       │
       ▼
  [입력 가드레일]
  ├─ 프롬프트 인젝션 탐지
  ├─ PII (개인정보) 마스킹
  ├─ 유해 콘텐츠 필터링
  └─ 토픽 제한 (off-topic 차단)
       │
       ▼
  [LLM 처리]
       │
       ▼
  [출력 가드레일]
  ├─ 환각 탐지 (출처 대조)
  ├─ 유해 콘텐츠 필터링
  ├─ 형식 검증 (JSON, 길이)
  └─ 브랜드 가이드라인 준수
       │
       ▼
  [사용자 응답]
```

| 위협 | 탐지 방법 | 대응 |
|------|----------|------|
| 프롬프트 인젝션 | 패턴 매칭 + 분류기 | 차단 + 로깅 |
| 환각 | 출처 문서와 대조 (RAG) | 경고 표시 |
| PII 노출 | NER 기반 탐지 | 마스킹 |
| 유해 콘텐츠 | 독성 분류기 | 차단 |
| 탈옥 시도 | 의도 분류 | 거부 응답 |

## 6. LLM 평가 프레임워크

```
LLM 평가 차원:

  [자동 평가]
  ├─ 관련성 (Relevance): 질문에 맞는 답인가?
  ├─ 충실성 (Faithfulness): 출처와 일치하는가? (RAG)
  ├─ 유해성 (Toxicity): 유해 콘텐츠가 포함되었는가?
  └─ 형식 (Format): 요구 형식을 따르는가?

  [사람 평가]
  ├─ 유용성 (Helpfulness)
  ├─ 정확성 (Correctness)
  └─ 자연스러움 (Naturalness)

  [LLM-as-Judge]
  └─ 다른 LLM으로 응답 품질 평가 (비용 효율적)
```

LLM 애플리케이션은 sd-03(검색)의 시맨틱 검색, sd-07(NLP)의 텍스트 처리, sd-10(서빙)의 인프라 지식이 모두 결합되는 종합 시스템입니다.

## 핵심 정리

- LLM 애플리케이션은 **직접 프롬프팅, RAG, 에이전트** 3가지 패턴으로 분류되며, 복잡도에 따라 선택합니다
- RAG 시스템의 품질은 **청킹 전략과 검색 품질**에 크게 의존하며, sd-03의 검색 기법이 기초입니다
- **비용 관리**는 모델 라우팅(복잡도별 모델 선택)과 시맨틱 캐싱으로 60-70% 절감이 가능합니다
- **가드레일**은 입력(인젝션, PII)과 출력(환각, 유해)을 이중으로 필터링하는 구조입니다
- LLM 평가는 **자동 메트릭 + 사람 평가 + LLM-as-Judge**를 조합하여 다차원으로 수행합니다
