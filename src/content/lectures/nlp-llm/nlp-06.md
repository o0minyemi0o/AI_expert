# LLM 사전학습과 스케일링 법칙

## 왜 스케일링 법칙이 중요한가

"더 큰 모델 + 더 많은 데이터 = 더 좋은 성능"은 직관적이지만, **얼마나 더 좋아지는지**를 정량적으로 예측할 수 있을까요? **스케일링 법칙(Scaling Laws)**은 모델 크기, 데이터 양, 계산량과 성능의 관계를 멱법칙(power law)으로 설명합니다. 이 법칙 덕분에 수억 달러를 투자하기 전에 최종 성능을 예측할 수 있게 되었습니다.

> **핵심 직관**: 스케일링 법칙의 핵심 발견은 **"성능이 모델/데이터/계산의 멱법칙을 따른다"**는 것입니다. Loss $\propto N^{-\alpha}$. 이는 10배 큰 모델이 10배 좋아지는 게 아니라 $10^{-\alpha}$만큼만 좋아진다는 뜻이며, 자원 배분의 과학적 근거를 제공합니다.

## 1. 스케일링 법칙 기초

```
Kaplan et al. (2020) — OpenAI 스케일링 법칙:

  L(N) ≈ (N_c / N)^α_N     (모델 크기 N)
  L(D) ≈ (D_c / D)^α_D     (데이터 크기 D)
  L(C) ≈ (C_c / C)^α_C     (계산량 C)

  L: 크로스 엔트로피 손실 (낮을수록 좋음)
  α_N ≈ 0.076, α_D ≈ 0.095, α_C ≈ 0.050

  핵심 발견:
  1. 멱법칙: log-log 스케일에서 직선
     → 자원을 10배 늘리면 일정 비율만큼 개선
  2. 세 변수가 독립적으로 영향
  3. 모델 구조(깊이/넓이 비율)보다 총 파라미터 수가 중요
  4. 수렴 전에 멈춰도 최종 성능 예측 가능

  Kaplan의 결론:
  "고정 예산에서 모델을 크게, 데이터를 적게"
  → 대형 모델을 짧게 학습하는 것이 효율적
  → GPT-3의 학습 전략에 반영
```

```
Chinchilla 법칙 (Hoffmann et al., 2022, DeepMind):

  Kaplan의 결론을 뒤집음!
  "고정 예산에서 모델과 데이터를 같은 비율로 키워라"

  최적 비율:
  D_opt ≈ 20 × N
  → 1B 모델에는 20B 토큰
  → 70B 모델에는 1.4T 토큰

  Chinchilla (70B) vs Gopher (280B):
  같은 계산 예산, Chinchilla가 더 작지만 더 우수!
  → Gopher는 "과대(over-parameterized)" 모델
  → 데이터가 부족했음

  | 모델 | 파라미터 | 학습 토큰 | 성능 |
  |------|---------|----------|------|
  | Gopher | 280B | 300B | 기준 |
  | Chinchilla | 70B | 1.4T | 더 우수 |

  산업 영향:
  ├─ LLaMA (65B, 1.4T tokens): Chinchilla 최적
  ├─ LLaMA-2 (70B, 2T tokens): 더 많은 데이터
  ├─ LLaMA-3 (70B, 15T+ tokens): Chinchilla 초과
  └─ 추론 비용 고려 시 작은 모델 + 더 많은 데이터가 유리
```

> **핵심 직관**: Chinchilla 법칙의 교훈은 **"학습 계산 최적과 추론 최적은 다르다"**는 것입니다. 학습 FLOP당 최적은 D=20N이지만, 추론 비용을 고려하면 작은 모델을 더 오래 학습(over-training)하는 것이 실무적으로 유리합니다. LLaMA-3가 이 전략을 따릅니다.

## 2. 대규모 사전학습 데이터

```
사전학습 데이터 파이프라인:

  수집 → 필터링 → 중복 제거 → 혼합 → 토크나이제이션

  1. 수집 (Raw Data):
     Common Crawl: 수 PB, 웹 전체 크롤
     Wikipedia: 고품질 백과사전
     Books: Project Gutenberg, Books3
     Code: GitHub (StarCoder, CodeLlama)
     학술: arXiv, PubMed

  2. 필터링 (Quality Filtering):
     ├─ 언어 감지: fastText로 언어 분류
     ├─ 품질 점수: 퍼플렉시티 기반 (Wikipedia로 학습한 LM)
     ├─ 유해 콘텐츠 제거: 분류기 기반
     ├─ 개인정보 제거: 이메일, 전화번호, 주소
     └─ 반복 텍스트 제거: "Click here click here..."

  3. 중복 제거 (Deduplication):
     ├─ Exact: 해시 기반 완전 중복 제거
     ├─ Near: MinHash + LSH로 유사 문서 제거
     │   → 30-50%의 데이터가 중복!
     └─ 문장 수준: 반복 문장 제거

  4. 데이터 혼합 (Data Mix):
     단순히 "많이"가 아니라 "적절한 비율"이 핵심

     LLaMA의 데이터 혼합:
     | 소스 | 비율 | 특성 |
     |------|------|------|
     | CommonCrawl | 67% | 다양성 |
     | C4 | 15% | 정제된 웹 |
     | Wikipedia | 4.5% | 사실적 |
     | Books | 4.5% | 장문 |
     | ArXiv | 2.5% | 학술 |
     | GitHub | 4.5% | 코드 |
     | StackExchange | 2% | Q&A |
```

## 3. 학습 안정성과 기법

```
대규모 학습의 도전:

  Loss Spike:
  학습 중 갑자기 loss가 급등하는 현상
  원인: 배치 내 이상치, 학습률 문제, 수치 불안정

  대처:
  ├─ 이전 체크포인트로 롤백 + 문제 배치 건너뛰기
  ├─ 학습률 일시 감소
  └─ 그래디언트 클리핑 강화

학습 하이퍼파라미터 (일반적 설정):

  옵티마이저: AdamW
    β₁ = 0.9, β₂ = 0.95, ε = 1e-8
    weight_decay = 0.1

  학습률 스케줄: Warmup + Cosine Decay
    Warmup: 2000 steps에서 0 → peak_lr
    Cosine: peak → peak × 0.1까지 감소

  배치 크기: 점진적 증가
    초기: 작게 (256K tokens)
    → 중기: 키움 (1M tokens)
    → 후기: 최대 (4M+ tokens)

  시퀀스 길이: 2048 → 4096 → 8192 (점진적)
  정밀도: BF16 (Brain Float 16) 또는 Mixed Precision
```

```
분산 학습 전략:

  모델이 단일 GPU 메모리에 안 들어감!
  70B × 2 bytes (BF16) = 140GB > GPU 메모리 (80GB)

  1. 데이터 병렬 (Data Parallel, DP):
     같은 모델 복사본을 여러 GPU에
     다른 배치를 각 GPU에서 처리
     그래디언트를 평균 → 동기화

  2. 텐서 병렬 (Tensor Parallel, TP):
     하나의 레이어를 여러 GPU에 분할
     어텐션 헤드를 GPU마다 나눠 계산
     → 노드 내 (NVLink 활용)

  3. 파이프라인 병렬 (Pipeline Parallel, PP):
     레이어를 GPU에 순차 배치
     GPU 0: 레이어 1-8, GPU 1: 레이어 9-16, ...
     → 마이크로배치로 파이프라인 효율 향상

  4. FSDP (Fully Sharded Data Parallel):
     ZeRO의 PyTorch 구현
     파라미터, 그래디언트, 옵티마이저 상태를 샤딩
     필요할 때만 all-gather → 메모리 절약

  LLaMA-2 70B 학습:
  2,000 A100 GPU × 수개월
  DP + TP(8) + PP(시퀀스 기반)
  → 수천만 달러 비용
```

## 4. 데이터 품질의 영향

```
"Scaling Data-Constrained Language Models" (2023):

  핵심 발견:
  1. 데이터 반복은 해롭다
     4회 이상 반복하면 수확 체감 급격
  2. 데이터 품질 > 양
     고품질 데이터 1T > 저품질 데이터 5T
  3. 데이터 혼합 비율이 중요
     코드를 너무 많이 넣으면 자연어 성능 저하

Phi 시리즈 (Microsoft):
  "교과서 수준의 데이터로 작은 모델도 강할 수 있다"

  Phi-1 (1.3B): 코딩 데이터만, 작은 모델인데 HumanEval 50%+
  Phi-2 (2.7B): 고품질 합성 데이터, 13B급 성능
  Phi-3 (3.8B): 더 정교한 데이터 필터링

  시사점:
  ├─ 데이터 품질의 극적인 영향
  ├─ 합성 데이터의 가능성
  └─ 작은 모델도 충분히 강할 수 있음

합성 데이터 (Synthetic Data):
  ├─ 강한 모델(GPT-4)로 학습 데이터 생성
  ├─ "Self-Instruct": LLM이 자기 학습 데이터 생성
  ├─ 위험: Model Collapse — 합성 데이터로만 학습 시
  │   다양성 감소, 꼬리 분포 손실
  └─ 해결: 실제 데이터와 합성 데이터의 적절한 혼합
```

## 5. Emergent Abilities과 스케일링의 한계

```
창발적 능력 (Emergent Abilities, Wei et al., 2022):

  특정 스케일 이상에서 갑자기 나타나는 능력
  ├─ Few-shot 산술: ~100B에서 등장
  ├─ Chain-of-Thought: ~60B에서 효과적
  ├─ 코드 실행 추적: ~100B에서 가능
  └─ 다단계 추론: ~100B+에서

  그래프:
  성능
  │          ┌─── 창발 (갑자기!)
  │         ╱
  │        │
  │───────╱
  │      ╱
  └──────────────── 모델 크기

  반론 (Schaeffer et al., 2023):
  "창발은 측정 지표의 artifact"
  ├─ 정확도(0/1) → 불연속적으로 보임
  ├─ 토큰 확률 → 연속적으로 개선
  └─ "창발"이 아니라 "비선형 지표의 phase transition"

  실무적 의미:
  ├─ 스케일업이 만능이 아님
  ├─ 작은 모델도 적절한 전략으로 많은 것을 달성
  ├─ 파인튜닝, CoT, 도구 사용으로 보완 가능
  └─ "충분히 큰 모델"의 정의가 태스크마다 다름
```

## 6. 사전학습의 미래 방향

```
현재 도전과 미래:

  데이터 벽 (Data Wall):
  ├─ 인터넷의 고품질 텍스트는 유한 (~10T 토큰?)
  ├─ 합성 데이터의 품질 한계
  ├─ 저작권 문제 증가
  └─ Model Collapse 위험

  계산 효율:
  ├─ MoE (Mixture of Experts): 활성 파라미터만 사용
  │   Mixtral: 47B total, 13B active → 성능은 70B급
  ├─ 효율적 아키텍처: Mamba (상태 공간 모델)
  └─ 학습 알고리즘 개선: 더 적은 스텝으로 수렴

  다음 패러다임:
  ├─ Test-time compute: 학습 시 계산 → 추론 시 계산
  │   → 더 작은 모델이 추론에 더 많은 시간을 쓸 수 있음
  ├─ Continual pretraining: 새 데이터로 지속 학습
  ├─ 다국어/다모달 통합 사전학습
  └─ Curriculum learning: 쉬운 것 → 어려운 것 순서

  | 전략 | 장점 | 한계 |
  |------|------|------|
  | 모델 스케일업 | 검증된 방법 | 비용, 데이터 벽 |
  | 데이터 품질 | 비용 효율적 | 수작업 필요 |
  | MoE | 추론 효율 | 학습 복잡도 |
  | 합성 데이터 | 확장 가능 | Collapse 위험 |
  | Test-time compute | 유연함 | 지연시간 증가 |
```

> **핵심 직관**: 스케일링 법칙은 **"더 많이 = 더 좋게"가 아니라 "정확히 얼마나 더 좋아지는지 예측 가능"**하다는 것입니다. 이 예측 가능성 덕분에 수억 달러의 투자 결정이 가능하고, 동시에 그 한계(데이터 벽, 수확 체감)도 명확히 드러납니다.

## 핵심 정리

- **Kaplan 스케일링 법칙**은 손실이 모델·데이터·계산의 멱법칙을 따른다는 것을 보여, 성능 예측을 가능하게 했습니다
- **Chinchilla 법칙**은 최적 학습에 $D \approx 20N$ 토큰이 필요하다고 밝혔으며, 추론 비용을 고려한 over-training 전략이 실무 표준입니다
- 사전학습 데이터는 **수집→필터링→중복제거→혼합**의 파이프라인을 거치며, 데이터 품질이 양보다 중요합니다
- 분산 학습은 **DP, TP, PP, FSDP**를 조합하며, 70B 모델은 수천 GPU로 수개월간 학습합니다
- **데이터 벽과 수확 체감**이 순수 스케일업의 한계이며, MoE, 합성 데이터, test-time compute 등이 차세대 전략입니다
