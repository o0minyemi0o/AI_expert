# 토크나이제이션과 어휘 설계

## 왜 토크나이제이션이 중요한가

LLM의 입력은 텍스트가 아니라 **토큰 ID의 시퀀스**입니다. "unhappiness"를 하나의 토큰으로 볼지, "un", "happiness"로 나눌지, "un", "happi", "ness"로 나눌지에 따라 모델의 어휘 크기, 시퀀스 길이, 다국어 성능이 달라집니다. 토크나이저는 LLM의 **"감각 기관"**이며, 그 설계가 모델 전체의 성능 상한을 결정합니다.

> **핵심 직관**: 토크나이제이션의 핵심 트레이드오프는 **"어휘 크기 vs 시퀀스 길이"**입니다. 글자 단위(어휘 ~256)는 시퀀스가 길어지고, 단어 단위(어휘 ~100K+)는 미등록 단어가 문제입니다. 서브워드(subword) 토크나이제이션은 이 둘 사이의 최적점을 찾습니다.

## 1. 토크나이제이션의 기본 개념

```
토크나이제이션 스펙트럼:

  문자 단위:
  "unhappiness" → [u, n, h, a, p, p, i, n, e, s, s]
  어휘: ~256 (ASCII) 또는 ~65K (Unicode)
  장점: OOV 없음, 어휘 작음
  단점: 시퀀스 매우 길어짐 → 어텐션 비용 O(N²)

  단어 단위:
  "unhappiness" → [unhappiness]
  어휘: 수만~수십만
  장점: 의미 단위 보존
  단점: OOV 문제, 임베딩 테이블 거대

  서브워드 단위 (현대 LLM의 표준):
  "unhappiness" → [un, happi, ness]
  어휘: 30K~100K
  장점: OOV 해결 + 적절한 시퀀스 길이
  핵심: 자주 나오는 패턴은 통째로, 드문 것은 분해

  | 방식 | 어휘 크기 | 시퀀스 길이 | OOV |
  |------|----------|-----------|-----|
  | 문자 | ~256 | 매우 길다 | 없음 |
  | 단어 | ~100K+ | 짧다 | 많음 |
  | 서브워드 | 32K~128K | 적절 | 거의 없음 |
```

## 2. BPE (Byte Pair Encoding)

```
BPE (Sennrich et al., 2015):
  가장 널리 사용되는 서브워드 토크나이제이션

  학습 알고리즘:
  1. 초기 어휘: 모든 문자 (또는 바이트)
  2. 코퍼스에서 가장 자주 등장하는 인접 쌍 찾기
  3. 그 쌍을 새 토큰으로 병합
  4. 원하는 어휘 크기에 도달할 때까지 반복

  예시:
  코퍼스: "low lower lowest new newer newest"
  초기: {l, o, w, e, r, s, t, n, _}

  Step 1: (e, r) 가장 빈번 → 'er' 추가
  Step 2: (e, s) 빈번 → 'es' 추가
  Step 3: (es, t) 빈번 → 'est' 추가
  Step 4: (l, o) 빈번 → 'lo' 추가
  Step 5: (lo, w) 빈번 → 'low' 추가
  Step 6: (n, e) 빈번 → 'ne' 추가
  Step 7: (ne, w) 빈번 → 'new' 추가

  결과: {l, o, w, e, r, s, t, n, _, er, es, est, lo, low, ne, new}

  토크나이제이션:
  "lowest" → [low, est]
  "newer" → [new, er]
  "unknown" → [u, n, k, n, o, w, n]  (드문 단어는 잘게)

GPT 계열의 BPE:
  GPT-2: 50,257 토큰 (바이트 수준 BPE)
  GPT-3/4: ~100K 토큰
  LLaMA: 32,000 토큰
  LLaMA-3: 128,000 토큰 (다국어 확장)
```

## 3. WordPiece와 SentencePiece

```
WordPiece (Schuster & Nakajima, 2012):
  BERT가 사용하는 방식

  BPE와의 차이:
  BPE: 빈도 기반 — 가장 자주 나오는 쌍 병합
  WordPiece: 우도 기반 — 병합 후 코퍼스 우도가 가장 증가하는 쌍

  병합 기준:
  score(a, b) = freq(ab) / (freq(a) × freq(b))
  → 개별 빈도 대비 동시 출현이 많은 쌍 우선

  표기: 단어 중간의 서브워드에 ## 접두사
  "unhappiness" → [un, ##happi, ##ness]
  "playing" → [play, ##ing]

  BERT 어휘: 30,522 토큰 (영어)
  [CLS], [SEP], [MASK], [PAD], [UNK] 등 특수 토큰 포함

SentencePiece (Kudo & Richardson, 2018):
  언어 독립적 토크나이저

  핵심 특징:
  ├─ 사전 토크나이제이션 불필요
  │   (공백 분리 없이 원시 텍스트에서 직접 학습)
  ├─ 공백을 특수 문자 ▁로 인코딩
  │   "I love cats" → [▁I, ▁love, ▁cats]
  ├─ BPE 또는 Unigram 알고리즘 선택 가능
  └─ 역변환이 무손실 (원래 텍스트 완벽 복원)

  왜 중요한가:
  영어는 공백으로 단어를 구분하지만,
  중국어, 일본어, 한국어는 공백이 없거나 다르게 사용
  → SentencePiece가 다국어 LLM의 표준 (T5, LLaMA)
```

```
Unigram 모델 (Kudo, 2018):
  BPE와 반대 방향 — 큰 어휘에서 시작하여 축소

  1. 모든 가능한 서브워드를 포함하는 큰 어휘로 시작
  2. 각 서브워드의 확률을 EM 알고리즘으로 추정
  3. 제거 시 코퍼스 우도 감소가 가장 작은 서브워드 제거
  4. 원하는 크기까지 반복

  토크나이제이션:
  주어진 텍스트의 가장 높은 확률 분해를 선택
  x* = argmax P(x₁)P(x₂)...P(xₙ)
  → Viterbi 알고리즘으로 최적 분해

  BPE vs Unigram:
  ├─ BPE: 결정적, 하나의 분해만 존재
  ├─ Unigram: 확률적, 여러 분해 가능 → 정규화 효과
  └─ Subword Regularization: 학습 시 다른 분해를 샘플링
     → 모델이 특정 분해에 과적합하지 않음
```

> **핵심 직관**: BPE는 **"자주 함께 나오는 것을 묶자"**(빈도 기반), WordPiece는 **"묶으면 전체적으로 더 잘 설명되는 것을 묶자"**(우도 기반), Unigram은 **"제거해도 설명력이 줄지 않는 것을 버리자"**(감산 기반)입니다. 결과적으로 성능 차이는 크지 않지만, SentencePiece + BPE/Unigram이 현대 LLM의 표준입니다.

## 4. 바이트 수준 토크나이제이션

```
Byte-level BPE (Radford et al., 2019, GPT-2):
  문자가 아닌 바이트(0~255)에서 시작

  왜 바이트인가:
  ├─ 어떤 언어/문자도 바이트로 표현 가능
  ├─ UTF-8: 영어 1바이트, 한국어 3바이트, 이모지 4바이트
  ├─ [UNK] 토큰이 완전히 불필요
  └─ 진정한 언어 독립성

  "안녕" = 0xEC 0x95 0x88 0xEB 0x85 0x95 (UTF-8, 6바이트)
  → BPE 학습 후: [안, 녕] 또는 [안녕] (빈도에 따라)

  단점: 비라틴 문자에 불리
  ├─ "hello" = 5바이트 → 1-2토큰
  ├─ "안녕하세요" = 15바이트 → 3-5토큰
  └─ 같은 의미를 전달하는데 더 많은 토큰 소모
  → "토큰 세금(token tax)" 문제

Byte-Pair BPE 어휘 크기의 영향:
  | 모델 | 어휘 크기 | 비고 |
  |------|----------|------|
  | GPT-2 | 50,257 | 영어 중심 |
  | LLaMA | 32,000 | 영어 + 코드 |
  | LLaMA-3 | 128,000 | 다국어 확장 |
  | Gemma | 256,000 | 대규모 다국어 |

  어휘가 클수록:
  ├─ 시퀀스 짧아짐 → 추론 빠름
  ├─ 다국어 커버리지 향상
  ├─ 임베딩 테이블 크기 증가 (메모리)
  └─ 드문 토큰의 임베딩 품질 저하
```

## 5. 토크나이저가 모델 성능에 미치는 영향

```
토큰화 품질의 실전 영향:

  1. 산술 능력:
     "12345 + 67890 = ?"
     토큰화: [123, 45, +, 678, 90] — 자릿수 경계 깨짐!
     → LLM의 산술 능력이 약한 주요 원인 중 하나

  2. 다국어 공정성:
     영어 "hello world" → 2 토큰
     한국어 "안녕 세상" → 4-6 토큰
     → 같은 문맥 창에서 비영어 언어의 정보량 감소
     → 비영어 언어의 추론 비용 증가 (토큰당 과금)

  3. 코드 생성:
     들여쓰기: "    " (4칸) → 1 토큰 vs 4 토큰
     → 코드 전용 토큰이 없으면 컨텍스트 낭비

  4. 희귀 단어/신조어:
     "ChatGPT" → [Chat, G, PT] (초기 모델)
     → 자주 등장하면 다음 학습에서 단일 토큰이 될 수 있음

토크나이저 설계 시 고려사항:
  ├─ 목표 언어의 특성 (공백, 형태소)
  ├─ 도메인 (코드, 수학, 의학)
  ├─ 어휘 크기 vs 시퀀스 길이 균형
  ├─ 특수 토큰 설계 (BOS, EOS, PAD, 도구 호출)
  └─ 정규화 (소문자화, 유니코드 정규화 NFKC)
```

## 6. 최신 토크나이제이션 동향

```
최신 연구 방향:

  1. Tokenizer-Free 모델:
     바이트 수준에서 직접 학습 (ByT5, MegaByte)
     → 토크나이저의 편향 완전 제거
     → 하지만 시퀀스가 3-5배 길어짐 → 계산 비용

  2. 적응형 토크나이제이션:
     도메인/언어에 따라 토크나이저 변경
     기존 모델의 토크나이저를 확장하는 연구
     → 임베딩 레이어만 재학습

  3. 다국어 균형:
     LLaMA-3: 128K 어휘로 다국어 토큰 효율 개선
     Gemma: 256K 어휘로 더 극적인 개선
     → 어휘 확장이 다국어 성능의 핵심

  4. 구조 인식 토크나이제이션:
     코드에서 AST 구조를 반영
     수학에서 수식 구조를 반영
     → 토큰 경계가 의미 경계와 일치

  실무 팁:
  ├─ 토크나이저와 모델은 반드시 쌍으로 사용
  │   (다른 토크나이저 사용 = 의미 없는 입력)
  ├─ 비용 계산 시 "토큰 수" 기준임을 인식
  ├─ 프롬프트 최적화 시 토큰 수 확인
  └─ 한국어 등 비영어 사용 시 토큰 효율 고려
```

> **핵심 직관**: 토크나이저는 LLM의 **"세계를 보는 안경"**입니다. 잘못된 안경(토크나이저)을 쓰면 아무리 좋은 눈(모델)이라도 세상을 정확히 볼 수 없습니다. 산술 실패, 다국어 편향, 코드 생성 문제—많은 LLM의 약점이 토크나이저에서 기인합니다.

## 핵심 정리

- 서브워드 토크나이제이션은 **"어휘 크기 vs 시퀀스 길이"** 트레이드오프의 최적점이며, BPE, WordPiece, Unigram이 대표적입니다
- **BPE**는 가장 빈번한 쌍을 병합하고, **WordPiece**는 우도 기반으로 병합하며, **Unigram**은 큰 어휘에서 불필요한 것을 제거합니다
- **SentencePiece**는 사전 토크나이제이션 없이 원시 텍스트에서 직접 학습하여 다국어 LLM의 표준이 되었습니다
- **바이트 수준 BPE**(GPT-2+)는 어떤 문자도 처리할 수 있지만, 비라틴 문자에 "토큰 세금"이 발생합니다
- 토크나이저 설계는 **산술 능력, 다국어 공정성, 코드 생성** 등 모델의 근본적 능력에 영향을 미치며, LLM과 반드시 쌍으로 사용해야 합니다
