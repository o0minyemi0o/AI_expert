# 정렬: RLHF와 DPO

## 왜 정렬이 필요한가

nlp-07의 SFT로 "질문에 답하는 모델"을 만들었지만, 여전히 문제가 있습니다. 유해한 내용을 생성하거나, 사실과 다른 것을 자신 있게 말하거나, 사용자의 의도와 다른 답변을 하기도 합니다. **정렬(Alignment)**은 모델의 행동을 인간의 가치와 선호에 맞추는 과정입니다. RLHF가 ChatGPT의 핵심 비결이었으며, DPO는 이를 더 간단하게 달성합니다.

> **핵심 직관**: 정렬의 핵심 문제는 **"좋은 답변을 수학적으로 정의하기 어렵다"**는 것입니다. "도움이 되고, 무해하고, 정직한" 답변의 손실 함수를 어떻게 쓸까요? RLHF의 해법은 "인간이 직접 비교 판단 → 그 판단을 학습한 보상 모델 → 보상을 최대화하도록 정책 학습"이라는 3단계입니다.

## 1. 정렬의 필요성과 역사

```
SFT만으로 부족한 이유:

  SFT 모델의 문제점:
  ├─ "Helpful but harmful": 폭탄 제조법도 친절하게 설명
  ├─ "Sycophancy": 사용자 의견에 무조건 동조
  ├─ "Hallucination": 모르는 것도 자신 있게 답변
  └─ "Verbosity": 불필요하게 긴 답변

  정렬의 3대 목표 (Anthropic HHH):
  ├─ Helpful (도움이 됨): 사용자 의도를 정확히 파악하고 도움
  ├─ Harmless (무해함): 위험한 내용 거부
  └─ Honest (정직함): 모르면 모른다고, 불확실하면 불확실하다고

  왜 SFT로는 안 되는가:
  SFT 데이터로 "좋은 답변"을 모두 커버하기 어려움
  "이건 하지 마라"보다 "이런 걸 해라"가 학습에 효과적
  → 비교 데이터(A > B)가 절대 평가(A = 좋음)보다 수집 쉬움
  → 인간의 선호를 "보상 함수"로 학습하는 RLHF 등장
```

## 2. RLHF 파이프라인

```
RLHF (Reinforcement Learning from Human Feedback):

  3단계 파이프라인:

  Step 1: SFT (Supervised Fine-Tuning)
    (instruction, response) 쌍으로 기본 학습
    → nlp-07에서 다룬 내용

  Step 2: Reward Model (RM) 학습
    같은 질문에 대한 두 응답을 인간이 비교
    "응답 A가 응답 B보다 좋다" (A > B)

    데이터: {x, y_w, y_l}  (x: 프롬프트, y_w: 선호, y_l: 비선호)

    보상 모델:
    L_RM = -E[log σ(r(x, y_w) - r(x, y_l))]
    → Bradley-Terry 모델: P(y_w > y_l) = σ(r_w - r_l)
    → 선호 응답의 보상이 높도록 학습

    보상 모델 구조:
    사전학습 LLM + 스칼라 출력 헤드
    → LLM의 마지막 토큰 히든 → Linear(d → 1) → r(x, y)

  Step 3: PPO (Proximal Policy Optimization)
    보상 모델을 사용하여 정책(LLM) 최적화

    목적 함수:
    max_π E_{x~D, y~π}[r(x, y)] - β · KL(π || π_ref)

    r(x, y): 보상 모델의 점수
    KL(π || π_ref): 현재 정책과 SFT 모델의 KL 발산
    β: KL 패널티 계수

    KL 패널티가 없으면:
    → 보상 해킹(Reward Hacking): 보상 모델을 속이는 텍스트 생성
    → 예: 반복적이지만 보상 높은 패턴 학습
    → 사전학습 지식 파괴
```

```
PPO for LLM의 구체적 작동:

  1. 프롬프트 x 샘플링
  2. 현재 정책 π로 응답 y 생성
  3. 보상 r(x, y) 계산 (보상 모델)
  4. KL 패널티 계산: KL(π(y|x) || π_ref(y|x))
  5. PPO로 정책 업데이트

  PPO의 핵심: Clipped Objective
  L = min(r_t(θ) · Â_t, clip(r_t(θ), 1-ε, 1+ε) · Â_t)

  r_t = π_θ(a|s) / π_old(a|s): 확률비
  Â_t: Advantage 추정
  → rl-08에서 PPO를 상세히 다룹니다

  RLHF의 실전 어려움:
  ├─ 4개 모델 동시 관리:
  │   정책(학습 중), 참조(고정), 보상, 가치(Value)
  ├─ 학습 불안정: 보상 해킹, 분포 이동
  ├─ 하이퍼파라미터 민감: β, lr, batch size
  └─ 비용: 인간 비교 데이터 수집 ($)
```

> **핵심 직관**: RLHF에서 KL 패널티의 역할은 **"보상 모델의 한계 내에서 최적화"**하는 것입니다. 보상 모델은 불완전하므로, 원래 모델(π_ref)에서 너무 벗어나면 보상 모델이 신뢰할 수 없는 영역에 진입합니다. KL 패널티는 "잘 아는 영역 내에서만 개선하라"는 제약입니다.

## 3. DPO (Direct Preference Optimization)

```
DPO (Rafailov et al., 2023):
  "RLHF에서 보상 모델과 RL을 제거하고 직접 최적화"

  핵심 통찰:
  RLHF의 최적 정책은 분석적으로 풀 수 있다!

  RLHF 목적: max E[r(x,y)] - β·KL(π||π_ref)
  최적 정책: π*(y|x) ∝ π_ref(y|x) · exp(r(x,y)/β)

  이것을 뒤집으면:
  r(x,y) = β · log(π*(y|x) / π_ref(y|x)) + β·log Z(x)

  → 보상을 정책의 로그 확률비로 표현 가능!
  → 보상 모델 학습이 불필요!

  DPO 손실:
  L_DPO = -E[log σ(β · log(π_θ(y_w|x)/π_ref(y_w|x))
                    - β · log(π_θ(y_l|x)/π_ref(y_l|x)))]

  직관: "선호 응답의 확률을 높이고,
         비선호 응답의 확률을 낮추되,
         참조 모델에서 너무 벗어나지 않게"

  RLHF vs DPO:
  | 속성 | RLHF | DPO |
  |------|------|-----|
  | 보상 모델 | 필요 | 불필요 |
  | RL 알고리즘 | PPO | 불필요 |
  | 학습 안정성 | 어려움 | 간단 |
  | 하이퍼파라미터 | 많음 | 적음 (β만) |
  | 메모리 | 4개 모델 | 2개 모델 |
  | 구현 복잡도 | 높음 | SFT와 비슷 |
  | 성능 | 기준 | 동등 ~ 약간 열위 |
```

```
DPO의 구현:

  필요한 것:
  1. 학습 모델 π_θ (학습 대상)
  2. 참조 모델 π_ref (고정, SFT 모델 사본)
  3. 선호 데이터: {x, y_w, y_l}

  학습 과정:
  for batch in preference_data:
      # 선호/비선호 응답의 로그 확률 계산
      logp_w = log π_θ(y_w | x)
      logp_l = log π_θ(y_l | x)
      ref_logp_w = log π_ref(y_w | x)  # 고정
      ref_logp_l = log π_ref(y_l | x)  # 고정

      # DPO 보상 차이
      reward_diff = β * ((logp_w - ref_logp_w) -
                         (logp_l - ref_logp_l))
      loss = -log(sigmoid(reward_diff))

  β 선택:
  ├─ β가 크면: KL 패널티 강함 → 참조에 가까움
  ├─ β가 작으면: 선호 데이터에 더 맞춤 → 과적합 위험
  └─ 일반적: β = 0.1 ~ 0.5
```

## 4. DPO 변형과 개선

```
IPO (Identity-Preference Optimization, Azar et al., 2024):
  DPO의 과적합 문제를 완화
  L_IPO = (log(π_θ(y_w)/π_ref(y_w)) - log(π_θ(y_l)/π_ref(y_l)) - 1/(2β))²
  → 선호 차이를 정확히 1/(2β)로 맞추도록

KTO (Kahneman-Tversky Optimization, Ethayarajh et al., 2024):
  쌍별 선호 대신 개별 좋음/나쁨 피드백
  → "이 응답이 좋다" 또는 "이 응답이 나쁘다"만으로 학습
  → 데이터 수집이 훨씬 쉬움 (좋아요/싫어요)

ORPO (Odds Ratio Preference Optimization):
  SFT와 정렬을 하나의 단계로 통합
  SFT loss + 선호 손실을 결합
  → 참조 모델도 불필요

SimPO (Simple Preference Optimization):
  참조 모델 없이 길이 정규화된 보상
  → 메모리 절반 감소

  | 방법 | 참조 모델 | 쌍 데이터 | 복잡도 |
  |------|----------|----------|--------|
  | RLHF | 필요 | 필요 | 높음 |
  | DPO | 필요 | 필요 | 낮음 |
  | IPO | 필요 | 필요 | 낮음 |
  | KTO | 필요 | 불필요 | 낮음 |
  | ORPO | 불필요 | 필요 | 매우 낮음 |
  | SimPO | 불필요 | 필요 | 매우 낮음 |
```

## 5. Constitutional AI와 자기 개선

```
Constitutional AI (Bai et al., 2022, Anthropic):

  인간 피드백 대신 "원칙(constitution)"으로 정렬

  과정:
  1. 유해한 질문에 대한 응답 생성
  2. 원칙에 따라 자체 비판 (Self-Critique)
     "이 응답이 '사람들을 해치지 않는다'는 원칙에 부합하는가?"
  3. 원칙에 맞게 수정된 응답 생성 (Revision)
  4. 수정 전후 쌍으로 RLAIF (AI Feedback) 학습

  원칙 예시:
  ├─ "사람을 해치는 방법을 제공하지 않는다"
  ├─ "편향된 고정관념을 강화하지 않는다"
  ├─ "불확실한 것은 불확실하다고 말한다"
  └─ "사용자의 자율성을 존중한다"

  장점:
  ├─ 인간 피드백 비용 절감
  ├─ 원칙을 명시적으로 문서화
  ├─ 확장 가능
  └─ 원칙 수정으로 행동 조정

정렬 세금 (Alignment Tax):
  정렬이 모델의 원래 능력을 약간 저하시키는 현상
  ├─ 코딩 능력 약간 감소
  ├─ 창의적 글쓰기 제한
  ├─ 과도한 거부 (over-refusal)
  └─ "미안합니다, 그 질문에는 답할 수 없습니다" 남발

  해결:
  ├─ 정교한 보상 모델링
  ├─ Helpful과 Harmless의 균형 조정
  └─ 맥락 인식 안전성 (무조건 거부가 아닌 판단)
```

## 6. 정렬의 미래와 도전

```
열린 문제:

  1. Goodhart's Law:
     "측정이 목표가 되면 좋은 측정이 아니게 된다"
     보상 모델을 완벽히 최적화 ≠ 실제로 좋은 모델
     → 보상 해킹, 스코어 게이밍

  2. 가치의 다양성:
     누구의 가치에 맞출 것인가?
     문화, 개인, 맥락에 따라 "좋은 답변"이 다름
     → "보편적 정렬"은 가능한가?

  3. 슈퍼 정렬 (Superalignment):
     인간보다 더 능력 있는 AI를 어떻게 정렬할 것인가?
     인간이 판단할 수 없는 영역에서의 정렬
     → Scalable Oversight: 약한 감독으로 강한 모델 제어

  4. 자기 개선 정렬:
     모델이 스스로 정렬을 개선
     Self-Play, Debate (AI끼리 토론)
     → Constitutional AI의 확장

  현재 실무 표준:
  SFT → DPO (또는 RLHF) → 반복적 인간 평가
  DPO가 단순성과 효과의 균형으로 빠르게 채택 중
  하지만 최고 성능에는 여전히 RLHF가 우위라는 보고도 있음
```

> **핵심 직관**: 정렬은 **"AI에게 '좋은 행동'을 가르치는 것"**이 아니라 **"'좋은 행동'이 무엇인지를 함께 정의하는 과정"**입니다. RLHF/DPO는 기술적 해법이지만, "무엇이 좋은가"라는 철학적 질문은 기술만으로 답할 수 없습니다.

## 핵심 정리

- **RLHF**는 인간 선호 비교 → 보상 모델 학습 → PPO로 정책 최적화의 3단계이며, ChatGPT의 핵심 기술입니다
- **DPO**는 보상 모델과 RL을 제거하고 선호 데이터에서 직접 정책을 학습하여, RLHF의 복잡성을 크게 줄였습니다
- **KL 패널티**는 보상 해킹을 방지하고 사전학습 지식을 보존하며, β가 탐색(작음)과 보존(큼)의 균형을 조절합니다
- **Constitutional AI**는 원칙 기반 자기 비판으로 인간 피드백 비용을 줄이며, AI 피드백(RLAIF)의 가능성을 보여줬습니다
- 정렬의 핵심 도전은 **보상 해킹, 가치의 다양성, 초인적 AI의 감독** 문제이며, DPO 변형(IPO, KTO, ORPO)이 활발히 연구되고 있습니다
