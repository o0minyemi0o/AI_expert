# LLM 평가와 벤치마킹

## 왜 LLM 평가가 어려운가

"GPT-4가 Claude보다 좋은가?"—이 질문에 답하는 것은 "피카소와 모네 중 누가 더 뛰어난가?"와 비슷합니다. LLM은 수백 가지 태스크를 수행하며, 각 태스크마다 다른 강점을 보입니다. 단일 숫자로 LLM을 비교하는 것은 불가능에 가까우며, 벤치마크 오염(contamination)과 측정 신뢰성 문제까지 더해집니다. 이 강의에서는 LLM 평가의 체계적 방법론을 다룹니다.

> **핵심 직관**: LLM 평가의 핵심 딜레마는 **"자동 평가는 정확하지 않고, 인간 평가는 확장 불가능하다"**는 것입니다. BLEU는 창의적 번역을 낮게 평가하고, 인간 평가는 비용이 수천 달러입니다. LLM-as-Judge는 이 둘의 중간을 시도하지만, 자체 편향이 있습니다.

## 1. 자동 벤치마크

```
주요 LLM 벤치마크:

  지식과 추론:
  ├─ MMLU (Massive Multitask Language Understanding):
  │   57개 분야 (수학, 역사, 의학, 법률 등)
  │   4지선다형, ~15K 문제
  │   GPT-4: ~86%, 인간 전문가: ~89%
  │
  ├─ MMLU-Pro: MMLU의 난이도 상향 버전
  │   10지선다, 더 많은 추론 필요
  │
  └─ ARC (AI2 Reasoning Challenge):
      초등~중학 수준 과학 문제

  코드:
  ├─ HumanEval: 164개 Python 함수 구현
  │   docstring → 코드 → 테스트케이스 통과율
  │   Pass@k: k번 시도 중 1번 이상 통과 확률
  │
  ├─ MBPP: 974개 간단한 프로그래밍 문제
  └─ SWE-bench: 실제 GitHub 이슈 해결

  수학:
  ├─ GSM8K: 초등 수학 문제 (산술 추론)
  ├─ MATH: 고등/대학 수준 수학
  └─ Minerva: 과학/수학 통합

  대화/지시:
  ├─ MT-Bench: 다턴 대화 품질 (80 대화)
  │   GPT-4가 판정 (LLM-as-Judge)
  │
  ├─ AlpacaEval: 단턴 지시 따르기
  │   Win rate vs reference model
  │
  └─ Chatbot Arena: 인간이 직접 비교 투표
      ELO 레이팅으로 순위 (가장 신뢰)

  | 벤치마크 | 측정 대상 | 지표 | 문제 수 |
  |---------|----------|------|--------|
  | MMLU | 지식/추론 | 정확도 | 15K |
  | HumanEval | 코드 | Pass@k | 164 |
  | GSM8K | 수학 | 정확도 | 1.3K |
  | MT-Bench | 대화 | 1-10 점수 | 80 |
```

## 2. 인간 평가

```
인간 평가 방법론:

  1. Side-by-Side (A/B 테스트):
     두 모델의 응답을 나란히 비교
     "어느 쪽이 더 좋은가?" (A > B, B > A, Tie)
     → Chatbot Arena의 방식

     장점: 직관적, 상대적 판단이 절대적보다 쉬움
     단점: 비용, 평가자 간 불일치, 스케일 불가

  2. Likert Scale:
     1-5점 또는 1-7점 척도로 평가
     품질, 관련성, 유해성 등 다차원 평가

     문제: 평가자마다 기준이 다름
     → 보정(calibration) 필요

  3. 오류 표시 (Error Annotation):
     응답의 구체적 오류를 표시
     사실 오류, 논리 오류, 문법 오류, 환각 등
     → 정성적 분석에 유용

Chatbot Arena (LMSYS):
  크라우드소싱 인간 평가의 표준
  ├─ 사용자가 두 익명 모델에 같은 질문
  ├─ 응답을 비교하고 선호 투표
  ├─ 결과를 ELO 레이팅으로 집계
  ├─ 100K+ 투표로 통계적 신뢰성 확보
  └─ 현재 가장 신뢰받는 LLM 순위

  한계:
  ├─ 선택 편향: 특정 유형의 질문에 치우침
  ├─ 길이 편향: 더 긴 응답을 선호하는 경향
  ├─ 형식 편향: 마크다운, 리스트를 선호
  └─ 영어 중심: 다국어 평가 부족
```

## 3. LLM-as-Judge

```
LLM-as-Judge (Zheng et al., 2023):
  강한 LLM(GPT-4)이 다른 모델의 출력을 평가

  Single Grading:
  "다음 응답을 1-10점으로 평가하세요.
   기준: 정확성, 관련성, 완성도
   응답: {response}
   점수:"

  Pairwise Comparison:
  "두 응답 중 어느 것이 더 좋은지 판단하세요.
   A: {response_A}
   B: {response_B}
   더 좋은 것: A/B/Tie"

  알려진 편향:
  ├─ 위치 편향: A 위치를 약간 선호 → 위치 교환 평균
  ├─ 장황함 편향: 긴 응답에 높은 점수
  ├─ 자기 편향: GPT-4가 GPT-4 출력을 선호하는 경향
  └─ 형식 편향: 구조화된 응답을 선호

  편향 완화:
  ├─ 위치 교환(swap positions)하여 평균
  ├─ 평가 기준을 명확히 제시
  ├─ 여러 LLM 판정자 사용 (앙상블)
  └─ 인간 평가와의 상관관계 정기 검증

  인간과의 상관:
  GPT-4 Judge vs 인간: ~80% 일치
  → 완벽하지 않지만 비용 대비 효과적
  → 대규모 비교에 실용적
```

> **핵심 직관**: LLM-as-Judge의 핵심 장점은 **"인간 수준의 판단을 API 호출 비용으로"**할 수 있다는 것입니다. 1,000개 응답의 인간 평가 비용이 $10,000라면, GPT-4 평가는 $50 수준입니다. 80%의 정확도에서 200배의 비용 절감—이것이 실무적 가치입니다.

## 4. 벤치마크의 한계

```
벤치마크 오염 (Benchmark Contamination):

  문제: 벤치마크 문제가 학습 데이터에 포함
  → 모델이 "풀이"가 아니라 "암기"로 정답

  탐지 방법:
  ├─ 문제의 일부를 변형했을 때 성능 급락
  ├─ 동일 개념의 새 문제에서 성능 저하
  └─ 학습 데이터에서 벤치마크 문제 직접 검색

  사례: MMLU 점수가 높지만
       같은 유형의 새 문제에서는 크게 저하
       → "MMLU를 학습한 것"일 수 있음

  대응:
  ├─ Private 벤치마크: 문제를 공개하지 않음
  ├─ 동적 벤치마크: 정기적으로 새 문제 생성
  ├─ 오염 탐지 포함: 벤치마크에 오염 검사 내장
  └─ 유사 문제 생성: 원래 문제의 변형 테스트

Goodhart's Law:
  "측정이 목표가 되면 좋은 측정이 아니게 된다"
  MMLU 최적화 → MMLU에서만 좋은 모델
  HumanEval 최적화 → HumanEval에서만 잘하는 모델

  → 벤치마크는 "방향"이지 "목표"가 아님

Saturation (포화):
  일부 벤치마크에서 모델이 인간 수준에 도달
  MMLU: 상위 모델 90%+ (인간 전문가 ~89%)
  → 변별력 상실
  → MMLU-Pro, GPQA 등 더 어려운 벤치마크 등장
```

## 5. 다면적 평가 체계

```
체계적 LLM 평가 프레임워크:

  차원 1: 능력별 평가
  ├─ 지식 (MMLU, TriviaQA)
  ├─ 추론 (GSM8K, ARC, GPQA)
  ├─ 코드 (HumanEval, SWE-bench)
  ├─ 수학 (MATH, Minerva)
  ├─ 대화 (MT-Bench, Arena)
  ├─ 지시 따르기 (IFEval)
  ├─ 다국어 (MGSM, XQuAD)
  └─ 장문 (∞Bench, RULER)

  차원 2: 안전성 평가
  ├─ 유해성 (ToxiGen, RealToxicityPrompts)
  ├─ 편향 (BBQ, WinoBias)
  ├─ 사실성 (TruthfulQA, FactScore)
  └─ 탈옥 저항성 (JailbreakBench)

  차원 3: 효율성 평가
  ├─ 추론 속도 (tokens/sec)
  ├─ 비용 ($/1M tokens)
  ├─ 메모리 사용량
  └─ 배치 처리량

  차원 4: 도메인별 평가
  ├─ 의료 (MedQA, PubMedQA)
  ├─ 법률 (LegalBench)
  ├─ 금융 (FinBench)
  └─ 과학 (ScienceQA, SciBench)

  실무 평가 전략:
  1. 공개 벤치마크로 기본 위치 파악
  2. 자체 도메인 평가 데이터 구축
  3. 인간 평가 + LLM-as-Judge 병행
  4. A/B 테스트로 실제 사용자 반응 측정
```

## 6. 평가의 미래

```
진화하는 평가 패러다임:

  1. 역량 기반 평가 (Capability-based):
     "이 모델이 X를 할 수 있는가?"에 초점
     → 태스크별 Pass/Fail 대신 능력 차원별 프로필

  2. 인터랙티브 평가:
     정적 문제가 아닌 동적 대화에서 평가
     → 다턴 대화, 도구 사용, 에이전트 행동

  3. 자동 레드팀:
     LLM이 다른 LLM의 약점을 자동으로 탐색
     → 취약점 발견의 자동화

  4. Process Evaluation:
     최종 답변뿐 아니라 추론 과정 평가
     → CoT의 각 단계가 올바른지 검증

  5. 실세계 평가:
     벤치마크가 아닌 실제 사용에서의 성능
     → 사용자 만족도, 재사용률, 오류 보고

  현실적 조언:
  ├─ 단일 벤치마크를 맹신하지 말 것
  ├─ 자체 도메인 평가를 반드시 구축할 것
  ├─ 모델 선택은 벤치마크 + 실제 태스크 테스트로
  ├─ 비용/속도도 평가 기준에 포함할 것
  └─ 정기적으로 재평가할 것 (모델이 빠르게 발전)
```

## 핵심 정리

- **자동 벤치마크**(MMLU, HumanEval, GSM8K)는 특정 능력을 측정하지만, 벤치마크 오염과 포화 문제로 해석에 주의가 필요합니다
- **인간 평가**가 가장 신뢰되며, **Chatbot Arena**의 ELO 레이팅이 현재 가장 신뢰받는 LLM 순위입니다
- **LLM-as-Judge**는 인간 평가의 80% 일치도에서 200배 비용 절감을 제공하지만, 위치/장황함/자기 편향에 주의해야 합니다
- 벤치마크 **오염**(학습 데이터 포함)과 **Goodhart's Law**(측정이 목표가 됨)는 벤치마크 점수의 신뢰성을 위협합니다
- 실무에서는 **공개 벤치마크 + 도메인 평가 + 인간/LLM 판정 + A/B 테스트**를 조합한 다면적 평가가 필수입니다
