# 파인튜닝: SFT와 PEFT

## 왜 파인튜닝이 필요한가

nlp-06에서 사전학습된 LLM은 "다음 토큰 예측"만 학습했습니다. "서울의 수도는?"이라고 물으면 "한국의 수도는 서울이다. 서울은..."처럼 백과사전을 이어쓸 뿐, 질문에 답하지 않습니다. **파인튜닝(Fine-Tuning)**은 이 사전학습 모델을 "질문에 답하는 어시스턴트"로 변환합니다. 전체 파라미터를 조정하는 Full Fine-Tuning부터 0.1%만 조정하는 LoRA까지, 효율과 성능의 트레이드오프를 다룹니다.

> **핵심 직관**: 파인튜닝의 핵심은 **"사전학습이 학습한 지식을 유지하면서, 출력 형식만 조정"**하는 것입니다. 모델은 이미 언어를 "알고" 있습니다. 파인튜닝은 "아는 것을 어떻게 표현할지"를 가르칩니다. 이것이 적은 데이터(수천~수만)로도 효과적인 이유입니다.

## 1. Supervised Fine-Tuning (SFT)

```
SFT (Supervised Fine-Tuning):
  (입력, 출력) 쌍으로 모델을 지도 학습

  입력: "다음 문장을 요약하세요: ..."
  출력: "이 글은 ...에 대한 내용입니다."

  학습:
  L = -Σ_t log P(y_t | x, y_{<t})
  → 입력 x가 주어졌을 때 출력 y의 확률 최대화
  → 입력 토큰은 loss 계산에서 제외 (출력만)

  데이터 형식:
  ├─ Instruction Following:
  │   {"instruction": "...", "input": "...", "output": "..."}
  ├─ Chat Format:
  │   [{"role":"user","content":"..."}, {"role":"assistant","content":"..."}]
  └─ 다양한 태스크를 하나의 포맷으로 통일

SFT 데이터 구축:
  1. 인간 작성 (Gold Standard):
     전문 작성자가 (질문, 답변) 쌍 생성
     품질 높지만 비용 높음 ($10-50 per example)
     → InstructGPT: ~13K 인간 작성 데이터

  2. 증류 (Distillation):
     강한 모델(GPT-4)의 출력으로 약한 모델 학습
     Self-Instruct: LLM이 instruction 자체를 생성
     Alpaca: GPT-3.5로 52K 데이터 생성 → LLaMA 학습

  3. 오픈소스 데이터:
     ShareGPT: 사용자 대화 로그
     OpenAssistant: 크라우드소싱 대화
     FLAN: 1,800+ 태스크 통합

  데이터 양의 영향:
  ├─ 1K-10K: 기본 instruction following
  ├─ 10K-100K: 다양한 태스크 커버
  └─ 100K+: 수확 체감, 품질이 더 중요
```

## 2. Full Fine-Tuning

```
Full Fine-Tuning:
  모든 파라미터를 업데이트

  장점:
  ├─ 최고 성능 (이론적 상한)
  ├─ 도메인 적응에 강함
  └─ 구현 단순

  단점:
  ├─ 메모리: 7B 모델 → 파라미터(14GB) + 그래디언트(14GB)
  │   + 옵티마이저(28GB for AdamW) = 56GB+ (BF16)
  ├─ 재앙적 망각(Catastrophic Forgetting):
  │   파인튜닝 태스크에 과적합 → 일반 능력 상실
  ├─ 저장: 태스크마다 전체 모델 사본 필요
  └─ 학습 데이터 수천 개면 과적합 위험

  실전 설정:
  학습률: 1e-5 ~ 5e-5 (사전학습보다 10-100배 작게)
  에폭: 1-3 (과적합 방지)
  배치: 16-64
  워밍업: 전체의 3-5%
  가중치 감쇠: 0.01-0.1

  왜 학습률이 작아야 하는가:
  사전학습 가중치는 이미 "좋은 위치"에 있음
  큰 학습률 → 사전학습 지식 파괴
  → co-06의 fine-tuning learning rate 분석과 동일 원리
```

## 3. LoRA (Low-Rank Adaptation)

```
LoRA (Hu et al., 2021):
  핵심 아이디어: 파인튜닝 시 가중치 변화가 저랭크(low-rank)

  기존: W' = W + ΔW    (ΔW ∈ R^{d×d}, 파라미터 d²개)
  LoRA: W' = W + BA     (B ∈ R^{d×r}, A ∈ R^{r×d})
                         → 파라미터 2dr개 (r << d)

  r = rank (보통 8, 16, 32, 64)
  d = 4096 (7B 모델), r = 16이면:
  원래: 4096² = 16.7M 파라미터
  LoRA: 2 × 4096 × 16 = 131K 파라미터
  → 0.78%만 학습!

  적용 위치:
  ├─ Q, K, V, O 프로젝션 행렬에 적용
  ├─ Q와 V만 적용해도 효과적 (원 논문)
  ├─ 모든 선형 레이어에 적용하면 더 좋음
  └─ FFN에도 적용 가능

  초기화:
  A: 정규 분포 (작은 값)
  B: 영행렬 (학습 시작 시 ΔW = 0)
  → 학습 초기에는 원래 모델과 동일하게 동작

  추론 시 병합:
  W' = W + BA → 추가 비용 없음!
  LoRA 어댑터를 원래 가중치에 더하면 끝
  → 추론 지연시간 증가 없음
```

```
LoRA의 수학적 직관:

  왜 저랭크가 작동하는가?
  ├─ 사전학습된 가중치는 이미 좋은 위치
  ├─ 파인튜닝은 "미세 조정" → 변화가 작음
  ├─ 작은 변화는 저랭크 근사로 충분히 포착
  └─ la-04의 SVD에서 배운 저랭크 근사와 동일 원리

  과적합 방지 효과:
  학습 가능 파라미터가 적으므로 자연스럽게 정규화
  → 적은 데이터에서도 안정적
  → gt-03의 VC 차원과 연결

  | 방법 | 학습 파라미터 | 메모리 | 성능 |
  |------|------------|--------|------|
  | Full FT | 100% | 매우 높음 | 최고 |
  | LoRA r=8 | ~0.1% | 낮음 | Full의 95%+ |
  | LoRA r=64 | ~0.8% | 중간 | Full에 근접 |
```

## 4. QLoRA와 기타 PEFT

```
QLoRA (Dettmers et al., 2023):
  양자화 + LoRA = 소비자 GPU에서 파인튜닝

  핵심:
  1. 기본 모델을 4-bit로 양자화 (NF4 데이터 타입)
  2. 양자화된 모델 위에 LoRA 어댑터 추가
  3. LoRA 어댑터만 BF16/FP16으로 학습

  메모리 절감:
  7B Full FT: ~56GB (A100 필요)
  7B LoRA: ~18GB (A100 or 2×3090)
  7B QLoRA: ~6GB (단일 3090!)

  NF4 (Normal Float 4):
  ├─ 사전학습 가중치가 정규 분포를 따르는 점을 활용
  ├─ 분위수 기반 양자화로 정보 손실 최소화
  └─ Double Quantization: 양자화 상수도 양자화

기타 PEFT (Parameter-Efficient Fine-Tuning):

  Adapter (Houlsby et al., 2019):
  ├─ 각 레이어에 작은 병목 모듈 삽입
  ├─ down(d→r) → 활성화 → up(r→d)
  └─ 단점: 추론 시 추가 지연시간

  Prefix Tuning (Li & Liang, 2021):
  ├─ 학습 가능한 "가상 토큰"을 입력 앞에 추가
  ├─ 각 레이어의 Key, Value에 prefix 추가
  └─ 모델 가중치 자체는 고정

  Prompt Tuning (Lester et al., 2021):
  ├─ 학습 가능한 연속 임베딩을 입력 앞에 추가
  ├─ Prefix Tuning보다 단순 (임베딩 레이어만)
  └─ 큰 모델에서 성능이 Full FT에 접근

  | 방법 | 추가 파라미터 | 추론 비용 | 구현 복잡도 |
  |------|------------|----------|----------|
  | LoRA | 0.1-1% | 없음 (병합) | 낮음 |
  | QLoRA | 0.1-1% | 없음 | 중간 |
  | Adapter | 2-5% | 약간 증가 | 낮음 |
  | Prefix | 0.1% | 약간 증가 | 중간 |
  | Prompt | <0.1% | 없음 | 낮음 |
```

> **핵심 직관**: LoRA가 성공하는 근본적 이유는 **"파인튜닝에 필요한 정보가 원래 가중치 공간의 작은 부분 공간에 존재한다"**는 것입니다. 전체 d×d 행렬을 움직일 필요 없이, d×r + r×d (r << d) 차원의 변화만으로 태스크 적응이 충분합니다.

## 5. 파인튜닝 실전 가이드

```
파인튜닝 전략 선택:

  데이터가 적음 (< 1K):
  → LoRA r=8-16 + 적은 에폭 (1-2)
  → 과적합 주의, 검증 손실 모니터링

  데이터가 보통 (1K-50K):
  → LoRA r=16-64 또는 QLoRA
  → 2-3 에폭, 학습률 2e-4

  데이터가 많음 (50K+):
  → Full FT (자원 있으면) 또는 LoRA r=64-128
  → 강력한 도메인 적응 가능

  도메인 적응 (의료, 법률 등):
  → Continual Pretraining (도메인 텍스트로 CLM 추가 학습)
  → 그 다음 SFT
  → 2단계 접근이 효과적

일반적인 파인튜닝 레시피:

  1. 데이터 준비:
     Chat 포맷으로 변환
     품질 필터링 (짧은 답변, 오류 제거)
     다양성 확보 (태스크/주제 균형)

  2. 학습:
     Base: LoRA r=16, alpha=32, dropout=0.05
     lr: 2e-4 (LoRA), 2e-5 (Full FT)
     batch: 4-8 (gradient accumulation으로 유효 배치 128)
     epochs: 2-3
     warmup: 10% steps

  3. 평가:
     Hold-out 검증 세트의 loss
     태스크별 벤치마크
     인간 평가 (side-by-side)

  흔한 실수:
  ├─ 학습률 너무 높음 → 사전학습 지식 파괴
  ├─ 에폭 너무 많음 → 과적합
  ├─ 데이터 품질 무시 → "garbage in, garbage out"
  └─ 입력 토큰에 loss 포함 → 불필요한 학습 신호
```

## 6. 파인튜닝의 한계와 대안

```
파인튜닝의 한계:

  1. 재앙적 망각:
     의료 데이터로 파인튜닝 → 일반 대화 능력 저하
     → 해결: 일반 데이터도 혼합, 정규화

  2. 환각 증가:
     파인튜닝이 "자신감 있게 답하기"를 학습
     → 모르는 것도 자신감 있게 (환각)
     → nlp-12에서 환각 문제를 상세히

  3. 데이터 저작권:
     학습 데이터의 출처와 라이선스
     → 합성 데이터 활용 증가

  4. 비용:
     태스크마다 별도 모델 → 관리 부담
     → 멀티태스크 파인튜닝으로 완화

대안적 접근:

  In-Context Learning (ICL):
  ├─ 파인튜닝 없이 프롬프트에 예시 제공
  ├─ 빠르고 유연하지만 문맥 창 제한
  └─ nlp-09에서 상세히

  RAG (Retrieval-Augmented Generation):
  ├─ 외부 지식을 검색하여 프롬프트에 추가
  ├─ 지식 업데이트가 파인튜닝 없이 가능
  └─ nlp-10에서 상세히

  파인튜닝 vs ICL vs RAG:
  | 속성 | Fine-tuning | ICL | RAG |
  |------|------------|-----|-----|
  | 지식 추가 | ○ (가중치) | △ (프롬프트) | ○ (DB) |
  | 형식 변경 | ○ | ○ | △ |
  | 비용 | 높음 | 낮음 | 중간 |
  | 업데이트 | 재학습 | 즉시 | 즉시 |
  | 최적 사용 | 도메인 적응 | 프로토타입 | 지식 집약 |
```

## 핵심 정리

- **SFT**는 (instruction, response) 쌍으로 사전학습 모델을 어시스턴트로 변환하며, 데이터 품질이 양보다 중요합니다
- **Full Fine-Tuning**은 성능이 최고이지만 메모리/재앙적 망각/저장 비용 문제가 있으며, 학습률을 사전학습의 1/10~1/100로 낮춰야 합니다
- **LoRA**는 가중치 변화를 저랭크 $BA$로 근사하여 0.1~1%의 파라미터만 학습하고, 추론 시 병합하여 추가 비용이 없습니다
- **QLoRA**는 4-bit 양자화 + LoRA로 6GB GPU에서도 7B 모델을 파인튜닝할 수 있게 만들었습니다
- 파인튜닝의 한계(재앙적 망각, 환각 증가)에 대해 **ICL과 RAG**가 보완적 대안이며, 실무에서는 이들을 조합하여 사용합니다
