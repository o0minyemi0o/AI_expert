# RAG 아키텍처

## 왜 RAG가 필요한가

LLM의 지식은 사전학습 시점에 고정됩니다. "오늘 날씨"나 "최신 논문"은 알 수 없고, 사내 문서나 개인 데이터는 학습에 포함되지 않았습니다. **RAG(Retrieval-Augmented Generation)**는 외부 지식을 실시간으로 검색하여 LLM에 제공하는 아키텍처입니다. 파인튜닝 없이 지식을 업데이트하고, 환각을 줄이며, 출처를 제공할 수 있습니다.

> **핵심 직관**: RAG의 핵심은 **"LLM의 매개변수에 모든 지식을 넣지 말고, 필요할 때 검색하라"**입니다. 사람이 모든 것을 외우지 않고 필요할 때 책을 찾아보듯, LLM도 외부 지식 소스를 참조합니다. 이것이 파인튜닝보다 효율적이고 업데이트가 쉬운 이유입니다.

## 1. RAG 기본 아키텍처

```
Naive RAG 파이프라인:

  사용자 질문
      │
      ▼
  [1. 질문 임베딩] → 질문을 벡터로 변환
      │
      ▼
  [2. 검색 (Retrieval)] → 벡터 DB에서 유사 문서 검색
      │
      ▼
  [3. 프롬프트 조합] → 검색 결과 + 질문 → 프롬프트
      │
      ▼
  [4. 생성 (Generation)] → LLM이 답변 생성
      │
      ▼
  응답 (+ 출처)

  핵심 구성요소:
  ├─ 임베딩 모델: 텍스트 → 벡터 (BERT, E5, BGE)
  ├─ 벡터 DB: 벡터 유사도 검색 (Chroma, Pinecone, Weaviate)
  ├─ 청킹: 문서를 검색 단위로 분할
  └─ LLM: 검색 결과를 기반으로 답변 생성

프롬프트 템플릿:
  "다음 문맥을 참고하여 질문에 답하세요.
   문맥에 답이 없으면 '모르겠습니다'라고 하세요.

   문맥:
   {retrieved_documents}

   질문: {user_question}
   답변:"
```

## 2. 청킹 전략

```
청킹 (Chunking) — 문서 분할:
  왜 필요한가:
  ├─ LLM의 문맥 창 제한 (4K~128K 토큰)
  ├─ 너무 큰 단위 → 노이즈 포함, 검색 부정확
  ├─ 너무 작은 단위 → 맥락 손실
  └─ 적절한 크기가 검색 품질을 결정

  청킹 방법:

  1. 고정 크기 (Fixed Size):
     512 토큰씩 분할, 50-100 토큰 오버랩
     단순하지만 의미 단위를 무시할 수 있음

  2. 의미 기반 (Semantic):
     문단, 섹션, 제목 기준으로 분할
     문서 구조를 활용 → 의미적 완결성 유지

  3. 재귀적 (Recursive):
     큰 구분자(##) → 작은 구분자(\n\n) → 문장
     청크가 목표 크기를 넘으면 더 작은 단위로 분할

  4. 문장 윈도우 (Sentence Window):
     중심 문장으로 검색하되, 주변 문장도 함께 반환
     검색 정확도(좁은 단위) + 문맥(넓은 단위) 동시 달성

  최적 청크 크기:
  ├─ 짧은 질문 답변: 256-512 토큰
  ├─ 기술 문서: 512-1024 토큰
  ├─ 법률/의료: 1024-2048 토큰 (맥락 중요)
  └─ 실험으로 최적값 결정이 필수
```

## 3. 임베딩과 벡터 검색

```
임베딩 모델:

  텍스트를 밀집 벡터로 변환 (768~4096차원)

  주요 모델:
  | 모델 | 차원 | 특징 |
  |------|------|------|
  | all-MiniLM-L6 | 384 | 가볍고 빠름 |
  | BGE-large | 1024 | 중국 BAAI, 다국어 |
  | E5-mistral-7B | 4096 | LLM 기반, 최고 성능 |
  | text-embedding-3 | 256-3072 | OpenAI, 가변 차원 |
  | Cohere embed-v3 | 1024 | 다국어, 압축 지원 |

  학습 방식:
  대조 학습 (Contrastive Learning):
  같은 의미의 쌍 → 가까이, 다른 의미 → 멀리

  L = -log(exp(sim(q, d⁺)/τ) / Σ_i exp(sim(q, d_i)/τ))

  Hard Negative Mining:
  BM25 상위지만 비관련 문서를 부정 예시로
  → 구분이 어려운 문서도 올바르게 거리 부여

벡터 DB와 검색:

  유사도 검색:
  ├─ 코사인 유사도: cos(q, d) = q·d / (||q||·||d||)
  ├─ 내적: q·d (정규화된 벡터에서 코사인과 동일)
  └─ L2 거리: ||q - d||₂

  근사 최근접 이웃 (ANN):
  수백만 벡터에서 정확한 검색은 O(N) → 느림
  ├─ HNSW: 그래프 기반, 높은 재현율
  ├─ IVF: 클러스터 기반, 메모리 효율적
  ├─ PQ (Product Quantization): 벡터 압축
  └─ FAISS, Annoy, ScaNN 등 라이브러리
```

## 4. Hybrid Search와 Reranking

```
키워드 검색의 장점:
  "에러 코드 0x80070005" → 정확한 키워드 매칭 필요
  벡터 검색: 의미는 비슷하지만 정확한 코드를 놓칠 수 있음
  키워드 검색 (BM25): 정확한 토큰 매칭에 강함

Hybrid Search:
  벡터 검색 + 키워드 검색을 결합

  방법 1: 점수 결합
  score = α × vector_score + (1-α) × bm25_score
  α = 0.5~0.7이 일반적

  방법 2: Reciprocal Rank Fusion (RRF)
  RRF_score(d) = Σ_r 1/(k + rank_r(d))
  각 검색 방법의 순위를 결합 (k=60 일반적)

  장점: 의미 검색(벡터)과 정확 매칭(키워드)의 장점 결합

Reranking (재순위화):
  1단계: 대량 후보 검색 (상위 20-50개)
  2단계: 정교한 모델로 재순위 (상위 3-5개 선택)

  Reranker 모델:
  ├─ Cross-Encoder: (질문, 문서) 쌍을 함께 인코딩
  │   → 더 정확하지만 느림 (모든 쌍 계산)
  ├─ 경량 Reranker: Cohere rerank, BGE reranker
  └─ LLM Reranker: LLM이 관련성 점수 부여

  성능 향상:
  검색만: MRR@5 = 0.65
  검색 + Reranking: MRR@5 = 0.78 (크게 개선!)
```

> **핵심 직관**: RAG 파이프라인에서 **검색(recall)과 정밀도(precision)는 다른 단계에서 최적화**합니다. 1단계 검색은 넓게 후보를 모으고(높은 recall), 2단계 리랭킹은 관련 문서만 선별합니다(높은 precision). 이 2단계 구조가 정보 검색의 보편적 패턴입니다.

## 5. 고급 RAG 패턴

```
Advanced RAG 패턴:

  1. Query Transformation:
     원래 질문을 검색에 최적화된 형태로 변환

     HyDE (Hypothetical Document Embeddings):
     질문 → LLM이 가상의 답변 생성 → 가상 답변으로 검색
     "왜 하늘은 파란가?" → "하늘이 파란 이유는 레일리 산란..."
     → 이 텍스트와 유사한 실제 문서 검색

     Multi-Query:
     하나의 질문을 여러 관점으로 변환
     "RAG의 장점?" → "RAG vs Fine-tuning 비교",
                      "RAG 사용 사례", "RAG의 기술적 이점"
     → 각 쿼리의 결과를 합집합

  2. 계층적 검색 (Hierarchical):
     요약 → 원문의 2단계 인덱스

     레벨 1: 문서/섹션 요약으로 검색 (넓은 범위)
     레벨 2: 관련 섹션의 원문 청크로 세부 검색
     → 긴 문서에서 효과적

  3. Self-RAG (Asai et al., 2023):
     LLM이 검색 필요 여부를 스스로 판단
     ├─ [Retrieve]: 검색이 필요한가?
     ├─ [IsRelevant]: 검색 결과가 관련 있는가?
     ├─ [IsSupported]: 답변이 검색 결과에 기반하는가?
     └─ [IsUseful]: 답변이 유용한가?
     → 불필요한 검색 감소, 환각 감지

  4. RAPTOR (Sarthi et al., 2024):
     문서를 트리 구조로 요약
     리프: 원본 청크
     내부: 하위 노드의 요약
     루트: 전체 문서 요약
     → 질문 수준에 따라 적절한 깊이에서 검색
```

## 6. RAG 평가와 실전 고려사항

```
RAG 평가 지표:

  검색 품질:
  ├─ Recall@K: 정답 문서가 상위 K에 포함되는 비율
  ├─ MRR (Mean Reciprocal Rank): 정답의 평균 역순위
  └─ NDCG: 순위 가중 관련성

  생성 품질:
  ├─ Faithfulness: 답변이 검색 결과에 기반하는가
  ├─ Answer Relevancy: 답변이 질문에 관련 있는가
  ├─ Context Precision: 검색 결과 중 관련 비율
  └─ Context Recall: 필요한 정보가 검색되었는가

  도구: RAGAS, TruLens, LangSmith

실전 고려사항:

  데이터 전처리:
  ├─ PDF/HTML/Word → 텍스트 추출 (품질이 핵심)
  ├─ 표, 이미지, 코드 블록 처리
  └─ 메타데이터 추출 (날짜, 저자, 카테고리)

  업데이트 전략:
  ├─ 증분 인덱싱: 새 문서만 추가
  ├─ 만료 정책: 오래된 문서 제거/갱신
  └─ 버전 관리: 문서 변경 이력 추적

  비용:
  ├─ 임베딩 비용: 문서 수 × 토큰 수
  ├─ 저장 비용: 벡터 × 차원 × 4 bytes
  ├─ 검색 비용: 쿼리당 ANN 검색
  └─ LLM 비용: 검색 결과 + 질문의 토큰 수

  | 구성요소 | 선택 기준 | 추천 |
  |---------|----------|------|
  | 임베딩 | 언어, 비용, 성능 | E5/BGE 시작 |
  | 벡터 DB | 규모, 관리 | Chroma(소), Pinecone(대) |
  | 청킹 | 문서 유형 | 재귀적 + 오버랩 |
  | 리랭킹 | 정확도 요구 | Cross-Encoder |
  | LLM | 비용, 품질 | GPT-4o/Claude |
```

## 핵심 정리

- **RAG**는 외부 지식 검색 + LLM 생성을 결합하여, 파인튜닝 없이 최신 지식 반영, 환각 감소, 출처 제공이 가능합니다
- **청킹 전략**은 검색 품질을 좌우하며, 의미 기반 분할 + 적절한 오버랩이 일반적으로 효과적입니다
- **Hybrid Search**(벡터 + BM25)는 의미 검색과 키워드 매칭을 결합하고, **Reranking**으로 정밀도를 크게 향상시킵니다
- 고급 패턴인 **HyDE**(가상 문서 검색), **Self-RAG**(자가 검색 판단), **RAPTOR**(계층적 요약)가 기본 RAG를 넘어섭니다
- RAG 평가는 **검색 품질**(Recall, MRR)과 **생성 품질**(Faithfulness, Relevancy)을 분리하여 측정하며, 각 구성요소를 독립적으로 개선합니다
