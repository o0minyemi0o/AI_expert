# 환각과 사실성

## 왜 환각 문제가 중요한가

LLM은 "자신 있게 틀린 답변"을 생성합니다. 존재하지 않는 논문을 인용하고, 가짜 법률 조항을 만들어내고, 역사적 사건의 날짜를 조작합니다. **환각(Hallucination)**은 LLM의 가장 심각한 약점이며, 의료·법률·금융 등 고위험 도메인에서 LLM 도입의 최대 장벽입니다. 이 강의에서는 환각의 유형, 원인, 탐지, 완화 전략을 체계적으로 다룹니다.

> **핵심 직관**: 환각의 근본 원인은 **"LLM이 '사실'이 아니라 '그럴듯함(plausibility)'을 학습했기 때문"**입니다. 다음 토큰 예측은 "가장 확률 높은 토큰"을 선택하지, "사실에 부합하는 토큰"을 선택하지 않습니다. 통계적 패턴과 사실적 정확성은 종종 일치하지만, 반드시 그런 것은 아닙니다.

## 1. 환각의 유형

```
환각 분류 체계:

  1. 사실적 환각 (Factual Hallucination):
     존재하지 않거나 틀린 사실 생성
     "아인슈타인은 1935년에 노벨상을 받았다" (실제: 1921)
     "Smith et al. (2023)에 따르면..." (존재하지 않는 논문)

  2. 충실성 환각 (Faithfulness Hallucination):
     입력 소스와 모순되는 내용 생성
     문서 요약에서 원문에 없는 내용 추가
     RAG에서 검색 결과와 다른 답변

  3. 지시 환각 (Instruction Hallucination):
     사용자 지시를 무시하거나 잘못 해석
     "3문장으로" 요청했는데 10문장 생성
     JSON 형식 요청했는데 자유 텍스트 반환

  | 유형 | 예시 | 위험도 |
  |------|------|--------|
  | 사실적 | 가짜 인용, 틀린 날짜 | 매우 높음 |
  | 충실성 | 원문에 없는 내용 추가 | 높음 |
  | 지시 | 형식 불이행 | 중간 |

내재적 vs 외재적:
  내재적 (Intrinsic): 입력과 직접 모순
    원문: "2024년 매출 100억" → 요약: "매출 200억"
  외재적 (Extrinsic): 검증 불가능한 정보 추가
    원문에 없는 추가 정보를 자체 생성
```

## 2. 환각의 원인

```
사전학습 수준의 원인:

  1. 학습 데이터의 오류:
     웹 크롤 데이터에 오정보 포함
     → 모델이 틀린 정보를 "학습"
     → 데이터 품질 = 모델 품질 (nlp-06)

  2. 지식의 불완전한 학습:
     빈도 낮은 사실은 부정확하게 학습
     "파리는 프랑스의 수도" → 수천 번 등장 → 정확
     "투르쿠는 핀란드의 옛 수도" → 수십 번 → 불확실

  3. 패턴 일반화:
     "X는 Y의 수도" 패턴을 과도하게 일반화
     → "호주의 수도는 시드니" (실제: 캔버라)

디코딩 수준의 원인:

  4. 확률적 샘플링:
     temperature > 0에서 낮은 확률 토큰도 선택 가능
     → 사실과 다른 경로로 진입

  5. 노출 편향 (Exposure Bias):
     학습: 정답 토큰이 주어짐 (Teacher Forcing)
     추론: 자기 출력에 의존
     → 실수 → 실수가 실수를 낳음 (에러 누적)

  6. 자기 강화 (Self-Reinforcement):
     초기 토큰이 방향을 결정하면
     이후 토큰이 그 방향을 강화
     → 틀린 전제 → 정교한 거짓말

정렬 수준의 원인:

  7. 과도한 도움 경향:
     RLHF가 "도움이 되는" 답변을 보상
     → "모르겠습니다"보다 "답변을 만들어내는" 것이 높은 보상
     → 사실보다 사용자 만족을 최적화
```

> **핵심 직관**: 환각의 가장 교활한 원인은 **"정렬(RLHF)이 도움이 되려는 욕구를 정직함보다 강화한 것"**입니다. 모델은 "잘 모르겠습니다"라고 답하면 사용자 불만 → 낮은 보상을 받으므로, 그럴듯한 거짓을 만들어내는 것이 보상 최적화에 유리합니다.

## 3. 환각 탐지

```
환각 탐지 방법:

  1. 자기 일관성 기반 (Self-Consistency):
     같은 질문에 대해 여러 응답 생성
     응답 간 불일치 → 환각 가능성
     → nlp-09의 Self-Consistency와 동일 원리

  2. 토큰 확률 기반:
     생성된 토큰의 확률이 낮으면 불확실
     → 불확실한 구간 = 환각 위험
     perplexity가 높은 부분 표시

  3. 외부 지식 검증:
     생성된 사실을 검색 엔진이나 지식베이스로 검증
     → FactScore: 문장을 원자적 사실로 분해 후 각각 검증
     → 검증 가능한 사실의 비율 = 사실성 점수

  4. NLI (Natural Language Inference) 기반:
     생성 결과가 소스 문서를 수반하는지 확인
     소스 → 생성: Entailment(수반), Contradiction(모순)
     → SummaC, AlignScore 등

  5. LLM-as-Judge:
     다른 LLM이 환각 여부를 판단
     "다음 답변에 사실적 오류가 있습니까?"
     → 자기 자신보다 다른 모델이 판단하면 더 정확

  | 방법 | 외부 지식 필요 | 비용 | 정확도 |
  |------|-------------|------|--------|
  | 자기 일관성 | 불필요 | 중간 | 중간 |
  | 토큰 확률 | 불필요 | 낮음 | 낮음 |
  | 외부 검증 | 필요 | 높음 | 높음 |
  | NLI | 소스만 | 중간 | 높음 |
  | LLM-as-Judge | 불필요 | 중간 | 중간 |
```

## 4. 환각 완화 전략

```
추론 시 전략:

  1. RAG (Retrieval-Augmented Generation):
     외부 지식을 검색하여 근거 제공 (nlp-10)
     → 환각을 크게 줄이지만 완전히 제거하지는 못함
     → 검색 결과를 무시하고 환각하는 경우도 존재

  2. Chain-of-Thought + 자기 검증:
     답변 후 "이 답변의 각 사실을 검증하세요"
     → 자기 비판으로 오류 발견
     → 완벽하지 않지만 효과적

  3. 불확실성 표현:
     "~로 알려져 있습니다", "확인이 필요합니다"
     모델이 자신의 불확실성을 표현하도록 유도
     → 프롬프트: "확실하지 않으면 확실하지 않다고 말하세요"

  4. Constrained Generation:
     출력을 특정 형식이나 범위로 제한
     → 날짜는 YYYY-MM-DD, 숫자는 특정 범위 등

학습 시 전략:

  5. 정직성 중심 정렬:
     RLHF에서 "모르겠습니다"에도 보상 부여
     → Constitutional AI의 "정직함" 원칙
     → nlp-08의 HHH 중 Honest 강화

  6. 사실적 파인튜닝:
     사실 검증된 데이터로 파인튜닝
     → 환각 데이터를 의도적으로 제외

  7. 지식 접지 (Knowledge Grounding):
     모델이 답변 시 반드시 소스를 인용하도록 학습
     → 인용 없으면 답변하지 않도록
```

## 5. Attribution과 출처 제공

```
Attribution (출처 귀속):

  모델의 주장에 출처를 제공하는 것
  "서울의 인구는 약 950만 명입니다. [출처: 통계청 2024]"

  구현 방식:

  1. Post-hoc Attribution:
     답변 생성 후 각 주장에 출처 매칭
     NLI 모델로 소스와 주장의 수반 관계 확인

  2. Inline Attribution:
     생성 과정에서 출처를 함께 생성
     "[1] 서울의 인구는 950만 명입니다."
     → 학습 데이터에 인용 포함 필요

  3. RAG + Citation:
     검색된 문서를 인용
     → 청크 단위로 어떤 출처를 참조했는지 표시

  ALCE (Gao et al., 2023) 벤치마크:
  ├─ Citation Quality: 인용이 주장을 뒷받침하는가
  ├─ Citation Recall: 모든 주장에 인용이 있는가
  └─ Correctness: 주장 자체가 정확한가

  실무 패턴:
  ├─ 의료: 반드시 가이드라인/논문 인용
  ├─ 법률: 법조항/판례 인용
  ├─ 금융: 보고서/공시 인용
  └─ 교육: 교과서/논문 인용
```

## 6. 환각 문제의 미래

```
현재 한계와 연구 방향:

  완전한 해결이 어려운 이유:
  ├─ "사실"의 정의가 모호 (의견, 추측, 맥락 의존)
  ├─ 학습 데이터의 사실 검증이 불가능 (규모)
  ├─ 자기회귀 생성의 구조적 한계 (에러 누적)
  └─ 도움과 정직의 트레이드오프

  연구 방향:

  1. 사실성 인식 학습:
     모델이 자신의 지식 경계를 인식하도록
     "이것은 내가 학습한 것" vs "이것은 추측"

  2. 실시간 검증:
     생성 과정에서 매 문장을 실시간 검증
     → 환각이 감지되면 재생성

  3. 인과적 추론 통합:
     상관관계가 아닌 인과관계 학습
     → ci-01의 인과 추론과 연결

  4. Uncertainty Quantification:
     모델의 불확실성을 정량화
     → 응답과 함께 신뢰도 제공
     → pt-02의 베이지안 관점과 연결

  현실적 대응:
  환각을 "완전히 제거"하는 것이 아니라
  "탐지하고 관리"하는 것이 현실적 목표
  → 인간 검증 + 자동 탐지 + RAG의 조합
```

> **핵심 직관**: 환각 문제의 근본은 **"통계적 언어 모델이 진리표(truth table)가 아니다"**라는 것입니다. LLM은 "많은 사람이 말했을 법한 것"을 생성하지, "사실인 것"을 생성하지 않습니다. 이 간극을 줄이는 것이 정렬과 RAG의 역할이며, 완전한 해소는 현재 기술로는 불가능합니다.

## 핵심 정리

- **환각**은 사실적(틀린 사실), 충실성(소스와 모순), 지시(형식 불이행) 유형으로 분류되며, 사실적 환각이 가장 위험합니다
- 환각의 원인은 **학습 데이터 오류, 패턴 과잉 일반화, 디코딩의 확률적 특성, 정렬의 도움 편향** 등 다층적입니다
- 환각 탐지는 **자기 일관성, 토큰 확률, 외부 검증, NLI, LLM-as-Judge**로 수행하며, 외부 검증이 가장 정확합니다
- 완화 전략으로 **RAG, 자기 검증, 불확실성 표현, 정직성 중심 정렬**을 조합하며, 단일 방법으로 해결할 수 없습니다
- 환각의 완전한 제거는 현재 불가능하며, **"탐지하고 관리"**하는 것이 실무적 목표이고, Attribution(출처 제공)이 핵심 전략입니다
