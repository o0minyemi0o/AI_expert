# 딥러닝 시계열 모델: LSTM, Temporal Fusion Transformer, N-BEATS, PatchTST

## 왜 딥러닝을 시계열에 적용하는가

ts-02~ts-04의 전통 통계 모델은 선형성과 정상성 가정에 의존하며, ts-09의 Prophet은 수작업 특징 설계가 필요합니다. 딥러닝은 이러한 제약 없이 비선형 패턴을 자동으로 학습하고, 대규모 다변량 시계열에서 교차 학습(cross-learning)을 통해 개별 모델로는 불가능한 성능을 달성합니다. 다만, 데이터가 충분하지 않으면 전통 모델에 못 미칠 수 있으므로 적절한 선택이 중요합니다.

---

## 1. 시계열을 위한 딥러닝의 기본 구조

### 1.1 입출력 설계

시계열 예측을 위한 딥러닝의 기본 입출력 구조입니다.

| 요소 | 설명 | 기호 |
|------|------|------|
| Look-back window | 입력으로 사용하는 과거 시점 수 | $L$ |
| Forecast horizon | 예측할 미래 시점 수 | $H$ |
| 입력 | $(Y_{t-L+1}, \ldots, Y_t)$ | $\mathbf{X} \in \mathbb{R}^{L \times d}$ |
| 출력 | $(\hat{Y}_{t+1}, \ldots, \hat{Y}_{t+H})$ | $\hat{\mathbf{Y}} \in \mathbb{R}^{H}$ |

### 1.2 예측 전략

| 전략 | 방법 | 장점 | 단점 |
|------|------|------|------|
| 직접 (Direct) | $H$개 모델 각각 훈련 | 오차 축적 없음 | 모델 수 많음 |
| 재귀 (Recursive) | 1-단계 예측 반복 | 모델 1개 | 오차 축적 |
| 다중 출력 (MIMO) | 한 번에 $H$개 출력 | 의존성 유지 | 모델 복잡 |

> **핵심 직관**: 딥러닝 모델은 대부분 **다중 출력(MIMO)** 전략을 사용하여 예측 수평선 전체를 한 번에 출력합니다.

---

## 2. RNN과 LSTM

### 2.1 기본 RNN의 문제

$$\mathbf{h}_t = \tanh(\mathbf{W}_h \mathbf{h}_{t-1} + \mathbf{W}_x \mathbf{x}_t + \mathbf{b})$$

기본 RNN은 **기울기 소실/폭발** 문제로 장기 의존성을 학습하기 어렵습니다.

### 2.2 LSTM (Long Short-Term Memory)

게이트 메커니즘으로 장기 기억을 유지합니다.

$$\mathbf{f}_t = \sigma(\mathbf{W}_f [\mathbf{h}_{t-1}, \mathbf{x}_t] + \mathbf{b}_f) \quad \text{(망각 게이트)}$$

$$\mathbf{i}_t = \sigma(\mathbf{W}_i [\mathbf{h}_{t-1}, \mathbf{x}_t] + \mathbf{b}_i) \quad \text{(입력 게이트)}$$

$$\tilde{\mathbf{c}}_t = \tanh(\mathbf{W}_c [\mathbf{h}_{t-1}, \mathbf{x}_t] + \mathbf{b}_c) \quad \text{(후보 셀)}$$

$$\mathbf{c}_t = \mathbf{f}_t \odot \mathbf{c}_{t-1} + \mathbf{i}_t \odot \tilde{\mathbf{c}}_t \quad \text{(셀 상태 갱신)}$$

$$\mathbf{o}_t = \sigma(\mathbf{W}_o [\mathbf{h}_{t-1}, \mathbf{x}_t] + \mathbf{b}_o) \quad \text{(출력 게이트)}$$

$$\mathbf{h}_t = \mathbf{o}_t \odot \tanh(\mathbf{c}_t)$$

| 게이트 | 역할 | 직관 |
|--------|------|------|
| 망각 $\mathbf{f}_t$ | 과거 정보 중 버릴 것 결정 | "이건 더 이상 중요하지 않다" |
| 입력 $\mathbf{i}_t$ | 새 정보 중 저장할 것 결정 | "이건 기억할 가치가 있다" |
| 출력 $\mathbf{o}_t$ | 현재 출력에 사용할 것 결정 | "지금은 이것만 내보낸다" |

> **핵심 직관**: LSTM의 셀 상태 $\mathbf{c}_t$는 "장기 기억 통로"이며, 게이트가 정보의 흐름을 선택적으로 제어합니다. ts-05의 칼만 필터와 유사하게 상태를 추적합니다.

```python
import torch
import torch.nn as nn

class LSTMForecaster(nn.Module):
    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):
        super().__init__()
        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_dim, output_dim)

    def forward(self, x):
        lstm_out, _ = self.lstm(x)
        return self.fc(lstm_out[:, -1, :])

model = LSTMForecaster(input_dim=1, hidden_dim=64, num_layers=2, output_dim=12)
```

---

## 3. Temporal Fusion Transformer (TFT)

Google이 제안한 해석 가능한 다중 수평선 예측 모델입니다.

### 3.1 아키텍처 구성

| 구성 요소 | 역할 |
|----------|------|
| Variable Selection Network | 입력 변수 중요도 학습 |
| Gated Residual Network (GRN) | 비선형 처리 + 스킵 연결 |
| LSTM Encoder-Decoder | 시간적 패턴 학습 |
| Multi-Head Attention | 장기 의존성 포착 |
| Quantile Output | 확률적 예측 (분위수) |

### 3.2 입력 유형

| 입력 유형 | 설명 | 예시 |
|----------|------|------|
| 과거 관측값 | 시계열 값 자체 | 과거 매출 |
| 알려진 미래 입력 | 미래에도 알 수 있는 변수 | 요일, 공휴일, 프로모션 |
| 정적 메타데이터 | 시간 불변 | 매장 위치, 제품 카테고리 |

```python
# PyTorch Forecasting 라이브러리 사용
from pytorch_forecasting import TemporalFusionTransformer, TimeSeriesDataSet

training = TimeSeriesDataSet(
    data,
    time_idx="time_idx",
    target="value",
    group_ids=["group"],
    max_encoder_length=60,
    max_prediction_length=20,
    time_varying_known_reals=["time_idx"],
    time_varying_unknown_reals=["value"],
    static_categoricals=["group"],
)

tft = TemporalFusionTransformer.from_dataset(
    training,
    hidden_size=32,
    attention_head_size=4,
    dropout=0.1,
    learning_rate=0.03,
)
```

> **핵심 직관**: TFT의 핵심 강점은 **해석 가능성**입니다. 변수 중요도, 시간별 어텐션 가중치를 시각화하여 "왜 이렇게 예측했는지" 설명할 수 있습니다.

---

## 4. N-BEATS

순수 MLP 기반의 시계열 예측 모델로, 시계열 특화 귀납 편향 없이도 우수한 성능을 보입니다.

### 4.1 아키텍처

이중 잔차(doubly residual) 스택 구조:

$$\hat{\mathbf{y}}_s, \hat{\mathbf{x}}_s = \text{Block}_s(\mathbf{x}_s)$$

$$\mathbf{x}_{s+1} = \mathbf{x}_s - \hat{\mathbf{x}}_s \quad \text{(backcast 잔차)}$$

$$\hat{\mathbf{y}} = \sum_s \hat{\mathbf{y}}_s \quad \text{(forecast 합산)}$$

### 4.2 해석 가능 버전 (N-BEATSi)

| 스택 | 기저 함수 | 역할 |
|------|---------|------|
| 추세 스택 | 다항식 | $\mathbf{v}_t = [1, t, t^2, \ldots]$ |
| 계절 스택 | 푸리에 | $\mathbf{v}_s = [\sin, \cos, \ldots]$ |

> **핵심 직관**: N-BEATS는 "단순한 MLP 블록의 잔차 연결"만으로 ARIMA를 능가하며, RNN이나 어텐션 없이도 시계열을 잘 다룰 수 있음을 보여주었습니다.

---

## 5. PatchTST와 Transformer 기반 모델

### 5.1 PatchTST

시계열을 **패치(patch)**로 나누어 Transformer에 입력하는 방법입니다.

$$\text{시계열} \xrightarrow{\text{패칭}} [\mathbf{p}_1, \mathbf{p}_2, \ldots, \mathbf{p}_N] \xrightarrow{\text{Transformer}} \hat{\mathbf{Y}}$$

| 설계 요소 | 설명 | 효과 |
|----------|------|------|
| Patching | 연속 시점을 묶어 토큰화 | 시퀀스 길이 축소, 로컬 정보 유지 |
| Channel Independence | 변수별 독립 처리 | 과적합 방지, 확장성 |
| Instance Normalization | 시계열별 정규화 | 분포 이동 처리 |

### 5.2 Transformer 기반 시계열 모델 비교

| 모델 | 핵심 아이디어 | 연도 |
|------|------------|------|
| Informer | ProbSparse Attention | 2021 |
| Autoformer | Auto-Correlation | 2021 |
| FEDformer | 주파수 도메인 어텐션 | 2022 |
| PatchTST | 패치 + Channel Independence | 2023 |
| iTransformer | 변수를 토큰으로 | 2024 |

```python
# PatchTST 개념적 구현
class PatchTST(nn.Module):
    def __init__(self, seq_len, patch_len, d_model, n_heads, n_layers, pred_len):
        super().__init__()
        self.patch_len = patch_len
        num_patches = seq_len // patch_len
        self.patch_embed = nn.Linear(patch_len, d_model)
        encoder_layer = nn.TransformerEncoderLayer(d_model, n_heads, batch_first=True)
        self.encoder = nn.TransformerEncoder(encoder_layer, n_layers)
        self.head = nn.Linear(d_model * num_patches, pred_len)

    def forward(self, x):  # x: (B, L, 1)
        # 패칭
        x = x.unfold(1, self.patch_len, self.patch_len)  # (B, N, P)
        x = self.patch_embed(x)  # (B, N, D)
        x = self.encoder(x)  # (B, N, D)
        x = x.flatten(1)  # (B, N*D)
        return self.head(x)  # (B, H)
```

> **핵심 직관**: PatchTST는 NLP의 토큰화를 시계열에 적용한 것입니다. 개별 시점이 아닌 **패치 단위**로 처리하여 효율성과 성능을 동시에 개선합니다.

---

## 6. 확률적 예측과 손실 함수

### 6.1 점 예측 vs. 확률적 예측

| 유형 | 출력 | 손실 함수 | 예시 |
|------|------|---------|------|
| 점 예측 | $\hat{y}$ | MSE, MAE | 단일 값 |
| 분위수 예측 | $\hat{q}_\tau$ | Quantile Loss | 예측 구간 |
| 분포 예측 | $p(y)$ | NLL | 전체 분포 |

### 6.2 Quantile Loss

$$L_\tau(y, \hat{q}_\tau) = \begin{cases} \tau (y - \hat{q}_\tau) & \text{if } y \geq \hat{q}_\tau \\ (1-\tau)(\hat{q}_\tau - y) & \text{if } y < \hat{q}_\tau \end{cases}$$

```python
def quantile_loss(y_pred, y_true, quantiles=[0.1, 0.5, 0.9]):
    losses = []
    for i, q in enumerate(quantiles):
        errors = y_true - y_pred[:, i]
        losses.append(torch.max((q - 1) * errors, q * errors).mean())
    return sum(losses)
```

> **핵심 직관**: ts-03에서 배운 ARIMA의 예측 구간이 정규 분포를 가정하는 반면, 분위수 예측은 **분포 가정 없이** 예측 불확실성을 표현합니다.

---

## 7. 딥러닝 vs. 전통 모델: 언제 무엇을 쓸 것인가

| 상황 | 추천 모델 | 이유 |
|------|---------|------|
| 단일 짧은 시계열 | ARIMA, ETS | 파라미터 적음 |
| 단일 긴 시계열 + 외생 변수 | SARIMAX, Prophet | 해석 가능 |
| 수천 개 유사 시계열 | N-BEATS, PatchTST | 교차 학습 |
| 다변량 + 해석 필요 | TFT | 변수 중요도 |
| 실시간 스트리밍 | LSTM, Online 모델 | 순차 처리 |

> **핵심 직관**: 딥러닝 시계열 모델은 **대규모 데이터**와 **교차 학습**이 가능할 때 진가를 발휘합니다. 단일 시계열 100개 시점에는 ARIMA가 여전히 경쟁력 있습니다.

---

## 핵심 정리

- **LSTM은 게이트 메커니즘으로 장기 의존성을 학습하며, 시계열의 순차적 패턴을 포착합니다.**
- **TFT는 변수 선택·어텐션·분위수 출력을 결합하여 해석 가능한 다중 수평선 확률 예측을 제공합니다.**
- **N-BEATS는 순수 MLP의 잔차 스택 구조로, 시계열 특화 구조 없이도 높은 예측 성능을 달성합니다.**
- **PatchTST는 시계열을 패치로 토큰화하여 Transformer에 입력하며, 채널 독립 설계로 과적합을 방지합니다.**
- **딥러닝 모델은 대규모·다변량·교차 학습 상황에서 강점을 보이지만, 소규모 단변량에서는 전통 모델이 여전히 유효합니다.**
