# 비용 최적화와 스케일링

## 왜 비용 최적화가 중요한가

ML 시스템의 인프라 비용은 조직의 ML 확산을 가로막는 가장 큰 장벽 중 하나입니다. GPU 클러스터의 학습 비용, 온라인 서빙의 상시 운영 비용, 데이터 저장 비용이 빠르게 증가하면서 "ML이 비즈니스에 가져오는 가치 > ML 인프라 비용"이라는 등식이 항상 성립하는 것은 아닙니다. 비용 최적화는 모델 성능을 유지하면서 인프라 비용을 줄이는 엔지니어링 역량입니다.

> **핵심 직관**: 비용 최적화의 목표는 "비용을 줄이는 것"이 아니라 "단위 비용당 가치를 극대화하는 것"입니다. 비용을 0으로 만드는 것은 시스템을 끄는 것이므로, 비즈니스 가치와의 균형이 핵심입니다.

## 1. GPU vs CPU 비용 분석

ML 워크로드에서 GPU와 CPU의 비용 효율성을 비교합니다.

| 항목 | CPU (c5.2xlarge) | GPU (p3.2xlarge) | 비용비 |
|------|-----------------|-----------------|--------|
| 시간당 비용 | ~$0.34 | ~$3.06 | 9x |
| 추론 처리량 (ResNet-50) | ~50 req/s | ~500 req/s | 10x |
| 비용 대비 처리량 | 147 req/$/h | 163 req/$/h | 1.1x |
| 추론 처리량 (BERT-base) | ~5 req/s | ~150 req/s | 30x |
| 비용 대비 처리량 | 14.7 req/$/h | 49 req/$/h | 3.3x |

```
GPU vs CPU 결정 흐름:

모델 유형은?
├── 경량 모델 (XGBoost, 소형 NN)
│   └── CPU 추론 → 비용 효율적
├── 중형 모델 (ResNet, 중형 Transformer)
│   └── QPS에 따라 결정
│       ├── QPS < 100 → CPU (양자화 적용)
│       └── QPS > 100 → GPU (배칭 최적화)
└── 대형 모델 (LLM, 대형 Vision)
    └── GPU 필수
        ├── 학습 → 멀티 GPU / 분산 학습
        └── 추론 → GPU + 양자화(mo-05) + 배칭
```

## 2. 스팟 인스턴스 활용

스팟 인스턴스(AWS) / 선점형 VM(GCP)은 여유 컴퓨팅 자원을 70~90% 할인된 가격에 제공하지만, 언제든 회수될 수 있습니다.

```python
# 스팟 인스턴스 기반 학습 - 체크포인트 전략
import torch
import signal
import boto3

class SpotTrainer:
    def __init__(self, model, checkpoint_dir="s3://checkpoints/"):
        self.model = model
        self.checkpoint_dir = checkpoint_dir
        self.current_epoch = 0

        # 스팟 인스턴스 종료 시그널 처리
        signal.signal(signal.SIGTERM, self.handle_termination)

    def handle_termination(self, signum, frame):
        """스팟 인스턴스 회수 시 즉시 체크포인트 저장"""
        print("스팟 인스턴스 종료 감지! 체크포인트 저장 중...")
        self.save_checkpoint(emergency=True)

    def save_checkpoint(self, emergency=False):
        checkpoint = {
            "epoch": self.current_epoch,
            "model_state": self.model.state_dict(),
            "optimizer_state": self.optimizer.state_dict(),
            "best_metric": self.best_metric,
        }
        path = f"{self.checkpoint_dir}/checkpoint_epoch_{self.current_epoch}.pt"
        torch.save(checkpoint, path)

    def resume_from_checkpoint(self):
        """최신 체크포인트에서 학습 재개"""
        latest = find_latest_checkpoint(self.checkpoint_dir)
        if latest:
            checkpoint = torch.load(latest)
            self.model.load_state_dict(checkpoint["model_state"])
            self.current_epoch = checkpoint["epoch"]
            print(f"에포크 {self.current_epoch}에서 학습 재개")
```

| 인스턴스 유형 | 비용 절감 | 적합한 워크로드 | 주의사항 |
|-------------|----------|----------------|---------|
| 온디맨드 | 0% (기준) | 프로덕션 서빙, 긴급 학습 | 안정적이지만 비쌈 |
| 예약 인스턴스 | 30~60% | 상시 서빙, 예측 가능한 워크로드 | 1~3년 약정 필요 |
| 스팟 인스턴스 | 70~90% | 학습, 배치 추론, 실험 | 중단 가능, 체크포인트 필수 |
| 세이빙스 플랜 | 20~40% | 유연한 컴퓨팅 | 시간당 약정, 인스턴스 유연 |

> **핵심 직관**: 스팟 인스턴스는 "저렴하지만 불안정한" 자원입니다. 핵심은 "중단에 대비한 설계"입니다. 체크포인트를 충분히 자주 저장하고, 학습 재개 로직을 견고하게 구현하면, 학습 비용을 70% 이상 절감할 수 있습니다.

## 3. 모델 압축 기법

비용 최적화의 가장 효과적인 방법 중 하나는 모델 자체를 작게 만드는 것입니다.

| 압축 기법 | 원리 | 크기 감소 | 정확도 영향 | 적용 난이도 |
|-----------|------|----------|------------|------------|
| 양자화(mo-05) | FP32 → INT8 변환 | 4x | 매우 적음 | 낮음 |
| 프루닝 | 중요하지 않은 가중치 제거 | 2~10x | 적음~중간 | 중간 |
| 지식 증류(mo-05) | 대형→소형 모델 전이 | 가변 | 적음 | 높음 |
| 행렬 분해 | 가중치 행렬을 저랭크로 분해 | 2~5x | 적음~중간 | 중간 |
| 아키텍처 탐색(NAS) | 효율적 구조 자동 탐색 | 가변 | 최소 | 매우 높음 |

```python
# 구조적 프루닝 예시
import torch.nn.utils.prune as prune

def apply_structured_pruning(model, amount=0.3):
    """모델의 Conv2d 레이어에 30% 구조적 프루닝 적용"""
    for name, module in model.named_modules():
        if isinstance(module, torch.nn.Conv2d):
            prune.ln_structured(module, name="weight", amount=amount, n=2, dim=0)
            prune.remove(module, "weight")  # 프루닝 영구 적용

    # 프루닝 후 통계
    total_params = sum(p.numel() for p in model.parameters())
    nonzero_params = sum((p != 0).sum().item() for p in model.parameters())
    print(f"프루닝 비율: {1 - nonzero_params/total_params:.1%}")
    return model
```

**시나리오: 서빙 비용 70% 절감**
자연어 처리 모델의 월 서빙 비용이 $30,000입니다. 최적화 3단계를 적용합니다: (1) ONNX 변환 → 추론 속도 2x 향상, GPU 인스턴스 절반 감소 ($15,000), (2) INT8 양자화 → GPU를 CPU로 전환 가능 ($5,000), (3) 응답 캐싱(히트율 40%) → 추론 요청 40% 감소 ($3,000). 총 비용: $30,000 → $9,000 (70% 절감).

## 4. 배치 최적화

추론 요청을 배치로 묶어 처리하면 GPU 활용률을 극대화하고 비용을 절감합니다.

```python
# 동적 배칭 서버 구현
import asyncio
from collections import deque
import time

class DynamicBatchingServer:
    def __init__(self, model, max_batch_size=32, max_wait_ms=10):
        self.model = model
        self.max_batch_size = max_batch_size
        self.max_wait_ms = max_wait_ms / 1000
        self.queue = deque()

    async def predict(self, features):
        """개별 요청을 큐에 넣고 배치 처리 결과를 기다림"""
        future = asyncio.Future()
        self.queue.append((features, future))

        # 배치 크기 도달 또는 타임아웃 시 처리
        if len(self.queue) >= self.max_batch_size:
            await self._process_batch()

        return await future

    async def _process_batch(self):
        """큐에 쌓인 요청을 한 번에 처리"""
        batch_items = []
        while self.queue and len(batch_items) < self.max_batch_size:
            batch_items.append(self.queue.popleft())

        features_batch = np.stack([item[0] for item in batch_items])
        results = self.model.predict(features_batch)  # GPU 한 번에 처리

        for (_, future), result in zip(batch_items, results):
            future.set_result(result)
```

```
배칭 효과 비교:

개별 처리 (batch_size=1):
  GPU 활용률: 15%  |  처리량: 100 req/s  |  지연시간: 10ms

동적 배칭 (batch_size=16):
  GPU 활용률: 85%  |  처리량: 800 req/s  |  지연시간: 15ms (+5ms 대기)

→ 처리량 8배 향상, 지연시간 5ms 증가 → GPU 인스턴스 수 87.5% 감소 가능
```

## 5. 비용 모니터링과 예산 관리

```python
# 비용 추적 및 알림 시스템
import boto3
from datetime import datetime, timedelta

def get_ml_infrastructure_cost(days=30):
    """ML 인프라 비용 항목별 추적"""
    ce = boto3.client("ce")
    response = ce.get_cost_and_usage(
        TimePeriod={
            "Start": (datetime.now() - timedelta(days=days)).strftime("%Y-%m-%d"),
            "End": datetime.now().strftime("%Y-%m-%d"),
        },
        Granularity="DAILY",
        Metrics=["UnblendedCost"],
        GroupBy=[{"Type": "TAG", "Key": "ml-workload"}],
    )
    return response

# 비용 이상 감지
def check_cost_anomaly(current_cost, historical_avg, threshold=1.5):
    if current_cost > historical_avg * threshold:
        send_alert(
            f"ML 비용 이상 감지: ${current_cost:.0f} "
            f"(평균의 {current_cost/historical_avg:.1f}배)"
        )
```

| 비용 항목 | 일반적 비율 | 최적화 전략 |
|-----------|------------|------------|
| GPU 학습 | 30~40% | 스팟 인스턴스, 효율적 하이퍼파라미터 탐색 |
| GPU/CPU 서빙 | 30~50% | 양자화, 배칭, 오토스케일링(mo-06) |
| 데이터 저장 | 10~20% | 수명주기 정책, 불필요 데이터 정리 |
| 데이터 전송 | 5~10% | 같은 리전 배치, 압축 전송 |

> **핵심 직관**: ML 비용 최적화는 "한 번 하고 끝"이 아니라 지속적인 프로세스입니다. 월간 비용 리뷰를 통해 불필요한 자원을 식별하고, 새로운 최적화 기회를 탐색해야 합니다. 특히 실험 환경의 좀비 인스턴스(사용 중단되었으나 종료되지 않은 인스턴스)를 정기적으로 정리해야 합니다.

## 6. 스케일링 전략

**시나리오: 스타트업의 단계별 ML 인프라 스케일링**
초기(0~100 QPS): 단일 CPU 서버에서 FastAPI로 서빙. 비용: $100/월. 성장기(100~1,000 QPS): Kubernetes에 3~10개 Pod 오토스케일링, 양자화 모델. 비용: $500/월. 성숙기(1,000~10,000 QPS): GPU 서빙 + 동적 배칭 + 카나리 배포. 비용: $3,000/월. 각 단계에서 과잉 설계를 피하고, 필요할 때 스케일 업하는 것이 핵심입니다.

```
스케일링 결정 매트릭스:

현재 QPS와 지연시간 SLA는?
├── QPS < 100, SLA > 100ms
│   └── 단일 서버 + CPU + 양자화 모델
├── QPS 100~1K, SLA > 50ms
│   └── K8s + HPA + CPU 클러스터 또는 소수 GPU
├── QPS 1K~10K, SLA > 20ms
│   └── K8s + GPU + 동적 배칭 + CDN/캐시
└── QPS > 10K, SLA < 10ms
    └── 멀티 리전 배포 + 엣지 캐싱 + 전용 가속기
```

## 핵심 정리

- GPU vs CPU 선택은 모델 유형과 QPS에 따라 결정하며, 경량 모델의 경우 CPU + 양자화가 GPU보다 비용 효율적일 수 있습니다
- 스팟 인스턴스는 학습 비용을 70~90% 절감하며, 체크포인트 기반 재개 로직이 핵심입니다
- 모델 압축(양자화, 프루닝, 증류)은 서빙 비용을 직접적으로 줄이는 가장 효과적인 방법입니다
- 동적 배칭은 GPU 활용률을 15%에서 85%로 끌어올려 처리량을 8배 향상시킬 수 있습니다
- 비용 최적화는 지속적 프로세스이며, 월간 비용 리뷰와 좀비 인스턴스 정리가 필수적입니다
