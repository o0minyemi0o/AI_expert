# 모델 서빙과 추론 최적화

## 왜 모델 서빙이 중요한가

아무리 뛰어난 모델(dl-01 참조)이라도 프로덕션 환경에서 안정적으로 예측을 제공하지 못하면 비즈니스 가치를 창출할 수 없습니다. 모델 서빙은 학습된 모델을 실제 사용자 요청에 대해 예측을 반환하는 시스템으로 전환하는 과정입니다. 지연시간, 처리량, 비용, 가용성 등 다양한 요구사항을 동시에 충족해야 하는 엔지니어링 도전입니다.

> **핵심 직관**: 모델 서빙은 "모델을 API로 감싸는 것"이 아니라, "예측을 SLA에 맞게 지속적으로 제공하는 시스템을 구축하는 것"입니다. 99.9% 가용성은 연간 8.7시간의 다운타임만 허용합니다.

## 1. 온라인 서빙 패턴

온라인 서빙은 실시간 요청에 대해 즉시 예측을 반환합니다.

```python
# FastAPI + 모델 서빙 예시
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import mlflow.pyfunc
import numpy as np
import time

app = FastAPI(title="Fraud Detection API")

# 모델 사전 로딩
model = mlflow.pyfunc.load_model("models:/fraud_detector/Production")

class PredictionRequest(BaseModel):
    transaction_amount: float
    merchant_category: str
    time_since_last_txn: float

class PredictionResponse(BaseModel):
    fraud_probability: float
    is_fraud: bool
    latency_ms: float

@app.post("/predict", response_model=PredictionResponse)
async def predict(request: PredictionRequest):
    start = time.time()
    features = preprocess(request.dict())
    prob = model.predict(np.array([features]))[0]
    latency = (time.time() - start) * 1000

    return PredictionResponse(
        fraud_probability=float(prob),
        is_fraud=prob > 0.7,
        latency_ms=round(latency, 2)
    )
```

| 서빙 프레임워크 | 특징 | 지원 형식 | 지연시간 |
|----------------|------|-----------|---------|
| TF Serving | TensorFlow 네이티브, gRPC 지원 | SavedModel | 매우 낮음 |
| Triton | 멀티 프레임워크, 동적 배칭 | ONNX, TRT, PyTorch | 매우 낮음 |
| TorchServe | PyTorch 네이티브 | TorchScript, eager | 낮음 |
| BentoML | 다양한 프레임워크 통합, 쉬운 패키징 | 범용 | 중간 |
| FastAPI | 범용 API, 빠른 프로토타이핑 | 범용 | 중간 |

## 2. 배치 서빙 패턴

대량의 예측을 미리 생성하여 저장해두는 방식입니다.

```python
# Spark 기반 배치 추론
from pyspark.sql import SparkSession
import mlflow.pyfunc

spark = SparkSession.builder.appName("batch_inference").getOrCreate()

# 대규모 데이터 로드
user_features = spark.read.parquet("s3://features/user_features/")

# UDF로 모델 적용
model_udf = mlflow.pyfunc.spark_udf(spark, "models:/recommender/Production")
predictions = user_features.withColumn("score", model_udf(*feature_columns))

# 결과 저장
predictions.write.parquet("s3://predictions/daily/")
```

```
온라인 vs 배치 서빙 결정 흐름:

예측 결과가 즉시 필요한가?
├── Yes → 지연시간 요구사항은?
│         ├── < 10ms  → 엣지 배포 / 사전 계산
│         ├── < 100ms → 온라인 서빙 (최적화 필수)
│         └── < 1s    → 온라인 서빙 (표준)
└── No  → 예측 대상 수는?
          ├── > 100만 건 → Spark 배치 추론
          └── < 100만 건 → 단일 노드 배치 추론
```

## 3. 모델 최적화: 양자화

양자화(Quantization)는 모델의 가중치와 활성화를 더 낮은 정밀도(FP32 → INT8/FP16)로 변환하여 추론 속도를 높이고 메모리 사용량을 줄이는 기법입니다.

| 양자화 유형 | 설명 | 정확도 영향 | 속도 향상 |
|------------|------|------------|-----------|
| 동적 양자화 | 추론 시 활성화를 동적으로 양자화 | 매우 적음 | 1.5~2x |
| 정적 양자화 | 캘리브레이션 데이터로 사전 양자화 | 적음 | 2~3x |
| 양자화 인식 학습(QAT) | 학습 중 양자화를 시뮬레이션 | 최소 | 2~4x |

```python
# PyTorch 동적 양자화 예시
import torch

# 원본 모델
model = load_trained_model()
original_size = get_model_size(model)

# 동적 양자화 적용
quantized_model = torch.quantization.quantize_dynamic(
    model, {torch.nn.Linear}, dtype=torch.qint8
)

quantized_size = get_model_size(quantized_model)
print(f"크기 감소: {original_size}MB → {quantized_size}MB")
print(f"압축률: {quantized_size/original_size:.1%}")
```

> **핵심 직관**: 양자화는 "정밀도를 약간 희생하여 속도를 크게 향상시키는" 트레이드오프입니다. 대부분의 실용적 시나리오에서 INT8 양자화 후 정확도 손실은 1% 미만이지만, 반드시 벤치마크로 검증해야 합니다.

## 4. 모델 최적화: 지식 증류

지식 증류(Knowledge Distillation, dl-08 참조)는 큰 모델(교사)의 지식을 작은 모델(학생)로 전이하는 기법입니다.

```python
# 지식 증류 학습 루프 핵심 코드
import torch.nn.functional as F

def distillation_loss(student_logits, teacher_logits, labels, temperature=4.0, alpha=0.7):
    # 소프트 타깃 손실 (교사 모델의 출력 분포 모방)
    soft_loss = F.kl_div(
        F.log_softmax(student_logits / temperature, dim=1),
        F.softmax(teacher_logits / temperature, dim=1),
        reduction="batchmean"
    ) * (temperature ** 2)

    # 하드 타깃 손실 (실제 레이블)
    hard_loss = F.cross_entropy(student_logits, labels)

    return alpha * soft_loss + (1 - alpha) * hard_loss
```

**시나리오: 모바일 추천 모델 최적화**
서버에서 BERT-large(340M 파라미터) 기반 추천 모델을 운영 중이었으나, 추론 비용이 월 $50,000에 달했습니다. 지식 증류로 DistilBERT(66M 파라미터)로 압축한 결과, 정확도는 97% 수준을 유지하면서 추론 속도 3배 향상, 비용 60% 절감을 달성했습니다.

## 5. 지연시간 관리

프로덕션 서빙에서 지연시간을 관리하기 위한 실전 기법들입니다.

```python
# 지연시간 최적화 기법 모음

# 1. 모델 사전 로딩 (콜드 스타트 방지)
model = load_model()  # 서버 시작 시 로드

# 2. 입력 배칭 (동적 배칭)
# Triton Inference Server 설정
# config.pbtxt
"""
dynamic_batching {
  preferred_batch_size: [4, 8, 16]
  max_queue_delay_microseconds: 5000
}
"""

# 3. 비동기 전처리
import asyncio
async def predict_batch(requests: list):
    features = await asyncio.gather(*[preprocess(r) for r in requests])
    return model.predict(np.stack(features))

# 4. 캐싱 (동일 입력에 대한 중복 추론 방지)
from functools import lru_cache

@lru_cache(maxsize=10000)
def cached_predict(feature_hash: str):
    return model.predict(decode_features(feature_hash))
```

| 최적화 기법 | 효과 | 구현 복잡도 | 적용 조건 |
|------------|------|------------|-----------|
| 모델 사전 로딩 | 콜드 스타트 제거 | 낮음 | 항상 적용 |
| 동적 배칭 | 처리량 2~5x 향상 | 중간 | QPS > 100 |
| ONNX 변환 | 추론 속도 1.5~3x | 낮음 | 표준 연산만 사용 |
| 양자화(INT8) | 속도 2~4x, 크기 4x 감소 | 중간 | 정확도 검증 필요 |
| 응답 캐싱 | 중복 요청 제거 | 낮음 | 입력 패턴 반복 시 |
| GPU → CPU 전환 | 비용 절감 | 낮음 | 배치 크기 작을 때 |

> **핵심 직관**: 지연시간 최적화는 프로파일링에서 시작합니다. 어디서 시간이 소모되는지 측정하지 않고 최적화하면, 효과 없는 곳에 노력을 낭비하게 됩니다. 전처리 → 추론 → 후처리 각 단계의 시간을 먼저 측정하십시오.

## 6. ONNX를 활용한 크로스 프레임워크 서빙

```python
# PyTorch → ONNX 변환
import torch.onnx

dummy_input = torch.randn(1, 10)
torch.onnx.export(model, dummy_input, "model.onnx",
                  input_names=["input"], output_names=["output"],
                  dynamic_axes={"input": {0: "batch_size"}})

# ONNX Runtime으로 추론
import onnxruntime as ort

session = ort.InferenceSession("model.onnx")
result = session.run(None, {"input": input_data.numpy()})
```

**시나리오: 지연시간 SLA 준수**
실시간 사기 탐지 시스템에서 p99 지연시간 50ms SLA를 준수해야 합니다. 프로파일링 결과: 전처리 15ms, 피처 스토어 조회 20ms, 모델 추론 30ms, 후처리 5ms로 총 70ms였습니다. 피처 스토어를 Redis로 전환(20ms → 5ms), 모델을 ONNX + INT8 양자화(30ms → 10ms)하여 총 35ms로 단축했습니다.

## 핵심 정리

- 온라인 서빙은 실시간 예측, 배치 서빙은 대량 사전 예측에 적합하며, 결정 기준은 지연시간 요구사항과 예측 대상 규모입니다
- 양자화(FP32→INT8)는 최소한의 정확도 손실로 추론 속도를 2~4배 향상시키며, 대부분의 프로덕션 모델에 적용 가능합니다
- 지식 증류는 대형 모델의 성능을 소형 모델로 전이하여 비용과 지연시간을 대폭 절감하는 핵심 기법입니다
- 지연시간 최적화는 반드시 프로파일링 결과에 기반해야 하며, 전처리/추론/후처리 각 단계를 개별적으로 분석합니다
- ONNX Runtime은 프레임워크 독립적인 서빙을 가능하게 하며, 추가적인 런타임 최적화를 자동으로 수행합니다
