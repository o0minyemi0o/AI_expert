# 학습 파이프라인 자동화

## 왜 학습 파이프라인 자동화가 중요한가

수동으로 모델을 학습시키고 배포하는 방식은 소규모 프로젝트에서는 가능하지만, 수십 개의 모델을 운영하는 조직에서는 지속 불가능합니다. 학습 파이프라인 자동화는 데이터 수집부터 모델 배포까지의 전 과정을 코드로 정의하고, 트리거 조건에 따라 자동으로 실행되도록 만드는 것입니다. 이를 통해 인적 오류를 줄이고, 모델 업데이트 주기를 단축하며, 재현성을 보장합니다.

> **핵심 직관**: 학습 파이프라인 자동화의 핵심은 "사람이 버튼을 누르지 않아도 모델이 최신 상태를 유지하는 것"입니다. 자동화 수준이 곧 MLOps 성숙도(mo-12 참조)를 결정합니다.

## 1. 오케스트레이션 도구 비교

학습 파이프라인을 관리하는 오케스트레이션 도구들을 비교합니다.

| 도구 | 특징 | ML 특화 기능 | 학습 곡선 |
|------|------|-------------|-----------|
| Airflow | 범용 워크플로 오케스트레이터 | 제한적 (플러그인 필요) | 중간 |
| Kubeflow Pipelines | K8s 네이티브 ML 파이프라인 | 실험 관리, GPU 지원 | 높음 |
| Prefect | 현대적 워크플로 엔진 | 동적 태스크, 쉬운 로컬 테스트 | 낮음 |
| Dagster | 데이터 자산 중심 오케스트레이터 | 데이터 품질 통합 | 중간 |
| Metaflow | Netflix 개발, 데이터 과학자 친화적 | 스케일링 자동화 | 낮음 |
| ZenML | MLOps 특화 프레임워크 | 스택 컴포넌트 추상화 | 낮음 |

## 2. DAG 기반 워크플로 설계

학습 파이프라인은 방향성 비순환 그래프(DAG)로 표현됩니다. 각 노드는 하나의 작업(Task)이고, 간선은 의존성을 나타냅니다.

```
                    ┌─── [피처 A 생성] ───┐
[데이터 추출] ──→ [데이터 검증] ──→ ├─── [피처 B 생성] ───├──→ [학습] ──→ [평가] ──→ [등록]
                    └─── [피처 C 생성] ───┘                        │
                                                                   ↓
                                                              [배포 결정]
                                                             /          \
                                                     [배포 승인]    [롤백/알림]
```

```python
# Prefect를 활용한 학습 파이프라인 DAG
from prefect import flow, task
from prefect.tasks import task_input_hash
from datetime import timedelta

@task(cache_key_fn=task_input_hash, cache_expiration=timedelta(hours=1))
def extract_data(source: str) -> pd.DataFrame:
    return load_from_source(source)

@task
def validate_data(df: pd.DataFrame) -> pd.DataFrame:
    assert df.shape[0] > 1000, "데이터 행 수 부족"
    assert df.isnull().mean().max() < 0.05, "결측치 비율 초과"
    return df

@task
def compute_features(df: pd.DataFrame, feature_group: str) -> pd.DataFrame:
    return feature_engineering(df, feature_group)

@task
def train_model(features: pd.DataFrame) -> Model:
    model = XGBClassifier(**load_config())
    model.fit(features.drop("label", axis=1), features["label"])
    return model

@task
def evaluate_model(model: Model, test_data: pd.DataFrame) -> dict:
    predictions = model.predict(test_data)
    return {"f1": f1_score(test_data["label"], predictions)}

@flow(name="training-pipeline")
def training_pipeline():
    raw_data = extract_data("s3://bucket/raw/")
    validated = validate_data(raw_data)
    features = compute_features(validated, "v3")
    model = train_model(features)
    metrics = evaluate_model(model, test_data)

    if metrics["f1"] > 0.85:
        register_model(model, metrics)
```

## 3. 재학습 트리거 설계

모델을 언제 재학습시킬지 결정하는 트리거 전략은 여러 가지가 있습니다.

| 트리거 유형 | 설명 | 적합한 상황 |
|------------|------|------------|
| 시간 기반 | 정해진 주기(매일/매주)로 재학습 | 데이터가 정기적으로 업데이트되는 경우 |
| 성능 기반 | 모니터링 메트릭 임계값 하회 시 | 모니터링 체계가 갖추어진 경우 |
| 데이터 기반 | 새 데이터 누적량이 임계값 초과 시 | 데이터 유입이 불규칙한 경우 |
| 드리프트 기반 | 데이터/예측 분포 변화 감지 시 | 외부 환경 변화가 빈번한 경우 |
| 수동 트리거 | 사람이 판단하여 실행 | 규제 산업, 초기 도입 단계 |

```python
# 성능 기반 재학습 트리거 예시
from evidently import ColumnMapping
from evidently.metrics import DataDriftTable

def check_retrain_trigger(current_metrics: dict, threshold: dict) -> bool:
    triggers = []
    triggers.append(current_metrics["f1"] < threshold["min_f1"])
    triggers.append(current_metrics["drift_score"] > threshold["max_drift"])
    triggers.append(current_metrics["data_count"] > threshold["min_new_data"])
    return any(triggers)

# 스케줄러에서 호출
if check_retrain_trigger(get_current_metrics(), THRESHOLDS):
    trigger_training_pipeline()  # Prefect/Airflow 파이프라인 실행
```

> **핵심 직관**: 재학습 트리거는 "충분한 근거"가 있을 때만 작동해야 합니다. 불필요한 재학습은 컴퓨팅 비용을 낭비하고, 오히려 모델 안정성을 해칠 수 있습니다. 성능 기반과 드리프트 기반(mo-08 참조)을 결합하는 것이 가장 효과적입니다.

## 4. 파이프라인 테스트 전략

학습 파이프라인도 소프트웨어이므로 반드시 테스트해야 합니다.

```python
# 파이프라인 단위 테스트 예시
import pytest

def test_feature_computation():
    """피처 변환 로직이 올바른지 검증"""
    sample_input = pd.DataFrame({"amount": [100, 200], "count": [5, 10]})
    result = compute_features(sample_input, "v3")
    assert "amount_per_count" in result.columns
    assert result["amount_per_count"].tolist() == [20.0, 20.0]

def test_data_validation_rejects_bad_data():
    """결측치가 많은 데이터를 거부하는지 검증"""
    bad_data = pd.DataFrame({"col1": [None] * 100, "col2": range(100)})
    with pytest.raises(AssertionError, match="결측치 비율 초과"):
        validate_data(bad_data)

def test_model_output_shape():
    """모델 출력의 형태가 올바른지 검증"""
    model = train_model(get_test_features())
    predictions = model.predict(get_test_input())
    assert predictions.shape[0] == get_test_input().shape[0]
    assert all(0 <= p <= 1 for p in predictions)
```

테스트는 다음과 같이 계층화합니다.

```
[단위 테스트] → 개별 태스크(피처 변환, 검증 로직) 검증
     ↓
[통합 테스트] → 태스크 간 데이터 흐름 검증
     ↓
[스모크 테스트] → 소규모 데이터로 전체 파이프라인 실행
     ↓
[성능 테스트] → 프로덕션 규모 데이터에서 시간/자원 소모 확인
```

## 5. 실패 처리와 복구

파이프라인은 반드시 실패합니다. 중요한 것은 실패를 우아하게 처리하는 것입니다.

**시나리오: 학습 파이프라인 실패 복구**
새벽 2시에 일일 재학습 파이프라인이 실행되었으나, 데이터 소스 DB의 일시적 장애로 추출 단계에서 실패했습니다. 파이프라인은 자동으로 3회 재시도(5분 간격)를 수행했고, 2차 시도에서 성공했습니다. 만약 3회 모두 실패했다면, 이전 모델을 유지하면서 온콜 엔지니어에게 PagerDuty 알림이 발송됩니다.

```python
# 재시도와 알림이 포함된 파이프라인
from prefect import flow, task
from prefect.blocks.notifications import SlackWebhook

@task(retries=3, retry_delay_seconds=300)
def extract_data(source: str):
    try:
        return load_from_source(source)
    except ConnectionError as e:
        logger.error(f"데이터 소스 연결 실패: {e}")
        raise  # 재시도 트리거

@flow(name="training-pipeline")
def training_pipeline():
    try:
        raw = extract_data("db://production")
        model = train_and_evaluate(raw)
        deploy_model(model)
    except Exception as e:
        slack = SlackWebhook.load("ml-alerts")
        slack.notify(f"학습 파이프라인 실패: {e}")
        # 이전 모델 유지 (자동 롤백 불필요)
```

> **핵심 직관**: 파이프라인 실패 시 "이전 모델 유지"가 기본 전략이어야 합니다. 실패한 파이프라인이 새 모델을 배포하는 것보다, 이전의 검증된 모델을 계속 서빙하는 것이 훨씬 안전합니다.

**시나리오: 멀티 모델 오케스트레이션**
이커머스 플랫폼에서 추천, 검색 랭킹, 사기 탐지 3개 모델을 운영합니다. 각 모델의 학습 주기가 다르고(일일/주간/월간), 일부 모델은 다른 모델의 출력을 피처로 사용합니다. DAG 기반 오케스트레이터로 의존성을 명시하고, 상위 모델이 재학습되면 하위 모델도 자동으로 재학습 트리거가 발동되도록 설계합니다.

## 핵심 정리

- 학습 파이프라인 오케스트레이션 도구는 팀의 기술 스택과 규모에 맞게 선택하며, Prefect/Dagster는 ML 팀에 친화적입니다
- DAG 기반 워크플로는 태스크 간 의존성을 명시적으로 정의하고, 병렬 실행과 캐싱을 자동으로 지원합니다
- 재학습 트리거는 시간/성능/데이터/드리프트 기반 중 적합한 방식을 조합하여 설계합니다
- 파이프라인 테스트는 단위 → 통합 → 스모크 → 성능의 계층 구조로 작성하며, CI/CD에 통합합니다(mo-07 참조)
- 실패 처리의 핵심 원칙은 "이전 모델 유지"이며, 재시도와 알림을 통해 안전하게 복구합니다
