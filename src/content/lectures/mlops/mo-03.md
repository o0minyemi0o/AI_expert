# 실험 관리와 모델 레지스트리

## 왜 실험 관리가 중요한가

ML 프로젝트에서는 수십에서 수백 번의 실험이 반복됩니다. 하이퍼파라미터 조합, 피처 집합, 데이터 전처리 방법을 변경하면서 최적의 모델을 탐색합니다. 체계적인 실험 관리 없이는 "지난주에 잘 되었던 설정이 뭐였지?"라는 질문에 답할 수 없게 됩니다. 이는 단순한 불편함이 아니라, 재현성(Reproducibility) 부재로 이어져 프로덕션 배포의 신뢰성을 근본적으로 훼손합니다.

> **핵심 직관**: 실험 관리는 "과거의 나와 소통하는 시스템"입니다. 3개월 후의 자신이 오늘의 실험을 정확히 재현할 수 있어야 합니다.

## 1. 실험 추적의 핵심 요소

체계적인 실험 추적을 위해 기록해야 할 요소는 다음과 같습니다.

| 추적 항목 | 설명 | 예시 |
|-----------|------|------|
| 하이퍼파라미터 | 모델 학습 설정값 | learning_rate=0.001, batch_size=64 |
| 메트릭 | 모델 성능 지표 | accuracy=0.92, f1=0.87, latency=12ms |
| 아티팩트 | 학습 결과물 | 모델 가중치, 혼동 행렬 이미지 |
| 환경 정보 | 실행 환경 | Python 3.10, PyTorch 2.1, GPU A100 |
| 데이터 버전 | 학습 데이터 식별자 | dataset_v3_20240115, hash: a3b2c1 |
| 소스 코드 | 학습 코드 버전 | git commit: 7f3a2b1 |

## 2. MLflow를 활용한 실험 추적

MLflow는 가장 널리 사용되는 오픈소스 실험 추적 도구입니다.

```python
import mlflow
import mlflow.pytorch
from sklearn.metrics import accuracy_score, f1_score

# 실험 설정
mlflow.set_experiment("fraud_detection_v2")

with mlflow.start_run(run_name="transformer_baseline"):
    # 하이퍼파라미터 로깅
    mlflow.log_params({
        "model_type": "transformer",
        "learning_rate": 0.001,
        "batch_size": 64,
        "num_layers": 4,
        "hidden_dim": 256,
        "dropout": 0.1,
        "epochs": 50,
    })

    # 학습 루프 (생략)
    model = train_model(config)
    predictions = model.predict(X_test)

    # 메트릭 로깅
    mlflow.log_metrics({
        "accuracy": accuracy_score(y_test, predictions),
        "f1_score": f1_score(y_test, predictions),
        "inference_time_ms": measure_latency(model),
    })

    # 모델 아티팩트 저장
    mlflow.pytorch.log_model(model, "model")
    mlflow.log_artifact("confusion_matrix.png")

    # 데이터셋 정보 기록
    mlflow.set_tag("dataset_version", "v3_20240115")
    mlflow.set_tag("git_commit", get_git_hash())
```

## 3. 하이퍼파라미터 로깅 전략

하이퍼파라미터를 효과적으로 관리하려면 설정 파일 기반 접근이 필요합니다.

```yaml
# config/experiment.yaml
experiment:
  name: fraud_detection
  version: 2

model:
  type: transformer
  hidden_dim: 256
  num_layers: 4
  dropout: 0.1

training:
  learning_rate: 0.001
  batch_size: 64
  epochs: 50
  optimizer: adamw
  scheduler: cosine_warmup

data:
  train_path: s3://data/train_v3.parquet
  val_split: 0.15
  augmentation: true
```

```python
# Hydra를 활용한 설정 관리
import hydra
from omegaconf import DictConfig

@hydra.main(config_path="config", config_name="experiment")
def train(cfg: DictConfig):
    mlflow.log_params(dict(cfg.model))
    mlflow.log_params(dict(cfg.training))
    # ...
```

> **핵심 직관**: 하이퍼파라미터를 코드에 하드코딩하지 마십시오. 설정 파일로 분리하면 실험 비교, 재현, 자동화가 모두 쉬워집니다. Hydra나 OmegaConf 같은 도구가 이를 체계적으로 지원합니다.

## 4. 모델 레지스트리

모델 레지스트리는 학습된 모델의 생명주기를 관리하는 중앙 저장소입니다. 단순한 파일 저장이 아니라, 모델의 버전, 상태, 메타데이터를 체계적으로 관리합니다.

```
모델 생명주기:

[실험 완료] → [모델 등록] → [Staging] → [검증/테스트] → [Production]
                                                              ↓
                                              [모니터링(mo-08)]
                                                              ↓
                                              [성능 저하 감지] → [Archived]
```

```python
# MLflow Model Registry 사용 예시
import mlflow

# 모델 등록
result = mlflow.register_model(
    model_uri="runs:/abc123/model",
    name="fraud_detector"
)

# 스테이지 전환
client = mlflow.tracking.MlflowClient()
client.transition_model_version_stage(
    name="fraud_detector",
    version=3,
    stage="Production",
    archive_existing_versions=True  # 이전 프로덕션 모델 자동 아카이브
)

# 프로덕션 모델 로드
model = mlflow.pyfunc.load_model("models:/fraud_detector/Production")
```

| 모델 상태 | 설명 | 접근 권한 |
|-----------|------|-----------|
| None | 등록만 된 상태 | 실험자 |
| Staging | 검증 중인 모델 | QA 팀 |
| Production | 프로덕션 서빙 중 | 서빙 시스템 |
| Archived | 더 이상 사용하지 않는 모델 | 감사 목적 |

## 5. 재현성 보장

ML 실험의 재현성을 보장하기 위해 다음 요소를 모두 관리해야 합니다.

**시나리오: 재현성 실패 사례**
데이터 사이언티스트 A가 로컬에서 F1 0.92를 달성한 모델을 보고했지만, 팀원 B가 동일한 코드로 재현했을 때 F1 0.85만 나왔습니다. 원인 분석 결과: (1) 랜덤 시드가 고정되지 않았고, (2) 라이브러리 버전이 달랐으며(PyTorch 2.0 vs 2.1), (3) 학습 데이터가 그 사이에 업데이트되었습니다.

```python
# 재현성 보장을 위한 체크리스트 구현
import torch
import numpy as np
import random

def ensure_reproducibility(seed: int = 42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

# 환경 정보 기록
def log_environment():
    mlflow.set_tag("python_version", platform.python_version())
    mlflow.set_tag("torch_version", torch.__version__)
    mlflow.set_tag("cuda_version", torch.version.cuda)
    mlflow.log_artifact("requirements.txt")
```

## 6. 실험 비교와 분석

다수의 실험을 체계적으로 비교하여 최적의 모델을 선정하는 과정도 자동화할 수 있습니다.

```python
# MLflow에서 실험 결과 비교
import mlflow
import pandas as pd

runs = mlflow.search_runs(
    experiment_names=["fraud_detection_v2"],
    filter_string="metrics.f1_score > 0.85",
    order_by=["metrics.f1_score DESC"],
    max_results=10
)

# 파레토 최적 모델 선정 (정확도 vs 추론 시간)
best_runs = runs[["params.model_type", "metrics.f1_score", "metrics.inference_time_ms"]]
print(best_runs.to_string())
```

모델 선정 시 단일 메트릭이 아닌 다중 기준(ms-03 참조)을 고려해야 합니다.

```
모델 선정 결정 흐름:

F1 > 0.85 충족?
├── No  → 탈락
└── Yes → 추론 시간 < 50ms?
          ├── No  → 모델 최적화(mo-05) 후 재평가
          └── Yes → 모델 크기 < 500MB?
                    ├── No  → 양자화/증류 검토
                    └── Yes → 프로덕션 후보 등록
```

> **핵심 직관**: 최고 성능 모델이 항상 최선의 프로덕션 모델은 아닙니다. 추론 비용, 유지보수 복잡도, 해석가능성(ms-05 참조)을 종합적으로 고려해야 합니다.

**시나리오: 모델 거버넌스**
금융 회사에서 신용 평가 모델을 배포하려면 모델 레지스트리에 등록 후, 리스크 팀의 공정성 검증 → 법무팀의 규제 준수 확인 → CTO 승인이라는 3단계 게이트를 통과해야 합니다. 모델 레지스트리의 상태 전환에 승인 워크플로를 연결하여 이를 자동화합니다.

## 핵심 정리

- 실험 추적은 하이퍼파라미터, 메트릭, 아티팩트, 환경 정보, 데이터 버전, 코드 버전을 모두 포함해야 완전합니다
- 하이퍼파라미터는 코드와 분리된 설정 파일(YAML)로 관리하고, Hydra 등의 도구로 체계적으로 주입합니다
- 모델 레지스트리는 None → Staging → Production → Archived 생명주기를 관리하며, 각 전환에 승인 게이트를 설정합니다
- 재현성은 랜덤 시드, 라이브러리 버전, 데이터 버전, 하드웨어 정보를 모두 고정해야 보장됩니다
- 모델 선정은 성능뿐 아니라 추론 비용, 모델 크기, 유지보수성을 종합적으로 고려하는 다중 기준 의사결정입니다
