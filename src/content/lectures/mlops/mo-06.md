# 컨테이너와 오케스트레이션

## 왜 컨테이너가 중요한가

"제 노트북에서는 잘 되는데요"는 ML 엔지니어링에서 가장 흔하면서도 위험한 말입니다. 데이터 사이언티스트의 로컬 환경과 프로덕션 서버의 환경이 다르면 모델 동작이 달라질 수 있습니다. 컨테이너는 코드, 의존성, 런타임 환경을 하나의 패키지로 묶어 어디서든 동일하게 실행되는 것을 보장합니다. Kubernetes는 이러한 컨테이너를 대규모로 배포하고 관리하는 오케스트레이션 플랫폼입니다.

> **핵심 직관**: 컨테이너는 "환경의 버전 관리"입니다. 코드에 Git을 사용하듯, 실행 환경에는 Docker를 사용합니다. 재현성(mo-03 참조)의 마지막 퍼즐 조각입니다.

## 1. Docker 기본 개념

Docker는 애플리케이션과 그 의존성을 격리된 환경(컨테이너)에서 실행합니다.

```dockerfile
# ML 모델 서빙을 위한 Dockerfile
FROM python:3.10-slim

# 시스템 의존성 설치
RUN apt-get update && apt-get install -y --no-install-recommends \
    build-essential && rm -rf /var/lib/apt/lists/*

# Python 의존성 설치 (캐시 활용을 위해 먼저 복사)
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# 모델 아티팩트 복사
COPY model/ /app/model/
COPY src/ /app/src/

WORKDIR /app
EXPOSE 8080

# 헬스체크 설정
HEALTHCHECK --interval=30s --timeout=5s \
    CMD curl -f http://localhost:8080/health || exit 1

CMD ["uvicorn", "src.serve:app", "--host", "0.0.0.0", "--port", "8080"]
```

```
Docker 이미지 빌드 전략:

멀티 스테이지 빌드 (이미지 크기 최소화):

[Stage 1: Builder]              [Stage 2: Runtime]
├── 전체 빌드 도구 설치          ├── 최소 런타임만 포함
├── 의존성 컴파일               ├── 컴파일된 패키지만 복사
├── 모델 변환/최적화            ├── 최적화된 모델만 복사
└── (이 스테이지는 최종 이미지에 포함 안 됨)  └── 최종 이미지 (작은 크기)
```

## 2. ML 모델 컨테이너화 최적화

ML 모델의 Docker 이미지는 일반 애플리케이션보다 훨씬 클 수 있습니다. 최적화 전략이 중요합니다.

```dockerfile
# 멀티 스테이지 빌드로 이미지 크기 최적화
# Stage 1: 모델 변환
FROM python:3.10 AS builder
COPY requirements-build.txt .
RUN pip install -r requirements-build.txt
COPY model_raw/ /tmp/model/
RUN python convert_to_onnx.py /tmp/model/ /opt/model/model.onnx

# Stage 2: 서빙 런타임
FROM python:3.10-slim
COPY requirements-serve.txt .
RUN pip install --no-cache-dir -r requirements-serve.txt
COPY --from=builder /opt/model/ /app/model/
COPY src/ /app/src/
WORKDIR /app
CMD ["uvicorn", "src.serve:app", "--host", "0.0.0.0", "--port", "8080"]
```

| 최적화 기법 | 효과 | 적용 난이도 |
|------------|------|------------|
| 멀티 스테이지 빌드 | 이미지 크기 50~70% 감소 | 낮음 |
| slim/alpine 베이스 이미지 | 베이스 이미지 크기 감소 | 낮음 |
| .dockerignore 활용 | 불필요한 파일 제외 | 낮음 |
| 레이어 캐싱 순서 최적화 | 빌드 시간 단축 | 중간 |
| 모델을 별도 볼륨으로 마운트 | 이미지 재빌드 없이 모델 업데이트 | 중간 |

> **핵심 직관**: Docker 이미지에서 "변경 빈도가 낮은 것을 먼저, 높은 것을 나중에" 복사합니다. 의존성은 거의 안 변하지만 코드는 자주 변하므로, requirements.txt를 먼저 COPY 하고 pip install 한 뒤에 소스 코드를 COPY 합니다.

## 3. Kubernetes 기본 개념

Kubernetes(K8s)는 컨테이너 오케스트레이션 플랫폼으로, ML 워크로드의 배포, 스케일링, 관리를 자동화합니다.

```yaml
# ML 모델 서빙 Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: fraud-detector
  labels:
    app: fraud-detector
    version: v2.3
spec:
  replicas: 3
  selector:
    matchLabels:
      app: fraud-detector
  template:
    metadata:
      labels:
        app: fraud-detector
    spec:
      containers:
        - name: model-server
          image: registry.example.com/fraud-detector:v2.3
          ports:
            - containerPort: 8080
          resources:
            requests:
              cpu: "500m"
              memory: "1Gi"
            limits:
              cpu: "2"
              memory: "4Gi"
          readinessProbe:
            httpGet:
              path: /health
              port: 8080
            initialDelaySeconds: 30
            periodSeconds: 10
          livenessProbe:
            httpGet:
              path: /health
              port: 8080
            initialDelaySeconds: 60
            periodSeconds: 30
---
apiVersion: v1
kind: Service
metadata:
  name: fraud-detector-svc
spec:
  selector:
    app: fraud-detector
  ports:
    - port: 80
      targetPort: 8080
  type: ClusterIP
```

## 4. 오토스케일링

트래픽 변동에 따라 Pod 수를 자동으로 조절합니다.

```yaml
# Horizontal Pod Autoscaler
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: fraud-detector-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: fraud-detector
  minReplicas: 2
  maxReplicas: 20
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 70
    - type: Pods
      pods:
        metric:
          name: inference_requests_per_second
        target:
          type: AverageValue
          averageValue: "100"
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 60
      policies:
        - type: Percent
          value: 50
          periodSeconds: 60
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
        - type: Percent
          value: 25
          periodSeconds: 120
```

```
오토스케일링 결정 흐름:

CPU 사용률 > 70%?
├── Yes → 스케일 업 (안정화 기간 60초)
│         └── 최대 50%씩 증가 (급격한 스케일링 방지)
└── No  → CPU 사용률 < 30%?
          ├── Yes → 스케일 다운 (안정화 기간 300초)
          │         └── 최대 25%씩 감소 (보수적 축소)
          └── No  → 현재 상태 유지
```

**시나리오: 블랙 프라이데이 트래픽 대응**
이커머스 추천 모델 서비스가 평소 QPS 500에서 블랙 프라이데이에 QPS 5,000으로 10배 증가했습니다. HPA가 CPU 사용률 기반으로 Pod를 3개에서 30개로 자동 확장했고, 사전에 Cluster Autoscaler로 노드도 자동 추가되도록 설정하여 서비스 무중단을 달성했습니다.

## 5. GPU 스케줄링

ML 추론에 GPU가 필요한 경우, Kubernetes에서의 GPU 스케줄링은 특별한 고려가 필요합니다.

```yaml
# GPU를 사용하는 모델 서빙 Pod
apiVersion: v1
kind: Pod
metadata:
  name: gpu-inference
spec:
  containers:
    - name: model-server
      image: registry.example.com/llm-server:v1.0
      resources:
        limits:
          nvidia.com/gpu: 1  # GPU 1개 할당
      env:
        - name: CUDA_VISIBLE_DEVICES
          value: "0"
      volumeMounts:
        - name: model-storage
          mountPath: /models
  volumes:
    - name: model-storage
      persistentVolumeClaim:
        claimName: model-pvc
  nodeSelector:
    gpu-type: a100  # 특정 GPU 타입 노드에 배치
  tolerations:
    - key: nvidia.com/gpu
      operator: Exists
      effect: NoSchedule
```

| GPU 스케줄링 전략 | 설명 | 활용 사례 |
|------------------|------|-----------|
| 전용 할당 | 1 Pod = 1 GPU | 대형 모델, 높은 처리량 |
| GPU 공유 (MIG) | A100을 최대 7개 파티션으로 분할 | 소형 모델 다수 운영 |
| GPU 시분할 | 여러 Pod가 시간 단위로 GPU 공유 | 간헐적 추론 워크로드 |
| CPU 폴백 | GPU 부족 시 CPU로 전환 | 지연시간 허용 가능한 서비스 |

> **핵심 직관**: GPU는 비싼 자원입니다. 모든 모델에 GPU가 필요한 것은 아닙니다. 양자화된 모델(mo-05 참조)은 CPU에서도 충분한 성능을 낼 수 있으며, GPU 비용 대비 효과를 항상 검증해야 합니다.

## 6. 컨테이너 보안과 모범 사례

**시나리오: 보안 취약점 대응**
보안 스캔 도구(Trivy)에서 모델 서빙 이미지의 베이스 이미지에 CVE 취약점이 발견되었습니다. 이미지 빌드 파이프라인에 자동 보안 스캔을 추가하고, 취약점 발견 시 빌드를 중단하도록 CI/CD(mo-07 참조)에 통합합니다.

```yaml
# 보안 모범 사례 체크리스트 (docker-compose.yaml)
version: "3.8"
services:
  model-server:
    image: registry.example.com/model:v2.3
    user: "1000:1000"        # non-root 사용자로 실행
    read_only: true           # 파일 시스템 읽기 전용
    security_opt:
      - no-new-privileges     # 권한 상승 방지
    tmpfs:
      - /tmp                  # 임시 파일만 쓰기 허용
    deploy:
      resources:
        limits:
          cpus: "2.0"
          memory: 4G
```

## 핵심 정리

- Docker는 ML 모델의 실행 환경을 버전 관리하여 "어디서든 동일하게 동작"하는 것을 보장합니다
- 멀티 스테이지 빌드와 레이어 캐싱 순서 최적화로 이미지 크기와 빌드 시간을 대폭 줄일 수 있습니다
- Kubernetes의 Deployment, Service, HPA를 조합하여 모델 서빙의 가용성과 확장성을 확보합니다
- GPU 스케줄링은 전용 할당, MIG 파티셔닝, 시분할 등 워크로드 특성에 맞는 전략을 선택합니다
- 컨테이너 보안(non-root 실행, 이미지 스캔, 읽기 전용 파일시스템)은 프로덕션 배포의 필수 요소입니다
