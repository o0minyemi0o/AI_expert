# 모니터링과 관측가능성

## 왜 모니터링이 중요한가

ML 모델은 배포 후 시간이 지남에 따라 성능이 저하됩니다. 전통적인 소프트웨어는 코드를 변경하지 않으면 동작이 동일하지만, ML 모델은 입력 데이터의 분포가 변하면 코드 변경 없이도 성능이 하락합니다. 이를 "모델 부패(Model Decay)" 또는 "모델 드리프트"라 합니다. 적절한 모니터링 없이는 성능 저하를 인지하지 못한 채 잘못된 예측을 제공하게 됩니다.

> **핵심 직관**: ML 모니터링은 "모델이 잘 동작하고 있는가?"를 넘어 "모델이 여전히 신뢰할 수 있는가?"를 지속적으로 검증하는 시스템입니다. 모니터링이 없는 ML 시스템은 눈을 감고 운전하는 것과 같습니다.

## 1. ML 모니터링의 네 가지 계층

ML 시스템의 모니터링은 전통적 인프라 모니터링 위에 ML 특화 계층이 추가됩니다.

| 계층 | 모니터링 대상 | 메트릭 예시 | 도구 |
|------|-------------|------------|------|
| 인프라 | CPU, 메모리, GPU, 네트워크 | CPU 사용률, GPU 메모리 | Prometheus, Grafana |
| 애플리케이션 | API 상태, 지연시간, 오류율 | p99 latency, 5xx rate | Datadog, New Relic |
| 데이터 | 입력 데이터 분포, 스키마 | 드리프트 점수, 결측치 비율 | Evidently, Whylogs |
| 모델 | 예측 품질, 편향, 공정성 | 정확도, F1, AUC 추이 | Evidently, Fiddler |

```
모니터링 계층 구조:

[모델 메트릭]     ← "예측이 정확한가?" (가장 중요하지만 감지 가장 느림)
     ↑
[데이터 메트릭]   ← "입력 데이터가 건강한가?" (선행 지표)
     ↑
[애플리케이션 메트릭] ← "서비스가 응답하는가?"
     ↑
[인프라 메트릭]   ← "서버가 살아있는가?" (가장 기본)
```

## 2. 데이터 드리프트 감지

데이터 드리프트는 모델이 학습한 데이터의 분포와 실제 서빙 시 입력되는 데이터의 분포가 달라지는 현상입니다.

```python
# Evidently를 활용한 드리프트 감지
from evidently.report import Report
from evidently.metric_preset import DataDriftPreset
from evidently.metrics import DataDriftTable

def detect_data_drift(reference_data, current_data):
    report = Report(metrics=[DataDriftPreset()])
    report.run(reference_data=reference_data, current_data=current_data)

    result = report.as_dict()
    drift_detected = result["metrics"][0]["result"]["dataset_drift"]
    drift_share = result["metrics"][0]["result"]["drift_share"]

    return {"drift_detected": drift_detected, "drift_share": drift_share}

# PSI (Population Stability Index) 직접 계산
import numpy as np

def compute_psi(expected, actual, bins=10):
    """PSI 계산: 두 분포의 안정성 측정"""
    expected_pct = np.histogram(expected, bins=bins)[0] / len(expected)
    actual_pct = np.histogram(actual, bins=bins)[0] / len(actual)

    # 0 방지
    expected_pct = np.clip(expected_pct, 1e-6, None)
    actual_pct = np.clip(actual_pct, 1e-6, None)

    psi = np.sum((actual_pct - expected_pct) * np.log(actual_pct / expected_pct))
    return psi
```

| PSI 값 | 해석 | 조치 |
|--------|------|------|
| < 0.1 | 안정적 | 모니터링 유지 |
| 0.1 ~ 0.2 | 경미한 변화 | 주의 관찰 |
| 0.2 ~ 0.25 | 유의미한 변화 | 원인 분석, 재학습 검토 |
| > 0.25 | 심각한 드리프트 | 즉시 재학습 트리거(mo-04 참조) |

> **핵심 직관**: 데이터 드리프트는 모델 성능 저하의 "선행 지표"입니다. 모델 성능이 하락하기 전에 데이터 분포 변화를 먼저 감지할 수 있으므로, 데이터 드리프트 모니터링은 사전 예방적 조치를 가능하게 합니다.

## 3. 모델 성능 저하 감지

프로덕션에서 모델 성능을 추적하는 것은 고유한 도전이 있습니다. 가장 큰 문제는 실제 레이블(ground truth)이 즉시 제공되지 않는다는 것입니다.

```python
# 모델 성능 모니터링 시스템
from prometheus_client import Histogram, Counter, Gauge
import time

# 메트릭 정의
prediction_latency = Histogram("model_prediction_latency_seconds", "추론 지연시간")
prediction_counter = Counter("model_predictions_total", "총 예측 수", ["result"])
model_accuracy = Gauge("model_accuracy", "모델 정확도 (7일 롤링)")
drift_score = Gauge("data_drift_score", "데이터 드리프트 점수", ["feature"])

@prediction_latency.time()
def predict(features):
    result = model.predict(features)

    # 예측 결과 카운터
    label = "positive" if result > 0.5 else "negative"
    prediction_counter.labels(result=label).inc()

    return result

# 주기적 성능 평가 (레이블 확보 후)
def evaluate_rolling_performance():
    """최근 7일간의 예측에 대해 실제 레이블과 비교"""
    predictions = get_recent_predictions(days=7)
    labels = get_confirmed_labels(predictions)

    accuracy = compute_accuracy(predictions, labels)
    model_accuracy.set(accuracy)

    if accuracy < PERFORMANCE_THRESHOLD:
        trigger_alert("model_performance_degradation", accuracy)
```

**시나리오: 조용한 성능 저하 감지**
신용 평가 모델이 6개월간 운영되면서 승인율이 서서히 증가했습니다. 월간 성능 리뷰에서 부도율이 예상치의 1.5배로 증가한 것을 발견했습니다. 원인: 코로나 이후 경제 환경 변화로 인한 개념 드리프트(concept drift). 주간 성능 모니터링과 드리프트 알림이 있었다면 2개월 차에 이미 감지할 수 있었습니다.

## 4. 알림 설계

효과적인 알림은 "중요한 것만, 적시에, 적절한 사람에게" 전달되어야 합니다.

```yaml
# Prometheus 알림 규칙 설정
groups:
  - name: ml_model_alerts
    rules:
      - alert: ModelLatencyHigh
        expr: histogram_quantile(0.99, model_prediction_latency_seconds_bucket) > 0.1
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "모델 추론 p99 지연시간 100ms 초과"

      - alert: DataDriftDetected
        expr: data_drift_score > 0.25
        for: 1h
        labels:
          severity: critical
        annotations:
          summary: "데이터 드리프트 심각 (PSI > 0.25)"

      - alert: PredictionDistributionShift
        expr: |
          abs(rate(model_predictions_total{result="positive"}[1h])
          / rate(model_predictions_total[1h]) - 0.05) > 0.03
        for: 30m
        labels:
          severity: warning
        annotations:
          summary: "예측 분포 비정상 변화 감지"
```

```
알림 설계 결정 흐름:

이상 감지됨
├── 인프라 문제? (서버 다운, OOM 등)
│   └── 즉시 알림 → 온콜 엔지니어 (PagerDuty)
├── 서빙 지연시간 SLA 초과?
│   └── 5분 지속 시 알림 → ML 엔지니어 (Slack)
├── 데이터 드리프트 감지?
│   └── 1시간 지속 시 알림 → 데이터 팀 (Slack)
└── 모델 성능 저하?
    └── 일일 리포트 → ML 팀 (Email/Dashboard)
```

> **핵심 직관**: 알림 피로(Alert Fatigue)는 진짜 문제를 놓치게 만드는 가장 큰 원인입니다. 모든 이상에 알림을 보내는 것이 아니라, 조치가 필요한 이상에만 알림을 보내야 합니다. 알림은 항상 "다음에 무엇을 해야 하는지"를 포함해야 합니다.

## 5. 관측가능성 스택 구축

관측가능성(Observability)은 메트릭, 로그, 트레이스 세 가지 신호를 통합하여 시스템 상태를 파악하는 능력입니다.

| 신호 | 역할 | 도구 | ML 활용 |
|------|------|------|---------|
| 메트릭(Metrics) | 수치적 측정값 집계 | Prometheus + Grafana | 정확도, 지연시간, 드리프트 점수 |
| 로그(Logs) | 개별 이벤트 기록 | ELK Stack, Loki | 예측 입출력, 오류 상세 |
| 트레이스(Traces) | 요청 경로 추적 | Jaeger, Zipkin | 전처리→추론→후처리 병목 분석 |

```python
# 구조화된 로깅으로 관측가능성 확보
import structlog

logger = structlog.get_logger()

async def predict(request):
    logger.info("prediction_request",
                user_id=request.user_id,
                feature_count=len(request.features))

    start = time.time()
    result = model.predict(request.features)
    latency = time.time() - start

    logger.info("prediction_response",
                user_id=request.user_id,
                prediction=float(result),
                confidence=float(max(result)),
                latency_ms=round(latency * 1000, 2),
                model_version="v2.3")

    return result
```

## 6. 대시보드 설계

**시나리오: ML 모니터링 대시보드 구성**
프로덕션 ML 시스템의 Grafana 대시보드를 다음과 같이 구성합니다.

```
┌─────────────────────────────────────────────────────────┐
│ [모델 건강 상태 대시보드]                                   │
│                                                          │
│ [1. 서비스 상태]  QPS: 1,234  에러율: 0.01%  p99: 45ms   │
│ [2. 모델 성능]   정확도: 0.92 ↓0.01  F1: 0.87  AUC: 0.95 │
│ [3. 데이터 드리프트]  전체 PSI: 0.08  드리프트 피처: 2/50   │
│ [4. 예측 분포]   양성 비율: 4.8% (기준: 5.0±1.0%)         │
│ [5. 인프라]     CPU: 45%  GPU: 72%  메모리: 3.2/8GB      │
│                                                          │
│ [알림 이력]                                               │
│ - 2024-01-15 09:30 데이터 드리프트 경고 (해결됨)           │
│ - 2024-01-10 14:15 지연시간 SLA 초과 (해결됨)             │
└─────────────────────────────────────────────────────────┘
```

## 핵심 정리

- ML 모니터링은 인프라, 애플리케이션, 데이터, 모델 네 가지 계층에서 이루어지며, 데이터와 모델 계층이 ML 특화 영역입니다
- 데이터 드리프트(PSI > 0.25)는 모델 성능 저하의 선행 지표이므로, 성능 하락 전에 선제적 대응이 가능합니다
- 모델 성능 모니터링의 핵심 과제는 레이블 지연(label delay)이며, 프록시 메트릭과 예측 분포 모니터링으로 보완합니다
- 알림 설계는 알림 피로를 방지하면서 조치 가능한(actionable) 알림만 전달하는 것이 핵심입니다
- 관측가능성은 메트릭(무엇이), 로그(왜), 트레이스(어디서)를 통합하여 문제의 근본 원인을 신속하게 파악하게 합니다
