# 데이터 파이프라인

## 왜 데이터 파이프라인이 중요한가

ML 시스템의 성능은 모델 알고리즘보다 데이터 품질에 더 크게 좌우됩니다. "Garbage In, Garbage Out"이라는 원칙은 ML에서 더욱 극적으로 적용됩니다. 아무리 정교한 딥러닝 모델(dl-01 참조)이라도 결함이 있는 데이터 위에서는 신뢰할 수 있는 예측을 생성할 수 없습니다. 데이터 파이프라인은 원시 데이터를 모델이 학습할 수 있는 형태로 안정적이고 반복 가능하게 변환하는 시스템입니다.

> **핵심 직관**: 데이터 파이프라인의 목표는 "데이터를 옮기는 것"이 아니라 "신뢰할 수 있는 데이터를 적시에 제공하는 것"입니다. 파이프라인이 실패하면 모델 전체가 멈춥니다.

## 1. ETL vs ELT 패턴

데이터 파이프라인의 두 가지 주요 패턴을 비교합니다.

| 비교 항목 | ETL (Extract-Transform-Load) | ELT (Extract-Load-Transform) |
|-----------|------------------------------|------------------------------|
| 변환 위치 | 중간 처리 계층 | 데이터 웨어하우스 내부 |
| 적합한 규모 | 중소 규모, 정형 데이터 | 대규모, 다양한 데이터 |
| 스토리지 비용 | 낮음 (필요한 데이터만 적재) | 높음 (원시 데이터 전체 적재) |
| 유연성 | 낮음 (스키마 사전 정의 필요) | 높음 (탐색적 분석 가능) |
| 대표 도구 | Informatica, Talend | dbt, BigQuery, Snowflake |
| ML 파이프라인 적합도 | 안정적인 피처 파이프라인 | 탐색적 피처 엔지니어링 |

```
ETL 패턴:
[소스 DB] → [Extract] → [Transform(Spark)] → [Load(DW)] → [피처 스토어]

ELT 패턴:
[소스 DB] → [Extract] → [Load(Data Lake)] → [Transform(dbt/SQL)] → [피처 스토어]
```

## 2. 데이터 검증

데이터 검증은 파이프라인의 각 단계에서 데이터 품질을 자동으로 확인하는 과정입니다. ML 모델의 "조용한 실패(silent failure)"를 방지하기 위해 반드시 필요합니다.

```python
# Great Expectations를 활용한 데이터 검증 예시
import great_expectations as gx

context = gx.get_context()
validator = context.sources.pandas_default.read_csv("training_data.csv")

# 기본 검증 규칙 정의
validator.expect_column_values_to_not_be_null("user_id")
validator.expect_column_values_to_be_between("age", min_value=0, max_value=150)
validator.expect_column_mean_to_be_between("purchase_amount", min_value=10, max_value=500)
validator.expect_column_proportion_of_unique_values_to_be_between("user_id", min_value=0.9)
```

검증해야 할 핵심 항목은 다음과 같습니다.

- **완전성(Completeness)**: NULL 비율이 허용 범위 내인가
- **정확성(Accuracy)**: 값의 범위가 논리적으로 타당한가
- **일관성(Consistency)**: 소스 간 데이터가 일치하는가
- **적시성(Timeliness)**: 데이터가 기대한 시간 내에 도착했는가

> **핵심 직관**: 데이터 검증은 "실패를 빨리 발견하는 것"이 핵심입니다. 학습 후 모델 성능이 하락한 것을 발견하는 것보다, 학습 전에 데이터 이상을 감지하는 것이 비용이 100배 적습니다.

## 3. 스키마 관리

데이터 스키마는 시간이 지남에 따라 변화합니다. 새로운 컬럼이 추가되거나, 기존 컬럼의 타입이 변경되거나, 값의 분포가 달라질 수 있습니다.

```yaml
# 스키마 정의 예시 (YAML 기반)
schema:
  name: user_features
  version: "2.1"
  columns:
    - name: user_id
      type: string
      nullable: false
      primary_key: true
    - name: age
      type: integer
      nullable: false
      constraints:
        min: 0
        max: 150
    - name: total_purchases
      type: float
      nullable: true
      default: 0.0
    - name: last_login
      type: timestamp
      nullable: true
  evolution_policy: backward_compatible
```

스키마 진화 전략은 세 가지로 구분됩니다.

| 전략 | 설명 | 예시 |
|------|------|------|
| 후방 호환(Backward) | 새 스키마가 이전 데이터를 읽을 수 있음 | 새 컬럼 추가 (기본값 포함) |
| 전방 호환(Forward) | 이전 스키마가 새 데이터를 읽을 수 있음 | 선택적 컬럼 삭제 |
| 완전 호환(Full) | 양방향 모두 호환 | 가장 엄격하지만 안전 |

## 4. 배치 파이프라인

배치 파이프라인은 정해진 시간 주기로 대량의 데이터를 처리합니다.

```python
# Airflow DAG 기반 배치 파이프라인 예시
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta

default_args = {"retries": 2, "retry_delay": timedelta(minutes=5)}

with DAG("feature_pipeline", schedule_interval="0 2 * * *",
         default_args=default_args, start_date=datetime(2024, 1, 1)) as dag:

    extract = PythonOperator(task_id="extract_data", python_callable=extract_from_source)
    validate = PythonOperator(task_id="validate_data", python_callable=run_validation)
    transform = PythonOperator(task_id="transform_features", python_callable=compute_features)
    load = PythonOperator(task_id="load_to_store", python_callable=load_feature_store)

    extract >> validate >> transform >> load
```

**시나리오: 일일 추천 피처 배치 파이프라인**
이커머스 플랫폼에서 매일 새벽 2시에 전일 사용자 행동 데이터를 집계하여 피처를 생성합니다. 데이터 소스에서 추출(10분) → 검증(2분) → 피처 변환(30분) → 피처 스토어 적재(5분) 순서로 진행되며, 검증 실패 시 Slack 알림이 발송되고 파이프라인이 중단됩니다.

## 5. 스트리밍 파이프라인

실시간 이벤트를 즉시 처리해야 하는 경우 스트리밍 파이프라인을 구축합니다.

```python
# Kafka + Faust 기반 스트리밍 파이프라인 예시
import faust

app = faust.App("feature-stream", broker="kafka://localhost:9092")

class ClickEvent(faust.Record):
    user_id: str
    item_id: str
    timestamp: float

click_topic = app.topic("clicks", value_type=ClickEvent)

@app.agent(click_topic)
async def process_clicks(events):
    async for event in events:
        features = compute_realtime_features(event)
        await feature_store.update(event.user_id, features)
```

```
배치 vs 스트리밍 결정 흐름:

데이터 지연 허용 시간은?
├── 수 시간 이상 → 배치 파이프라인 (Airflow + Spark)
├── 수 분 ~ 1시간 → 마이크로 배치 (Spark Streaming)
└── 수 초 이하 → 스트리밍 파이프라인 (Kafka + Flink/Faust)
    └── 정확히 한 번 처리(exactly-once) 필요?
        ├── Yes → Kafka + Flink (트랜잭션 지원)
        └── No  → 경량 스트리밍 (Kafka + Consumer)
```

> **핵심 직관**: 스트리밍 파이프라인은 강력하지만 복잡도와 운영 비용이 높습니다. "정말 실시간이 필요한가?"를 먼저 질문하고, 배치로 해결 가능하다면 배치를 선택하는 것이 현명합니다.

## 6. 데이터 계보와 관측가능성

데이터가 어디서 왔고, 어떤 변환을 거쳤는지 추적하는 것을 데이터 계보(Data Lineage)라 합니다. 모델 예측에 문제가 생겼을 때 원인을 역추적하는 데 필수적입니다.

**시나리오: 모델 성능 저하 원인 추적**
추천 모델의 CTR이 갑자기 15% 하락했습니다. 데이터 계보를 추적한 결과, 3일 전 업스트림 서비스에서 사용자 세그먼트 코드 체계를 변경했고, 이로 인해 피처 값이 왜곡된 것을 발견했습니다. 데이터 계보가 없었다면 원인 파악에 수일이 걸렸을 것입니다.

## 핵심 정리

- ETL은 변환 후 적재, ELT는 적재 후 변환 방식이며, ML 파이프라인에서는 탐색적 분석에 유리한 ELT가 점점 더 선호됩니다
- 데이터 검증은 완전성, 정확성, 일관성, 적시성 네 가지 차원에서 자동화되어야 하며, 학습 전 게이트로 작동해야 합니다
- 스키마 관리는 후방 호환성을 기본으로 하되, 파이프라인의 모든 소비자에게 변경사항이 전파되어야 합니다
- 배치와 스트리밍의 선택은 "지연 허용 시간"이 핵심 기준이며, 불필요한 스트리밍은 복잡도만 증가시킵니다
- 데이터 계보(Lineage) 추적은 모델 문제 발생 시 근본 원인 분석(RCA)을 가능하게 하는 핵심 인프라입니다
