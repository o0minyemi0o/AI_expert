# CI/CD for ML

## 왜 ML CI/CD가 중요한가

전통적인 소프트웨어 CI/CD는 코드 변경에 대한 빌드, 테스트, 배포를 자동화합니다. ML 시스템에서는 코드 변경뿐만 아니라 데이터 변경과 모델 변경이라는 두 가지 추가적인 변경 축이 존재합니다. 이 세 가지 축을 모두 관리하는 것이 ML CI/CD의 핵심 과제입니다. 체계적인 CI/CD 없이는 모델 배포가 수동 작업에 의존하게 되고, 이는 오류, 지연, 재현성 부재로 이어집니다.

> **핵심 직관**: ML CI/CD는 "코드 CI/CD + 데이터 CI/CD + 모델 CI/CD"의 합집합입니다. 세 가지 중 하나라도 빠지면 파이프라인에 구멍이 생깁니다.

## 1. ML CI/CD의 세 가지 축

| 변경 축 | 트리거 | 테스트 항목 | 예시 |
|---------|--------|------------|------|
| 코드 변경 | Git push/PR | 단위 테스트, 린팅, 타입 검사 | 피처 엔지니어링 로직 수정 |
| 데이터 변경 | 새 데이터 도착 | 스키마 검증, 분포 검증, 품질 게이트 | 일일 학습 데이터 갱신 |
| 모델 변경 | 재학습 완료 | 성능 벤치마크, 편향 검사, 지연시간 | 새 모델 버전 생성 |

```
ML CI/CD 전체 흐름:

[코드 Push] → [CI: 코드 테스트] → [CI: 파이프라인 빌드] → [학습 실행]
                                                              ↓
[데이터 도착] → [CI: 데이터 검증] ──────────────────────→ [학습 실행]
                                                              ↓
                                                        [모델 생성]
                                                              ↓
                                                    [CD: 모델 테스트]
                                                              ↓
                                                    [CD: 스테이징 배포]
                                                              ↓
                                                    [CD: 섀도 테스트]
                                                              ↓
                                                    [CD: 프로덕션 배포]
```

## 2. 코드 테스트: ML 파이프라인의 CI

ML 코드에 대한 CI는 전통적인 소프트웨어 테스트에 ML 특화 테스트를 추가합니다.

```yaml
# GitHub Actions CI 파이프라인
name: ML Pipeline CI
on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main]

jobs:
  code-quality:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: "3.10"
      - run: pip install -r requirements-dev.txt
      - run: ruff check src/
      - run: mypy src/ --ignore-missing-imports
      - run: pytest tests/unit/ -v --cov=src --cov-report=xml

  pipeline-test:
    runs-on: ubuntu-latest
    needs: code-quality
    steps:
      - uses: actions/checkout@v4
      - run: pip install -r requirements.txt
      - name: 파이프라인 스모크 테스트
        run: python -m pipeline.train --config configs/test.yaml --max-samples 100
      - name: 모델 출력 형태 검증
        run: pytest tests/integration/test_model_output.py
```

```python
# ML 특화 단위 테스트 예시
import pytest
import numpy as np

def test_feature_pipeline_deterministic():
    """동일 입력에 대해 동일 피처 생성 확인"""
    input_data = create_sample_data(seed=42)
    features_1 = feature_pipeline(input_data)
    features_2 = feature_pipeline(input_data)
    np.testing.assert_array_equal(features_1, features_2)

def test_model_prediction_range():
    """모델 출력이 유효 범위 내인지 확인"""
    model = load_test_model()
    predictions = model.predict(get_test_batch())
    assert np.all(predictions >= 0) and np.all(predictions <= 1)

def test_model_not_all_same():
    """모델이 동일한 예측만 반환하지 않는지 확인"""
    model = load_test_model()
    predictions = model.predict(get_diverse_test_batch())
    assert len(set(predictions.round(2))) > 1
```

## 3. 데이터 테스트

데이터 품질을 자동으로 검증하는 테스트를 CI/CD에 통합합니다.

```python
# 데이터 테스트 프레임워크
import great_expectations as gx

def test_training_data_quality():
    """학습 데이터 품질 게이트"""
    context = gx.get_context()
    result = context.run_checkpoint(checkpoint_name="training_data_check")

    # 핵심 검증 항목
    assert result.success, f"데이터 검증 실패: {result.get_failed_validation_results()}"

def test_feature_distribution_stability():
    """피처 분포의 안정성 검증 (훈련-서빙 편향 방지)"""
    train_stats = load_training_stats()
    current_stats = compute_current_stats()

    for feature in KEY_FEATURES:
        drift = compute_psi(train_stats[feature], current_stats[feature])
        assert drift < 0.2, f"피처 {feature}의 PSI={drift:.3f}로 드리프트 감지"

def test_label_distribution():
    """레이블 분포 검증 (데이터 수집 오류 감지)"""
    labels = load_current_labels()
    positive_ratio = labels.mean()
    assert 0.01 < positive_ratio < 0.10, f"양성 비율 {positive_ratio:.3f} 비정상"
```

> **핵심 직관**: 데이터 테스트는 모델 테스트보다 먼저 실행되어야 합니다. 잘못된 데이터로 학습한 모델을 테스트하는 것은 시간 낭비입니다. "데이터가 건강해야 모델이 건강합니다."

## 4. 모델 테스트

새로 학습된 모델이 프로덕션에 배포될 자격이 있는지 자동으로 검증합니다.

```python
# 모델 품질 게이트 테스트
def test_model_performance_regression():
    """현재 프로덕션 모델 대비 성능 하락 여부 확인"""
    current_model = load_model("production")
    new_model = load_model("candidate")
    test_data = load_holdout_set()

    current_f1 = evaluate(current_model, test_data)
    new_f1 = evaluate(new_model, test_data)

    # 성능이 1% 이상 하락하면 배포 차단
    assert new_f1 >= current_f1 * 0.99, \
        f"성능 하락: {current_f1:.4f} → {new_f1:.4f}"

def test_model_fairness():
    """공정성 메트릭 검증"""
    model = load_model("candidate")
    for group in ["gender", "age_group"]:
        group_metrics = evaluate_by_group(model, test_data, group)
        max_gap = max(group_metrics.values()) - min(group_metrics.values())
        assert max_gap < 0.05, f"{group} 그룹 간 성능 차이 {max_gap:.3f} 초과"

def test_model_latency():
    """추론 지연시간 SLA 검증"""
    model = load_model("candidate")
    latencies = [measure_inference_time(model) for _ in range(1000)]
    p99 = np.percentile(latencies, 99)
    assert p99 < 50, f"p99 지연시간 {p99:.1f}ms > SLA 50ms"
```

| 테스트 유형 | 검증 내용 | 차단 기준 예시 |
|------------|----------|---------------|
| 성능 회귀 | 현재 모델 대비 성능 | F1 하락 > 1% |
| 공정성 | 그룹 간 성능 차이 | 차이 > 5% |
| 지연시간 | p99 추론 시간 | > 50ms |
| 모델 크기 | 아티팩트 크기 | > 500MB |
| 슬라이스 성능 | 세그먼트별 성능 | 특정 세그먼트 F1 < 0.7 |

## 5. 섀도 배포 (Shadow Deployment)

섀도 배포는 새 모델이 실제 트래픽을 받지만, 결과는 기록만 하고 사용자에게 반환하지 않는 방식입니다.

```python
# 섀도 배포 프록시 구현
from fastapi import FastAPI
import asyncio
import httpx

app = FastAPI()

@app.post("/predict")
async def predict(request: dict):
    async with httpx.AsyncClient() as client:
        # 프로덕션 모델 (실제 응답)
        prod_response = await client.post("http://model-prod:8080/predict", json=request)

        # 섀도 모델 (결과 기록만)
        asyncio.create_task(shadow_predict(client, request))

    return prod_response.json()

async def shadow_predict(client, request):
    try:
        shadow_response = await client.post("http://model-shadow:8080/predict", json=request)
        log_comparison(request, prod_response=None, shadow_response=shadow_response.json())
    except Exception as e:
        logger.warning(f"섀도 예측 실패: {e}")
```

```
배포 전략 비교:

[섀도 배포] → 0% 트래픽 영향, 100% 관측
     ↓ (검증 통과)
[카나리 배포] → 5% 트래픽에 새 모델, 점진적 확대
     ↓ (검증 통과)
[블루-그린 배포] → 100% 전환, 즉시 롤백 가능
```

> **핵심 직관**: 섀도 배포는 "위험 없이 실전 테스트하는 유일한 방법"입니다. 오프라인 평가에서 좋은 성능이 온라인에서도 보장되지 않으므로(ms-03 참조), 실제 트래픽으로 검증하는 단계가 반드시 필요합니다.

## 6. CD 파이프라인 자동화

**시나리오: 엔드투엔드 ML CD 파이프라인**
사기 탐지 모델의 주간 재학습이 완료되면 다음 과정이 자동 실행됩니다: (1) 모델 테스트 게이트 통과 → (2) 스테이징 환경 자동 배포 → (3) 24시간 섀도 배포 → (4) 예측 분포 비교 통과 → (5) 5% 카나리 배포(mo-10 참조) → (6) 72시간 모니터링(mo-08 참조) → (7) 전체 배포. 어느 단계에서든 문제가 감지되면 자동으로 이전 모델로 롤백됩니다.

```yaml
# CD 파이프라인 설정 (GitHub Actions)
name: Model CD
on:
  workflow_dispatch:
    inputs:
      model_version:
        description: "배포할 모델 버전"
        required: true

jobs:
  model-tests:
    runs-on: ubuntu-latest
    steps:
      - name: 모델 성능 테스트
        run: python tests/model/test_performance.py --version ${{ inputs.model_version }}
      - name: 공정성 테스트
        run: python tests/model/test_fairness.py --version ${{ inputs.model_version }}
      - name: 지연시간 테스트
        run: python tests/model/test_latency.py --version ${{ inputs.model_version }}

  deploy-staging:
    needs: model-tests
    runs-on: ubuntu-latest
    steps:
      - name: 스테이징 배포
        run: kubectl set image deployment/model-staging model=${{ inputs.model_version }}
      - name: 섀도 테스트 시작
        run: python scripts/start_shadow_test.py --duration 24h

  deploy-production:
    needs: deploy-staging
    runs-on: ubuntu-latest
    steps:
      - name: 카나리 배포 (5%)
        run: python scripts/canary_deploy.py --percentage 5 --duration 72h
      - name: 전체 배포
        run: kubectl set image deployment/model-prod model=${{ inputs.model_version }}
```

**시나리오: 롤백 자동화**
카나리 배포 중 새 모델의 오류율이 기존 모델 대비 2배 증가한 것이 감지되었습니다. 자동 롤백 시스템이 3분 이내에 모든 트래픽을 이전 모델로 전환하고, 관련 팀에 슬랙 알림과 인시던트 보고서를 자동 생성했습니다.

## 핵심 정리

- ML CI/CD는 코드, 데이터, 모델 세 가지 변경 축을 모두 관리해야 하며, 각 축에 대한 자동화된 테스트가 필요합니다
- 데이터 테스트(스키마, 분포, 품질)는 모델 테스트보다 선행되어야 하며, 데이터 품질 게이트가 파이프라인의 첫 관문입니다
- 모델 테스트는 성능 회귀, 공정성, 지연시간, 세그먼트별 성능을 포함하며, 모든 항목이 통과해야 배포 가능합니다
- 섀도 배포는 사용자 영향 없이 실제 트래픽으로 새 모델을 검증하는 핵심 안전장치입니다
- 자동 롤백 메커니즘은 프로덕션 배포의 필수 요소이며, 문제 감지 시 분 단위로 이전 모델로 복원해야 합니다
