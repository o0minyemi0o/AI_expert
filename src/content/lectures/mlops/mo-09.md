# 피처 스토어 심화

## 왜 피처 스토어가 중요한가

ML 시스템이 복잡해지면 피처 관리가 가장 큰 병목이 됩니다. 동일한 피처를 여러 모델에서 중복 계산하거나, 학습 시와 서빙 시의 피처 계산 로직이 달라 "학습-서빙 편향(Training-Serving Skew)"이 발생하는 문제가 반복됩니다. 피처 스토어는 피처를 중앙에서 정의, 저장, 서빙하여 이러한 문제를 근본적으로 해결하는 인프라입니다.

> **핵심 직관**: 피처 스토어는 "ML을 위한 데이터 웨어하우스"입니다. 코드에 패키지 매니저가 있듯이, ML 시스템에는 피처를 관리하는 중앙 저장소가 필요합니다.

## 1. 온라인 피처 스토어 vs 오프라인 피처 스토어

피처 스토어는 두 가지 저장 계층으로 구성됩니다.

| 비교 항목 | 오프라인 피처 스토어 | 온라인 피처 스토어 |
|-----------|--------------------|-------------------|
| 용도 | 학습 데이터 생성, 배치 분석 | 실시간 추론 시 피처 조회 |
| 지연시간 | 수 초 ~ 수 분 | 수 밀리초 (< 10ms) |
| 저장소 | S3, BigQuery, Hive | Redis, DynamoDB, Bigtable |
| 데이터 규모 | 수 TB ~ PB | 최신 상태만 (수 GB) |
| 쿼리 패턴 | 범위 스캔, 집계 | 키 기반 포인트 조회 |
| 비용 | 상대적 저렴 | 상대적 높음 (상시 운영) |

```
피처 스토어 아키텍처:

[배치 파이프라인(mo-02)] → [오프라인 스토어(S3/BigQuery)]
                                    ↓ (물질화)
                           [온라인 스토어(Redis)]
                                    ↓
[스트리밍 파이프라인] ──────→ [온라인 스토어(Redis)] ← [서빙 API(mo-05)]
                                                          ↓
[학습 파이프라인(mo-04)] ← [오프라인 스토어] ← [시간 여행 쿼리]
```

## 2. Feast를 활용한 피처 스토어 구현

Feast는 가장 널리 사용되는 오픈소스 피처 스토어입니다.

```python
# feature_store/definitions.py
from feast import Entity, FeatureView, Field, FileSource
from feast.types import Float32, Int64, String
from datetime import timedelta

# 엔티티 정의
user = Entity(name="user_id", join_keys=["user_id"], description="사용자 식별자")

# 데이터 소스 정의
user_features_source = FileSource(
    path="s3://features/user_features.parquet",
    timestamp_field="event_timestamp",
    created_timestamp_column="created_timestamp",
)

# 피처 뷰 정의
user_features = FeatureView(
    name="user_features",
    entities=[user],
    ttl=timedelta(days=1),  # 피처 유효 기간
    schema=[
        Field(name="total_purchases", dtype=Float32),
        Field(name="avg_order_value", dtype=Float32),
        Field(name="days_since_last_order", dtype=Int64),
        Field(name="preferred_category", dtype=String),
    ],
    online=True,  # 온라인 스토어에도 물질화
    source=user_features_source,
)
```

```python
# 피처 스토어 활용 - 학습 데이터 생성
from feast import FeatureStore
import pandas as pd

store = FeatureStore(repo_path="feature_store/")

# 학습 데이터 생성 (시간 여행 포함)
entity_df = pd.DataFrame({
    "user_id": ["user_001", "user_002", "user_003"],
    "event_timestamp": pd.to_datetime(["2024-01-15", "2024-01-15", "2024-01-15"]),
})

training_df = store.get_historical_features(
    entity_df=entity_df,
    features=["user_features:total_purchases", "user_features:avg_order_value"],
).to_df()

# 온라인 서빙 시 피처 조회
online_features = store.get_online_features(
    features=["user_features:total_purchases", "user_features:avg_order_value"],
    entity_rows=[{"user_id": "user_001"}],
).to_dict()
```

## 3. 피처 일관성 보장

학습-서빙 편향(Training-Serving Skew)은 ML 시스템에서 가장 위험한 버그 중 하나입니다.

| 편향 유형 | 원인 | 영향 | 해결 방법 |
|-----------|------|------|-----------|
| 로직 편향 | 학습/서빙에서 다른 전처리 코드 사용 | 예측 품질 저하 | 피처 정의 단일화 |
| 데이터 편향 | 학습 시 미래 데이터 누출 | 과대평가된 성능 | 시간 여행 쿼리 |
| 분포 편향 | 학습/서빙 데이터 분포 차이 | 성능 저하 | 드리프트 모니터링(mo-08) |

```python
# 피처 일관성 검증 테스트
def test_feature_consistency():
    """오프라인과 온라인 피처 값이 일치하는지 검증"""
    store = FeatureStore(repo_path="feature_store/")

    # 같은 시점의 피처를 오프라인/온라인에서 각각 조회
    entity = {"user_id": "user_001"}
    timestamp = datetime.now()

    offline_features = store.get_historical_features(
        entity_df=pd.DataFrame({**entity, "event_timestamp": [timestamp]}),
        features=["user_features:total_purchases"],
    ).to_df()

    online_features = store.get_online_features(
        features=["user_features:total_purchases"],
        entity_rows=[entity],
    ).to_dict()

    # 오차 허용 범위 내에서 일치 확인
    assert abs(
        offline_features["total_purchases"].iloc[0]
        - online_features["total_purchases"][0]
    ) < 1e-5, "오프라인-온라인 피처 불일치 감지"
```

> **핵심 직관**: 피처 일관성 문제는 "조용하게" 성능을 저하시킵니다. 모델이 학습 시 본 피처와 서빙 시 받는 피처가 다르면, 오프라인 평가는 좋지만 온라인 성능이 나쁜 현상이 발생합니다. 이를 방지하려면 학습과 서빙에서 동일한 피처 정의를 공유해야 합니다.

## 4. 시간 여행 (Point-in-Time Join)

시간 여행은 "과거 특정 시점에서 알 수 있었던 정보만"을 사용하여 피처를 생성하는 기법입니다. 미래 데이터 누출(data leakage)을 방지하는 핵심 메커니즘입니다.

```
시간 여행 개념:

시간축 ──────────────────────────────────────────→
        t1        t2        t3(예측시점)    t4

피처A:  [값=10]   [값=15]                  [값=20]
피처B:            [값=3]    [값=5]          [값=7]

Point-in-Time Join at t3:
  피처A = 15 (t2 기준, t3 이전 최신값)
  피처B = 5  (t3 기준, t3 시점의 값)
  ❌ 피처A = 20 (t4는 미래 데이터이므로 사용 불가!)
```

```python
# 시간 여행 쿼리의 SQL 표현
TIME_TRAVEL_QUERY = """
SELECT
    e.user_id,
    e.event_timestamp,
    f.total_purchases,
    f.avg_order_value
FROM
    prediction_events e
LEFT JOIN LATERAL (
    SELECT total_purchases, avg_order_value
    FROM user_features f
    WHERE f.user_id = e.user_id
      AND f.event_timestamp <= e.event_timestamp  -- 미래 데이터 차단
    ORDER BY f.event_timestamp DESC
    LIMIT 1
) f ON true
"""
```

**시나리오: 데이터 누출 방지**
사기 탐지 모델에서 "거래 후 30일 이내 차지백 발생 여부"를 레이블로 사용합니다. 만약 시간 여행 없이 피처를 생성하면, 거래 시점 이후에 발생한 다른 거래 정보가 피처에 포함되어 미래 정보가 누출됩니다. 오프라인 AUC 0.99라는 비현실적 성능이 나오지만, 실제 서빙에서는 AUC 0.75로 급락합니다.

## 5. 피처 서빙 최적화

온라인 피처 서빙의 성능을 최적화하는 전략입니다.

```python
# 피처 서빙 최적화 기법들

# 1. 배치 피처 조회 (다수 엔티티 한 번에 조회)
entities = [{"user_id": f"user_{i:03d}"} for i in range(100)]
batch_features = store.get_online_features(
    features=["user_features:total_purchases", "user_features:avg_order_value"],
    entity_rows=entities,
).to_dict()

# 2. 피처 캐싱 (자주 조회되는 피처)
from functools import lru_cache

@lru_cache(maxsize=50000)
def get_cached_features(user_id: str):
    return store.get_online_features(
        features=FEATURE_LIST,
        entity_rows=[{"user_id": user_id}],
    ).to_dict()

# 3. 피처 사전 계산 (precomputed features)
# 복잡한 집계 피처를 배치로 미리 계산하여 온라인 스토어에 저장
def precompute_aggregated_features():
    """사용자별 7일/30일 집계 피처를 배치로 계산"""
    agg_features = compute_aggregations(window="7d")
    store.materialize(end_date=datetime.now())
```

| 최적화 기법 | 지연시간 효과 | 적용 조건 |
|------------|-------------|-----------|
| 배치 조회 | 다수 개별 호출 대비 5~10x | 다수 엔티티 동시 조회 |
| 로컬 캐싱 | 캐시 히트 시 < 1ms | 조회 패턴이 반복적 |
| 사전 계산 | 온라인 연산 제거 | 집계 피처, 윈도우 피처 |
| 피처 임베딩 | 차원 축소로 전송량 감소 | 고차원 범주형 피처 |

> **핵심 직관**: 피처 서빙 지연시간은 모델 추론 지연시간과 합산됩니다. 모델을 아무리 최적화해도(mo-05 참조) 피처 조회에 100ms가 걸리면 전체 지연시간이 100ms 이상이 됩니다. 피처 서빙 최적화는 모델 최적화만큼 중요합니다.

## 6. 피처 스토어 거버넌스

**시나리오: 대규모 조직에서의 피처 재사용**
데이터 팀이 100개 이상의 피처를 등록한 피처 스토어에서, 새로운 프로젝트 팀이 기존 피처를 검색하고 재사용합니다. 피처 카탈로그에서 "user_purchase_count_30d" 피처를 발견하고, 설명, 소유자, 데이터 계보(mo-02 참조), 사용 중인 모델 목록을 확인한 후 자신의 모델에 바로 적용합니다. 피처 중복 개발 시간 2주를 절약했습니다.

```yaml
# 피처 메타데이터 관리
feature_metadata:
  name: user_purchase_count_30d
  description: "최근 30일간 사용자 구매 횟수"
  owner: data-team@company.com
  created: "2024-01-01"
  sla: "1시간 이내 업데이트"
  data_source: "orders 테이블"
  used_by_models:
    - fraud_detector_v2
    - recommender_v3
    - churn_predictor_v1
  quality_checks:
    - "NOT NULL"
    - "value >= 0"
    - "value < 10000"
```

## 핵심 정리

- 피처 스토어는 오프라인(학습용, 대규모 배치)과 온라인(서빙용, 저지연) 두 계층으로 구성되며, 양 계층 간 일관성이 핵심입니다
- 학습-서빙 편향(Training-Serving Skew)은 피처 정의를 단일화하고 피처 스토어를 통해 학습과 서빙에 동일한 파이프라인을 공유하여 방지합니다
- 시간 여행(Point-in-Time Join)은 과거 시점에서 알 수 있었던 정보만 사용하여 데이터 누출을 방지하는 필수 메커니즘입니다
- 온라인 피처 서빙 지연시간은 배치 조회, 캐싱, 사전 계산 등의 기법으로 최적화하며, 모델 지연시간에 직접 영향을 줍니다
- 피처 카탈로그와 메타데이터 관리는 대규모 조직에서 피처 재사용을 촉진하고 중복 개발을 방지합니다
