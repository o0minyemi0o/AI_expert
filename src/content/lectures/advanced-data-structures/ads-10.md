# 캐시 친화적 자료구조

## 왜 캐시를 고려해야 하는가

알고리즘 분석에서 RAM 모델은 모든 메모리 접근이 동일 비용이라 가정합니다. 하지만 현실에서 L1 캐시 접근은 ~1ns, RAM은 ~100ns, 디스크는 ~10ms입니다. **캐시 미스 한 번이 연산 수백 번보다 비쌉니다.** dbi-01에서 DB가 페이지 단위로 I/O를 하는 이유, B-트리(dbi-02)가 이진 트리보다 빠른 이유가 모두 캐시에 있습니다.

> **핵심 직관**: 현대 하드웨어에서 "데이터를 가져오는 비용"이 "연산하는 비용"보다 훨씬 큽니다. 캐시 친화적 자료구조는 **데이터 접근 패턴을 메모리 계층에 맞추는 것**이 핵심입니다.

## 1. 메모리 계층과 캐시 모델

```
메모리 계층:

  L1 캐시    ~32KB     ~1ns     ████
  L2 캐시    ~256KB    ~4ns     ████████
  L3 캐시    ~8MB      ~10ns    ████████████
  RAM        ~16GB     ~100ns   ████████████████████████████
  SSD        ~1TB      ~100μs   ████████████████████████████████████

  캐시 라인: 보통 64 bytes
  → 메모리 접근 시 64 bytes 단위로 캐시에 적재
  → int 16개 또는 pointer 8개가 한 캐시 라인

외부 메모리 모델 (I/O Model):

  메모리 크기: M
  블록(캐시 라인) 크기: B
  N개 원소 → N/B 블록

  순차 스캔: O(N/B) I/O
  이진 탐색: O(log₂(N/B)) I/O ← 각 비교가 캐시 미스
  B-트리:     O(logB(N)) I/O  ← 노드 하나가 한 블록
```

## 2. 배열 vs 연결 리스트 (캐시 관점)

```
배열 순차 접근:
  [1][2][3][4][5][6][7][8]...
  └──── 캐시 라인 1 ────┘└──── 캐시 라인 2 ────┘

  → 첫 접근만 캐시 미스, 이후 15개는 히트
  → Prefetcher가 다음 라인을 미리 적재

연결 리스트 순차 접근:
  [1]→ ... →[2]→ ... →[3]→ ...
   ↑ 캐시미스  ↑ 캐시미스  ↑ 캐시미스

  → 매 노드 접근마다 캐시 미스 가능
  → Prefetcher가 예측 불가

  실측: 연결 리스트 순회가 배열보다 5-10배 느림
  → 포인터 기반 자료구조는 캐시에 불리
```

## 3. Cache-Oblivious 알고리즘

```
Cache-Oblivious:
  캐시 크기(M)와 블록 크기(B)를 모르고도
  모든 캐시 계층에 최적인 알고리즘

  vs Cache-Aware: B-트리처럼 B를 알고 노드 크기 결정

  장점: 모든 계층(L1, L2, L3, RAM-디스크)에 동시 최적
  단점: 상수 인자가 Cache-Aware보다 약간 큼

Cache-Oblivious 순차 스캔: O(N/B)
  → 그냥 배열 순회 (자동으로 최적)

Cache-Oblivious 정렬: O((N/B) logM/B(N/B))
  → Funnel Sort: 재귀적 머지
```

> **핵심 직관**: Cache-Oblivious의 핵심 아이디어는 **"재귀적으로 문제를 나누면, 어느 시점에서 서브문제가 캐시에 딱 맞게 된다"**는 것입니다. 캐시 크기를 몰라도 재귀 분할이 자동으로 캐시를 활용합니다.

## 4. Van Emde Boas Layout

```
이진 탐색 트리의 캐시 문제:

  BFS 순서 배열: [루트, L1, R1, L2, R2, ...]
  → 깊이가 깊어지면 자식이 멀리 떨어짐 → 캐시 미스

Van Emde Boas (vEB) Layout:

  트리를 높이 절반으로 나누어 재귀적 배치

  높이 h 트리:
  ├─ 상위 h/2: 한 블록에 배치
  └─ 하위 h/2: 각 서브트리를 연속 배치

  결과: 탐색 시 O(logB N) 캐시 미스
  → B-트리와 동일한 I/O 복잡도!
  → 캐시 크기를 몰라도 달성 (Cache-Oblivious)

  실제 효과:
  BFS layout: ~log₂N 캐시 미스
  vEB layout: ~logB N 캐시 미스 (B ≈ 16이면 4배 감소)
```

## 5. B-트리: 궁극의 캐시 최적화

```
B-트리가 이진 트리보다 빠른 이유:

  이진 트리 (높이 20, 100만 노드):
  탐색: 20번 노드 접근 → 20번 캐시 미스

  B-트리 (차수 128, 높이 3, 100만 노드):
  탐색: 3번 노드 접근 → 3번 캐시 미스
  각 노드 내 이진 탐색: 캐시 히트 (노드가 1-2 캐시 라인)

  → 캐시 미스 20번 → 3번: 6배 이상 빠름

  노드 크기 선택:
  ├─ L1 캐시 라인 (64B): CPU 캐시 최적
  ├─ 페이지 (4-16KB): 디스크 I/O 최적 (dbi-02)
  └─ 목적에 따라 다르게 설정
```

## 6. 실전 캐시 최적화 팁

```
1. 배열 > 연결 리스트
   가능하면 포인터 대신 인덱스 기반

2. 구조체 배열 (AoS) vs 배열의 구조체 (SoA)
   AoS: [{x,y,z}, {x,y,z}, ...]  ← x만 접근 시 낭비
   SoA: {[x,x,...], [y,y,...], [z,z,...]}  ← x만 접근 시 캐시 효율

3. 순차 접근 패턴 유지
   행렬 순회: 행 우선 > 열 우선 (C/Python)

4. 데이터 크기 줄이기
   int64 → int32 (캐시 라인에 2배 더 들어감)
   구조체 패딩 최소화

5. Prefetch 힌트
   __builtin_prefetch(ptr) (C/C++)
   → 접근 전에 미리 캐시에 적재
```

> **핵심 직관**: "알고리즘이 $O(N \log N)$이면 충분히 빠르다"는 이론적 관점이고, 실무에서는 **캐시 미스 패턴이 상수 인자를 10배까지 바꿀 수 있습니다**. 빅오가 같더라도 캐시 친화적인 구현이 압도적으로 빠릅니다.

캐시 최적화는 dbi-01의 버퍼 풀, dbi-02의 B-트리 설계, dp-02의 Spark 셔플 최적화의 근본 원리입니다.

## 핵심 정리

- 현대 하드웨어에서 **캐시 미스가 연산보다 100배 비싸므로**, 자료구조 설계에서 캐시를 고려해야 합니다
- **Cache-Oblivious** 알고리즘은 캐시 크기를 모르고도 재귀적 분할로 모든 메모리 계층에 최적화됩니다
- **Van Emde Boas Layout**은 이진 트리를 재귀적으로 배치하여 B-트리와 동일한 $O(\log_B N)$ 캐시 미스를 달성합니다
- **B-트리**는 노드 크기를 캐시 라인/페이지에 맞춰 캐시 미스를 최소화하는 궁극의 캐시 최적화입니다
- 실전에서는 배열 우선, SoA 레이아웃, 순차 접근 패턴, 데이터 크기 축소가 가장 효과적인 최적화입니다
