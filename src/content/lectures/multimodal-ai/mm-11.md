# 멀티모달 평가와 벤치마크

## 왜 평가가 어려운가

mm-01~10에서 다양한 멀티모달 모델을 봤습니다. 하지만 "어떤 모델이 더 좋은가?"에 답하기는 생각보다 어렵습니다. 이미지 분류는 정확도 하나로 비교할 수 있지만, "이 이미지를 설명해"의 좋은 답은 하나가 아닙니다. **멀티모달 평가**는 여러 모달리티에 걸친 이해와 생성을 종합적으로 측정해야 하며, 벤치마크 설계 자체가 연구의 핵심 과제입니다.

> **핵심 직관**: 멀티모달 평가의 근본적 어려움은 **"무엇을 측정하는가"**입니다. "이 VLM이 이미지를 이해하는가?"는 너무 광범위합니다. 객체 인식? 공간 추론? OCR? 상식? 각각 다른 능력이며, 하나의 숫자로 요약하면 핵심 약점이 가려집니다. 좋은 벤치마크는 **구체적인 능력을 분리하여 측정**하면서도 **실제 사용 사례를 반영**해야 합니다.

## 1. VLM 평가의 핵심 차원

```
멀티모달 모델이 갖추어야 할 능력:

  1. 시각적 인식 (Perception):
     ├─ 객체 인식: "이 이미지에 무엇이 있나?"
     ├─ 속성 파악: "무슨 색/크기/재질인가?"
     ├─ 텍스트 인식 (OCR): "간판에 뭐라고 써있나?"
     └─ 기본적이지만 필수

  2. 공간적 이해 (Spatial):
     ├─ 위치 관계: "A의 왼쪽에 B가 있나?"
     ├─ 크기 비교: "가장 큰 객체는?"
     ├─ 수량: "몇 개가 있나?"
     └─ 현재 모델의 주요 약점 중 하나

  3. 추론 (Reasoning):
     ├─ 상식 추론: "이 장면은 어떤 계절인가?"
     ├─ 인과 추론: "왜 이런 상황이 발생했을까?"
     ├─ 수학/논리: "그래프에서 최대값은?"
     └─ 고수준 이해의 핵심

  4. 지식 (Knowledge):
     ├─ 세계 지식: "이 건물의 이름은?"
     ├─ 전문 지식: "이 X-ray에서 이상 소견은?"
     ├─ 문화적 지식: "이 풍습의 의미는?"
     └─ LLM의 지식이 시각에 결합

  5. 생성 품질 (Generation):
     ├─ 유창성: 자연스러운 문장
     ├─ 충실도: 이미지 내용과 일치
     ├─ 상세함: 적절한 수준의 디테일
     └─ 환각 없음: 없는 내용 생성 안 함

  6. 안전성 (Safety):
     ├─ 유해 콘텐츠 거부
     ├─ 편향 없는 응답
     ├─ 프라이버시 보호
     └─ 악의적 사용 방지

  평가의 유형:

  | 유형 | 방식 | 장점 | 단점 |
  |------|------|------|------|
  | 자동 메트릭 | 정답 비교 | 빠르고 재현 가능 | 뉘앙스 놓침 |
  | LLM 심판 | GPT-4가 판정 | 유연함 | 비용, 편향 |
  | 인간 평가 | 사람이 판정 | 가장 정확 | 느리고 비쌈 |
  | 아레나 | 1:1 비교 투표 | 실전적 | 재현 어려움 |
```

## 2. 주요 멀티모달 벤치마크

```
종합 벤치마크:

  MMMU (Yue et al., 2024):
  Massive Multi-discipline Multimodal Understanding
  ├─ 11,500 문제, 30개 학문 분야
  ├─ 대학 수준의 전문 지식 요구
  │   → 의학: X-ray 판독
  │   → 공학: 회로도 분석
  │   → 예술: 미술사 지식
  ├─ 객관식 (4지선다)
  ├─ 이미지 + 전문 질문
  └─ 인간 전문가: 88.6%, GPT-4V: 56.8%
  → 멀티모달 AI와 인간 전문가의 격차를 측정

  MMBench (Liu et al., 2024):
  ├─ 3,217 문제, 20개 능력 차원
  ├─ 체계적 능력 분해:
  │   L1: 인식, 추론
  │   L2: 세부 속성, 관계, 논리, ...
  │   L3: 색상, 형태, 공간 관계, ...
  ├─ 순환 평가: 선택지 순서를 바꿔 여러 번 평가
  │   → 위치 편향(첫 선택지 선호) 제거
  └─ 다국어 버전 (중국어, 영어)

  MME (Fu et al., 2024):
  ├─ Yes/No 질문으로 간단한 평가
  ├─ 14개 하위 과제
  ├─ Perception + Cognition으로 분류
  └─ 간단하지만 폭넓은 스크리닝

  MM-Vet (Yu et al., 2024):
  ├─ 6개 핵심 VL 능력 평가
  │   인식, 지식, 공간 이해, 언어 생성, OCR, 수학
  ├─ 개방형 질문 (자유 답변)
  ├─ GPT-4가 채점
  └─ 통합적 능력 측정

  SEED-Bench (Li et al., 2024):
  ├─ 19K 질문, 12개 평가 차원
  ├─ 이미지 + 비디오 모두 평가
  ├─ 시간적 이해 포함 (비디오)
  └─ 장면 이해, 인스턴스 속성, 공간 관계 등

  | 벤치마크 | 문제 수 | 유형 | 난이도 | 초점 |
  |---------|--------|------|--------|------|
  | MMMU | 11.5K | 객관식 | 매우 높음 | 전문 지식 |
  | MMBench | 3.2K | 객관식 | 중간 | 체계적 능력 |
  | MME | 2.4K | Yes/No | 쉬움 | 폭넓은 스크리닝 |
  | MM-Vet | 200 | 개방형 | 높음 | 통합 능력 |
  | SEED-Bench | 19K | 객관식 | 중간 | 이미지+비디오 |
```

## 3. 환각 평가

```
멀티모달 환각 (Hallucination):
  이미지에 없는 내용을 생성하는 문제

  유형:
  ├─ 객체 환각: 없는 객체 언급
  │   이미지: 개 1마리 → "두 마리의 개가..."
  ├─ 속성 환각: 잘못된 속성 부여
  │   빨간 차 → "파란 차가..."
  ├─ 관계 환각: 잘못된 관계 기술
  │   개가 고양이 옆에 → "개가 고양이 위에..."
  └─ 존재 환각: 이미지와 무관한 내용
     바다 사진 → "산 위에서..."

  POPE (Li et al., 2023):
  Polling-based Object Probing Evaluation
  ├─ "이 이미지에 [객체]가 있나요?" (Yes/No)
  ├─ 긍정/부정 질문 균형
  ├─ Random / Popular / Adversarial 세팅
  │   Adversarial: 자주 함께 나오는 객체로 질문
  │   → "소파가 있으면 TV도 있을 것" 편향 테스트
  └─ 간단하지만 효과적인 환각 측정

  CHAIR (Rohrbach et al., 2018):
  캡션에서 환각된 객체 비율 측정
  ├─ CHAIR_i: 환각된 객체 / 전체 언급 객체
  ├─ CHAIR_s: 환각 포함 캡션 / 전체 캡션
  └─ 낮을수록 좋음

  HallusionBench (Guan et al., 2024):
  ├─ 시각적 착시와 환각을 유도하는 이미지
  ├─ 원본 이미지 + 조작된 이미지
  │   → 원본: "사과가 3개" / 조작: "사과가 2개"
  │   → 모델이 조작을 감지하는지 테스트
  ├─ GPT-4V도 많은 문제에서 실패
  └─ 시각 vs 언어 지식의 충돌을 측정

  환각의 원인:
  ├─ 언어 편향: LLM의 텍스트 통계가 시각을 압도
  │   → "개가 있으면 고양이도 있을 것"
  ├─ 학습 데이터 편향: 캡션이 항상 양성
  │   → "이 이미지에 X가 있다" 패턴에 과적합
  ├─ 시각 토큰 부족: 세밀한 시각 정보 전달 실패
  └─ 주의 부족: Cross-Attention이 잘못된 영역에 주목
```

> **핵심 직관**: 멀티모달 환각은 LLM의 **"유창함과 정확함의 갈등"**이 시각으로 확장된 것입니다. LLM은 유창한 문장을 생성하도록 학습되어, "이미지에 없지만 있을 법한" 내용을 자신있게 생성합니다. "공원에 개가 있다"고 하면 LLM은 자연스럽게 "풀밭에서 공놀이를 하는 개"로 확장하지만, 실제 이미지에서는 개가 앉아있을 수 있습니다.

## 4. 모달리티별 평가

```
이미지 이해 평가:

  일반 VQA: VQAv2, GQA
  지식 VQA: OK-VQA, A-OKVQA
  텍스트 VQA: TextVQA, OCRBench
  문서 VQA: DocVQA, InfographicVQA
  차트 VQA: ChartQA
  과학 VQA: ScienceQA, AI2D

  → mm-04에서 상세히 다룸

오디오 평가:

  음성 인식: WER (Word Error Rate)
  ├─ LibriSpeech, CommonVoice
  └─ 언어별, 도메인별 평가

  오디오 이해: AudioBench
  ├─ 환경음 분류
  ├─ 음악 이해
  └─ 오디오 캡셔닝

비디오 평가:

  Video QA: MSRVTT-QA, ActivityNet-QA, NExT-QA
  비디오 캡셔닝: MSRVTT, YouCook2
  시간적 그라운딩: Charades-STA
  긴 비디오: EgoSchema, Video-MME

  Video-MME (Fu et al., 2024):
  ├─ 짧은/중간/긴 비디오 모두 평가
  ├─ 자막 유무에 따른 성능 차이 측정
  ├─ 비디오+오디오 통합 이해 측정
  └─ 가장 종합적인 비디오 벤치마크

생성 평가:

  이미지 생성:
  ├─ FID (Frechet Inception Distance): 분포 유사도
  ├─ CLIPScore: CLIP 유사도 (텍스트-이미지)
  ├─ 인간 평가: 품질, 프롬프트 따르기
  └─ T2I-CompBench: 구성적 생성 능력

  음성 합성:
  ├─ MOS (Mean Opinion Score): 인간 자연스러움 평가
  ├─ WER: 합성 음성의 인식 정확도
  └─ 화자 유사도: 화자 임베딩 코사인 유사도
```

## 5. 벤치마크 설계 원칙

```
좋은 벤치마크의 조건:

  1. 진단적 (Diagnostic):
     전체 점수뿐 아니라 세부 능력별 점수
     ├─ "시각 인식 80%, 공간 추론 40%, OCR 90%"
     ├─ 약점이 어디인지 명확히 파악 가능
     └─ MMBench의 L1/L2/L3 분해가 좋은 예

  2. 오염 방지 (Contamination-resistant):
     학습 데이터에 벤치마크가 포함되면 무의미
     ├─ 문제: 웹 크롤링 학습 데이터에 벤치마크 포함 가능
     ├─ 해결: 새로운 이미지, 동적 문제 생성
     └─ MMMU-Pro: 선택지를 10개로 + 비전 중심 질문

  3. 강건한 평가 (Robust evaluation):
     ├─ 선택지 순서 변경해도 답이 같아야
     │   → 많은 모델이 "A"를 선호하는 편향
     ├─ 질문 표현 변경에도 일관된 답
     └─ MMBench의 순환 평가가 좋은 예

  4. 실전 반영 (Practical relevance):
     ├─ 학술 벤치마크 vs 실제 사용
     ├─ "여권 사진에서 이름 추출" (실전)
     │   vs "이 그림의 작가는?" (학술)
     ├─ WildVision: 실제 사용자 질문 수집
     └─ Chatbot Arena: 실제 사용자 1:1 비교

  5. 다양성 (Diversity):
     ├─ 다양한 이미지 유형 (자연, 문서, 차트, 의료)
     ├─ 다양한 질문 유형 (인식, 추론, 지식)
     ├─ 다양한 난이도 (쉬움 ~ 전문가)
     └─ 다국어/다문화 포함

현재의 한계:
  ├─ 벤치마크 포화: 모델이 높은 점수 → 차별화 어려움
  ├─ 레이크드 메트릭: 벤치마크 최적화 ≠ 실제 능력
  ├─ 생성 품질의 주관성: 같은 캡션도 사람마다 다르게 평가
  └─ 교차 모달리티 평가 부족: 오디오+비디오+텍스트 동시 평가
```

## 6. 평가의 미래 방향

```
LLM-as-Judge:
  LLM이 멀티모달 출력을 평가

  ├─ GPT-4V/Gemini가 두 모델의 답변을 비교
  ├─ 장점: 유연, 개방형 질문 평가 가능
  ├─ 단점: 자기 자신(또는 유사 모델)에 편향
  │   → GPT-4가 GPT-4 답변을 선호할 수 있음
  └─ 해결: 여러 LLM 심판의 앙상블

  LMSys Chatbot Arena:
  ├─ 사용자가 두 모델을 비교 투표
  ├─ Elo 레이팅으로 모델 순위
  ├─ Vision Arena: 멀티모달 버전
  └─ 가장 실전적인 평가

동적 벤치마크:
  ├─ 매월 새로운 문제 생성
  ├─ 데이터 오염 원천 차단
  ├─ LiveBench, Chatbot Arena 방식
  └─ 정적 벤치마크의 한계 극복

능력 기반 평가 프레임워크:
  단일 점수 대신 능력 프로필

  모델 A: [인식:90, 추론:60, OCR:85, 지식:70, 안전:95]
  모델 B: [인식:85, 추론:80, OCR:70, 지식:90, 안전:90]

  → 용도에 따라 다른 모델이 최적
  → 의료: 지식과 안전이 중요 → 모델 B
  → 문서: OCR이 중요 → 모델 A

  미래 평가의 방향:
  ├─ 인터랙티브 평가: 다중 턴 대화에서의 능력
  ├─ 에이전트 평가: 도구 사용, 행동 수행 능력
  ├─ 안전 평가: Jailbreak, 편향, 유해 콘텐츠
  ├─ 효율 평가: 정확도뿐 아니라 속도/비용 함께
  └─ 실세계 과제: 실제 업무에서의 성능 측정
```

## 핵심 정리

- 멀티모달 평가는 **인식, 공간 이해, 추론, 지식, 생성 품질, 안전성**의 6개 차원으로 나뉘며, 단일 점수로 요약하면 핵심 약점이 가려집니다
- **MMMU**(대학 수준 전문 지식), **MMBench**(체계적 능력 분해), **SEED-Bench**(이미지+비디오)가 주요 종합 벤치마크이며, 각각 다른 관점에서 모델을 평가합니다
- **환각**은 VLM의 핵심 문제로, **POPE**(객체 존재 여부)와 **HallusionBench**(시각 착시)로 측정하며, 원인은 LLM의 언어 편향이 시각 정보를 압도하기 때문입니다
- 좋은 벤치마크는 **진단적**(세부 능력별), **오염 방지**, **강건**(순서 무관), **실전 반영**, **다양**해야 하며, 현재는 벤치마크 포화와 교차 모달리티 평가 부족이 한계입니다
- 평가의 미래는 **LLM-as-Judge**, **동적 벤치마크**(매월 새 문제), **능력 프로필**(다차원 점수), **에이전트 평가**(도구 사용/행동) 방향으로 발전하고 있습니다
