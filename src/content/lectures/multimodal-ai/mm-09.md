# 비디오-언어 이해

## 왜 비디오를 언어로 이해하는가

mm-04에서 이미지에 대한 질의응답을 다뤘습니다. 하지만 세상의 대부분은 **동적**입니다. "이 사람이 무엇을 하고 있나?"는 이미지 한 장으로도 답할 수 있지만, "이 사람이 무엇을 한 **다음에** 무엇을 했나?"는 **시간적 흐름**이 필요합니다. 비디오는 이미지+시간의 모달리티이며, 여기에 언어를 결합하면 행동 이해, 이벤트 추론, 비디오 검색 등 풍부한 과제가 열립니다.

> **핵심 직관**: 비디오 이해의 핵심 도전은 **"시간적 중복과 변화의 균형"**입니다. 30fps 비디오의 연속 프레임은 거의 동일합니다. 하지만 "문을 열기 전에 노크했다"를 이해하려면 시간적 순서가 핵심입니다. 효율적으로 중복을 제거하면서도 중요한 시간적 변화를 포착하는 것이 비디오-언어 모델의 설계 핵심입니다.

## 1. 비디오 표현의 도전

```
비디오의 데이터 특성:
  이미지의 시간적 확장

  30fps, 1분 비디오:
  ├─ 1,800 프레임
  ├─ 각 프레임을 ViT로 처리하면:
  │   1,800 × 256 패치 = 460,800 토큰!
  ├─ Transformer Self-Attention: O(n²)
  │   → 460K² ≈ 2×10¹¹ 연산 → 불가능
  └─ 효율적 표현이 필수

  프레임 샘플링 전략:
  ├─ 균등 샘플링: N 프레임 균등 간격으로 추출
  │   → 1분 비디오 → 32프레임 (2초 간격)
  │   → 간단하지만 빠른 동작 놓칠 수 있음
  ├─ 키프레임 추출: 변화가 큰 프레임만
  │   → 장면 전환, 동작 변화 포착
  │   → 중요하지 않은 구간 건너뛰기
  └─ 동적 샘플링: 내용에 따라 밀도 조절
     → 빠른 동작: 밀집 샘플링
     → 정적 장면: 희소 샘플링

  시간적 정보의 유형:
  ├─ 순간적: "공을 던진다" (1-2초)
  ├─ 단기: "차를 주차한다" (10-30초)
  ├─ 중기: "요리를 만든다" (수 분)
  ├─ 장기: "영화의 줄거리" (수 시간)
  └─ 시간 스케일에 따라 다른 접근 필요

비디오-언어 과제 유형:

  Video QA: 비디오에 대한 질문 답변
  Video Captioning: 비디오 설명 생성
  Temporal Grounding: 텍스트가 가리키는 시간 구간 찾기
  Dense Captioning: 비디오의 시간별 설명
  Video Retrieval: 텍스트로 비디오 검색
  Video Summarization: 긴 비디오 요약
```

## 2. Video LLM 아키텍처

```
Video LLM의 기본 구조:
  mm-03의 VLM을 비디오로 확장

  접근 1: 다중 프레임 입력
  비디오 → N 프레임 샘플링
  → 각 프레임을 ViT로 인코딩
  → 모든 프레임 토큰을 LLM에 입력

  문제: 토큰 수 폭발
  32프레임 × 256패치 = 8,192 토큰
  + 텍스트 토큰 → LLM 컨텍스트 한계에 근접

  접근 2: 시각 토큰 압축

  Video-ChatGPT (Maaz et al., 2023):
  ├─ CLIP ViT로 각 프레임 인코딩
  ├─ 시간 축 평균 풀링: (T, H×W, D) → (H×W, D)
  ├─ 공간 축 평균 풀링: (T, H×W, D) → (T, D)
  ├─ 두 표현을 결합하여 LLM에 입력
  └─ 대폭적 토큰 수 감소

  Video-LLaVA (Lin et al., 2023):
  ├─ LLaVA(mm-03)를 비디오로 확장
  ├─ LanguageBind: 비디오-텍스트 정렬된 인코더
  ├─ 이미지와 비디오를 통합 처리
  └─ MLP로 LLM에 연결 (LLaVA와 동일)

  접근 3: 시간적 어텐션 추가

  LLaVA-NeXT-Video (2024):
  ├─ 이미지용 LLaVA-NeXT를 비디오로
  ├─ AnyRes: 동적 해상도 + 다중 프레임
  ├─ 프레임 수를 점진적으로 증가
  └─ 단순하지만 효과적

  VideoChat2 (Li et al., 2024):
  ├─ UMT-L 비디오 인코더
  ├─ Q-Former로 비디오 토큰 압축
  ├─ 단계적 학습: 이미지 → 비디오 → 지시
  └─ 비디오 특화 Q-Former

  | 모델 | 비디오 인코더 | 압축 | LLM |
  |------|------------|------|-----|
  | Video-ChatGPT | CLIP ViT | 풀링 | LLaMA |
  | Video-LLaVA | LanguageBind | MLP | Vicuna |
  | VideoChat2 | UMT | Q-Former | Vicuna |
  | LLaVA-NeXT-Video | SigLIP | AnyRes | Qwen2 |
```

## 3. 비디오 질의응답

```
Video QA 데이터셋:

  MSRVTT-QA:
  ├─ 10K 비디오, 243K 질문
  ├─ "이 사람은 무엇을 하고 있나요?"
  └─ 짧은 비디오 (10-30초), 단순 질문

  ActivityNet-QA:
  ├─ 5.8K 비디오, 58K 질문
  ├─ 긴 비디오 (평균 3분)
  ├─ 행동, 공간, 시간 추론
  └─ "이 사람이 첫 번째로 한 동작은?"

  NExT-QA:
  ├─ 인과적/시간적 추론 강조
  ├─ "왜 그 사람이 웃었나?" (인과)
  ├─ "문을 열기 전에 무엇을 했나?" (시간)
  └─ 단순 인식을 넘어 추론 필요

  EgoSchema:
  ├─ 1인칭(Ego-centric) 비디오
  ├─ 3분 비디오, 5지선다 질문
  ├─ 매우 긴 시간적 이해 필요
  └─ 현재 모델에게 매우 어려움

  Video QA의 유형:
  ├─ 기술적(Descriptive): "무엇을/누가/어디서"
  ├─ 시간적(Temporal): "언제/이후에/전에"
  ├─ 인과적(Causal): "왜/어떻게/결과"
  ├─ 카운팅: "몇 번/몇 명"
  └─ 추론(Reasoning): 여러 단서를 종합

비디오 이해의 수준:

  Level 1 - 프레임 수준:
  "이 비디오에 개가 있나?" → 이미지 QA와 유사
  → 한 프레임만 봐도 답 가능

  Level 2 - 단기 동작:
  "이 사람이 무엇을 하고 있나?" → 몇 프레임 필요
  → 동작 인식

  Level 3 - 시간 순서:
  "A를 한 후 B를 했는가?" → 프레임 순서 중요
  → 시간적 추론

  Level 4 - 인과 관계:
  "왜 넘어졌는가?" → 원인-결과 추론
  → 물리 상식, 사회적 이해

  Level 5 - 장기 이해:
  "이 영화의 주제는?" → 전체 내용 종합
  → 가장 어려운 수준
```

## 4. 시간적 그라운딩과 Dense Captioning

```
Temporal Grounding (시간적 그라운딩):
  텍스트 → 비디오에서 해당 시간 구간 찾기

  mm-04의 Visual Grounding의 시간 버전

  입력: 비디오 + "남자가 기타를 치는 장면"
  출력: [1:23 - 2:45] (시작-끝 시간)

  데이터셋:
  ├─ Charades-STA: 실내 활동 비디오
  ├─ ActivityNet Captions: 다양한 활동
  ├─ QVHighlights: 뉴스, 브이로그 등
  └─ 자연어 쿼리 → 비디오 구간

  접근 방법:
  ├─ Proposal-based: 후보 구간 생성 → 텍스트와 매칭
  ├─ Proposal-free: 직접 시작/끝 시간 예측
  └─ LLM-based: 비디오 LLM이 시간을 텍스트로 출력
     → "이 장면은 1분 23초부터 2분 45초입니다"

  Moment Retrieval:
  ├─ 자연어로 비디오의 특정 순간 검색
  ├─ "그 사람이 처음 웃는 순간" → 타임스탬프
  └─ 긴 비디오에서 핵심 순간 빠르게 찾기

Dense Video Captioning:
  비디오의 시간별 설명 자동 생성

  입력: 요리 비디오 (5분)
  출력:
  [0:00-0:30] "양파를 칼로 썰고 있다"
  [0:30-1:15] "프라이팬에 기름을 두르고 양파를 볶는다"
  [1:15-2:00] "달걀을 깨서 프라이팬에 넣는다"
  ...

  → 시간 구간 탐지 + 각 구간 설명 생성
  → 비디오 인덱싱, 검색, 요약에 핵심

  Vid2Seq (Yang et al., 2023, Google):
  ├─ 시간 토큰: 특수 토큰으로 시간 표현
  │   <t=0.00> 양파를 썰고 있다 <t=0.30>
  ├─ AR 디코딩으로 (시간, 설명) 쌍 생성
  └─ 단일 모델로 탐지+캡셔닝 동시에
```

> **핵심 직관**: 비디오의 시간적 그라운딩은 이미지의 공간적 그라운딩(바운딩 박스)과 대응합니다. "이 사건이 **어디서** 일어나는가" → 바운딩 박스, "이 사건이 **언제** 일어나는가" → 시간 구간. 최신 Video LLM은 공간(좌표)과 시간(타임스탬프) 모두를 텍스트 토큰으로 출력하여, 통합된 그라운딩을 수행합니다.

## 5. 긴 비디오 이해

```
Long Video Understanding:
  현재 비디오 AI의 가장 어려운 도전

  문제의 크기:
  1시간 비디오 (30fps):
  = 108,000 프레임
  = 27,648,000 패치 토큰 (256 패치/프레임)
  → 어떤 Transformer도 처리 불가

  접근 전략:

  1. 계층적 처리:
     프레임 → 클립(수초) → 장면(수분) → 전체
     ├─ 각 수준에서 요약/압축
     └─ 상위 수준에서 전체 이해

  2. 메모리 기반:
     MovieChat (Song et al., 2024):
     ├─ 비디오를 순차적으로 처리
     ├─ 짧은기억(최근 프레임) + 긴기억(과거 요약)
     ├─ 긴기억: 토큰 병합으로 압축
     └─ 2시간 영화까지 처리 가능

  3. 텍스트 중간 표현:
     LLoVi (Zhang et al., 2024):
     ├─ 각 프레임/클립을 VLM으로 캡셔닝
     ├─ 캡션 모음 → LLM으로 종합 추론
     └─ "시각 → 텍스트 → 추론" 파이프라인

     장점: LLM의 긴 컨텍스트 활용
     단점: 캡셔닝 단계의 정보 손실

  4. 희소 어텐션:
     긴 시퀀스에서 중요한 프레임에만 어텐션
     ├─ 질문 관련 프레임을 동적 선택
     └─ 불필요한 프레임 무시

  Long Video 벤치마크:
  | 벤치마크 | 비디오 길이 | 과제 | 난이도 |
  |---------|-----------|------|--------|
  | MSRVTT | 10-30초 | QA | 쉬움 |
  | ActivityNet | 3분 | QA | 중간 |
  | EgoSchema | 3분 | 5지선다 | 어려움 |
  | MovieChat-1K | 수 시간 | QA | 매우 어려움 |
  | Video-MME | 다양 | 종합 | 어려움 |
```

## 6. 비디오-언어의 응용과 미래

```
실전 응용:

  비디오 검색:
  ├─ 자연어로 비디오 라이브러리 검색
  ├─ "고양이가 상자에 들어가는 장면"
  ├─ CLIP 기반: 프레임 단위 텍스트-이미지 유사도
  └─ 비디오 특화: 시간적 맥락까지 고려

  비디오 요약:
  ├─ 1시간 회의 → 3분 핵심 요약
  ├─ 스포츠 하이라이트 자동 생성
  ├─ 강의 비디오 챕터 분할
  └─ Dense Captioning 기반

  비디오 편집 보조:
  ├─ "이 장면에서 배경 음악을 슬프게"
  ├─ "첫 등장 인물의 대사를 모아줘"
  └─ 자연어 기반 편집 인터페이스

  교육:
  ├─ 강의 비디오 자동 요약
  ├─ 비디오 기반 QA (학습 보조)
  └─ 실습 동영상 분석

  보안/감시:
  ├─ 이상 행동 감지
  ├─ 자연어 쿼리로 CCTV 검색
  └─ 실시간 장면 설명

  비디오 생성과의 연결:
  ├─ cv-12의 Sora: 텍스트 → 비디오 생성
  ├─ 비디오 이해와 생성은 동전의 양면
  ├─ 이해 모델이 생성 모델의 평가자 역할
  └─ 생성 모델이 이해 모델의 학습 데이터 생성

  미래 방향:
  ├─ 실시간 스트리밍 비디오 이해
  ├─ 시간 이상의 비디오 처리
  ├─ 오디오+비디오+텍스트 통합 이해
  │   → mm-10의 Gemini가 시도
  ├─ 인터랙티브: 비디오를 보며 실시간 대화
  └─ 에이전트: 비디오 속 세계를 이해하고 행동 계획
```

## 핵심 정리

- 비디오-언어 이해의 핵심 도전은 **시간적 중복**(연속 프레임의 유사성)과 **토큰 수 폭발**(32프레임×256패치=8K+ 토큰)이며, 프레임 샘플링과 토큰 압축이 필수입니다
- **Video LLM**은 VLM(mm-03)을 비디오로 확장하며, 풀링(Video-ChatGPT), MLP(Video-LLaVA), Q-Former(VideoChat2) 등으로 비디오 토큰을 압축합니다
- **비디오 QA**는 기술적/시간적/인과적 질문 유형이 있으며, 시간 순서 이해(NExT-QA)와 장기 이해(EgoSchema)가 현재 모델의 주요 약점입니다
- **시간적 그라운딩**은 텍스트가 가리키는 비디오 구간을 찾고, **Dense Captioning**은 시간별 설명을 생성하며, Vid2Seq처럼 시간 토큰으로 통합 접근이 가능합니다
- **긴 비디오 이해**는 계층적 처리, 메모리 기반(MovieChat), 텍스트 중간 표현(LLoVi) 등으로 접근하며, 시간 이상의 비디오를 종합 이해하는 것이 핵심 미해결 과제입니다
