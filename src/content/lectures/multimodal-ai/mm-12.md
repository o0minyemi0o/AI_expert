# 새로운 모달리티와 미래

## 왜 더 많은 감각이 필요한가

mm-01~11에서 텍스트, 이미지, 오디오, 비디오의 멀티모달 AI를 다뤘습니다. 하지만 인간의 세계 이해는 **5감을 넘어섭니다**. 물체를 잡을 때의 촉감, 공간을 이동할 때의 균형감각, 도구를 사용할 때의 힘 조절. **Embodied AI**는 AI가 물리적 몸을 가지고 세상과 상호작용하며, **World Models**은 세상의 물리 법칙을 학습합니다. 멀티모달 AI의 궁극적 목표는 **"보고, 듣고, 만지고, 행동하는"** 범용 에이전트입니다.

> **핵심 직관**: 현재 멀티모달 AI는 **"관찰자"**입니다. 이미지를 보고 설명하지만 직접 만지지 않고, 소리를 듣지만 직접 만들지 않습니다. **Embodied AI**는 AI를 **"행위자"**로 전환합니다. 로봇이 "컵을 집어"라는 명령을 이해하려면 컵의 모양(시각), 무게(촉각), 잡는 방법(운동 제어)을 통합해야 합니다. 이것이 멀티모달의 최종 목표이며, 단순한 모달리티 추가가 아닌 **세상과의 상호작용**입니다.

## 1. 촉각과 로봇 감각

```
촉각 (Tactile Sensing):
  로봇이 "느끼는" 감각

  왜 촉각이 필요한가:
  ├─ 시각만으로는 부족한 정보
  │   → 물체의 무게, 질감, 온도, 미끄러움
  │   → 유리컵과 종이컵은 비슷하게 보이지만 다르게 잡아야
  ├─ 가려진 물체: 주머니 속의 열쇠를 만져서 찾기
  └─ 세밀한 조작: 달걀을 깨뜨리지 않고 잡기

  촉각 센서:
  ├─ GelSight: 카메라 기반 촉각 센서
  │   → 투명 젤 + 카메라 + LED
  │   → 접촉 시 젤 변형 → 이미지로 촬영
  │   → 고해상도 3D 접촉 정보
  ├─ DIGIT: GelSight의 소형화 버전 (Meta)
  │   → 로봇 손가락에 장착 가능
  └─ 압력 센서 어레이: 분포된 압력 측정

  Touch and Go (Yang et al., 2022):
  ├─ 시각 + 촉각으로 재질 인식
  ├─ 시각: "이것은 천으로 보인다"
  ├─ 촉각: "만져보니 실크다" (면 vs 실크 구분)
  └─ 시각+촉각 > 시각만 or 촉각만

  로봇의 다중 감각:

  | 감각 | 센서 | 정보 | 용도 |
  |------|------|------|------|
  | 시각 | 카메라 | 형태, 색, 위치 | 인식, 내비게이션 |
  | 촉각 | GelSight, 압력 | 질감, 압력, 미끄러움 | 잡기, 조작 |
  | 힘/토크 | F/T 센서 | 접촉 힘, 토크 | 힘 제어 |
  | 고유수용감각 | 관절 인코더 | 관절 각도, 속도 | 자세 제어 |
  | IMU | 가속도/자이로 | 기울기, 가속 | 균형 |
  | 깊이 | LiDAR, 깊이 카메라 | 3D 거리 | 장애물 회피 |

  → 로봇은 인간보다 더 많은 "감각"을 가질 수 있음
  → 문제: 이 모든 감각을 어떻게 통합할 것인가?
```

## 2. Embodied AI: 몸을 가진 AI

```
Embodied AI의 정의:
  물리적 세계에서 행동하는 AI

  핵심 차이:
  기존 멀티모달: 관찰 → 이해 → 텍스트 응답
  Embodied AI: 관찰 → 이해 → 물리적 행동

  Embodied AI의 루프:
  관찰(시각+촉각+...) → 이해 → 계획 → 행동 → 관찰 → ...
  → rl-01의 에이전트-환경 상호작용!
  → 멀티모달 AI + RL의 융합

  핵심 과제:

  1. Navigation (내비게이션):
     "부엌으로 가서 빨간 컵을 찾아라"
     ├─ 자연어 명령 이해
     ├─ 환경 탐색 (지도 없이)
     ├─ 목표 물체 인식
     └─ 경로 계획 + 장애물 회피

  2. Manipulation (조작):
     "서랍을 열고 가위를 꺼내"
     ├─ 서랍 손잡이 인식
     ├─ 잡기 계획 (그립 포인트)
     ├─ 당기기 (힘 제어)
     ├─ 가위 인식 + 잡기
     └─ 긴 시퀀스의 다단계 조작

  3. Rearrangement (재배치):
     "책상을 정리해"
     ├─ 현재 상태 파악 (어떤 물체가 어디에)
     ├─ 목표 상태 추론 ("정리된" 상태는?)
     ├─ 행동 계획 (어떤 순서로 이동)
     └─ 상식 필요: "연필은 필통에, 책은 책꽂이에"

시뮬레이션 환경:

  Habitat (Meta):
  ├─ 실제 3D 스캔 환경에서 시뮬레이션
  ├─ Matterport3D, Gibson 환경
  └─ 내비게이션 + 조작 과제

  AI2-THOR (Allen AI):
  ├─ 인터랙티브 실내 환경
  ├─ 물체 상호작용 (열기, 집기, 놓기)
  └─ ALFRED: 자연어 지시 따르기

  RLBench (James et al.):
  ├─ 100개 로봇 조작 과제
  ├─ 다양한 난이도
  └─ 데모 데이터 제공

  → 시뮬레이션에서 학습 → 실세계 전이 (rl-14의 Sim-to-Real)
```

## 3. Foundation Models for Robotics

```
LLM/VLM을 로봇에 적용:

  SayCan (Ahn et al., 2022, Google):
  "LLM의 계획 + 로봇의 행동"

  ├─ LLM: "코카콜라를 가져다줘"를 단계로 분해
  │   1. 부엌으로 이동
  │   2. 냉장고를 열기
  │   3. 코카콜라를 집기
  │   4. 냉장고를 닫기
  │   5. 사용자에게 가져가기
  ├─ Affordance: 각 단계의 실행 가능성 평가
  │   → "냉장고를 열기" → 로봇이 현재 할 수 있나?
  ├─ LLM 확률 × Affordance 확률 → 최종 행동 선택
  └─ LLM의 상식 + 로봇의 물리적 제약 결합

  RT-2 (Brohan et al., 2023, Google):
  Vision-Language-Action Model

  ├─ VLM(PaLI-X)을 로봇 행동 생성에 직접 사용
  ├─ 입력: 카메라 이미지 + 자연어 명령
  ├─ 출력: 로봇 행동 토큰 (관절 속도, 그리퍼)
  ├─ 행동을 텍스트 토큰처럼 취급!
  │   → 행동 = "1 128 91 241 5 101 127"
  │   → 각 숫자: 관절 속도의 양자화된 값
  ├─ 웹 데이터의 시각-언어 지식을 로봇에 전이
  └─ "바나나 근처의 물체를 집어" (본 적 없는 조합도 가능!)

  RT-2의 놀라운 일반화:
  ├─ "코카콜라 캔을 집어" (학습 데이터에 없는 객체)
  │   → 웹 지식으로 코카콜라 인식 → 로봇 행동 생성
  ├─ "쓰레기를 치워" (추상적 명령)
  │   → "쓰레기"가 뭔지 VLM이 판단
  └─ VLM의 세계 지식 → 로봇의 일반화 능력

  Open X-Embodiment (2024):
  ├─ 22개 로봇, 527개 기술의 대규모 데이터셋
  ├─ 다양한 로봇/환경의 데이터를 통합
  ├─ RT-X: 여러 로봇 형태에서 일반화되는 정책
  └─ "ImageNet for Robotics"를 향해

  | 모델 | 접근 | LLM/VLM 역할 | 일반화 |
  |------|------|-------------|--------|
  | SayCan | LLM 계획 | 단계 분해 | 중간 |
  | RT-2 | VLM→행동 | 직접 행동 생성 | 높음 |
  | RT-X | 다중 로봇 | 범용 정책 | 높음 |
```

> **핵심 직관**: RT-2의 핵심 통찰은 **"행동도 언어다"**라는 것입니다. "고양이"가 텍스트 토큰이듯, "관절을 30도 회전"도 토큰입니다. VLM이 이미 학습한 시각-언어 지식(코카콜라가 어떻게 생겼는지, 쓰레기가 무엇인지)을 행동 생성에 **직접 활용**할 수 있습니다. 이것은 mm-08의 "오디오도 토큰"과 같은 원리의 로봇 확장입니다.

## 4. World Models

```
World Models:
  세상의 물리 법칙을 이해하는 모델

  rl-09에서 Dreamer의 세계 모델을 다뤘지만,
  여기서는 "대규모 비디오 기반" 세계 모델

  Sora (OpenAI, 2024):
  ├─ 텍스트 → 1분 비디오 생성
  ├─ 물리 법칙을 "이해"하는 것처럼 보임
  │   → 물이 흐르고, 물체가 떨어지고, 빛이 반사
  ├─ OpenAI: "World Simulator"로 지칭
  └─ 세계 모델인가, 패턴 매칭인가? (논쟁 중)

  비디오 생성 = 세계 모델?

  찬성:
  ├─ 물리 법칙에 맞는 비디오 생성
  ├─ 인과 관계 반영 (공을 던지면 떨어짐)
  ├─ 시뮬레이션의 대체 가능
  └─ 충분한 비디오 = 충분한 물리 이해

  반대:
  ├─ 물리 법칙을 "외우는" 것일 뿐 이해가 아님
  ├─ 학습하지 않은 상황에서 실패
  │   → 거꾸로 흐르는 물, 무중력 등
  ├─ 행동에 의한 결과를 예측하지 못함
  └─ 상호작용 없이 관찰만으로는 부족

  Genie (Bruce et al., 2024, Google):
  ├─ 비디오에서 **상호작용 가능한** 환경 생성
  ├─ 인터넷 비디오(게임, 실세계)에서 학습
  ├─ 잠재 행동 추론: 프레임 사이의 "행동" 학습
  │   → 비디오에 행동 라벨 없이!
  │   → 프레임 차이에서 암묵적 행동 추출
  ├─ 사용자 입력 → 다음 프레임 생성
  └─ "비디오에서 게임을 만든다"

  Genie의 핵심 통찰:
  비디오 = 관찰 시퀀스
  관찰 사이의 차이 = 행동의 결과
  → 행동을 모르고도 행동의 효과를 학습!
  → rl-09의 Model-Based RL과 연결

  | 모델 | 입력 | 출력 | 상호작용 |
  |------|------|------|---------|
  | Sora | 텍스트 | 비디오 | 불가 |
  | Genie | 행동 | 다음 프레임 | 가능 |
  | Dreamer | 행동 | 잠재 상태 | 가능 |
  | UniSim | 행동 | 시뮬레이션 | 가능 |
```

## 5. 범용 에이전트를 향해

```
멀티모달 에이전트:

  GUI 에이전트:
  ├─ 컴퓨터 화면을 보고 마우스/키보드 조작
  ├─ "항공편을 예약해줘" → 브라우저 탐색 → 예약
  ├─ WebVoyager, OSWorld 벤치마크
  └─ 시각(스크린샷) + 행동(클릭/타이핑)

  CogAgent (Hong et al., 2024):
  ├─ GUI 스크린샷 이해 + 행동 생성
  ├─ 고해상도 GUI 이미지 처리
  └─ 웹, 모바일 앱 자동화

  게임 에이전트:
  ├─ CRADLE: 게임 화면을 보고 게임 플레이
  ├─ Voyager: Minecraft에서 자율 탐험
  │   → GPT-4로 코드 생성 → 게임 행동
  └─ 시각 입력 + 행동 계획 + 코드 생성

범용 에이전트의 비전:

  현재 (2024-2025):
  ├─ 텍스트+이미지+오디오+비디오 이해
  ├─ 텍스트+이미지+오디오 생성
  ├─ 도구 사용 (코드 실행, API 호출)
  └─ 간단한 GUI 조작

  단기 미래 (2025-2027):
  ├─ 로봇 제어와 VLM의 깊은 통합
  ├─ 실시간 멀티모달 스트리밍
  ├─ 긴 기억 (과거 경험 축적)
  └─ 자율적 목표 설정과 계획

  장기 비전:
  ├─ 물리적 세계를 이해하는 World Model
  ├─ 인간 수준의 감각 통합
  ├─ 자율적 학습과 적응
  └─ 범용 로봇: "무엇이든 할 수 있는" 에이전트

  핵심 기반 기술 연결:
  ├─ mm-02 CLIP: 시각-언어 정렬의 기반
  ├─ mm-03 VLM: 시각 이해의 핵심
  ├─ mm-06~08: 오디오 이해와 생성
  ├─ mm-10: 통합 모달리티 처리
  ├─ rl-08~14: 행동 학습과 최적화
  ├─ cv-13: 3D 공간 이해
  └─ dl-01~16: 모든 것의 기초
```

## 6. 열린 문제와 전망

```
근본적 열린 문제:

  1. 그라운딩 (Grounding):
     언어의 의미가 물리적 세계에 어떻게 연결되는가?
     ├─ "무거운"의 의미를 텍스트만으로 이해할 수 있는가?
     ├─ 직접 들어봐야 "무겁다"를 진정으로 이해?
     ├─ Symbol Grounding Problem (Harnad, 1990)
     └─ Embodied AI가 해결할 수 있는가? (활발한 논쟁)

  2. 상식 물리 (Intuitive Physics):
     물이 기울면 쏟아진다
     공을 던지면 포물선으로 날아간다
     유리는 깨지기 쉽다
     → 인간은 직관적으로 아는 것, AI는?
     → 비디오에서 학습 가능? 상호작용이 필수?

  3. 통합 표현 (Unified Representation):
     모든 모달리티를 하나의 표현으로?
     ├─ 현재: 모달리티별 인코더 + 결합
     ├─ 미래: 모달리티에 무관한 범용 표현?
     ├─ mm-02의 ImageBind가 시도
     └─ 근본적으로 가능한가? (철학적 질문)

  4. 효율성:
     ├─ 현재 멀티모달 모델은 매우 큼
     ├─ Gemini Ultra: 수천 GPU로 학습
     ├─ 로봇: 실시간, 저전력 필요
     └─ 경량화, 온디바이스 추론이 핵심

  5. 안전과 윤리:
     ├─ 딥페이크: 이미지+오디오+비디오 조작
     ├─ 자율 에이전트의 안전: 잘못된 행동의 물리적 결과
     ├─ 편향: 학습 데이터의 문화적/사회적 편향
     └─ 투명성: AI의 판단 근거 설명

  멀티모달 AI 코스 전체 흐름:
  mm-01: 기초 (모달리티, 융합)
  mm-02~03: 비전-언어 (CLIP, VLM)
  mm-04~05: 시각 이해/생성 (VQA, Diffusion)
  mm-06~08: 오디오 (표현, ASR/TTS, AudioLM)
  mm-09: 비디오-언어
  mm-10: 통합 (Gemini, GPT-4o)
  mm-11: 평가
  mm-12: 미래 (Embodied, World Models, 에이전트)

  → CLIP의 정렬에서 시작하여
  → VLM으로 이미지를 이해하고
  → Diffusion으로 이미지를 생성하고
  → 오디오를 듣고 말하며
  → 비디오를 이해하고
  → 모든 것을 통합하여
  → 물리적 세계에서 행동하는 AI를 향해

  이 모든 것의 기반은:
  "서로 다른 형태의 정보를 연결하여
   더 완전한 세계 이해를 만든다"
```

## 핵심 정리

- **촉각 센서**(GelSight, DIGIT)는 시각으로 파악할 수 없는 질감/무게/미끄러움 정보를 제공하며, 세밀한 로봇 조작에서 시각+촉각 통합이 필수적입니다
- **Embodied AI**는 AI를 관찰자에서 행위자로 전환하며, 내비게이션/조작/재배치 과제에서 시각-언어-행동의 통합이 핵심이고, Habitat/AI2-THOR 시뮬레이션에서 학습합니다
- **RT-2**는 VLM의 시각-언어 지식을 로봇 행동 생성에 직접 활용하여, 학습하지 않은 객체/명령에도 일반화하며, "행동도 토큰이다"라는 원칙이 핵심입니다
- **World Models**은 비디오에서 물리 법칙을 학습하며, Sora(관찰 기반)와 Genie(상호작용 가능)가 대표적이고, "비디오 생성 = 세계 이해인가"는 활발한 논쟁 중입니다
- 멀티모달 AI의 궁극적 방향은 **"보고, 듣고, 만지고, 행동하는" 범용 에이전트**이며, 그라운딩, 상식 물리, 통합 표현, 효율성, 안전이 핵심 열린 문제입니다
