# 텍스트-이미지 생성

## 왜 텍스트로 이미지를 만드는가

mm-04까지 이미지를 **이해**하는 과제를 다뤘습니다. 이제 텍스트로 이미지를 **생성**하는 방향을 봅니다. "석양이 지는 해변에서 걷는 고양이"라고 쓰면 그런 이미지가 만들어집니다. **텍스트-이미지 생성**은 cv-07의 Diffusion Model과 mm-02의 CLIP을 결합한 것이며, DALL-E에서 Stable Diffusion까지 폭발적으로 발전했습니다. 이 강의는 cv-07의 이미지 생성 기초 위에 **텍스트 조건부 생성의 원리**에 집중합니다.

> **핵심 직관**: 텍스트-이미지 생성의 핵심은 **"텍스트가 Diffusion의 방향을 안내한다"**는 것입니다. cv-07의 Diffusion Model은 노이즈에서 이미지를 만들지만, 어떤 이미지를 만들지 모릅니다. 텍스트 임베딩이 "고양이를 그려라"라고 방향을 지시하면, 노이즈 제거가 고양이 방향으로 진행됩니다. 이 방향 지시가 **Cross-Attention**으로 구현됩니다.

## 1. 텍스트 조건부 Diffusion

```
조건부 생성의 원리:

  무조건 Diffusion (cv-07 복습):
  노이즈 z_T → ... → z_1 → z_0 (이미지)
  ε_θ(z_t, t): 노이즈 예측 네트워크
  → 어떤 이미지가 나올지 제어 불가

  조건부 Diffusion:
  ε_θ(z_t, t, c): c = 텍스트 조건
  → 텍스트 c에 맞는 이미지로 노이즈 제거

  텍스트 조건 c는 어떻게 주입하는가?

  방법 1: Cross-Attention (가장 일반적)
  U-Net/DiT의 각 블록에 Cross-Attention 추가

  Self-Attention: Q=K=V=이미지 특징
  Cross-Attention: Q=이미지 특징, K=V=텍스트 임베딩

  → 이미지의 각 영역이 텍스트의 관련 단어에 주목
  → "빨간 차"에서 차 영역이 "빨간"에 어텐션
  → 텍스트가 이미지 생성을 공간적으로 안내

  방법 2: Adaptive Layer Norm (DiT)
  타임스텝 t와 텍스트 c를 AdaLN으로 주입
  → cv-07의 DiT 구조
  → 전역적 조건 주입

텍스트 인코더:

  CLIP Text Encoder:
  ├─ SD 1.5: CLIP ViT-L/14 텍스트 인코더
  ├─ 77 토큰 제한, 768차원
  ├─ mm-02의 대조 학습으로 사전학습
  └─ 이미지-텍스트 정렬이 핵심

  OpenCLIP + T5:
  ├─ SDXL: OpenCLIP ViT-bigG + CLIP ViT-L (이중)
  ├─ SD 3: T5-XXL + CLIP (텍스트 이해 강화)
  ├─ T5: 더 긴 텍스트, 더 나은 언어 이해
  └─ 복잡한 프롬프트에 더 잘 반응

  텍스트 인코더의 영향:
  "a red car parked in front of a blue house"
  ├─ CLIP만: 색상-객체 바인딩 실패 가능 (빨간 집, 파란 차)
  ├─ CLIP + T5: 바인딩 개선
  └─ 텍스트 이해력이 생성 품질을 직접 좌우
```

## 2. DALL-E 시리즈

```
DALL-E (Ramesh et al., 2021, OpenAI):
  텍스트-이미지 생성의 시작

  구조:
  ├─ dVAE: 이미지를 이산 토큰으로 인코딩
  │   256×256 이미지 → 32×32 = 1024개 토큰
  ├─ BPE: 텍스트를 256개 토큰으로
  ├─ Transformer: [텍스트 256] + [이미지 1024] 시퀀스
  │   → Autoregressive하게 이미지 토큰 생성
  └─ dVAE 디코더: 이산 토큰 → 이미지

  → GPT처럼 "다음 토큰 예측"으로 이미지 생성!
  → 12B 파라미터, 2.5억 이미지-텍스트 쌍

DALL-E 2 (Ramesh et al., 2022):
  CLIP + Diffusion의 결합

  2단계 과정:
  1. Prior: 텍스트 임베딩 → CLIP 이미지 임베딩 예측
     텍스트 "a corgi" → CLIP 이미지 공간의 벡터
     → Diffusion Prior 또는 AR Prior

  2. Decoder: CLIP 이미지 임베딩 → 이미지
     unCLIP: CLIP 임베딩을 조건으로 이미지 생성
     → 수정된 GLIDE 모델 사용

  왜 2단계인가:
  ├─ 텍스트 → 이미지: 너무 큰 도약
  ├─ 텍스트 → CLIP 이미지 임베딩: 같은 공간에서 이동
  ├─ CLIP 임베딩 → 이미지: 시각 정보 복원
  └─ 중간 표현(CLIP)이 다리 역할

  특징:
  ├─ 이미지 변형: CLIP 임베딩 보간으로 스타일 변환
  ├─ 텍스트 기반 편집: 기존 이미지의 CLIP 임베딩 수정
  └─ 고품질이지만 프롬프트 따르기가 완벽하지 않음

DALL-E 3 (Betker et al., 2023):
  프롬프트 따르기의 대폭 개선

  핵심 혁신: 캡션 재작성
  ├─ 기존 데이터셋 캡션: "A dog" (너무 간단)
  ├─ 상세 캡션 생성: LLM으로 재작성
  │   → "A golden retriever sitting on a green lawn..."
  ├─ 상세 캡션으로 학습 → 상세 프롬프트를 잘 따름
  └─ 간단한 사용자 프롬프트 → GPT-4가 상세 프롬프트로 변환

  → "데이터가 모델을 만든다"의 또 다른 증거
  → 캡션 품질이 생성 품질을 결정

  | 모델 | 구조 | 프롬프트 따르기 | 시기 |
  |------|------|--------------|------|
  | DALL-E 1 | AR + dVAE | 낮음 | 2021 |
  | DALL-E 2 | CLIP + Diffusion | 중간 | 2022 |
  | DALL-E 3 | 캡션 재작성 + Diffusion | 높음 | 2023 |
```

## 3. Stable Diffusion

```
Stable Diffusion (Rombach et al., 2022):
  오픈소스 텍스트-이미지 생성의 표준

  cv-07에서 다룬 Latent Diffusion의 텍스트 조건 버전

  구조:
  이미지 x → VAE Encoder → 잠재 z (4×64×64)
  텍스트 → CLIP Text Encoder → 텍스트 임베딩 c
  노이즈 z_T → U-Net(z_t, t, c) → 디노이징 → z_0
  z_0 → VAE Decoder → 이미지 x̂

  U-Net의 Cross-Attention:
  각 해상도 블록에서:
  ├─ ResNet Block (공간 특징)
  ├─ Self-Attention (공간 내 관계)
  └─ Cross-Attention (텍스트↔이미지 관계)

  Cross-Attention:
  Attention(Q, K, V) = softmax(QK^T/√d)V
  Q = W_Q × z  (이미지 특징에서)
  K = W_K × c  (텍스트 임베딩에서)
  V = W_V × c  (텍스트 임베딩에서)

  → 이미지의 각 위치가 텍스트의 어떤 단어에 주목할지 학습
  → "a red hat" 생성 시: 모자 영역의 Q가 "red"의 K에 높은 어텐션

  SD 1.5 → SDXL → SD 3 진화:

  SD 1.5 (2022):
  ├─ U-Net 860M, CLIP ViT-L
  ├─ 512×512 기본 해상도
  ├─ LAION-5B에서 학습
  └─ 오픈소스 생태계의 시작

  SDXL (2023):
  ├─ U-Net 2.6B (3배 증가)
  ├─ 이중 텍스트 인코더 (OpenCLIP + CLIP)
  ├─ 1024×1024 기본 해상도
  ├─ Refiner 모델: 세부 디테일 향상
  └─ 크기 조건: 원하는 해상도 지정 가능

  SD 3 (2024):
  ├─ U-Net → DiT (Diffusion Transformer)
  ├─ MMDiT: 텍스트/이미지를 결합 어텐션
  ├─ 삼중 텍스트 인코더 (CLIP×2 + T5-XXL)
  ├─ Flow Matching: DDPM 대신 rectified flow
  └─ 텍스트 렌더링 크게 개선

  | 모델 | 백본 | 텍스트 인코더 | 해상도 |
  |------|------|-------------|--------|
  | SD 1.5 | U-Net 860M | CLIP-L | 512 |
  | SDXL | U-Net 2.6B | CLIP-L+G | 1024 |
  | SD 3 | DiT | CLIP×2+T5 | 1024 |
```

> **핵심 직관**: Stable Diffusion의 Cross-Attention은 **"화가에게 지시하는 것"**과 같습니다. "빨간 모자를 쓴 소녀"라고 하면, U-Net이 이미지를 그릴 때 모자 영역에서 "빨간"이라는 지시를 참조하고, 소녀 영역에서 "소녀"라는 지시를 참조합니다. Cross-Attention의 어텐션 맵을 시각화하면, 실제로 텍스트 단어가 이미지의 해당 영역에 높은 가중치를 갖는 것을 볼 수 있습니다.

## 4. Classifier-Free Guidance

```
CFG (Classifier-Free Guidance, Ho & Salimans, 2022):
  텍스트-이미지 생성의 핵심 기법

  문제:
  조건부 Diffusion만으로는 텍스트를 강하게 따르지 않음
  → 텍스트와 무관한 일반적 이미지가 생성되는 경향

  Classifier Guidance (이전 방법):
  별도의 분류기를 학습하여 생성을 유도
  → 분류기 별도 학습 필요 → 번거로움

  Classifier-Free Guidance:
  분류기 없이, 조건부/무조건부 모델을 동시에 학습

  학습 시:
  일정 확률(10~20%)로 텍스트 조건을 제거 (빈 텍스트로 대체)
  → 하나의 모델이 조건부 + 무조건부 모두 학습

  추론 시:
  ε_guided = ε_uncond + w × (ε_cond - ε_uncond)

  ε_uncond: 조건 없는 노이즈 예측 (텍스트 무시)
  ε_cond: 조건부 노이즈 예측 (텍스트 반영)
  w: Guidance Scale (가이던스 강도)

  w의 효과:
  ├─ w = 1: 기본 조건부 생성 (약한 텍스트 반영)
  ├─ w = 7.5: 일반적 기본값 (좋은 균형)
  ├─ w = 15: 강한 텍스트 반영 (선명하지만 과포화)
  ├─ w = 30+: 과도한 강조 (부자연스러운 이미지)
  └─ w = 0: 무조건부 생성 (텍스트 무시)

  직관:
  ε_cond - ε_uncond = "텍스트가 만드는 차이"
  w로 이 차이를 증폭!
  → "텍스트 방향으로 더 강하게 밀어라"

  w 크면: 텍스트를 잘 따르지만, 다양성 감소 + 아티팩트
  w 작으면: 다양하지만, 텍스트와 관련 약함

  CFG의 비용:
  매 디노이징 스텝마다 2번의 추론 필요
  (1번 조건부 + 1번 무조건부)
  → 속도 2배 느려짐
  → 최근: Distillation으로 CFG 없이도 좋은 결과

Negative Prompting:
  원하지 않는 요소를 명시적으로 제거

  ε_guided = ε_neg + w × (ε_cond - ε_neg)

  ε_neg: 네거티브 프롬프트의 노이즈 예측
  → "blurry, low quality, deformed" 등을 네거티브로

  → 빈 텍스트 대신 네거티브 프롬프트 사용
  → 품질 향상에 매우 효과적
```

## 5. 최신 텍스트-이미지 모델

```
Imagen (Saharia et al., 2022, Google):
  ├─ T5-XXL 텍스트 인코더 (언어 모델 강조)
  ├─ Cascaded Diffusion: 64→256→1024 단계적 업스케일
  ├─ 핵심 발견: 텍스트 인코더가 이미지 생성기보다 중요
  └─ Dynamic Thresholding: 높은 CFG에서도 안정적

FLUX (Black Forest Labs, 2024):
  ├─ SD 3의 후속, Stability AI 핵심 인력
  ├─ MMDiT (Multi-Modal DiT) 아키텍처
  ├─ Flow Matching 기반
  ├─ FLUX.1 [dev]: 12B 파라미터
  ├─ 텍스트 렌더링, 손 생성 크게 개선
  └─ 현재 오픈소스 최고 수준

Midjourney:
  ├─ 비공개 모델, Discord 기반 서비스
  ├─ 예술적 품질에 특화
  ├─ v6: 텍스트 렌더링, 사실적 인물 크게 개선
  └─ 상용 서비스로 큰 성공

Ideogram:
  ├─ 텍스트 렌더링에 특화
  ├─ 이미지 내 텍스트를 정확하게 생성
  └─ 로고, 포스터 등 텍스트가 필요한 생성에 강점

텍스트-이미지의 남은 도전:

  1. 텍스트 렌더링:
     이미지 내 텍스트 생성이 여전히 어려움
     → FLUX, SD 3에서 크게 개선 but 완벽하지 않음

  2. 수량과 공간 관계:
     "3개의 빨간 공과 2개의 파란 공"
     → 정확한 수량 제어 어려움

  3. 속성 바인딩:
     "빨간 모자를 쓴 소녀와 파란 모자를 쓴 소년"
     → 속성(빨간/파란)이 객체(소녀/소년)에 정확히 바인딩 안 됨

  4. 일관성:
     같은 캐릭터를 여러 이미지에서 일관되게 유지
     → IP-Adapter, Reference 이미지로 부분 해결

  5. 손/얼굴:
     인체 구조가 부자연스러운 경우
     → FLUX에서 크게 개선
```

## 6. 텍스트-이미지 생성의 활용

```
실전 응용:

  크리에이티브:
  ├─ 컨셉 아트: 게임/영화 초기 디자인
  ├─ 광고 이미지: 제품 광고 시안
  ├─ 일러스트: 책, 블로그 삽화
  └─ 패션 디자인: 의상 컨셉

  산업:
  ├─ 제품 디자인: 프로토타입 시각화
  ├─ 건축: 외관/인테리어 렌더링
  ├─ 교육: 교재 삽화 자동 생성
  └─ 마케팅: A/B 테스트용 이미지 다량 생성

  제어된 생성 (cv-08 연결):
  ├─ ControlNet: 포즈, 엣지, 깊이맵으로 구조 제어
  ├─ IP-Adapter: 참조 이미지의 스타일/내용 유지
  ├─ Inpainting: 이미지 일부를 텍스트로 수정
  └─ Image-to-Image: 기존 이미지를 텍스트로 변환

  윤리적 고려:
  ├─ 저작권: 학습 데이터의 저작물 사용 문제
  ├─ 딥페이크: 가짜 인물/장면 생성 악용
  ├─ 편향: 학습 데이터의 편향이 생성에 반영
  ├─ 워터마크: AI 생성 이미지 식별
  └─ 규제: EU AI Act 등 법적 프레임워크 논의 중

  프롬프트 작성 팁:
  ├─ 구체적으로: "a cat" → "a fluffy orange tabby cat"
  ├─ 스타일 명시: "oil painting style", "photorealistic"
  ├─ 구도 지정: "close-up", "aerial view", "golden hour"
  ├─ 네거티브 활용: 원치 않는 요소 명시적 제거
  └─ 단계적 개선: 기본 → 세부 추가 → 네거티브 조정
```

## 핵심 정리

- 텍스트-이미지 생성의 핵심은 **Cross-Attention**으로, U-Net/DiT의 이미지 특징(Q)이 텍스트 임베딩(K,V)을 참조하여 텍스트가 이미지 생성의 방향을 안내합니다
- **DALL-E** 시리즈는 AR+dVAE → CLIP+Diffusion → 캡션 재작성으로 진화했으며, 특히 DALL-E 3의 **상세 캡션 학습**이 프롬프트 따르기를 크게 개선했습니다
- **Stable Diffusion**은 SD 1.5(U-Net) → SDXL(이중 텍스트 인코더) → SD 3(DiT+Flow Matching)으로 진화하며, 오픈소스 생성 AI 생태계의 중심입니다
- **Classifier-Free Guidance**(CFG)는 조건부/무조건부 예측의 차이를 w배 증폭하여 텍스트 따르기를 강화하며, w=7.5가 품질-다양성의 일반적 균형점입니다
- 남은 도전은 **텍스트 렌더링**, **수량/공간 제어**, **속성 바인딩**, **일관성**이며, FLUX/SD 3 등 최신 모델이 점진적으로 개선하고 있습니다
