# 음성 인식과 합성

## 왜 음성이 핵심 인터페이스인가

mm-06에서 오디오의 기초 표현을 배웠습니다. 이제 가장 중요한 두 가지 응용인 **음성 인식(ASR)**과 **음성 합성(TTS)**을 다룹니다. **Whisper**는 거의 모든 언어를 받아쓰고, **VALL-E**는 3초의 음성만으로 화자를 복제합니다. 음성은 키보드보다 빠르고 자연스러운 입력이며, 인간에 가까운 AI 인터페이스의 핵심입니다.

> **핵심 직관**: ASR과 TTS는 **역방향** 과제입니다. ASR은 음성→텍스트(인코딩), TTS는 텍스트→음성(디코딩)입니다. 하지만 둘 다 "언어의 소리 구조를 이해해야" 한다는 점에서 같은 지식을 공유합니다. 최근에는 하나의 모델이 ASR과 TTS를 동시에 수행하는 통합 접근이 등장하고 있습니다.

## 1. 음성 인식의 진화

```
음성 인식 (Automatic Speech Recognition, ASR):
  음성 → 텍스트

  전통적 파이프라인 (2010년대 이전):
  음성 → MFCC → 음향 모델(HMM-DNN) → 언어 모델 → 텍스트
  ├─ 음향 모델: 음성 프레임 → 음소
  ├─ 발음 사전: 음소 → 단어
  ├─ 언어 모델: 단어 시퀀스 확률
  └─ 디코딩: 빔 서치로 최적 문장
  → 각 구성요소를 따로 학습, 복잡한 파이프라인

  End-to-End ASR (2014~):
  음성 → 단일 신경망 → 텍스트

  CTC (Connectionist Temporal Classification):
  ├─ 입력: 음성 프레임 시퀀스
  ├─ 출력: 문자/토큰 시퀀스 (입력보다 짧음)
  ├─ 공백(blank) 토큰으로 정렬 문제 해결
  ├─ 모든 가능한 정렬의 합 = 목표 시퀀스 확률
  └─ DeepSpeech 2 (Baidu, 2015)

  Attention-based (Seq2Seq):
  ├─ Encoder: 음성 → 숨은 표현
  ├─ Decoder: Attention으로 텍스트 자동 회귀 생성
  ├─ LAS (Listen, Attend and Spell, 2015)
  └─ 정렬을 Attention이 자동 학습

  RNN-T (Recurrent Neural Transducer):
  ├─ CTC + Attention의 장점 결합
  ├─ 스트리밍 ASR에 적합 (실시간)
  ├─ Google의 온디바이스 ASR에 사용
  └─ 현재 산업용 ASR의 표준

  Conformer (Gulati et al., 2020):
  ├─ CNN + Transformer 결합
  ├─ CNN: 지역적 패턴 (음소)
  ├─ Transformer: 전역적 의존성 (문맥)
  ├─ Convolution → Self-Attention → FFN
  └─ ASR 인코더의 사실상 표준

  | 방식 | 정렬 | 스트리밍 | 정확도 |
  |------|------|---------|--------|
  | CTC | 암묵적 | 가능 | 좋음 |
  | Attention | 자동 | 어려움 | 매우 좋음 |
  | RNN-T | 암묵적 | 가능 | 매우 좋음 |
  | Conformer+RNN-T | - | 가능 | SOTA |
```

## 2. Whisper: 범용 음성 인식

```
Whisper (Radford et al., 2023, OpenAI):
  "거의 모든 언어, 거의 모든 상황"

  핵심 철학:
  복잡한 아키텍처보다 대규모 데이터!
  → CLIP이 시각에서 한 것을 음성에서

  데이터:
  ├─ 680,000시간의 인터넷 음성 데이터
  ├─ 99개 언어 (영어가 ~65%)
  ├─ 약한 지도학습: 기존 자막/전사를 라벨로
  │   → 완벽하지 않은 라벨 but 양이 압도적
  └─ 다양한 도메인: 팟캐스트, YouTube, 오디오북

  구조:
  간단한 Encoder-Decoder Transformer

  Encoder:
  ├─ 입력: 30초 Mel Spectrogram (80×3000)
  ├─ 2개 Conv 층으로 다운샘플
  ├─ Transformer Encoder (L개 층)
  └─ 출력: 인코딩된 오디오 표현 (1500 프레임)

  Decoder:
  ├─ Transformer Decoder (Autoregressive)
  ├─ 특수 토큰으로 과제 지정:
  │   <|startoftranscript|>
  │   <|ko|> (언어)
  │   <|transcribe|> or <|translate|> (과제)
  │   <|notimestamps|> or <|0.00|> (타임스탬프)
  └─ 텍스트 토큰을 하나씩 생성

  멀티태스크:
  하나의 모델이 여러 과제를 수행
  ├─ 음성 인식: 음성 → 같은 언어 텍스트
  ├─ 음성 번역: 음성 → 영어 텍스트
  ├─ 언어 감지: 음성 → 언어 식별
  ├─ 타임스탬프: 단어별 시간 정보
  └─ VAD: 음성 구간 감지

  모델 크기:
  | 모델 | 파라미터 | 영어 WER | 다국어 |
  |------|---------|---------|--------|
  | tiny | 39M | 7.6% | 보통 |
  | base | 74M | 5.0% | 좋음 |
  | small | 244M | 3.4% | 좋음 |
  | medium | 769M | 2.9% | 매우 좋음 |
  | large-v3 | 1.5B | 2.1% | 최고 |

  Whisper의 한계:
  ├─ 30초 단위 처리 (긴 오디오는 분할 필요)
  ├─ 환각: 반복 생성, 없는 내용 생성
  ├─ 실시간 처리 어려움 (인코더-디코더 구조)
  └─ 소수 언어는 여전히 부정확

  Whisper의 영향:
  ├─ 오픈소스로 공개 → 연구/산업 광범위 활용
  ├─ faster-whisper: CTranslate2로 4배 빠른 추론
  ├─ whisper.cpp: C++ 포팅, 모바일/엣지 실행
  └─ 음성 인식의 "민주화"
```

> **핵심 직관**: Whisper의 설계 철학은 CLIP, GPT와 동일합니다: **"복잡한 아키텍처보다 대규모 데이터와 간단한 모델"**. wav2vec 2.0이 자기지도학습의 정교함으로 성능을 올린 반면, Whisper는 68만 시간의 (불완전한) 라벨 데이터로 더 강력한 범용 모델을 만들었습니다. 특수 토큰으로 언어/과제를 지정하는 방식은 mm-03의 VLM 지시 튜닝과 같은 원리입니다.

## 3. 음성 합성 (TTS) 기초

```
Text-to-Speech (TTS):
  텍스트 → 자연스러운 음성

  전통적 파이프라인:
  텍스트 → 텍스트 분석 → 음소 → 운율 → 음향 모델 → 보코더 → 파형
  → 각 단계가 수작업 규칙 + 별도 모델

  현대 TTS의 2단계:
  1. 텍스트 → 중간 표현 (Mel Spectrogram)
  2. 중간 표현 → 파형 (보코더)

  Tacotron 2 (Shen et al., 2018, Google):
  ├─ Encoder: 텍스트 → 문자 임베딩 → Transformer/LSTM
  ├─ Attention: 문자와 Mel 프레임의 정렬
  ├─ Decoder: Autoregressive하게 Mel Spectrogram 생성
  └─ WaveNet 보코더: Mel → 파형

  FastSpeech 2 (Ren et al., 2021):
  ├─ Non-autoregressive: 병렬 생성 (빠름!)
  ├─ Duration Predictor: 각 음소의 길이 예측
  ├─ Pitch Predictor: 음높이 예측
  ├─ Energy Predictor: 에너지 예측
  └─ 실시간보다 빠른 합성 가능

  보코더 (Vocoder):
  Mel Spectrogram → 파형

  WaveNet (2016): Autoregressive, 고품질 but 매우 느림
  WaveGlow (2019): Flow 기반, 실시간 가능
  HiFi-GAN (2020): GAN 기반, 빠르고 고품질 → 현재 표준
  Vocos (2023): ConvNeXt 기반, 더 빠르고 간단

  | TTS 모델 | 생성 방식 | 속도 | 자연스러움 |
  |---------|----------|------|----------|
  | Tacotron 2 | AR | 느림 | 좋음 |
  | FastSpeech 2 | Non-AR | 빠름 | 좋음 |
  | VITS | E2E | 빠름 | 매우 좋음 |
```

## 4. VITS와 Neural Codec TTS

```
VITS (Kim et al., 2021):
  End-to-End TTS의 대표

  핵심:
  ├─ 텍스트 → 파형 직접 생성 (중간 Mel 불필요!)
  ├─ VAE + Flow + GAN 결합
  ├─ Posterior Encoder: 음성 → 잠재 표현 z
  ├─ Prior Encoder: 텍스트 → 잠재 분포
  ├─ Flow: Prior와 Posterior 연결
  ├─ Decoder: z → 파형 (HiFi-GAN 기반)
  └─ 단일 모델, End-to-End 학습

  장점:
  ├─ 보코더 별도 학습 불필요
  ├─ 자연스러운 음성 (MOS 4.3+)
  ├─ 다화자 지원 (화자 임베딩 추가)
  └─ 실시간보다 빠른 추론

VALL-E (Wang et al., 2023, Microsoft):
  "3초의 음성으로 화자 복제"

  패러다임 전환:
  TTS를 "언어 모델링"으로 재정의!
  → mm-06의 Neural Audio Codec(EnCodec) 활용

  구조:
  1. 텍스트 + 3초 프롬프트 음성
  2. 프롬프트 음성 → EnCodec → 이산 오디오 토큰
  3. AR 모델: 첫 번째 코드북의 토큰 생성
  4. NAR 모델: 나머지 코드북의 토큰 생성
  5. EnCodec Decoder: 토큰 → 파형

  핵심 아이디어:
  ├─ 음성을 이산 토큰으로 표현 (EnCodec)
  ├─ GPT처럼 "다음 토큰 예측"으로 음성 생성
  ├─ 프롬프트 음성의 토큰이 화자 정보를 제공
  └─ 3초 음성만으로 화자의 특성, 감정까지 복제!

  학습:
  ├─ LibriLight 60K시간의 영어 음성
  ├─ Zero-shot TTS: 학습 시 본 적 없는 화자도 복제
  └─ In-Context Learning: 프롬프트가 화자 정보 역할

  VALL-E 2 (2024):
  ├─ Repetition Aware Sampling
  ├─ Grouped Code Modeling
  ├─ 인간과 구분 불가능한 음성 품질 달성
  └─ Robustness 크게 개선

  | TTS 모델 | 접근 | 화자 복제 | 데이터 |
  |---------|------|----------|--------|
  | VITS | VAE+Flow+GAN | 미세조정 필요 | 수십 시간 |
  | VALL-E | Codec LM | 3초 프롬프트 | 60K 시간 |
  | VALL-E 2 | 개선 Codec LM | 3초 프롬프트 | 60K 시간 |
```

## 5. Voice Cloning과 음성 번역

```
Voice Cloning (음성 복제):

  VALL-E 방식:
  ├─ Zero-shot: 3초 프롬프트만으로
  ├─ 학습 시 화자별 미세조정 불필요
  └─ 누구의 목소리로든 텍스트를 읽을 수 있음

  XTTS (Coqui):
  ├─ 오픈소스 음성 복제
  ├─ GPT 기반 TTS
  ├─ 다국어 지원 (17개 언어)
  └─ 6초 프롬프트로 복제

  StyleTTS 2 (Li et al., 2023):
  ├─ Diffusion 기반 스타일 모델링
  ├─ 인간 수준의 자연스러움 (LJSpeech)
  └─ 스타일 제어 가능 (속도, 감정, 톤)

  윤리적 우려:
  ├─ 딥보이스: 음성으로 사기 (전화 사기)
  ├─ 동의 없는 음성 복제
  ├─ 가짜 뉴스 (유명인 음성 조작)
  └─ 워터마크와 감지 기술이 중요

음성 번역:

  Cascaded:
  음성(언어A) → ASR → 텍스트(A) → MT → 텍스트(B) → TTS → 음성(B)
  → 에러 전파, 지연 큼

  End-to-End:
  음성(언어A) → 단일 모델 → 음성(언어B)

  SeamlessM4T (Meta, 2023):
  ├─ 음성-음성, 음성-텍스트, 텍스트-음성, 텍스트-텍스트
  ├─ 100개 언어 지원
  ├─ 단일 모델로 모든 변환
  └─ Seamless Communication의 시작

  SeamlessStreaming:
  ├─ 실시간 스트리밍 번역
  ├─ EMMA (Efficient Monotonic Multihead Attention)
  ├─ 2초 이내 지연으로 실시간 번역
  └─ 화자의 감정/톤까지 보존

  | 방법 | 지연 | 품질 | 언어 |
  |------|------|------|------|
  | Cascaded | 높음 | 좋음 | 제한 없음 |
  | Whisper+MT+TTS | 중간 | 좋음 | 99개+ |
  | SeamlessM4T | 낮음 | 좋음 | 100개 |
  | SeamlessStreaming | 매우 낮음 | 좋음 | 100개 |
```

## 6. 음성 AI의 실전과 미래

```
실전 응용:

  음성 비서:
  ├─ Siri, Alexa, Google Assistant
  ├─ ASR + NLU + TTS 파이프라인
  ├─ GPT-4o: 음성-음성 직접 처리 (mm-10)
  └─ 더 자연스러운 대화형 AI

  접근성:
  ├─ 실시간 자막: 청각장애인 지원
  ├─ 음성 합성: 시각장애인을 위한 텍스트 읽기
  ├─ 다국어 번역: 언어 장벽 해소
  └─ 의사소통 보조 장치

  콘텐츠 제작:
  ├─ 오디오북: TTS로 자동 생성
  ├─ 팟캐스트: 음성 편집, 노이즈 제거
  ├─ 더빙: 영상의 다국어 더빙
  └─ 내레이션: 광고, 교육 콘텐츠

  의료:
  ├─ 진료 기록: 의사의 음성을 자동 전사
  ├─ 음성 바이오마커: 음성에서 질병 감지
  │   (파킨슨병, 우울증, COVID-19)
  └─ 원격 진료: 실시간 의료 통역

  미래 방향:
  ├─ 감정 제어 TTS: 특정 감정으로 합성
  ├─ 실시간 음성 변환: 라이브 Voice Cloning
  ├─ 음성-음성 직접 모델: ASR+TTS 통합
  │   → GPT-4o가 선구자 (mm-10)
  ├─ 초저지연: 10ms 이내 실시간 대화
  └─ 음성 에이전트: 전화 응대, 고객 서비스
```

## 핵심 정리

- 음성 인식은 HMM-DNN에서 **End-to-End**(CTC, Attention, RNN-T)로 진화했으며, **Conformer**(CNN+Transformer)가 인코더의 사실상 표준입니다
- **Whisper**는 68만 시간의 약한 지도학습 데이터로 99개 언어의 범용 ASR을 달성하며, 특수 토큰으로 인식/번역/타임스탬프를 멀티태스크 처리합니다
- 현대 TTS는 **VITS**(VAE+Flow+GAN End-to-End)와 **VALL-E**(Neural Codec + Language Model)로 나뉘며, VALL-E는 3초 프롬프트만으로 Zero-shot 화자 복제를 달성합니다
- **SeamlessM4T**는 100개 언어의 음성-음성/음성-텍스트 번역을 단일 모델로 수행하며, 스트리밍 버전은 2초 이내 지연의 실시간 번역을 제공합니다
- 음성 AI의 핵심 윤리 과제는 **딥보이스**(음성 복제 악용)이며, 워터마크와 감지 기술의 발전이 기술 발전과 함께 이루어져야 합니다
