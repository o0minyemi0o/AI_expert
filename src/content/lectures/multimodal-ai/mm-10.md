# 통합 멀티모달 아키텍처

## 왜 하나의 모델인가

mm-01~09에서 비전-언어, 오디오-언어, 비디오-언어를 **각각** 다뤘습니다. 하지만 인간은 텍스트, 이미지, 오디오, 비디오를 **동시에** 처리합니다. 영화를 볼 때 화면(시각)과 대사(오디오)와 자막(텍스트)을 통합하여 이해합니다. **통합 멀티모달 모델**은 **하나의 모델**로 모든 모달리티를 입력받고 출력합니다. **Gemini**와 **GPT-4o**가 이 방향의 선두이며, "범용 AI"를 향한 핵심 단계입니다.

> **핵심 직관**: 통합 모델의 가치는 **"모달리티 간 상호 강화"**입니다. 비디오를 볼 때 배경 음악이 슬프면 장면을 더 슬프게 해석하고, 대사의 톤이 화나면 표정을 더 주의 깊게 봅니다. 모달리티를 따로 처리하면 이런 **교차 모달리티 추론**이 불가능합니다. 하나의 모델이 모든 모달리티를 함께 처리할 때, "보면서 듣고 읽는" 인간의 이해에 가까워집니다.

## 1. 통합 아키텍처의 설계 원칙

```
Any-to-Any 모델의 목표:
  어떤 모달리티든 입력, 어떤 모달리티든 출력

  입력: 텍스트, 이미지, 오디오, 비디오 (조합)
  출력: 텍스트, 이미지, 오디오 (과제에 따라)

  설계 스펙트럼:

  1. 전문 모델 앙상블 (Toolbox):
     각 모달리티 전문 모델 + 라우터
     ├─ 이미지 질문 → VLM
     ├─ 음성 인식 → Whisper
     ├─ 이미지 생성 → DALL-E
     └─ 장점: 각 분야 최강 / 단점: 교차 추론 불가

  2. 공유 인코더 + 전문 디코더:
     하나의 인코더로 모든 모달리티 표현
     ├─ 텍스트/이미지/오디오 → 공유 Transformer
     ├─ 출력: 과제별 전문 디코더
     └─ 장점: 교차 추론 가능 / 단점: 생성 제한

  3. 완전 통합 (True Any-to-Any):
     하나의 Transformer가 모든 것
     ├─ 모든 모달리티를 토큰화
     ├─ 하나의 시퀀스로 결합
     ├─ Autoregressive 또는 Diffusion으로 생성
     └─ 장점: 완전한 교차 추론 / 단점: 학습 어려움

  토큰화 전략:

  | 모달리티 | 토큰화 | 예시 |
  |---------|--------|------|
  | 텍스트 | BPE/SentencePiece | 기존 LLM 토크나이저 |
  | 이미지 | ViT 패치 | 256~576 토큰 |
  | 오디오 | Mel+Encoder / Codec | 연속/이산 |
  | 비디오 | 프레임×패치 | 수천 토큰 |

  핵심 설계 결정:
  ├─ 별도 인코더 vs 공유 인코더
  ├─ 연속 표현 vs 이산 토큰
  ├─ 입력 전용 vs 입출력 모두
  └─ 학습 단계와 데이터 구성
```

## 2. Gemini: 네이티브 멀티모달

```
Gemini (Team et al., 2023, Google DeepMind):
  "처음부터 멀티모달로 학습된 모델"

  핵심 차별점:
  GPT-4V: LLM + 시각 모듈을 결합 (모듈러)
  Gemini: 처음부터 텍스트+이미지+오디오+비디오로 학습 (네이티브)

  구조:
  ├─ Transformer Decoder 기반 (GPT 스타일)
  ├─ 이미지: SigLIP 인코더의 토큰 사용
  ├─ 오디오: USM(Universal Speech Model) 특징
  ├─ 비디오: 프레임 토큰 시퀀스
  ├─ 모든 모달리티 토큰이 하나의 시퀀스
  └─ Autoregressive하게 다음 토큰 예측

  모델 크기:
  ├─ Gemini Ultra: 최대 모델
  ├─ Gemini Pro: 중간 (API 제공)
  ├─ Gemini Nano: 온디바이스 (모바일)
  └─ 정확한 파라미터 수 비공개

  긴 컨텍스트:
  ├─ 1M 토큰 컨텍스트 (Gemini 1.5 Pro)
  ├─ 1시간 비디오 처리 가능!
  ├─ 11시간 오디오 처리 가능
  ├─ 10K 페이지 문서 처리 가능
  └─ "In-Context Learning"의 극대화

  Gemini 1.5 Pro의 능력:
  ├─ 1시간 영화를 보고 질문에 답변
  ├─ 비디오의 특정 장면 시간 찾기
  ├─ 오디오+비디오 동시 이해
  │   "이 장면에서 무슨 소리가 나나?" 답변 가능
  ├─ 다국어 음성 이해
  └─ 이미지+텍스트 혼합 문서 분석

Gemini 2.0 (2024):
  ├─ 네이티브 이미지 생성 (Imagen 3 통합)
  ├─ 네이티브 TTS (음성 출력)
  ├─ Tool Use 강화 (코드 실행, 검색)
  ├─ "Agentic" 능력 강화
  └─ 입력+출력 모두 멀티모달

  | Gemini 버전 | 입력 | 출력 | 컨텍스트 |
  |-----------|------|------|---------|
  | 1.0 | T+I+A+V | T | 32K |
  | 1.5 Pro | T+I+A+V | T | 1M |
  | 2.0 Flash | T+I+A+V | T+I+A | 1M |
```

## 3. GPT-4o: 네이티브 음성 대화

```
GPT-4o (OpenAI, 2024):
  "omni" — 모든 모달리티를 네이티브로

  GPT-4V vs GPT-4o:
  GPT-4V: 텍스트 LLM + 비전 모듈 (파이프라인)
  GPT-4o: 텍스트+이미지+오디오를 단일 모델로

  음성 대화의 혁신:
  기존 (GPT-4 + Whisper + TTS):
  사용자 음성 → Whisper(ASR) → 텍스트
  → GPT-4 → 텍스트 응답
  → TTS → 음성 응답
  지연: 3-5초 (각 단계 누적)

  GPT-4o (End-to-End):
  사용자 음성 → GPT-4o → 음성 응답
  지연: 0.3초! (인간 대화 수준)

  왜 빨라졌는가:
  ├─ 3단계 파이프라인 → 1단계
  ├─ 중간 텍스트 변환 불필요
  ├─ 음성의 톤/감정을 직접 처리
  └─ 텍스트를 거치지 않으므로 정보 손실 없음

  네이티브 음성의 장점:
  ├─ 감정 인식: 사용자 음성의 감정을 직접 감지
  ├─ 감정 표현: 다양한 톤으로 응답
  ├─ 끊어 말하기: 자연스러운 턴 테이킹
  ├─ 비언어: "음...", "아!" 같은 추임새
  └─ 다국어: 중간 번역 없이 직접 이해/생성

  시각 능력:
  ├─ GPT-4V 수준의 이미지 이해
  ├─ 실시간 카메라 입력 (시연)
  │   → 칠판 수학 문제를 보고 풀기
  │   → 주변 환경 설명
  ├─ 문서, 차트, 코드 이미지 분석
  └─ 멀티모달 추론: 이미지+텍스트 동시 이해

GPT-4o의 한계:
  ├─ 음성 출력의 환각 (잘못된 정보를 자신있게)
  ├─ 긴 대화에서 일관성 저하
  ├─ 이미지 생성은 DALL-E에 위임 (네이티브 아님)
  ├─ 비디오 입력 제한적
  └─ 안전 필터: 특정 음성(유명인) 생성 방지
```

> **핵심 직관**: GPT-4o의 0.3초 음성 응답이 가능한 이유는 **"중간 텍스트를 거치지 않기"** 때문입니다. 기존에는 음성→텍스트→이해→텍스트→음성의 5단계였지만, GPT-4o는 음성→이해→음성의 3단계입니다. 더 중요한 것은, 텍스트로 변환할 때 **잃어버리는 정보**(톤, 감정, 속도, 강세)를 보존한다는 것입니다. "괜찮아"를 화난 톤으로 말하면 의미가 다르듯, 음성의 비언어적 정보가 핵심입니다.

## 4. Unified-IO와 범용 아키텍처

```
Unified-IO (Lu et al., 2023):
  "모든 과제를 하나의 프레임워크로"

  핵심 아이디어:
  모든 입출력을 토큰 시퀀스로 통일

  입력 토큰화:
  ├─ 텍스트: SentencePiece 토큰
  ├─ 이미지: VQ-GAN 이산 토큰
  ├─ 바운딩 박스: 좌표를 1000 bin으로 양자화
  ├─ 키포인트: 좌표 토큰
  └─ 모든 것이 이산 토큰 시퀀스!

  Unified-IO 2 (2024):
  ├─ 텍스트, 이미지, 오디오, 비디오, 행동
  ├─ 220개 이상의 과제
  ├─ 입출력 모두 토큰화
  ├─ Encoder-Decoder Transformer
  └─ 7B 파라미터로 범용 멀티모달

  수행 과제 예시:
  ├─ 이미지 → 캡션 (캡셔닝)
  ├─ 이미지 + 질문 → 답변 (VQA)
  ├─ 텍스트 → 이미지 토큰 → 이미지 (생성)
  ├─ 이미지 → 바운딩 박스 토큰 (탐지)
  ├─ 이미지 → 세그멘테이션 토큰 (분할)
  ├─ 오디오 → 텍스트 (ASR)
  └─ 비디오 → 캡션 (비디오 캡셔닝)

  → 모든 것이 "토큰 입력 → 토큰 출력"!

4M (Bachmann et al., 2024):
  ├─ 다양한 모달리티의 토큰화 + Transformer
  ├─ RGB, 깊이, 법선, 세그멘테이션, 텍스트 등
  ├─ Any-to-Any: 어떤 조합이든 처리
  └─ 멀티모달 = 멀티태스크의 극대화

CoDi (Tang et al., 2023):
  ├─ Composable Diffusion
  ├─ 어떤 모달리티든 → 어떤 모달리티든
  ├─ 각 모달리티의 Diffusion 모델 + 정렬
  ├─ 텍스트+오디오 → 비디오 등 조합 생성
  └─ Any-to-Any 생성의 초기 시도

  | 모델 | 접근 | 모달리티 | 입력 | 출력 |
  |------|------|---------|------|------|
  | Unified-IO 2 | 토큰화 | 5개 | Any | Any |
  | 4M | 토큰화 | 10개+ | Any | Any |
  | CoDi | Diffusion | 4개 | Any | Any |
  | Gemini | 네이티브 | 4개 | Any | T(+I+A) |
  | GPT-4o | 네이티브 | 3개 | Any | T+A |
```

## 5. 모달리티 확장의 미래

```
새로운 입력 모달리티:

  센서 데이터:
  ├─ LiDAR: 자율주행의 3D 포인트 클라우드
  ├─ IMU: 가속도/자이로스코프 (모바일, 로봇)
  ├─ EEG: 뇌파 신호 → 의도 해석
  └─ 촉각: 로봇의 접촉 감각 (mm-12)

  구조화 데이터:
  ├─ 테이블/스프레드시트
  ├─ 그래프/지식 그래프
  ├─ 코드: 프로그래밍 언어
  └─ 3D 장면: NeRF, Gaussian Splatting (cv-13)

  인터랙티브 데이터:
  ├─ GUI: 스크린샷 + 클릭/타이핑
  ├─ 웹: HTML/CSS + 상호작용
  ├─ 게임: 게임 화면 + 행동
  └─ 로봇: 시각 + 관절 상태 + 행동

새로운 출력 모달리티:

  행동 (Action):
  ├─ 로봇 명령: 관절 토크, 그리퍼 개폐
  ├─ GUI 조작: 클릭 위치, 입력 텍스트
  ├─ API 호출: 도구 사용
  └─ "멀티모달 에이전트"의 핵심

  3D:
  ├─ 3D 객체 생성
  ├─ 3D 장면 구성
  └─ cv-13의 DreamFusion + 텍스트 조건

  코드:
  ├─ 시각 입력 → 코드 생성
  ├─ UI 디자인 → HTML/CSS
  └─ 다이어그램 → 코드

통합의 확장성:

  현재 (2024):
  ├─ 텍스트 + 이미지 + 오디오 + 비디오 통합
  ├─ 1M 토큰 컨텍스트 (Gemini 1.5)
  └─ 네이티브 음성 대화 (GPT-4o)

  다음 단계:
  ├─ 행동 출력: 에이전트로서 세계와 상호작용
  ├─ 무한 컨텍스트: 평생의 경험 축적
  ├─ 실시간 스트리밍: 연속적 멀티모달 입력 처리
  └─ mm-12의 Embodied AI로 연결
```

## 6. 통합 모델의 도전과 트레이드오프

```
학습의 도전:

  데이터 불균형:
  ├─ 텍스트: 수조 토큰 (인터넷 전체)
  ├─ 이미지-텍스트 쌍: 수십억
  ├─ 비디오-텍스트 쌍: 수억
  ├─ 오디오-텍스트 쌍: 수천만
  └─ 모달리티별 데이터 양의 극심한 차이

  해결:
  ├─ 단계적 학습: 텍스트 → +이미지 → +오디오 → +비디오
  ├─ 데이터 증강: 모달리티 간 변환으로 데이터 생성
  ├─ 커리큘럼 학습: 쉬운 과제 → 어려운 과제
  └─ 합성 데이터: AI로 데이터 생성 (DALL-E 3의 캡션)

  모달리티 간 간섭:
  ├─ 하나의 모달리티 학습이 다른 것을 방해
  ├─ "텍스트 잘하면 이미지가 나빠짐"
  ├─ 해결: Mixture of Experts (MoE)
  │   → 모달리티별 전문 파라미터 + 공유 파라미터
  └─ Gemini가 MoE 사용 (추정)

  전문 모델 vs 통합 모델:
  | 측면 | 전문 모델 | 통합 모델 |
  |------|----------|----------|
  | 단일 과제 성능 | 최고 | 좋음 |
  | 교차 추론 | 불가 | 가능 |
  | 유지보수 | 복잡(N개 모델) | 간단(1개) |
  | 학습 비용 | 각각 적음 | 총합 매우 높음 |
  | 확장성 | 모달리티 추가 어려움 | 토큰 추가로 확장 |

  현재 트렌드:
  ├─ 이해: 통합 모델이 승리 (Gemini, GPT-4o)
  ├─ 생성: 전문 모델이 아직 우세 (DALL-E 3, Suno)
  ├─ 하지만 Gemini 2.0이 생성도 통합하는 추세
  └─ 장기적으로 통합 모델이 대세
```

## 핵심 정리

- 통합 멀티모달 아키텍처는 모든 모달리티를 **토큰 시퀀스로 통일**하여 하나의 Transformer로 처리하며, 전문 모델 앙상블 → 공유 인코더 → 완전 통합의 스펙트럼이 있습니다
- **Gemini**는 처음부터 텍스트/이미지/오디오/비디오로 네이티브 학습되었으며, 1.5 Pro의 **1M 토큰 컨텍스트**로 1시간 비디오를 처리하고, 2.0에서 이미지/오디오 생성까지 통합합니다
- **GPT-4o**는 음성을 네이티브로 처리하여 **0.3초 응답**을 달성하며, 중간 텍스트 변환 없이 톤/감정/비언어적 정보를 보존하는 것이 핵심 혁신입니다
- **Unified-IO 2**와 **4M**은 모든 입출력을 이산 토큰으로 변환하여 Any-to-Any 처리를 달성하며, 바운딩 박스/세그멘테이션까지 토큰으로 통일합니다
- 통합 모델의 핵심 도전은 **데이터 불균형**, **모달리티 간 간섭**, **학습 비용**이며, 단계적 학습과 MoE로 해결하는 방향이며, 이해는 통합 모델이, 생성은 아직 전문 모델이 우세합니다
