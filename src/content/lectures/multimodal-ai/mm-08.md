# 오디오-언어 모델

## 왜 오디오를 언어처럼 다루는가

mm-07에서 Whisper(ASR)와 VALL-E(TTS)를 다뤘습니다. 이들은 음성↔텍스트 변환에 특화되어 있습니다. 하지만 **"이 소리는 무엇인가?"**, **"슬픈 느낌의 피아노 음악을 만들어줘"**처럼 오디오를 자연어로 **이해하고 생성**하는 더 넓은 능력이 필요합니다. **AudioLM**은 오디오를 언어 모델링으로 생성하고, **MusicLM**은 텍스트로 음악을 만들며, **AudioPaLM**은 음성과 텍스트를 통합합니다.

> **핵심 직관**: mm-06의 Neural Audio Codec(EnCodec)이 오디오를 "토큰"으로 바꿔놓자, **LLM의 모든 기법이 오디오에 적용 가능**해졌습니다. 텍스트 생성이 "다음 단어 예측"이듯, 오디오 생성은 "다음 오디오 토큰 예측"입니다. Autoregressive, Transformer, In-Context Learning 등 언어 모델의 성공 패턴을 오디오에 그대로 가져오는 것이 이 분야의 핵심 아이디어입니다.

## 1. AudioLM: 오디오 언어 모델

```
AudioLM (Borsos et al., 2023, Google):
  오디오를 Language Model로 생성

  핵심 통찰:
  오디오 = 이산 토큰 시퀀스 → LM으로 생성!

  2종류의 토큰:

  1. Semantic Tokens (의미 토큰):
     w2v-BERT(wav2vec + BERT)에서 추출
     → K-means로 이산화 (HuBERT와 유사)
     → 음성의 내용(무엇을 말하는가)을 표현
     → 언어적 정보, 높은 수준의 구조

  2. Acoustic Tokens (음향 토큰):
     SoundStream(Neural Codec)에서 추출
     → RVQ의 각 코드북에서 토큰
     → 음성의 스타일(어떻게 말하는가)을 표현
     → 화자 특성, 음색, 환경음

  계층적 생성:
  Stage 1: Semantic 토큰 생성
  ├─ 고수준 구조 결정 (무슨 말? 어떤 멜로디?)
  └─ Transformer LM으로 자동 회귀 생성

  Stage 2: Coarse Acoustic 토큰 생성
  ├─ Semantic 토큰을 조건으로
  ├─ 대략적 음향 특성 생성
  └─ RVQ의 상위 코드북

  Stage 3: Fine Acoustic 토큰 생성
  ├─ Coarse 토큰을 조건으로
  ├─ 세부 음향 디테일 추가
  └─ RVQ의 하위 코드북

  → 계층적: 의미 → 대략적 소리 → 세부 소리
  → 각 단계가 더 세밀한 정보를 추가

  결과:
  ├─ 음성 프롬프트 → 자연스러운 음성 연속 생성
  ├─ 내용(텍스트)과 스타일(화자)을 모두 유지
  ├─ 피아노 음악의 자연스러운 연속 생성
  └─ 텍스트 조건 없이도 일관된 오디오 생성

  AudioLM의 의의:
  ├─ "오디오 생성 = 언어 모델링" 패러다임 확립
  ├─ Semantic + Acoustic 이중 표현의 효과 증명
  ├─ mm-07의 VALL-E에 직접 영향
  └─ 이후 MusicLM, SoundStorm 등의 기반
```

## 2. MusicLM: 텍스트→음악 생성

```
MusicLM (Agostinelli et al., 2023, Google):
  "텍스트로 음악을 만든다"

  핵심:
  AudioLM의 계층적 생성 + MuLan의 텍스트-음악 정렬

  MuLan:
  ├─ mm-02의 CLIP을 음악에 적용!
  ├─ 음악-텍스트 대조 학습
  ├─ 음악 오디오와 텍스트 설명을 같은 공간에
  └─ "A calm piano melody" ↔ 해당 음악

  MusicLM 구조:
  1. 텍스트 → MuLan 텍스트 임베딩
  2. MuLan 임베딩 → Semantic 토큰 생성 (AudioLM Stage 1)
  3. Semantic → Acoustic 토큰 생성 (AudioLM Stage 2-3)
  4. Acoustic 토큰 → SoundStream → 파형

  → 텍스트 설명이 음악의 "방향"을 안내
  → AudioLM이 그 방향으로 음악 생성

  특징:
  ├─ 24kHz, 분 단위 음악 생성
  ├─ 텍스트 설명의 의미를 잘 반영
  │   "A soothing jazz piece with saxophone"
  │   → 재즈 느낌, 색소폰 포함 음악
  ├─ 멜로디 조건: 허밍/휘파람으로 멜로디 지정 가능
  └─ 다양한 장르, 악기, 분위기 제어

MusicGen (Copet et al., 2023, Meta):
  ├─ EnCodec + Transformer LM
  ├─ 단일 LM으로 음악 생성 (계층적 분리 없이)
  ├─ Codebook Interleaving:
  │   여러 코드북의 토큰을 인터리빙하여 하나의 시퀀스로
  │   → 단일 패스로 모든 코드북 생성
  ├─ 텍스트 또는 멜로디 조건
  ├─ 오픈소스로 공개!
  └─ 1.5B 파라미터

  | 모델 | 조건 | 구조 | 공개 |
  |------|------|------|------|
  | MusicLM | 텍스트+멜로디 | 계층적 LM | 비공개 |
  | MusicGen | 텍스트+멜로디 | 단일 LM | 오픈소스 |
  | Stable Audio | 텍스트+길이 | Latent Diffusion | 부분 공개 |
```

## 3. 범용 오디오 생성

```
SoundStorm (Borsos et al., 2023, Google):
  AudioLM의 Acoustic 토큰 생성을 병렬화

  AudioLM의 문제:
  Acoustic 토큰을 한 개씩 생성 → 매우 느림
  1초 오디오 = 수백 개 토큰 → 수백 번 추론

  SoundStorm의 해결:
  ├─ MaskGIT 스타일 병렬 디코딩
  ├─ 처음: 모든 토큰이 마스크
  ├─ 각 스텝: 가장 확신 있는 토큰들을 채움
  ├─ 반복: 점점 더 많은 토큰 채우기
  └─ ~수십 스텝이면 완성 (수백 → 수십으로 단축)

  → AudioLM 대비 100배 빠른 생성!
  → 실시간 오디오 생성 가능

텍스트→오디오:

  AudioGen (Kreuk et al., 2023, Meta):
  ├─ 환경음/효과음 생성에 특화
  ├─ "Dogs barking in the rain" → 해당 소리 생성
  ├─ EnCodec 토큰 + Transformer LM
  └─ 텍스트 조건으로 다양한 소리 생성

  Make-An-Audio (Huang et al., 2023):
  ├─ Latent Diffusion 기반 오디오 생성
  ├─ Mel Spectrogram을 잠재 공간에서 생성
  └─ Stable Diffusion의 오디오 버전

  AudioLDM 2 (Liu et al., 2024):
  ├─ 음성, 음악, 효과음을 하나의 모델로
  ├─ Language of Audio (LOA) 표현
  │   → AudioMAE + GPT-2로 범용 오디오 표현
  ├─ Latent Diffusion + LOA 조건
  └─ 범용 오디오 생성의 통합

상용 서비스:

  Suno:
  ├─ 텍스트 → 노래 (보컬 + 반주) 생성
  ├─ 가사 + 스타일 지정 가능
  ├─ 2분 이상의 완성된 노래
  └─ 크리에이터 도구로 큰 인기

  Udio:
  ├─ 유사한 텍스트→음악 서비스
  ├─ 다양한 장르와 스타일
  └─ 음악 산업의 AI 논쟁 촉발

  ElevenLabs:
  ├─ 고품질 TTS + Voice Cloning
  ├─ 25개 언어 지원
  ├─ 감정 제어, 스타일 조절
  └─ 팟캐스트, 오디오북 제작에 활용
```

> **핵심 직관**: 오디오 생성에서 **Semantic/Acoustic 분리**는 매우 강력한 원칙입니다. 글을 쓸 때 "무엇을 말할까"(내용)를 먼저 정하고 "어떻게 쓸까"(문체)를 결정하듯, 오디오도 "무슨 소리인가"(semantic)를 먼저 결정하고 "어떤 음색인가"(acoustic)를 채웁니다. 이 분리 덕분에 같은 내용을 다른 화자로, 같은 멜로디를 다른 악기로 변환할 수 있습니다.

## 4. AudioPaLM과 음성-언어 통합

```
AudioPaLM (Rubenstein et al., 2023, Google):
  PaLM(LLM) + AudioLM의 통합

  핵심 아이디어:
  텍스트 토큰과 오디오 토큰을 하나의 LM에서 처리
  → 텍스트와 음성의 경계를 허문다!

  구조:
  ├─ PaLM (540B LLM)을 기반
  ├─ AudioLM의 오디오 토큰화 기법 통합
  ├─ 입력/출력이 텍스트 또는 오디오 토큰
  └─ 단일 모델로 다양한 조합 처리

  수행 가능 과제:
  ├─ ASR: 오디오 토큰 → 텍스트 토큰
  ├─ TTS: 텍스트 토큰 → 오디오 토큰
  ├─ S2ST: 음성(언어A) → 음성(언어B)
  ├─ AST: 음성(언어A) → 텍스트(언어B)
  └─ 모든 조합이 하나의 모델!

  음성 번역에서의 장점:
  ├─ 화자의 목소리를 보존하면서 번역
  ├─ 영어 화자의 음성 → 한국어 음성 (같은 목소리)
  └─ 기존 Cascaded 방식에서 불가능했던 것

  → mm-10의 GPT-4o와 Gemini의 선행 연구

SpeechGPT / SALMONN / Qwen-Audio:
  오픈소스 음성-언어 통합 모델

  Qwen-Audio (Bai et al., 2023):
  ├─ Whisper 인코더 + Qwen LLM
  ├─ 음성 + 오디오 + 음악 이해
  ├─ 지시에 따른 오디오 분석
  │   "이 음악의 장르는?" → "재즈"
  │   "화자의 감정은?" → "화가 나 있습니다"
  └─ Qwen2-Audio: 멀티턴 오디오 대화

  SALMONN (Tang et al., 2024):
  ├─ Whisper + BEATs → Q-Former → LLM
  ├─ 음성 + 비음성 오디오 모두 처리
  ├─ 오디오 캡셔닝, 질의응답
  └─ BLIP-2(mm-03) 아키텍처를 오디오에 적용

  | 모델 | 오디오 입력 | 텍스트 입력 | 오디오 출력 |
  |------|-----------|-----------|-----------|
  | Whisper | 음성 | - | - |
  | Qwen-Audio | 범용 | 질문 | - |
  | AudioPaLM | 음성 | 텍스트 | 음성 |
  | GPT-4o | 범용 | 텍스트 | 음성 |
```

## 5. 오디오 캡셔닝과 이해

```
오디오 캡셔닝:
  소리를 자연어로 설명 (mm-04의 이미지 캡셔닝의 오디오 버전)

  입력: 오디오 클립
  출력: "A dog barking followed by a car horn in the distance"

  데이터셋:
  ├─ AudioCaps: YouTube에서 추출, 46K 캡션
  ├─ Clotho: 다양한 환경음, 크라우드소싱 캡션
  └─ WavCaps: ChatGPT로 캡션 생성 (대규모)

  접근:
  ├─ Encoder-Decoder: 오디오 인코더 + 텍스트 디코더
  ├─ CLAP 기반: 오디오-텍스트 정렬로 검색
  └─ LLM 기반: Qwen-Audio처럼 LLM이 직접 설명

Sound Event Detection (SED):
  시간 구간별 소리 이벤트 검출

  0-2초: [새 울음] [바람 소리]
  2-5초: [자동차 경적] [사람 말소리]
  5-8초: [비 오는 소리]

  → 시간 축의 객체 탐지와 유사
  → 보안, 모니터링, 로봇 인식에 활용

음악 이해:
  ├─ 장르 분류: 클래식, 재즈, 록, 힙합
  ├─ 감정 분석: 슬픔, 기쁨, 평화
  ├─ 악기 인식: 피아노, 기타, 드럼
  ├─ 비트/BPM 추정: 리듬 분석
  └─ 음악 전사: 오디오 → 악보 (Music Transcription)

  MusicCaps (Google):
  ├─ 5.5K 음악 클립 + 전문가 캡션
  ├─ MusicLM 평가에 사용
  └─ "Upbeat electronic dance music with heavy bass"

오디오-시각 대응:
  소리와 이미지의 관계 이해
  ├─ "이 소리는 어떤 장면에서 날까?"
  ├─ 개 짖는 소리 → 공원 이미지
  ├─ 파도 소리 → 해변 이미지
  └─ mm-02의 ImageBind가 이 관계를 학습
```

## 6. 오디오-언어의 미래

```
현재 상태와 미래 방향:

  현재 달성:
  ├─ ASR: 인간 수준 (Whisper large-v3)
  ├─ TTS: 인간과 구분 불가 (VALL-E 2)
  ├─ 음악 생성: 2분 이상 일관된 음악 (Suno)
  ├─ 오디오 이해: 환경음+음성+음악 통합 (Qwen-Audio)
  └─ 음성 번역: 100개 언어 실시간 (SeamlessM4T)

  열린 문제:

  1. 긴 오디오 이해:
     현재: 30초~1분 단위 처리
     목표: 1시간 팟캐스트, 콘서트 전체 이해
     → 긴 시퀀스 처리의 근본 과제

  2. 음악의 의미 이해:
     "이 곡이 왜 슬프게 느껴지는가?"
     → 음악 이론 + 감정의 교차점
     → 현재 모델은 표면적 패턴 매칭

  3. 다화자 분리:
     여러 사람이 동시에 말할 때 분리
     → Cocktail Party Problem
     → 아직 실용적 해결 어려움

  4. 실시간 대화:
     자연스러운 대화: 끊김, 겹침, 추임새
     → GPT-4o가 시도하지만 아직 초기 단계
     → mm-10에서 상세히 다룸

  통합 방향:
  ├─ 단일 모델: 텍스트+이미지+오디오+비디오
  │   → GPT-4o, Gemini가 선도
  ├─ Any-to-Any: 어떤 모달리티든 입출력
  ├─ 실시간: 스트리밍 입력, 즉각 응답
  └─ mm-10의 통합 아키텍처로 연결
```

## 핵심 정리

- **AudioLM**은 Semantic(내용)+Acoustic(스타일) 이중 토큰화로 오디오를 언어 모델링하여, 계층적 생성(의미→대략적 소리→세부 소리)으로 자연스러운 오디오를 생성합니다
- **MusicLM**은 MuLan(음악-텍스트 CLIP)+AudioLM 구조로 텍스트 설명에서 음악을 생성하고, **MusicGen**은 단일 LM+Codebook Interleaving으로 오픈소스 대안을 제공합니다
- **AudioPaLM**은 PaLM에 오디오 토큰을 통합하여 ASR/TTS/음성번역을 단일 모델로 수행하며, 화자 목소리를 보존하면서 번역하는 것이 핵심 장점입니다
- **Qwen-Audio/SALMONN** 등 오디오-언어 모델은 VLM 아키텍처(Whisper 인코더+LLM)를 오디오에 적용하여, 음성+환경음+음악의 범용 이해를 달성합니다
- 오디오 AI의 핵심 원리는 **Neural Codec으로 오디오를 이산 토큰화**하여 LLM의 모든 기법(AR 생성, In-Context Learning, 멀티태스크)을 적용하는 것이며, 이것이 mm-10의 통합 모델로 이어집니다
