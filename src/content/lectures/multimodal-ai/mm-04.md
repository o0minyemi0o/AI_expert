# 시각 질의응답과 캡셔닝

## 왜 이미지에 대해 질문하는가

mm-03에서 VLM의 아키텍처를 다뤘습니다. 이제 VLM이 실제로 수행하는 **핵심 과제**를 살펴봅니다. **시각 질의응답(VQA)**은 이미지에 대한 질문에 답하는 것이고, **이미지 캡셔닝**은 이미지를 문장으로 설명하는 것입니다. 이 두 과제는 "이미지를 이해하는가"를 측정하는 가장 기본적인 벤치마크이며, VLM 발전의 원동력이었습니다.

> **핵심 직관**: VQA의 핵심 도전은 **"질문이 어디를 봐야 하는지 알려준다"**는 것입니다. "이 사진에서 가장 큰 물체는?"이라는 질문은 크기 비교를, "왼쪽 사람이 입은 옷 색깔은?"은 특정 위치와 속성을, "이 사람들은 행복해 보이나요?"는 감정 추론을 요구합니다. 같은 이미지에 대해 질문에 따라 **완전히 다른 시각적 추론**이 필요합니다.

## 1. VQA: 시각 질의응답

```
VQA의 정의:
  입력: 이미지 I + 질문 Q (자연어)
  출력: 답변 A (자연어)

  예시:
  이미지: 공원에서 프리스비를 잡는 개
  Q: "이 동물은 무엇인가요?" → A: "개"
  Q: "개가 무엇을 하고 있나요?" → A: "프리스비를 잡고 있습니다"
  Q: "배경은 어디인가요?" → A: "공원"
  Q: "날씨가 어떤가요?" → A: "맑은 날"

  질문 유형별 분류:
  ├─ 인식: "이것은 무엇인가요?" (객체 인식)
  ├─ 속성: "무슨 색인가요?" (속성 파악)
  ├─ 수량: "몇 개인가요?" (카운팅)
  ├─ 공간: "왼쪽에 무엇이 있나요?" (공간 관계)
  ├─ 행동: "무엇을 하고 있나요?" (활동 인식)
  ├─ 추론: "왜 웃고 있을까요?" (고차원 추론)
  └─ 상식: "이 음식이 매울까요?" (외부 지식)

  VQA 데이터셋 역사:

  VQA v1.0 (2015):
  ├─ 204K 이미지, 614K 질문
  ├─ 10명의 답변자 → 다수결 답변
  └─ 문제: 언어 편향 (이미지 안 보고도 답변 가능)

  VQA v2.0 (2017):
  ├─ 같은 질문에 다른 답을 가진 이미지 쌍 추가
  ├─ "이것은 무슨 색?" → 빨간 차 이미지 + 파란 차 이미지
  └─ 이미지를 반드시 봐야 답할 수 있도록 설계

  편향 문제:
  Q: "바나나는 무슨 색?" → 모델: "노란색" (항상)
  → 초록 바나나 이미지에서도 "노란색"이라 답함!
  → 이미지를 보지 않고 언어 통계로 답하는 문제

  VQA 평가 메트릭:
  Accuracy = min(# humans that said a / 3, 1)
  → 10명 중 3명 이상이 동의한 답변이면 정답

  | 데이터셋 | 이미지 | 질문 | 특징 |
  |---------|-------|------|------|
  | VQA v2 | 204K | 1.1M | 균형 쌍 |
  | GQA | 113K | 22M | 장면 그래프 기반 |
  | OK-VQA | 14K | 14K | 외부 지식 필요 |
  | TextVQA | 28K | 45K | 이미지 내 텍스트 |
  | DocVQA | 12K | 50K | 문서 이해 |
```

## 2. 고급 VQA: 지식, 텍스트, 문서

```
OK-VQA / A-OKVQA:
  외부 지식이 필요한 VQA

  Q: "이 건물은 어느 나라에 있나요?"
  → 이미지만으로는 불충분 → 에펠탑 지식 필요 → "프랑스"

  Q: "이 음식의 칼로리는 대략 얼마인가요?"
  → 음식 인식 + 영양 지식 필요

  → VLM의 LLM이 가진 세계 지식이 핵심!
  → GPT-4V가 특히 강한 영역

TextVQA:
  이미지 내 텍스트를 읽어야 하는 VQA

  Q: "이 간판에 뭐라고 써있나요?"
  → OCR 능력 필요
  → 이미지 속 텍스트 인식 + 질문 이해

  과거: OCR 모듈 별도 → 파이프라인 복잡
  현재: VLM이 직접 텍스트 인식 (End-to-End)

DocVQA:
  문서 이미지에서 정보 추출

  입력: 청구서, 영수증, 양식 이미지
  Q: "총 금액은 얼마인가요?"
  → 레이아웃 이해 + 텍스트 인식 + 관계 파악

  문서 이해 VLM:
  ├─ LayoutLMv3: 텍스트 + 레이아웃 + 이미지
  ├─ Donut: OCR 없이 문서 직접 이해
  ├─ Pix2Struct: 스크린샷 → 구조 파싱
  └─ 최신 VLM: GPT-4V, Qwen2-VL이 직접 문서 이해

ChartQA:
  차트/그래프에서 정보 추출

  Q: "2023년 매출은 전년 대비 몇 % 증가했나요?"
  → 차트 구조 인식 + 수치 추출 + 계산

  → mm-03의 고해상도 처리가 핵심
  → 작은 숫자, 범례, 축 라벨을 정확히 읽어야 함

  | VQA 유형 | 핵심 능력 | 어려움 | 대표 데이터 |
  |---------|----------|--------|-----------|
  | 일반 VQA | 시각 인식 | 중간 | VQA v2 |
  | 지식 VQA | 세계 지식 | 높음 | OK-VQA |
  | 텍스트 VQA | OCR | 중간 | TextVQA |
  | 문서 VQA | 레이아웃+OCR | 높음 | DocVQA |
  | 차트 VQA | 구조+수치 | 매우 높음 | ChartQA |
```

## 3. 이미지 캡셔닝

```
이미지 캡셔닝:
  이미지 → 자연어 설명

  입력: 해변에서 서핑하는 사람 사진
  출력: "A person is surfing on a wave at the beach"

  캡션의 수준:
  ├─ 간단: "A dog" (1~2단어)
  ├─ 기본: "A dog is running in the park" (1문장)
  ├─ 상세: "A golden retriever is running through..." (여러 문장)
  └─ Dense: 이미지의 모든 영역을 상세히 설명

  진화:

  Show and Tell (2015):
  ├─ CNN (이미지) + LSTM (텍스트 생성)
  ├─ CNN 마지막 특징 → LSTM 초기 상태
  ├─ LSTM이 한 단어씩 생성
  └─ 최초의 End-to-End 캡셔닝

  Show, Attend and Tell (2015):
  ├─ Attention 도입!
  ├─ 단어 생성 시 이미지의 관련 영역에 주목
  ├─ "dog" 생성 시 → 개 영역에 어텐션
  └─ 캡셔닝 + 시각적 설명 가능성

  현대 VLM 캡셔닝:
  ├─ BLIP-2, LLaVA 등이 직접 캡셔닝
  ├─ 더 자연스럽고 상세한 설명
  ├─ 지시에 따라 스타일 조절 가능
  │   "간단히 설명해" vs "매우 자세히 설명해"
  └─ 별도 캡셔닝 모델이 불필요해짐

  Dense Captioning:
  이미지의 모든 영역을 각각 설명

  이미지 영역 1: "왼쪽의 빨간 차"
  이미지 영역 2: "차 옆에 서있는 여성"
  이미지 영역 3: "배경의 높은 건물"
  → 객체 탐지 + 캡셔닝의 결합

캡셔닝 평가 메트릭:

  BLEU (Papineni et al., 2002):
  n-gram 정밀도 (기계번역에서 유래)
  → 짧은 캡션에 유리, 다양성 반영 못함

  CIDEr (Vedantam et al., 2015):
  TF-IDF 가중 n-gram 유사도
  → 캡셔닝에 특화, 가장 널리 사용

  METEOR, ROUGE-L:
  동의어, 어순 변형 고려

  문제: 자동 메트릭과 인간 판단의 불일치
  → CLIPScore: CLIP 유사도로 평가 (메트릭 프리!)
  → GPT-4 기반 평가: LLM이 직접 품질 판단
```

> **핵심 직관**: 캡셔닝의 "좋은 설명"은 **목적에 따라 다릅니다**. 시각 장애인을 위한 설명은 공간 배치와 행동을 상세히, 이미지 검색용 설명은 핵심 키워드를, 예술 비평용 설명은 분위기와 구도를 강조해야 합니다. 현대 VLM은 "이 이미지를 시각 장애인에게 설명해 주세요"처럼 **지시에 따라 스타일을 조절**할 수 있으며, 이것이 범용 VLM의 힘입니다.

## 4. Visual Grounding

```
Visual Grounding:
  텍스트가 가리키는 이미지 영역 찾기

  입력: 이미지 + "빨간 셔츠를 입은 남자"
  출력: 해당 영역의 Bounding Box [x, y, w, h]

  → VQA의 역방향: 답→위치 (VQA: 이미지→답)

  참조 표현 이해 (Referring Expression Comprehension):
  ├─ "왼쪽에 앉아있는 여성" → 바운딩 박스
  ├─ "가장 큰 빨간 물체" → 바운딩 박스
  ├─ "테이블 위의 두 번째 책" → 바운딩 박스
  └─ 자연어로 객체를 지칭 → 위치 찾기

  데이터셋:
  ├─ RefCOCO/RefCOCO+/RefCOCOg
  ├─ 다양한 난이도의 참조 표현
  └─ "the man" (쉬움) ~ "the second person from left" (어려움)

  전통적 접근:
  ├─ 2단계: 후보 영역 생성 → 텍스트와 매칭
  ├─ UNITER, MDETR 등
  └─ 복잡한 파이프라인

  VLM 기반 접근:
  최신 VLM이 직접 좌표 출력!

  방법 1: 텍스트로 좌표 생성
  Q: "빨간 차의 위치는?"
  A: "[0.12, 0.35, 0.58, 0.72]"
  → 좌표를 텍스트 토큰으로 생성

  방법 2: 특수 토큰
  <box>x1 y1 x2 y2</box>
  → Qwen-VL, Ferret 등이 사용

  Grounded Captioning:
  캡셔닝 + Grounding 동시에

  "A [dog](0.2, 0.3, 0.6, 0.8) is playing
   with a [ball](0.7, 0.6, 0.9, 0.8) in the park"

  → 각 명사구에 바운딩 박스 연결
  → 설명이 이미지의 어디를 말하는지 명확!

  Grounding DINO + SAM:
  cv-11에서 다룬 조합
  ├─ Grounding DINO: 텍스트 → 바운딩 박스
  ├─ SAM: 바운딩 박스 → 세그멘테이션 마스크
  └─ 텍스트로 아무 객체나 분할 가능!
```

## 5. 시각 추론

```
시각 추론 (Visual Reasoning):
  단순 인식을 넘어 추론이 필요한 과제

  수준별 시각 이해:
  Level 1 - 인식: "이것은 고양이다" (객체 인식)
  Level 2 - 속성: "회색 고양이가 소파에 앉아있다" (속성+관계)
  Level 3 - 추론: "고양이가 불안해 보인다" (감정 추론)
  Level 4 - 상식: "이 고양이는 아마 실내 고양이일 것이다" (추론)
  Level 5 - 복합: "이 사진은 반려동물 입양 광고에 적합하다" (종합)

  구성적 추론 (Compositional Reasoning):
  "빨간 공 위의 파란 상자" vs "파란 공 위의 빨간 상자"
  → 속성(색상)과 관계(위/아래)를 올바르게 조합
  → 현재 VLM의 약점 중 하나

  공간적 추론:
  "A의 왼쪽에 있는 B" → 공간 관계 이해
  "가장 가까운 물체" → 깊이 추정
  → 2D 이미지에서 3D 공간 관계 추론

  시간적 추론 (비디오):
  "남자가 문을 열기 전에 무엇을 했나?" → 시간 순서
  → mm-09의 비디오-언어 이해로 확장

  수학적 추론:
  "이 그래프에서 최대값은 언제인가?" → 차트 분석
  "두 물체 사이의 각도는 약 몇 도인가?" → 기하 추론

  시각 추론 벤치마크:
  | 벤치마크 | 초점 | 난이도 |
  |---------|------|--------|
  | GQA | 구성적 추론 | 중간 |
  | CLEVR | 3D 공간 추론 | 쉬움 |
  | Winoground | 구성적 이해 | 어려움 |
  | MMMU | 대학 수준 추론 | 매우 어려움 |
  | MathVista | 수학+시각 | 어려움 |
```

## 6. 실전 응용과 한계

```
실전 응용:

  접근성:
  ├─ 시각 장애인을 위한 이미지 설명
  ├─ Be My Eyes + GPT-4V: 실시간 시각 보조
  ├─ 장면 설명, 텍스트 읽기, 내비게이션 도움
  └─ 가장 직접적인 사회적 영향

  전자상거래:
  ├─ 상품 이미지 → 자동 설명 생성
  ├─ "이 옷과 어울리는 조합은?"
  ├─ 시각적 유사 상품 검색
  └─ 리뷰 이미지 분석

  교육:
  ├─ 교과서 이미지에 대한 질문-답변
  ├─ 수학 문제 이미지 풀이
  ├─ 과학 실험 사진 분석
  └─ 다이어그램 설명

  의료:
  ├─ X-ray/CT 이미지 판독 보조
  ├─ 병리 슬라이드 분석
  ├─ 의료 보고서 자동 생성
  └─ 주의: 진단 보조, 최종 판단은 의사

  현재 한계:

  환각 (Hallucination):
  ├─ 이미지에 없는 내용을 자신있게 생성
  ├─ "이 이미지에 고양이가 있나요?" → "네" (없는데)
  ├─ 텍스트 생성의 유창함이 정확성을 압도
  └─ POPE 벤치마크로 측정

  카운팅:
  ├─ "몇 개?" 질문에 부정확
  ├─ 특히 많은 수 (>5)에서 실패
  └─ 시각적 카운팅은 여전히 어려운 문제

  세밀한 구분:
  ├─ 유사한 조류/곤충 종 구분 어려움
  ├─ 미세한 시각적 차이 포착 부족
  └─ 전문가 수준의 세밀한 인식은 한계

  공간 관계:
  ├─ "왼쪽/오른쪽", "위/아래" 혼동
  ├─ 이미지 좌표와 자연어 방향의 불일치
  └─ 개선 중이지만 여전히 약점
```

## 핵심 정리

- **VQA**는 이미지에 대한 자연어 질문에 답하는 과제로, 인식/속성/수량/공간/추론 등 다양한 유형이 있으며 **언어 편향**(이미지 안 보고 답하기)이 핵심 도전입니다
- VQA는 일반(VQA v2)에서 **지식(OK-VQA)**, **텍스트(TextVQA)**, **문서(DocVQA)**, **차트(ChartQA)**로 확장되며, 각각 세계 지식, OCR, 레이아웃, 수치 추론 능력을 요구합니다
- **이미지 캡셔닝**은 Show&Tell(CNN+LSTM)에서 현대 VLM의 지시 기반 설명 생성으로 진화했으며, CIDEr/CLIPScore/GPT-4 평가가 사용됩니다
- **Visual Grounding**은 텍스트가 가리키는 영역을 찾는 과제로, 최신 VLM은 좌표를 텍스트 토큰으로 직접 생성하며, Grounding DINO+SAM과 결합하여 텍스트 기반 분할이 가능합니다
- VLM의 현재 한계는 **환각**(이미지에 없는 내용 생성), **카운팅**, **세밀한 구분**, **공간 관계** 이해이며, 접근성/전자상거래/교육/의료에서 실전 응용이 확대되고 있습니다
