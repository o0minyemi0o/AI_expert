# 멀티모달 AI 개론

## 왜 여러 감각을 결합하는가

인간은 세상을 **하나의 감각**으로 이해하지 않습니다. 강의를 들을 때 교수의 **목소리**(오디오), **슬라이드**(시각), **말의 내용**(텍스트)을 동시에 처리합니다. 맛있는 음식은 **맛**(미각), **향**(후각), **모양**(시각)이 함께 느껴집니다. **멀티모달 AI**는 텍스트, 이미지, 오디오, 비디오 등 여러 **모달리티(modality)**를 결합하여 이해하고 생성하는 AI입니다. 단일 모달리티의 한계를 넘어, 인간처럼 풍부한 세계 이해를 목표로 합니다.

> **핵심 직관**: 멀티모달의 가치는 **"1+1 > 2"**입니다. 텍스트 "bank"는 은행인지 강둑인지 모호하지만, 강 사진과 함께 보면 즉시 강둑입니다. 의료 진단에서 X-ray 이미지만으로는 불확실하지만, 환자의 증상 설명(텍스트)과 함께 보면 정확도가 올라갑니다. 각 모달리티는 다른 모달리티의 **모호성을 해소**합니다.

## 1. 모달리티란 무엇인가

```
모달리티의 정의:
  정보를 표현하는 특정 형태/채널

  주요 모달리티:
  ├─ 텍스트: 단어, 문장, 문서 (이산적, 순차적)
  ├─ 이미지: 2D 픽셀 배열 (연속적, 공간적)
  ├─ 오디오: 파형, 스펙트럼 (연속적, 시간적)
  ├─ 비디오: 이미지 시퀀스 (공간+시간적)
  ├─ 3D: 포인트 클라우드, 메쉬 (공간적)
  ├─ 센서: IMU, LiDAR, 터치 (도메인 특화)
  └─ 구조화 데이터: 테이블, 그래프

  각 모달리티의 특성:

  | 모달리티 | 차원 | 시간성 | 밀도 | 모호성 |
  |---------|------|--------|------|--------|
  | 텍스트 | 1D | 순차 | 희소 | 높음 |
  | 이미지 | 2D | 없음 | 밀집 | 중간 |
  | 오디오 | 1D | 연속 | 밀집 | 높음 |
  | 비디오 | 3D | 연속 | 매우 밀집 | 중간 |

  모달리티 간 정보 차이:

  텍스트: "빨간 차가 빠르게 달린다"
  → 색상, 객체, 동작을 명시적으로 기술
  → 하지만 정확한 모양, 배경은 모호

  이미지: 빨간 차 사진
  → 모양, 배경, 조명이 정확
  → 하지만 "빠르게 달린다"는 정보 없음

  비디오: 달리는 차 영상
  → 모양 + 동작 모두 포함
  → 하지만 설명/의도는 없음

  → 모달리티마다 다른 정보를 담음
  → 결합하면 더 완전한 이해!
```

## 2. 멀티모달 학습의 핵심 과제

```
5가지 핵심 과제 (Baltrusaitis et al., 2019):

  1. Representation (표현):
     각 모달리티의 데이터를 어떻게 표현하고 결합할 것인가?
     ├─ 텍스트: 토큰 임베딩 (512~4096차원)
     ├─ 이미지: 패치/픽셀 임베딩
     ├─ 오디오: 스펙트로그램 임베딩
     └─ 문제: 차원, 분포, 의미 체계가 모두 다름!

  2. Alignment (정렬):
     모달리티 간 대응 관계 학습
     ├─ 이미지의 "고양이" 영역 ↔ 텍스트의 "cat"
     ├─ 오디오의 "야옹" 구간 ↔ 비디오의 고양이 장면
     └─ 대응이 1:1이 아닐 수 있음 (1:N, N:M)

  3. Fusion (융합):
     여러 모달리티의 정보를 합치는 방법
     → 다음 섹션에서 상세히!

  4. Translation (변환):
     한 모달리티를 다른 모달리티로 변환
     ├─ 텍스트 → 이미지 (DALL-E, Stable Diffusion)
     ├─ 이미지 → 텍스트 (Captioning)
     ├─ 텍스트 → 오디오 (TTS)
     └─ 생성형 AI의 핵심!

  5. Co-learning (공동 학습):
     한 모달리티의 지식으로 다른 모달리티 학습 도움
     ├─ Zero-shot: 텍스트 설명만으로 이미지 분류
     ├─ Transfer: 이미지 사전학습 → 비디오에 전이
     └─ 데이터가 부족한 모달리티에 특히 유용

  → 이 5가지가 멀티모달 AI의 전체 문제 공간
  → 각 과제가 독립적이 아니라 서로 얽혀 있음
```

## 3. 융합 전략: Early, Late, Cross

```
융합(Fusion)의 세 가지 전략:

  1. Early Fusion (초기 융합):
     입력 단계에서 모달리티를 결합

     텍스트 토큰: [CLS] the cat sits
     이미지 패치: [IMG] p1 p2 p3 p4
     → 연결: [CLS] the cat sits [IMG] p1 p2 p3 p4
     → 하나의 Transformer로 처리!

     장점:
     ├─ 모달리티 간 상호작용이 모든 층에서 발생
     ├─ 미세한 관계 포착 가능
     └─ 구현이 단순 (하나의 모델)

     단점:
     ├─ 시퀀스 길이 폭발 (이미지 패치가 매우 많음)
     ├─ 사전학습된 단일 모달 모델 활용 어려움
     └─ 모달리티 간 간섭 가능

     예시: VisualBERT, Unified-IO, Fuyu

  2. Late Fusion (후기 융합):
     각 모달리티를 독립 처리 후 결과만 결합

     텍스트 → 텍스트 인코더 → 텍스트 특징 ─┐
                                            ├→ 결합 → 예측
     이미지 → 이미지 인코더 → 이미지 특징 ─┘

     결합 방법:
     ├─ Concatenation: [f_text; f_image]
     ├─ 평균/최대 풀링: (f_text + f_image) / 2
     ├─ Attention: f_text가 f_image에 어텐션
     └─ MLP: MLP([f_text; f_image])

     장점:
     ├─ 각 모달리티에 최적 인코더 사용 가능
     ├─ 사전학습 모델 활용 용이
     └─ 모달리티 하나가 없어도 동작 (유연성)

     단점:
     ├─ 모달리티 간 상호작용이 제한적
     └─ 미세한 관계 (단어↔영역) 포착 어려움

     예시: CLIP, 대부분의 검색 시스템

  3. Cross-Modal Fusion (교차 융합):
     중간 층에서 모달리티가 서로 참조

     텍스트 → 텍스트 인코더 ─→ Cross-Attention ─→ 텍스트 특징'
                               ↑
     이미지 → 이미지 인코더 ─→ Cross-Attention ─→ 이미지 특징'

     Cross-Attention:
     Q = 텍스트, K/V = 이미지
     → 텍스트의 각 단어가 이미지의 관련 영역에 주목
     → "cat"이 고양이 영역에 어텐션

     장점:
     ├─ 모달리티 간 세밀한 상호작용
     ├─ 사전학습 인코더 활용 + 상호작용
     └─ 현재 가장 인기 있는 접근

     단점:
     ├─ Cross-Attention 계산 비용
     └─ 설계 복잡성 증가

     예시: Flamingo, BLIP-2, LLaVA

  | 전략 | 상호작용 | 유연성 | 복잡도 | 대표 모델 |
  |------|---------|--------|--------|----------|
  | Early | 전체 층 | 낮음 | 낮음 | Unified-IO |
  | Late | 마지막만 | 높음 | 낮음 | CLIP |
  | Cross | 중간 층 | 중간 | 중간 | Flamingo |
```

> **핵심 직관**: 융합 전략은 **"언제 대화를 시작하는가"**로 이해할 수 있습니다. Early Fusion은 처음부터 모든 정보를 한 방에 넣고 대화시키는 것, Late Fusion은 각자 결론을 내린 후 마지막에 투표하는 것, Cross Fusion은 중간중간 서로 의견을 교환하는 것입니다. 현재 가장 성공적인 모델들(Flamingo, GPT-4V)은 **Cross Fusion**을 사용합니다.

## 4. 멀티모달 AI의 역사

```
멀티모달 연구의 흐름:

  1세대: 수작업 특징 (2000년대):
  ├─ 텍스트: Bag of Words, TF-IDF
  ├─ 이미지: SIFT, HOG
  ├─ 오디오: MFCC
  ├─ 융합: SVM, CRF로 결합
  └─ 과제: 감정 인식, 비디오 검색

  2세대: 딥러닝 (2014~2019):
  ├─ CNN + LSTM 결합
  ├─ VQA (Visual Question Answering, 2015)
  │   → 이미지 + 질문 → 답변
  ├─ Image Captioning (2015)
  │   → CNN 특징 → LSTM으로 문장 생성
  ├─ Visual Grounding (2016)
  │   → 텍스트가 이미지의 특정 영역을 가리킴
  └─ Attention이 핵심 메커니즘으로 부상

  3세대: Transformer 기반 (2019~2022):
  ├─ ViLBERT (2019): 두 개의 BERT 스트림 + Co-Attention
  ├─ CLIP (2021): 대조 학습으로 이미지-텍스트 정렬
  │   → cv-11에서 상세히 다룸
  ├─ DALL-E (2021): 텍스트 → 이미지 생성
  ├─ Flamingo (2022): Few-shot 멀티모달 학습
  └─ 대규모 웹 데이터 활용이 핵심 전환

  4세대: 대형 멀티모달 모델 (2023~):
  ├─ GPT-4V (2023): 텍스트 + 이미지 이해
  ├─ Gemini (2023): 텍스트/이미지/오디오/비디오 통합
  ├─ GPT-4o (2024): 음성 입출력까지 네이티브
  ├─ 단일 모델로 모든 모달리티 처리
  └─ "범용 멀티모달 에이전트"를 향해

  스케일의 변화:
  | 세대 | 데이터 | 파라미터 | 대표 모델 |
  |------|-------|---------|----------|
  | 1세대 | 수천 | 수만 | SVM |
  | 2세대 | 수십만 | 수백만 | Show&Tell |
  | 3세대 | 수억 | 수십억 | CLIP |
  | 4세대 | 수조 토큰 | 수천억 | Gemini |
```

## 5. 멀티모달의 도전

```
핵심 도전 과제:

  1. 모달리티 격차 (Modality Gap):
     같은 의미여도 모달리티마다 표현이 매우 다름
     ├─ 텍스트 "고양이": 2토큰, 이산적
     ├─ 이미지 "고양이": 수만 픽셀, 연속적
     ├─ 소리 "야옹": 수천 프레임, 시간적
     └─ 이 격차를 어떻게 연결할 것인가?

  2. 데이터 정렬의 어려움:
     모달리티 간 대응이 불완전
     ├─ 웹 이미지-텍스트 쌍: 텍스트가 이미지의 일부만 설명
     ├─ "A dog in the park" → 개, 잔디, 벤치, 하늘 중 뭘 설명?
     ├─ 노이즈 많은 대규모 데이터 vs 정교한 소규모 데이터
     └─ CLIP의 성공: 노이즈를 양으로 극복

  3. 모달리티 불균형:
     학습 시 한 모달리티가 지배적
     ├─ 텍스트가 너무 강하면 이미지를 무시
     ├─ 이미지가 쉬우면 오디오를 활용 안 함
     ├─ "Lazy modality" 문제
     └─ 해결: 그래디언트 조절, 모달리티 드롭아웃

  4. 계산 비용:
     모달리티가 많을수록 비용 급증
     ├─ 이미지: 256 패치 (ViT-L 기준)
     ├─ 비디오: 256 패치 × T 프레임
     ├─ 오디오: 수천 프레임
     ├─ 시퀀스 길이: 수만 토큰
     └─ Attention: O(n²) → 매우 비쌈

  5. 환각 (Hallucination):
     이미지에 없는 내용을 생성
     ├─ 이미지: 개 한 마리
     ├─ 모델 출력: "두 마리의 개가 공놀이"
     ├─ 텍스트 지식이 시각 정보를 덮어씀
     └─ mm-11에서 평가 방법 상세히 다룸
```

## 6. 멀티모달 AI의 응용

```
현재 주요 응용:

  시각 이해:
  ├─ 이미지/비디오 설명 생성
  ├─ 시각 질의응답 (VQA)
  ├─ 문서 이해 (OCR + 레이아웃 + 텍스트)
  └─ 의료 이미지 분석 + 보고서 생성

  콘텐츠 생성:
  ├─ 텍스트 → 이미지 (DALL-E, Midjourney)
  ├─ 텍스트 → 비디오 (Sora, Runway)
  ├─ 텍스트 → 음악 (MusicLM, Suno)
  └─ 이미지 → 3D (DreamFusion)

  멀티모달 대화:
  ├─ GPT-4V/4o: 이미지를 보고 대화
  ├─ Gemini: 비디오를 보고 질문에 답변
  ├─ 음성 어시스턴트: 음성 + 화면 이해
  └─ "보고, 듣고, 말하는" AI

  검색:
  ├─ 텍스트로 이미지 검색 (CLIP 기반)
  ├─ 이미지로 유사 이미지 검색
  ├─ 멀티모달 RAG: 텍스트 + 이미지 + 표
  └─ 교차 모달리티 검색

  코스 로드맵:
  mm-01: 개론 (지금)
  mm-02~03: 비전-언어 기초 (CLIP, VLM)
  mm-04~05: 시각 이해/생성
  mm-06~08: 오디오 모달리티
  mm-09: 비디오-언어
  mm-10: 통합 아키텍처 (Gemini, GPT-4o)
  mm-11: 평가와 벤치마크
  mm-12: 미래 방향 (Embodied AI)
```

## 핵심 정리

- **멀티모달 AI**는 텍스트, 이미지, 오디오, 비디오 등 여러 모달리티를 결합하여 "1+1 > 2"의 시너지를 만들며, 각 모달리티는 다른 모달리티의 **모호성을 해소**합니다
- 멀티모달 학습의 5대 과제는 **표현, 정렬, 융합, 변환, 공동 학습**이며, 이들이 서로 얽혀 전체 문제 공간을 형성합니다
- **Early Fusion**은 입력부터, **Late Fusion**은 출력에서, **Cross-Modal Fusion**은 중간 층에서 모달리티를 결합하며, 현재 Cross Fusion(Cross-Attention)이 가장 성공적입니다
- CLIP(2021)의 대조 학습과 Flamingo(2022)의 Few-shot 능력을 거쳐, **GPT-4V/Gemini**(2023~)가 단일 모델로 모든 모달리티를 처리하는 시대가 열렸습니다
- 핵심 도전은 **모달리티 격차**, **정렬 노이즈**, **모달리티 불균형**, **계산 비용**, **환각**이며, 스케일과 아키텍처 혁신으로 점진적으로 해결되고 있습니다
