# 비전-언어 모델 아키텍처

## 왜 VLM이 필요한가

mm-02의 CLIP은 이미지와 텍스트를 **같은 공간에 매핑**하여 검색과 분류를 가능하게 했습니다. 하지만 "이 이미지에서 무슨 일이 일어나고 있나요?"라는 질문에 **자유로운 문장**으로 답하지는 못합니다. **비전-언어 모델(VLM)**은 이미지를 **이해**하고 텍스트를 **생성**합니다. CLIP이 "매칭"이라면, VLM은 "대화"입니다. Flamingo에서 시작하여 LLaVA, GPT-4V까지, VLM은 이미지를 보고 생각하고 말하는 AI의 핵심입니다.

> **핵심 직관**: VLM의 핵심 설계 질문은 **"LLM에 시각을 어떻게 연결하는가"**입니다. LLM은 이미 강력한 언어 능력을 가지고 있으므로, 시각 정보를 LLM이 이해할 수 있는 형태로 "번역"해 주면 됩니다. 이 "시각→언어 번역기"를 어떻게 설계하느냐가 VLM 아키텍처의 핵심이며, 크게 세 가지 접근이 있습니다: Cross-Attention(Flamingo), Projection(LLaVA), Abstraction(BLIP-2).

## 1. VLM의 기본 구조

```
VLM의 세 가지 구성 요소:

  1. Vision Encoder: 이미지 → 시각 특징
     ├─ CLIP ViT, SigLIP ViT (사전학습된)
     ├─ 출력: 패치 토큰 시퀀스
     │   ViT-L/14, 224px: 256개 패치 토큰
     │   ViT-L/14, 336px: 576개 패치 토큰
     └─ 보통 고정(freeze)하거나 약하게 학습

  2. Connector: 시각 특징 → LLM 입력 형태로 변환
     ├─ 이것이 VLM 아키텍처의 핵심 차이!
     ├─ Linear Projection (LLaVA)
     ├─ Q-Former (BLIP-2)
     ├─ Cross-Attention (Flamingo)
     └─ Perceiver Resampler (Flamingo)

  3. LLM: 텍스트 이해 + 생성
     ├─ Vicuna, LLaMA, Mistral 등
     ├─ 시각 토큰 + 텍스트 토큰을 함께 처리
     └─ Autoregressive하게 답변 생성

  추론 흐름:
  이미지 → Vision Encoder → 시각 토큰들
  시각 토큰 + 질문 텍스트 → LLM → 답변 생성

  "이 사진에 뭐가 있어?"
  → [이미지 토큰 256개] + "이 사진에 뭐가 있어?"
  → LLM: "이 사진에는 해변에서 놀고 있는 강아지가..."

  핵심 설계 결정:
  ├─ Vision Encoder: 어떤 모델? 해상도? 고정 여부?
  ├─ Connector: 어떤 방식? 토큰 수 압축?
  ├─ LLM: 어떤 모델? 크기?
  └─ 학습: 무엇을 학습? 무엇을 고정?
```

## 2. Flamingo: Cross-Attention 접근

```
Flamingo (Alayrac et al., 2022, DeepMind):
  Few-shot 멀티모달 학습의 선구자

  구조:
  ├─ Vision Encoder: NFNet-F6 (고정)
  ├─ Connector: Perceiver Resampler
  ├─ LLM: Chinchilla (70B, 고정)
  └─ 추가: Gated Cross-Attention 층

  Perceiver Resampler:
  ├─ 가변 수의 시각 토큰 → 고정 수로 압축
  ├─ 학습 가능한 쿼리 64개
  ├─ Cross-Attention: 쿼리가 시각 토큰에 어텐션
  └─ 출력: 64개의 시각 요약 토큰
  → 해상도가 달라도 항상 64개 토큰!

  Gated Cross-Attention:
  LLM의 기존 층 사이에 삽입

  원래 LLM 층: Self-Attention → FFN
  Flamingo:     Self-Attention → Gated XAttn → FFN

  Gated XAttn:
  ├─ Q = 텍스트 토큰, K/V = 시각 토큰
  ├─ tanh(α) × CrossAttn(text, visual)
  ├─ α 초기값 = 0 → 처음에 XAttn 출력 = 0
  └─ → 학습 시작 시 원래 LLM처럼 동작!

  왜 Gate가 중요한가:
  LLM은 이미 잘 학습됨
  → 갑자기 시각 정보를 넣으면 LLM이 망가질 수 있음
  → Gate로 시각 정보를 서서히 도입
  → 사전학습된 LLM 능력 보존!

  Few-shot 능력:
  Flamingo는 여러 이미지-텍스트 예시를 컨텍스트로
  ├─ 예시 1: [이미지1] "이것은 고양이입니다"
  ├─ 예시 2: [이미지2] "이것은 강아지입니다"
  ├─ 질문: [이미지3] "이것은 무엇입니까?"
  └─ → "이것은 토끼입니다" (Few-shot!)

  결과:
  ├─ 16개 벤치마크 중 6개에서 SOTA
  ├─ Few-shot으로 미세조정 모델과 경쟁
  └─ 80B 파라미터 (LLM 70B + 10B 추가)
```

## 3. BLIP-2: Q-Former 접근

```
BLIP-2 (Li et al., 2023, Salesforce):
  "효율적인 비전-언어 사전학습"

  핵심 아이디어:
  사전학습된 Vision Encoder와 LLM을 모두 고정하고,
  그 사이의 경량 브릿지(Q-Former)만 학습!

  Q-Former (Querying Transformer):
  ├─ 학습 가능한 쿼리 32개
  ├─ 각 쿼리가 이미지에서 관련 정보를 추출
  ├─ Self-Attention (쿼리 간) + Cross-Attention (쿼리→이미지)
  └─ 출력: 32개 토큰 (768차원)

  2단계 학습:

  Stage 1: Vision-Language Representation Learning
  Q-Former를 이미지-텍스트 정렬에 학습
  ├─ ITC (Image-Text Contrastive): CLIP과 유사
  ├─ ITM (Image-Text Matching): 매칭 여부 이진 분류
  ├─ ITG (Image-grounded Text Generation): 이미지 조건 생성
  └─ Vision Encoder 고정, Q-Former만 학습

  Stage 2: Vision-to-Language Generative Learning
  Q-Former 출력을 LLM에 연결
  ├─ Q-Former 출력 → Linear → LLM 입력 공간
  ├─ LLM은 고정! Q-Former + Linear만 학습
  └─ Q-Former가 "시각 → 언어" 번역기 역할

  효율성:
  ├─ 학습 파라미터: 188M (Q-Former + Linear)
  ├─ Vision Encoder (ViT-G): 1B (고정)
  ├─ LLM (FlanT5-XXL): 11B (고정)
  └─ 전체 12B 중 188M만 학습! (1.6%)

  결과:
  ├─ VQA, Captioning에서 강력한 성능
  ├─ Flamingo보다 적은 학습 비용
  ├─ 다양한 LLM(OPT, FlanT5)에 적용 가능
  └─ InstructBLIP: 지시 튜닝으로 추가 개선

  BLIP-2 vs Flamingo:
  | 속성 | Flamingo | BLIP-2 |
  |------|---------|--------|
  | 연결 방식 | Cross-Attention | Q-Former |
  | 시각 토큰 | 64개 | 32개 |
  | LLM 수정 | 층 삽입 | 수정 없음 |
  | 학습 파라미터 | ~10B | ~188M |
  | Few-shot | 강함 | 중간 |
```

## 4. LLaVA: 간단하지만 강력한 접근

```
LLaVA (Liu et al., 2023):
  "Visual Instruction Tuning"

  핵심 철학:
  복잡한 브릿지 모듈 대신, 단순한 선형 투영!

  구조:
  이미지 → CLIP ViT-L/14 → 패치 토큰 (256개)
  → Linear Projection → LLM 입력 공간
  → [시각 토큰 256개] + [텍스트 토큰] → LLaMA → 답변

  Linear Projection:
  W × 시각 토큰 (768 → 4096)
  → 단 하나의 선형 변환!
  → Q-Former의 수백만 파라미터 vs Linear의 수백만 파라미터

  왜 이렇게 간단한 게 작동하는가:
  ├─ CLIP이 이미 좋은 시각 표현을 학습
  ├─ LLM이 이미 강력한 언어 능력 보유
  ├─ 둘 사이를 연결하는 건 "차원 변환"만으로 충분
  └─ 복잡한 모듈보다 데이터 품질이 더 중요!

  학습 데이터 생성 (GPT-4 활용):
  1. 이미지 + 캡션을 GPT-4에 제공
  2. GPT-4가 다양한 질문-답변 쌍 생성
     "이 이미지를 자세히 설명해 주세요"
     "이 이미지에서 어떤 활동이 일어나고 있나요?"
  3. 158K개의 시각 지시 데이터 생성
  → 고품질 지시 데이터가 핵심 기여!

  2단계 학습:
  Stage 1: Alignment Pre-training
  ├─ 558K CC3M 이미지-캡션 쌍
  ├─ Vision Encoder + LLM 고정
  ├─ Linear Projection만 학습
  └─ 시각 특징을 LLM 공간에 정렬

  Stage 2: Visual Instruction Tuning
  ├─ 158K 시각 지시 데이터 (GPT-4 생성)
  ├─ Vision Encoder 고정
  ├─ Linear Projection + LLM 전체 학습
  └─ 지시를 따르는 멀티모달 대화 능력

LLaVA 1.5 (Liu et al., 2023):
  간단한 개선으로 큰 성능 향상

  변경사항:
  ├─ Linear → MLP (2층): W₂ × GELU(W₁ × x)
  ├─ Vision: CLIP ViT-L → SigLIP ViT-SO400M
  ├─ 해상도: 224 → 336px (더 세밀한 시각)
  ├─ LLM: Vicuna 7B/13B
  └─ 데이터: 665K 사전학습 + 665K 지시 튜닝

  결과:
  ├─ 12개 벤치마크에서 BLIP-2, InstructBLIP 능가
  ├─ 학습 비용: A100 1대, ~1일
  └─ 오픈소스 VLM의 새 표준

  | 모델 | 연결 | 학습 비용 | 성능 |
  |------|------|----------|------|
  | Flamingo-80B | Gated XAttn | 매우 높음 | 좋음 |
  | BLIP-2 | Q-Former | 중간 | 좋음 |
  | LLaVA-1.5-13B | MLP | 낮음 | 매우 좋음 |
```

> **핵심 직관**: LLaVA의 교훈은 **"간단한 아키텍처 + 좋은 데이터가 복잡한 아키텍처를 이긴다"**입니다. Q-Former나 Gated Cross-Attention 같은 정교한 모듈 대신, 2층 MLP 하나로 CLIP과 LLM을 연결합니다. 대신 GPT-4로 생성한 고품질 지시 데이터가 핵심입니다. 이것은 dl-01에서 다룬 "데이터가 모델보다 중요하다"의 멀티모달 버전입니다.

## 5. GPT-4V, Qwen-VL, 최신 VLM

```
GPT-4V (OpenAI, 2023):
  상용 VLM의 새 기준

  특징:
  ├─ GPT-4에 시각 입력 추가
  ├─ 아키텍처 비공개 (추정: Cross-Attention 계열)
  ├─ 매우 높은 시각 이해력
  │   → OCR, 차트 분석, 코드 이미지 이해
  │   → 복잡한 추론이 포함된 시각 질문
  ├─ 다중 이미지 입력 가능
  └─ 안전 필터 적용

  GPT-4V의 강점:
  ├─ 세밀한 텍스트 인식 (OCR)
  ├─ 공간 관계 이해 ("왼쪽 위의 객체는?")
  ├─ 차트/그래프 분석
  ├─ 유머/밈 이해
  └─ 다단계 시각 추론

Qwen-VL (Bai et al., 2023, Alibaba):
  ├─ Qwen LLM + ViT-G
  ├─ 단일 이미지 연결자 (Cross-Attention 1층)
  ├─ 다중 이미지, 다중 언어 (중/영)
  ├─ Bounding Box 출력 가능 (Grounding)
  └─ Qwen2-VL: 동적 해상도, 비디오 이해

InternVL (Chen et al., 2024):
  ├─ InternViT-6B: 대규모 Vision Encoder
  ├─ Progressive 정렬: ViT→CLIP→VLM 단계적 학습
  ├─ InternVL 2.0: 다양한 크기 (1B~108B)
  └─ 중국어+영어 벤치마크에서 강력

LLaVA-OneVision (2024):
  ├─ 단일 이미지, 다중 이미지, 비디오 통합
  ├─ SigLIP + Qwen2
  ├─ AnyRes: 임의 해상도/비율 처리
  └─ 오픈소스 VLM의 최전선

해상도 처리 전략:

  고정 해상도:
  224×224 또는 336×336으로 리사이즈
  → 세밀한 텍스트, 작은 객체 손실

  동적 해상도 (AnyRes, Dynamic Resolution):
  ├─ 이미지를 여러 타일로 분할
  │   예: 1024×768 → 3×2 = 6개 타일 (각 336×336)
  ├─ 각 타일을 ViT로 처리
  ├─ 전체 축소 이미지도 함께 처리 (글로벌 컨텍스트)
  └─ 토큰 수 증가하지만 세밀한 이해 가능

  → 문서 OCR, 차트 분석에서 큰 성능 차이!
```

## 6. VLM 설계 원칙과 비교

```
VLM 설계의 트레이드오프:

  1. 시각 토큰 수:
     많을수록 → 세밀한 이해, but 계산 비용 증가
     ├─ LLaVA: 576개 (336px) → 세밀
     ├─ BLIP-2: 32개 → 효율적
     ├─ Flamingo: 64개 → 중간
     └─ 동적: 수백~수천개 (AnyRes)

  2. LLM 학습 여부:
     고정: 원래 LLM 능력 보존, 학습 효율적
     학습: 더 나은 시각-언어 통합, but 비용 높음
     → LLaVA: Stage 2에서 LLM 학습 → 더 좋은 성능

  3. Vision Encoder 학습 여부:
     고정: 대부분의 모델 (안정적)
     학습: InternVL (더 나은 정렬, but 불안정할 수 있음)

  학습 데이터 유형:
  ├─ 이미지-캡션 쌍: 웹 크롤링 (수억 개, 노이즈)
  ├─ VQA 데이터: 질문-답변 (수십만, 정교)
  ├─ 지시 데이터: GPT-4 생성 (수십만, 고품질)
  ├─ OCR 데이터: 문서, 차트 (특화)
  └─ 대화 데이터: 다중 턴 대화 (자연스러움)

  학습 레시피 (일반적):
  1. Alignment: 이미지-캡션으로 연결자 학습
  2. Pre-training: 다양한 데이터로 VLM 사전학습
  3. Instruction Tuning: 지시 데이터로 미세조정
  4. RLHF/DPO: 인간 선호 정렬 (선택적)

  종합 비교:
  | 모델 | 연결 | ViT | LLM | 특징 |
  |------|------|-----|-----|------|
  | Flamingo | Gated XAttn | NFNet | Chinchilla 70B | Few-shot |
  | BLIP-2 | Q-Former | ViT-G | FlanT5 | 효율적 |
  | LLaVA-1.5 | MLP | SigLIP | Vicuna 13B | 간단+강력 |
  | GPT-4V | 비공개 | 비공개 | GPT-4 | 최강 성능 |
  | Qwen2-VL | XAttn | ViT | Qwen2 | 동적 해상도 |
  | InternVL2 | MLP | InternViT | InternLM2 | 대규모 ViT |
```

## 핵심 정리

- VLM은 **Vision Encoder + Connector + LLM**의 세 요소로 구성되며, Connector 설계(Cross-Attention, Q-Former, MLP)가 아키텍처의 핵심 차이입니다
- **Flamingo**는 Perceiver Resampler + Gated Cross-Attention으로 LLM에 시각 정보를 삽입하며, Few-shot 멀티모달 학습의 선구자입니다
- **BLIP-2**는 Q-Former(32개 쿼리)로 시각 정보를 압축하여 고정된 LLM에 전달하며, 전체 파라미터의 **1.6%만 학습**하는 효율적 접근입니다
- **LLaVA**는 단순한 MLP 투영 + GPT-4 생성 지시 데이터로 복잡한 모델을 능가하며, "간단한 아키텍처 + 좋은 데이터"의 원칙을 증명했습니다
- 최신 VLM(**GPT-4V, Qwen2-VL, InternVL2**)은 동적 해상도, 다중 이미지, 비디오 입력으로 진화하며, 문서 OCR과 차트 분석까지 범용 시각 이해를 달성하고 있습니다
