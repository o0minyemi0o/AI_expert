# 오디오 기초와 음성 표현

## 왜 오디오를 다루는가

mm-01~05에서 시각과 언어의 결합을 다뤘습니다. 하지만 인간의 소통에서 **소리**는 필수적입니다. 대화의 의미는 말의 내용뿐 아니라 **톤, 억양, 감정**에도 담겨 있습니다. GPT-4o가 음성으로 대화하고, Whisper가 모든 언어를 받아쓰며, MusicLM이 텍스트로 음악을 만듭니다. 오디오 모달리티의 기초를 이해하면 mm-07~08의 음성 인식, 합성, 오디오-언어 모델을 제대로 이해할 수 있습니다.

> **핵심 직관**: 오디오와 이미지의 근본적 차이는 **"시간"**입니다. 이미지는 하나의 순간을 포착하지만, 오디오는 시간에 따라 흐릅니다. 1초의 음성에 16,000개의 샘플이 있고(16kHz), 이 시간적 구조 속에 음소, 단어, 문장, 감정이 계층적으로 담겨 있습니다. 오디오 AI의 핵심 과제는 이 **시간적 계층 구조를 효율적으로 표현**하는 것입니다.

## 1. 소리의 물리와 디지털 표현

```
소리의 본질:
  공기의 진동 → 압력 파동 → 귀의 고막 진동

  아날로그 → 디지털 변환:
  1. 샘플링 (Sampling):
     연속 신호를 일정 간격으로 측정
     ├─ 샘플레이트: 초당 측정 횟수
     ├─ 16 kHz: 음성 (전화 품질)
     ├─ 22.05 kHz: 저품질 음악
     ├─ 44.1 kHz: CD 품질
     ├─ 48 kHz: 영상/방송 표준
     └─ 나이퀴스트 정리: fs > 2 × f_max
        인간 청각 ~20kHz → 44.1kHz면 충분

  2. 양자화 (Quantization):
     각 샘플의 진폭을 이산 값으로
     ├─ 16-bit: 65,536 레벨 (CD)
     ├─ 24-bit: 스튜디오 품질
     └─ 32-bit float: 딥러닝 처리

  데이터 크기:
  1초 음성 (16kHz, 16-bit) = 32 KB
  1분 음악 (44.1kHz, 16-bit, 스테레오) = 10.6 MB
  → 이미지(수백 KB)와 비교하면 매우 밀집된 데이터

  파형 (Waveform):
  가장 원시적인 표현
  x = [x₁, x₂, ..., x_T]
  T = 샘플레이트 × 시간(초)
  → 1초 = 16,000개 값 (16kHz 기준)
  → 직접 다루기엔 너무 길고 저수준

  문제:
  파형은 주파수 정보를 직접 보여주지 않음
  "도" 음과 "레" 음의 파형 차이는 알기 어려움
  → 주파수 분석이 필요!
```

## 2. 스펙트로그램과 Mel 스펙트로그램

```
푸리에 변환:
  시간 → 주파수 변환

  DFT: 전체 신호의 주파수 성분 분석
  → 문제: "언제" 그 주파수가 나타났는지 모름

  STFT (Short-Time Fourier Transform):
  신호를 짧은 창(window)으로 나누어 각각 FFT

  파라미터:
  ├─ Window Size (n_fft): 보통 1024~2048 샘플
  │   → 주파수 해상도 결정 (클수록 정밀)
  ├─ Hop Length: 창 이동 간격 (보통 n_fft/4)
  │   → 시간 해상도 결정 (작을수록 정밀)
  └─ Window Function: Hann, Hamming 등

  출력: 스펙트로그램
  ├─ x축: 시간 (프레임)
  ├─ y축: 주파수 (Hz)
  ├─ 값: 에너지/진폭 (dB)
  └─ 크기: (n_fft/2+1) × T_frames

  → "시간에 따른 주파수 에너지 분포"
  → 음성의 "지문" 같은 역할

Mel Scale:
  인간 청각에 맞춘 주파수 스케일

  인간의 청각:
  ├─ 저주파에서 민감 (100Hz vs 200Hz = 큰 차이)
  ├─ 고주파에서 둔감 (8000Hz vs 8100Hz = 구분 어려움)
  └─ 로그 스케일에 가까운 비선형 인식

  Mel 변환:
  m = 2595 × log₁₀(1 + f/700)

  Mel Spectrogram:
  STFT → Mel Filter Bank → Log 변환
  ├─ Mel Filter: 삼각 필터 뱅크 (보통 80~128개)
  ├─ 저주파: 좁은 필터 (세밀한 해상도)
  ├─ 고주파: 넓은 필터 (거친 해상도)
  └─ Log: 에너지를 dB 스케일로

  출력: Mel Spectrogram
  크기: n_mels × T_frames (예: 80 × 400)
  → 원시 파형 (16000 × 1) 대비 크게 압축
  → 인간 청각에 맞는 표현
  → 대부분의 오디오 AI 모델의 입력!

MFCC (Mel-Frequency Cepstral Coefficients):
  전통적 음성 특징

  Mel Spectrogram → DCT (이산 코사인 변환)
  → 상위 13~20개 계수만 사용
  → 매우 압축된 표현

  ├─ 전통 음성 인식의 표준 입력
  ├─ HMM-GMM 시대의 핵심 특징
  ├─ 이제는 Mel Spectrogram을 직접 사용하는 추세
  └─ 딥러닝이 더 좋은 표현을 스스로 학습

  | 표현 | 차원/초 | 정보량 | 용도 |
  |------|---------|--------|------|
  | 파형 | 16,000 | 최대 | WaveNet, wav2vec |
  | Spectrogram | ~800×513 | 높음 | 시각화 |
  | Mel Spec | ~80×100 | 중간 | 대부분의 모델 |
  | MFCC | ~13×100 | 낮음 | 전통 ASR |
```

> **핵심 직관**: Mel Spectrogram을 "오디오의 이미지"라고 생각할 수 있습니다. 실제로 Mel Spectrogram을 이미지처럼 취급하여 CNN으로 처리하는 것이 오디오 AI의 일반적 접근이었습니다. cv-10의 ViT가 이미지를 패치로 나누듯, AST(Audio Spectrogram Transformer)는 Mel Spectrogram을 패치로 나누어 Transformer로 처리합니다.

## 3. 자기지도 음성 표현: wav2vec 2.0

```
wav2vec 2.0 (Baevski et al., 2020, Meta):
  "음성의 BERT"

  동기:
  ├─ 음성 데이터는 풍부 (인터넷, 팟캐스트)
  ├─ 라벨(전사)은 비쌈 (수작업 필요)
  ├─ 라벨 없는 음성에서 좋은 표현을 학습하자!
  └─ cv-09의 자기지도 학습을 오디오에 적용

  구조:
  1. Feature Encoder:
     원시 파형 → CNN (7개 층)
     → 20ms 간격의 잠재 표현 z_t
     → 16kHz 파형 → 49Hz 프레임으로 압축

  2. Quantization Module:
     z_t를 이산 코드 q_t로 양자화
     → Codebook: G개 그룹 × V개 엔트리
     → Gumbel-Softmax로 미분 가능한 양자화
     → cv-07의 VQ-VAE와 유사한 아이디어

  3. Context Network:
     Transformer가 z_t 시퀀스를 처리
     → 컨텍스트 표현 c_t 생성
     → 과거와 미래의 음성 정보를 통합

  자기지도 학습:
  마스킹 + 대조 학습

  1. 입력 z의 일부를 마스킹 (약 50%)
  2. Transformer가 마스킹된 위치의 c_t 예측
  3. 대조 목표:
     L = -log( exp(sim(c_t, q_t)) / Σ exp(sim(c_t, q̃)) )
     → c_t가 정답 q_t와 가까워지도록
     → cv-09의 SimCLR과 유사한 대조 학습!

  결과:
  ├─ Librispeech 960h → 사전학습
  ├─ 10분의 라벨 데이터만으로 WER 4.8%
  │   (960시간 지도학습과 비슷!)
  ├─ 1시간 라벨 → WER 2.3% (SOTA)
  └─ 라벨 효율성을 극대화

  저자원 언어의 구세주:
  ├─ 영어: 라벨 데이터 풍부
  ├─ 소수 언어: 라벨 거의 없음
  ├─ wav2vec 2.0으로 라벨 없이 사전학습
  └─ 소량의 라벨로 미세조정 → 음성 인식 가능!
```

## 4. HuBERT

```
HuBERT (Hsu et al., 2021, Meta):
  Hidden-Unit BERT

  wav2vec 2.0과의 차이:
  wav2vec 2.0: 대조 학습 (양성/음성 쌍)
  HuBERT: 유사 라벨 예측 (BERT의 MLM처럼)

  핵심 아이디어:
  음성에는 자연스러운 "클래스"가 있다 (음소와 유사)
  → K-means로 이산 유사 라벨을 만들고
  → 마스킹된 위치의 유사 라벨을 예측

  반복 학습:
  Iteration 1:
  ├─ MFCC에 K-means → 유사 라벨 (100 클러스터)
  ├─ HuBERT 학습: 마스킹된 위치의 유사 라벨 예측
  └─ 초기 표현 학습

  Iteration 2:
  ├─ Iter 1의 HuBERT 특징에 K-means → 더 나은 유사 라벨
  ├─ HuBERT 재학습: 개선된 라벨로
  └─ 더 좋은 표현 → 더 좋은 유사 라벨 → 선순환!

  → 보통 2~3회 반복이면 수렴
  → 유사 라벨이 실제 음소와 높은 상관!

  구조:
  파형 → CNN Feature Encoder → Transformer
  → wav2vec 2.0과 구조 동일
  → 학습 목적만 다름

  결과:
  ├─ wav2vec 2.0과 동등하거나 더 나은 성능
  ├─ 학습이 더 안정적 (대조 학습의 불안정성 회피)
  ├─ 이산 표현의 품질이 높음
  └─ 다운스트림 과제에서 범용적

  | 모델 | 자기지도 목표 | 양자화 | 안정성 |
  |------|-------------|--------|--------|
  | wav2vec 2.0 | 대조 학습 | Gumbel-Softmax | 중간 |
  | HuBERT | 유사 라벨 예측 | K-means | 높음 |
```

## 5. 범용 오디오 표현

```
음성을 넘어선 오디오 이해:

  AST (Audio Spectrogram Transformer, Gong et al., 2021):
  ├─ Mel Spectrogram을 ViT처럼 패치로 분할
  ├─ ImageNet 사전학습 ViT를 오디오에 전이!
  │   → 이미지와 오디오 스펙트로그램의 유사성 활용
  ├─ AudioSet(527 클래스)에서 미세조정
  └─ 환경음, 음악, 음성 모두 분류 가능

  BEATs (Chen et al., 2023):
  ├─ Audio Tokenizer + MLM 스타일 학습
  ├─ 반복적 자기지도 학습 (HuBERT 아이디어를 범용 오디오에)
  └─ AudioSet에서 SOTA

  오디오 과제의 다양성:

  음성 관련:
  ├─ ASR (음성 인식): 음성 → 텍스트 (mm-07)
  ├─ TTS (음성 합성): 텍스트 → 음성 (mm-07)
  ├─ SER (감정 인식): 음성 → 감정
  ├─ Speaker ID: 음성 → 화자 식별
  └─ Voice Conversion: 음성 → 다른 화자 음성

  비음성:
  ├─ 환경음 분류: "자동차", "새 울음", "빗소리"
  ├─ 음악 분류: 장르, 악기, 분위기
  ├─ Sound Event Detection: 시간별 소리 이벤트
  └─ Audio Captioning: 소리 → 텍스트 설명

Neural Audio Codec:
  음성/오디오를 이산 토큰으로 압축

  EnCodec (Défossez et al., 2022, Meta):
  ├─ 오디오 → 이산 코드 시퀀스
  ├─ RVQ (Residual Vector Quantization)
  │   → 여러 코드북의 계층적 양자화
  │   → 첫 코드북: 대략적 구조
  │   → 이후 코드북: 세부 디테일
  ├─ 24kHz 오디오를 1.5~24 kbps로 압축
  └─ mm-07의 VALL-E, mm-08의 AudioLM의 기반

  SoundStream (Zeghidour et al., 2022, Google):
  ├─ EnCodec과 유사한 Neural Codec
  ├─ RVQ + adversarial 학습
  └─ MusicLM의 기반

  → 오디오를 "토큰"으로 만들면
  → LLM처럼 다음 토큰 예측으로 오디오 생성 가능!
  → mm-07~08의 핵심 기반 기술

  | 표현 | 연속/이산 | 정보 | 용도 |
  |------|---------|------|------|
  | wav2vec 2.0 | 연속 | 음성 | ASR, SER |
  | HuBERT | 연속+이산 | 음성 | ASR, TTS |
  | AST | 연속 | 범용 | 분류 |
  | EnCodec | 이산 (RVQ) | 범용 | 생성 |
```

## 6. 오디오 표현의 선택과 응용

```
어떤 표현을 선택할 것인가:

  음성 인식 (ASR):
  → wav2vec 2.0 / HuBERT 특징
  → Whisper는 자체 Mel Spectrogram 입력 (mm-07)

  음성 합성 (TTS):
  → Mel Spectrogram (전통) 또는 EnCodec 토큰 (최신)
  → mm-07에서 상세히 다룸

  오디오 분류:
  → AST, BEATs (Mel Spectrogram 기반)
  → 범용 오디오 이해

  오디오 생성:
  → EnCodec / SoundStream 토큰
  → LM으로 토큰 시퀀스 생성 → 디코딩

  멀티모달 통합:
  → CLAP (mm-02의 AudioCLIP 확장)
  → ImageBind (6개 모달리티 통합)

  오디오 표현의 발전 방향:
  ├─ 자기지도 → 더 적은 라벨로 더 나은 성능
  ├─ Neural Codec → 오디오를 "언어처럼" 다루기
  ├─ 멀티모달 정렬 → 오디오+텍스트+이미지 통합
  └─ mm-10의 Gemini, GPT-4o에서 오디오 네이티브 통합
```

## 핵심 정리

- 오디오의 디지털 표현은 **파형**(16kHz=초당 16,000 샘플)에서 시작하며, **STFT**로 시간-주파수 분석 후 **Mel Scale**로 인간 청각에 맞춘 Mel Spectrogram이 대부분의 모델 입력입니다
- **wav2vec 2.0**은 원시 파형에서 마스킹+대조 학습으로 음성 표현을 자기지도 학습하여, 10분의 라벨만으로 960시간 지도학습에 근접하는 음성 인식 성능을 달성합니다
- **HuBERT**는 K-means 유사 라벨의 반복 정제로 wav2vec 2.0보다 안정적인 학습을 달성하며, 이산 유사 라벨이 실제 음소와 높은 상관을 보입니다
- **AST**는 Mel Spectrogram을 ViT 패치처럼 처리하여 범용 오디오 분류를 수행하고, **EnCodec/SoundStream**의 Neural Audio Codec은 오디오를 이산 토큰으로 변환하여 LM 기반 생성을 가능케 합니다
- 오디오 표현은 **연속(wav2vec, HuBERT)**과 **이산(EnCodec)**으로 나뉘며, 이해 과제는 연속 표현, 생성 과제는 이산 토큰이 주로 사용됩니다
