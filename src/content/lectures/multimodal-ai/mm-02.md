# 크로스모달 표현 학습

## 왜 모달리티를 정렬하는가

mm-01에서 멀티모달 학습의 5대 과제 중 **정렬(Alignment)**을 다뤘습니다. "고양이" 텍스트와 고양이 이미지는 같은 개념이지만, 표현 공간이 완전히 다릅니다. **크로스모달 표현 학습**은 서로 다른 모달리티를 **같은 공간에 매핑**하여, 텍스트와 이미지가 직접 비교 가능하게 합니다. **CLIP**이 이 분야를 혁명적으로 바꾸었고, 현대 멀티모달 AI의 거의 모든 시스템의 기반이 됩니다.

> **핵심 직관**: 크로스모달 정렬은 **"번역"**과 같습니다. 한국어와 영어는 다른 체계이지만, "사랑 = love"처럼 의미를 대응시킬 수 있습니다. 마찬가지로 고양이 사진과 "a photo of a cat"은 다른 형태이지만, 같은 의미 공간의 같은 점에 매핑되어야 합니다. CLIP은 4억 개의 이미지-텍스트 쌍에서 이 "번역 사전"을 학습합니다.

## 1. 대조 학습의 원리

```
대조 학습 (Contrastive Learning):
  비슷한 것은 가깝게, 다른 것은 멀게

  기본 아이디어:
  ├─ 양성 쌍 (positive): 같은 의미 → 가깝게
  │   (이미지, 그 이미지의 캡션)
  ├─ 음성 쌍 (negative): 다른 의미 → 멀게
  │   (이미지, 다른 이미지의 캡션)
  └─ 임베딩 공간에서의 거리로 유사도 측정

  InfoNCE Loss:
  L = -log( exp(sim(z_i, z_j)/τ) / Σ_k exp(sim(z_i, z_k)/τ) )

  z_i: 앵커 (이미지 임베딩)
  z_j: 양성 (매칭 텍스트 임베딩)
  z_k: 모든 샘플 (양성 + 음성)
  τ: 온도 (보통 0.07)
  sim: 코사인 유사도

  cv-09에서 다룬 SimCLR의 NT-Xent와 동일한 원리!
  차이: SimCLR은 이미지-이미지, 여기서는 이미지-텍스트

  배치 내 음성 (In-batch Negatives):
  배치 크기 N일 때:
  ├─ N개의 (이미지, 텍스트) 쌍
  ├─ 각 이미지에 대해: 1개 양성 + (N-1)개 음성
  ├─ 유사도 행렬: N × N
  │   대각선 = 양성 쌍 (높아야 함)
  │   비대각선 = 음성 쌍 (낮아야 함)
  └─ 큰 배치 = 더 많은 음성 = 더 나은 학습

  온도 τ의 역할:
  ├─ τ 작음 (0.01): 분포가 뾰족 → "가장 가까운 것만" 집중
  ├─ τ 큼 (1.0): 분포가 평평 → 모든 음성에 균등 패널티
  ├─ τ = 0.07: CLIP의 기본값 (학습 가능)
  └─ 적절한 τ가 학습의 질을 크게 좌우
```

## 2. CLIP: 이미지-텍스트 정렬의 혁명

```
CLIP (Contrastive Language-Image Pretraining, Radford et al., 2021):
  OpenAI, 현대 멀티모달 AI의 기반

  구조:
  이미지 → Image Encoder → 이미지 임베딩 (512/768차원)
  텍스트 → Text Encoder → 텍스트 임베딩 (512/768차원)
  → 코사인 유사도로 매칭

  Image Encoder: ViT-B/16, ViT-L/14 등 (cv-10의 ViT)
  Text Encoder: Transformer (GPT-2 스타일)

  학습 데이터:
  WIT (WebImageText): 4억 이미지-텍스트 쌍
  ├─ 인터넷에서 수집
  ├─ 이미지 alt-text를 캡션으로 사용
  ├─ 필터링: 영어, 최소 품질
  └─ 노이즈가 많지만, 양이 압도적

  학습:
  배치 크기 32,768 (매우 큼!)
  ├─ 32K × 32K 유사도 행렬
  ├─ 대각선 32K개 = 양성
  ├─ 나머지 ~10억 개 = 음성
  └─ 256 GPU, 12일 학습

  CLIP의 Zero-Shot 분류:
  학습 시 본 적 없는 클래스도 분류 가능!

  방법:
  1. 각 클래스에 대해 텍스트 프롬프트 생성
     "a photo of a {class}"
     → "a photo of a cat", "a photo of a dog", ...
  2. 텍스트 임베딩 계산 (클래스당 하나)
  3. 입력 이미지의 이미지 임베딩 계산
  4. 코사인 유사도가 가장 높은 클래스 선택

  → ImageNet Zero-Shot: 76.2% (ResNet-50 지도학습과 동등!)
  → 학습 데이터에 ImageNet 라벨 없이 달성!

  프롬프트 엔지니어링:
  "a photo of a {class}" → 기본
  "a centered satellite photo of {class}" → 위성 사진
  "a black and white photo of {class}" → 흑백 사진
  → 프롬프트에 따라 성능 크게 변화

  80개 프롬프트 앙상블:
  여러 프롬프트의 텍스트 임베딩을 평균
  → 3.5% 성능 향상

  | 모델 | ImageNet Top-1 | 학습 데이터 | 라벨 필요 |
  |------|---------------|-----------|----------|
  | ResNet-50 (지도) | 76.1% | ImageNet 1.3M | 필요 |
  | CLIP ViT-B/16 (제로샷) | 68.3% | WIT 400M | 불필요 |
  | CLIP ViT-L/14 (제로샷) | 76.2% | WIT 400M | 불필요 |
```

> **핵심 직관**: CLIP의 혁신은 **"라벨 없이 분류"**입니다. 기존 ImageNet 분류는 1000개 클래스의 라벨이 필요했지만, CLIP은 "a photo of a cat"이라는 자연어만으로 분류합니다. 새로운 클래스가 추가되면? 그냥 텍스트를 추가하면 됩니다. 이것이 **오픈 보캐뷸러리(Open-Vocabulary)** 인식의 시작이며, cv-11에서 다룬 Open-Vocabulary Detection의 기반입니다.

## 3. CLIP 이후: ALIGN, SigLIP, EVA-CLIP

```
ALIGN (Jia et al., 2021, Google):
  CLIP과 유사하지만 데이터 규모 확대

  차이점:
  ├─ 데이터: 18억 이미지-텍스트 쌍 (CLIP의 4.5배)
  ├─ 필터링 최소화: 노이즈 있는 데이터를 그대로 사용
  ├─ Image Encoder: EfficientNet (CNN 기반)
  └─ 결론: 데이터 양이 품질보다 중요할 수 있음

  교훈:
  데이터 필터링을 줄여도 양이 충분하면 작동
  → 웹 스케일 데이터의 힘

SigLIP (Zhai et al., 2023, Google):
  CLIP의 손실 함수 개선

  CLIP의 Softmax 문제:
  ├─ N × N 유사도 행렬 계산 필요
  ├─ 배치 크기 N에 대해 GPU 간 통신 필요
  ├─ N = 32K일 때 메모리와 통신 병목
  └─ 배치 크기 증가가 어려움

  SigLIP의 해결: Sigmoid Loss
  각 쌍을 독립적으로 이진 분류

  L = -Σ log σ(y_ij × (sim(z_i, z_j)/τ - b))

  y_ij = +1 (매칭 쌍) or -1 (비매칭 쌍)
  b: 학습 가능한 바이어스

  장점:
  ├─ 전체 행렬 불필요 → 메모리 절약
  ├─ GPU 간 통신 감소
  ├─ 더 큰 배치 크기 가능
  └─ CLIP과 동등하거나 더 나은 성능

  → LLaVA 1.5 등 최신 VLM에서 SigLIP 채택

OpenCLIP (LAION):
  오픈소스 CLIP 재현

  ├─ LAION-2B: 20억 이미지-텍스트 쌍 (공개 데이터셋)
  ├─ LAION-5B: 50억 쌍
  ├─ ViT-G/14: OpenAI CLIP보다 강력
  ├─ 커뮤니티 주도의 오픈소스 노력
  └─ 대부분의 연구가 OpenCLIP 기반

EVA-CLIP (Fang et al., 2023):
  CLIP 학습의 효율화

  ├─ EVA로 이미지 인코더 사전학습 (MAE 기반)
  ├─ 적은 데이터로 강력한 CLIP
  ├─ EVA-CLIP ViT-E: 18B 파라미터
  └─ ImageNet Zero-Shot 82.0% (최고 수준)

  | 모델 | 데이터 | 손실 | ImageNet Zero-Shot |
  |------|-------|------|-------------------|
  | CLIP ViT-L/14 | 400M | Softmax | 76.2% |
  | ALIGN | 1.8B | Softmax | 76.4% |
  | OpenCLIP ViT-G/14 | 2B | Softmax | 80.1% |
  | SigLIP ViT-L | 10B | Sigmoid | 78.4% |
  | EVA-CLIP ViT-E | 2B | Softmax | 82.0% |
```

## 4. 크로스모달 표현의 활용

```
CLIP 임베딩의 다양한 활용:

  1. Zero-Shot 분류:
     위에서 다룬 방식
     → 어떤 분류 문제든 텍스트만 바꾸면 적용

  2. 이미지-텍스트 검색:
     텍스트 → 가장 유사한 이미지 (Text-to-Image Retrieval)
     이미지 → 가장 유사한 텍스트 (Image-to-Text Retrieval)
     → 코사인 유사도로 순위 매김

  3. 이미지 생성의 조건:
     Stable Diffusion의 텍스트 조건 = CLIP 텍스트 인코더
     → mm-05에서 상세히 다룸
     → 텍스트 임베딩이 생성 방향을 안내

  4. Open-Vocabulary 인식:
     cv-11에서 다룬 OWL-ViT, Grounding DINO
     → CLIP 공간에서 텍스트와 영역을 매칭
     → "학습하지 않은 객체"도 감지 가능

  5. VLM의 시각 인코더:
     LLaVA, BLIP-2 등 비전-언어 모델의 이미지 인코더
     → mm-03에서 상세히 다룸
     → CLIP의 이미지 표현 = "시각 언어"

  6. 멀티모달 RAG:
     문서의 이미지와 텍스트를 함께 인덱싱
     → 질문과 가장 관련된 이미지/텍스트 검색
     → 답변 생성 시 시각 정보 활용

임베딩 공간의 성질:

  선형 산술:
  "king" - "man" + "woman" ≈ "queen" (텍스트)
  마찬가지로 CLIP 공간에서도:
  개 이미지 - "사진" + "그림" ≈ 개 그림의 임베딩

  모달리티 갭:
  ├─ 같은 공간이지만 이미지와 텍스트가 완벽히 섞이지 않음
  ├─ 이미지 임베딩과 텍스트 임베딩이 각각 클러스터 형성
  ├─ 코사인 유사도 ≈ 0.2~0.3 (같은 의미여도)
  └─ 모달리티 갭을 줄이는 것이 활발한 연구 주제
```

## 5. 다른 모달리티로의 확장

```
CLIP의 원리를 다른 모달리티에 적용:

  AudioCLIP (Guzhov et al., 2022):
  ├─ 이미지-텍스트-오디오 3개 모달리티
  ├─ CLIP에 오디오 인코더 추가
  ├─ 오디오 → "개 짖는 소리" 텍스트와 매칭
  └─ Zero-shot 오디오 분류 가능

  CLAP (Wu et al., 2023):
  ├─ Contrastive Language-Audio Pretraining
  ├─ 오디오-텍스트 쌍에 CLIP 원리 적용
  ├─ HTSAT 오디오 인코더 + RoBERTa 텍스트 인코더
  └─ mm-08에서 오디오-언어 모델의 기반

  PointCLIP (Zhang et al., 2022):
  ├─ 3D 포인트 클라우드에 CLIP 적용
  ├─ 포인트 클라우드를 여러 각도로 투영 → 2D 이미지
  ├─ CLIP으로 각 투영 분류 → 앙상블
  └─ 3D 학습 데이터 없이 3D 분류!

  ImageBind (Goel et al., 2023, Meta):
  ├─ 6개 모달리티를 하나의 공간에!
  │   이미지, 텍스트, 오디오, 깊이, 열화상, IMU
  ├─ 핵심: 이미지를 "앵커 모달리티"로
  │   이미지-텍스트 쌍 + 이미지-오디오 쌍 + ...
  │   → 이미지를 거쳐 모든 모달리티가 연결
  ├─ 오디오 → 이미지 검색도 가능 (직접 쌍 없이!)
  └─ "Emergent Zero-Shot": 학습 안 한 모달리티 조합도 동작

  ImageBind의 핵심 통찰:
  (텍스트, 이미지) 정렬 + (이미지, 오디오) 정렬
  → (텍스트, 오디오) 정렬이 자동으로!
  → 이미지가 "허브 모달리티" 역할

  | 모델 | 모달리티 | 정렬 방식 | 특징 |
  |------|---------|----------|------|
  | CLIP | 이미지+텍스트 | 대조 | 기본 |
  | CLAP | 오디오+텍스트 | 대조 | 오디오 |
  | ImageBind | 6개 | 이미지 앵커 | 범용 |
```

> **핵심 직관**: ImageBind의 "앵커 모달리티" 아이디어는 놀랍도록 간단합니다. 한국어-영어 번역과 영어-일본어 번역이 있으면, 한국어-일본어 번역이 **영어를 거쳐** 가능합니다. ImageBind는 이미지를 이 "영어" 역할로 사용합니다. 이미지-텍스트 정렬과 이미지-오디오 정렬만 학습하면, 텍스트-오디오 정렬이 **자동으로 나타납니다**.

## 6. 크로스모달 학습의 실전

```
실전 고려사항:

  인코더 선택:
  ├─ 이미지: ViT-L/14 (CLIP), ViT-SO400M (SigLIP)
  ├─ 텍스트: CLIP Text Encoder, BERT 계열
  ├─ 오디오: HTSAT, AST
  └─ 사전학습된 인코더를 활용하는 것이 표준

  학습 전략:
  ├─ 처음부터 학습: 대규모 데이터 + 큰 배치 필요
  ├─ 파인튜닝: 사전학습 CLIP + 도메인 데이터
  │   → 의료: CLIP + X-ray 캡션 쌍
  │   → 패션: CLIP + 상품-설명 쌍
  ├─ Linear Probe: CLIP 고정 + 분류 헤드만 학습
  │   → 적은 데이터로 강력한 분류기
  └─ 프롬프트 튜닝: 텍스트 프롬프트만 최적화 (CoOp)

  배치 크기의 중요성:
  ├─ 대조 학습은 큰 배치가 핵심
  ├─ CLIP: 32,768 / SigLIP: 32,768
  ├─ 작은 배치: 음성 샘플 부족 → 학습 품질 저하
  └─ 해결: Gradient Accumulation, SigLIP(독립 쌍)

  CLIP 공간의 한계:
  ├─ 세밀한 차이 구분 어려움
  │   → "빨간 차"와 "파란 차"의 구분이 약함
  ├─ 구성적 이해 부족
  │   → "개 위의 고양이" ≠ "고양이 위의 개" 구분 어려움
  ├─ 수량 이해 약함
  │   → "3개의 사과"와 "5개의 사과" 유사도 비슷
  └─ VLM(mm-03)이 이 한계를 보완

  | 전략 | 데이터 양 | 성능 | 비용 |
  |------|---------|------|------|
  | 처음부터 | 수억 쌍 | 최고 | 매우 높음 |
  | 파인튜닝 | 수만~수십만 | 좋음 | 중간 |
  | Linear Probe | 수백~수천 | 좋음 | 낮음 |
  | 프롬프트 튜닝 | 수백 | 좋음 | 매우 낮음 |
```

## 핵심 정리

- **대조 학습**은 양성 쌍은 가깝게, 음성 쌍은 멀게 배치하며, **InfoNCE 손실**과 큰 배치 크기가 핵심이고 온도 τ가 학습 품질을 좌우합니다
- **CLIP**은 4억 이미지-텍스트 쌍에서 대조 학습으로 시각-언어 정렬을 학습하여, ImageNet Zero-Shot 76.2%를 달성하며 현대 멀티모달 AI의 기반이 됩니다
- **SigLIP**은 Sigmoid 손실로 CLIP의 Softmax 병목을 해결하고, **EVA-CLIP**은 MAE 사전학습으로 효율을 높여 82.0% Zero-Shot을 달성합니다
- **ImageBind**는 이미지를 앵커 모달리티로 6개 모달리티를 하나의 공간에 정렬하여, 직접 학습하지 않은 모달리티 조합도 Zero-Shot으로 동작합니다
- CLIP 임베딩은 **Zero-Shot 분류, 검색, 이미지 생성 조건, Open-Vocabulary 인식, VLM 시각 인코더** 등 멀티모달 AI 전반의 기초로 활용됩니다
