# 서포트 벡터 머신: 최대 마진 분류기

## 왜 마진을 최대화해야 하는가

cm-01에서 다룬 퍼셉트론은 분류 경계를 찾지만 그 경계가 유일하지 않습니다. 무한히 많은 선형 분류기 중 "가장 안전한" 경계를 선택하는 원리가 바로 마진 최대화이며, 이것이 서포트 벡터 머신(SVM)의 핵심 아이디어입니다. SVM은 통계적 학습 이론의 VC 차원과 구조적 위험 최소화에 이론적 근거를 두고 있습니다.

---

## 1. 기하학적 마진과 함수적 마진

**함수적 마진(functional margin)**: 샘플 $(\mathbf{x}_i, y_i)$에 대해

$$\hat{\gamma}_i = y_i(\mathbf{w}^\top \mathbf{x}_i + b)$$

함수적 마진은 $\mathbf{w}$의 스케일에 의존하므로, 정규화가 필요합니다.

**기하학적 마진(geometric margin)**: 결정 경계까지의 실제 거리

$$\gamma_i = \frac{y_i(\mathbf{w}^\top \mathbf{x}_i + b)}{\|\mathbf{w}\|}$$

전체 데이터셋에 대한 마진은 가장 가까운 점의 거리입니다:

$$\gamma = \min_{i=1,\ldots,n} \gamma_i$$

> **핵심 직관**: 기하학적 마진은 결정 경계의 "안전 지대" 폭입니다. 마진이 넓을수록 미래의 데이터에 대한 오분류 확률이 줄어듭니다.

```python
import numpy as np

def geometric_margin(w, b, X, y):
    functional_margins = y * (X @ w + b)
    return np.min(functional_margins) / np.linalg.norm(w)
```

---

## 2. 하드 마진 SVM: 원초 문제

모든 데이터를 정확히 분류하면서 마진을 최대화하는 최적화 문제를 정의합니다.

$$\max_{\mathbf{w}, b} \frac{2}{\|\mathbf{w}\|}$$

$$\text{s.t.} \quad y_i(\mathbf{w}^\top \mathbf{x}_i + b) \geq 1, \quad \forall i$$

이는 동치인 최소화 문제로 변환됩니다:

$$\min_{\mathbf{w}, b} \frac{1}{2}\|\mathbf{w}\|^2$$

$$\text{s.t.} \quad y_i(\mathbf{w}^\top \mathbf{x}_i + b) \geq 1, \quad \forall i$$

이 문제는 **co-08에서 배운 볼록 이차 계획(Quadratic Programming)** 문제입니다. 목적 함수는 이차 볼록 함수이고, 제약 조건은 선형이므로 전역 최적해가 존재합니다.

| 성질 | 설명 |
|------|------|
| 목적 함수 | 이차 볼록 |
| 제약 조건 | 선형 부등식 |
| 해의 존재성 | 선형 분리 가능 시 유일한 최적해 |
| 계산 복잡도 | $O(n^3)$ (일반 QP) |

> **핵심 직관**: $\|\mathbf{w}\|$를 최소화하면 마진 $\frac{2}{\|\mathbf{w}\|}$이 최대화됩니다. 이는 "가장 단순한 모델"을 선택하는 오컴의 면도날 원리와 일맥상통합니다.

---

## 3. 서포트 벡터와 KKT 조건

최적해에서 **KKT(Karush-Kuhn-Tucker) 조건**이 성립합니다.

라그랑주 함수:

$$\mathcal{L}(\mathbf{w}, b, \boldsymbol{\alpha}) = \frac{1}{2}\|\mathbf{w}\|^2 - \sum_{i=1}^{n} \alpha_i \left[y_i(\mathbf{w}^\top \mathbf{x}_i + b) - 1\right]$$

KKT 조건:

$$\alpha_i \geq 0, \quad y_i(\mathbf{w}^\top \mathbf{x}_i + b) - 1 \geq 0$$

$$\alpha_i \left[y_i(\mathbf{w}^\top \mathbf{x}_i + b) - 1\right] = 0 \quad (\text{상보 조건})$$

상보 조건에 의해:
- $\alpha_i > 0$이면 $y_i(\mathbf{w}^\top \mathbf{x}_i + b) = 1$ (**서포트 벡터**)
- $\alpha_i = 0$이면 해당 샘플은 결정에 관여하지 않음

**가중치 벡터의 표현**:

$$\mathbf{w}^* = \sum_{i=1}^{n} \alpha_i y_i \mathbf{x}_i = \sum_{i \in \text{SV}} \alpha_i y_i \mathbf{x}_i$$

> **핵심 직관**: 결정 경계는 오직 서포트 벡터에 의해서만 결정됩니다. 수천 개의 데이터 중 서포트 벡터 몇 개만으로도 동일한 경계를 복원할 수 있습니다.

---

## 4. 소프트 마진 SVM

현실 데이터는 대부분 선형 분리 불가능합니다. **슬랙 변수(slack variables)** $\xi_i \geq 0$을 도입합니다.

$$\min_{\mathbf{w}, b, \boldsymbol{\xi}} \frac{1}{2}\|\mathbf{w}\|^2 + C \sum_{i=1}^{n} \xi_i$$

$$\text{s.t.} \quad y_i(\mathbf{w}^\top \mathbf{x}_i + b) \geq 1 - \xi_i, \quad \xi_i \geq 0$$

하이퍼파라미터 $C$는 마진 크기와 오분류 허용 사이의 균형을 제어합니다.

| $C$ 값 | 마진 | 오분류 허용 | 모델 경향 |
|--------|------|-----------|----------|
| $C \to \infty$ | 좁음 | 거의 없음 | 과적합 (하드 마진에 수렴) |
| $C$ 큼 | 좁음 | 적음 | 낮은 편향, 높은 분산 |
| $C$ 작음 | 넓음 | 많음 | 높은 편향, 낮은 분산 |
| $C \to 0$ | 매우 넓음 | 매우 많음 | 과소적합 |

```python
from sklearn.svm import SVC

# C가 클수록 오분류에 대한 벌점이 커짐
svm_hard = SVC(kernel='linear', C=1000)   # 하드 마진에 가까움
svm_soft = SVC(kernel='linear', C=0.01)   # 넓은 마진, 오분류 허용
```

> **핵심 직관**: $C$는 "마진의 넓이"와 "분류 정확도" 사이의 트레이드오프를 제어하는 단일 노브입니다. cm-12에서 배울 교차 검증으로 최적 $C$를 선택합니다.

---

## 5. 힌지 손실과 정규화 관점

소프트 마진 SVM은 다음의 비제약 최적화 문제와 동치입니다:

$$\min_{\mathbf{w}, b} \frac{1}{n}\sum_{i=1}^{n} \max(0, 1 - y_i f(\mathbf{x}_i)) + \frac{\lambda}{2}\|\mathbf{w}\|^2$$

여기서 $\max(0, 1 - y_i f(\mathbf{x}_i))$가 **힌지 손실(hinge loss)**입니다.

**손실 함수 비교**:

| 손실 함수 | 수식 | $yf(\mathbf{x}) = 1$에서 | 미분 |
|-----------|------|------------------------|------|
| 0-1 손실 | $\mathbb{1}[yf < 0]$ | 불연속 | 불가 |
| 힌지 손실 | $\max(0, 1 - yf)$ | 0 (꺾임점) | 부분적 |
| 교차 엔트로피 | $\log(1 + e^{-yf})$ | $\log 2$ 근처 | 전구간 |
| 지수 손실 | $e^{-yf}$ | $e^{-1}$ | 전구간 |

힌지 손실의 핵심 특성:
- $yf(\mathbf{x}) \geq 1$이면 손실이 정확히 0 (마진 밖의 점은 무시)
- cm-01에서 다룬 교차 엔트로피와 달리, 정확히 분류된 점은 학습에 영향을 주지 않음

> **핵심 직관**: 힌지 손실은 "충분히 잘 분류된" 샘플($yf \geq 1$)의 손실을 정확히 0으로 만듭니다. 이것이 SVM의 희소 서포트 벡터 구조를 만드는 원인입니다.

---

## 6. SVM의 이론적 근거: 구조적 위험 최소화

SVM의 마진 최대화는 **VC 이론**에 이론적 근거를 둡니다.

**일반화 오차 상한**:

$$R(\mathbf{w}) \leq R_{\text{emp}}(\mathbf{w}) + \sqrt{\frac{h(\log(2n/h) + 1) - \log(\delta/4)}{n}}$$

여기서 $h$는 VC 차원이고, 마진 $\gamma$인 선형 분류기의 VC 차원은:

$$h \leq \min\left(\frac{R^2}{\gamma^2}, d\right) + 1$$

$R$은 데이터를 포함하는 구의 반지름입니다. 마진이 클수록 VC 차원이 작아지고, 일반화 오차 상한이 줄어듭니다.

```python
from sklearn.svm import SVC
from sklearn.model_selection import cross_val_score

svm = SVC(kernel='linear', C=1.0)
scores = cross_val_score(svm, X, y, cv=5)
# MNIST에서 선형 SVM은 ~94% 정확도로, 로지스틱 회귀(~92%)보다 약간 우수합니다
```

> **핵심 직관**: 마진 최대화는 단순히 기하학적 직관이 아니라, 통계적 학습 이론에서 일반화 능력을 보장하는 원리입니다.

---

## 7. 구현과 실용적 고려 사항

| 고려 사항 | 권장 |
|-----------|------|
| 피처 스케일링 | 반드시 필요 ($\|\mathbf{w}\|$가 스케일에 의존) |
| 클래스 불균형 | `class_weight='balanced'` 사용 |
| $C$ 선택 | 로그 스케일 그리드 서치 ($10^{-3}$ ~ $10^3$) |
| 대규모 데이터 | `LinearSVC` (liblinear) 또는 SGD |

```python
from sklearn.svm import LinearSVC
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline

pipe = Pipeline([
    ('scaler', StandardScaler()),
    ('svm', LinearSVC(C=1.0, loss='hinge', max_iter=10000))
])
pipe.fit(X_train, y_train)
```

cm-03에서는 이 원초 문제를 라그랑주 쌍대 문제로 변환하여, 커널 트릭을 통한 비선형 SVM으로 확장합니다.

> **핵심 직관**: 실무에서 SVM은 피처 스케일링 없이 사용하면 성능이 급격히 저하됩니다. 이는 마진이 피처의 절대 스케일에 영향을 받기 때문입니다.

---

## 핵심 정리

- **SVM은 기하학적 마진 $\frac{2}{\|\mathbf{w}\|}$를 최대화하는 볼록 이차 계획 문제로, 선형 분류기 중 가장 안전한 결정 경계를 선택합니다**
- **서포트 벡터는 마진 경계에 위치한 소수의 핵심 데이터 포인트이며, KKT 상보 조건에 의해 이들만이 결정 경계를 결정합니다**
- **소프트 마진 SVM은 슬랙 변수와 하이퍼파라미터 $C$를 통해 마진 크기와 오분류 허용 사이의 균형을 제어합니다**
- **힌지 손실은 마진 밖의 정확히 분류된 샘플에 대해 손실을 0으로 만들어, 서포트 벡터의 희소성을 유도합니다**
- **마진 최대화는 VC 이론에 근거하여 모델의 복잡도를 제한하고 일반화 성능을 보장하는 구조적 위험 최소화 원리입니다**
