# EM 알고리즘과 혼합 모델

## 왜 EM 알고리즘을 배워야 하는가

많은 실세계 데이터는 관측되지 않은 잠재 변수(latent variable)를 포함합니다. 예를 들어 고객을 세분화할 때 "어떤 고객군에 속하는가"는 관측되지 않습니다. EM 알고리즘은 이런 잠재 변수가 있는 모델에서 최대 우도 추정을 수행하는 일반적이고 우아한 방법입니다. 가우시안 혼합 모델(GMM)은 EM의 가장 대표적인 응용이며, cm-07에서 다룬 생성 모델의 확장입니다.

---

## 1. 잠재 변수와 불완전 데이터

**관측 변수** $\mathbf{x}$와 **잠재 변수** $\mathbf{z}$가 있을 때:

- 완전 데이터 우도: $p(\mathbf{x}, \mathbf{z} \mid \boldsymbol{\theta})$ (만약 $\mathbf{z}$를 알면 간단)
- 불완전 데이터 우도: $p(\mathbf{x} \mid \boldsymbol{\theta}) = \sum_{\mathbf{z}} p(\mathbf{x}, \mathbf{z} \mid \boldsymbol{\theta})$

$\mathbf{z}$에 대한 합/적분이 로그 안에 들어가면 직접 최적화가 어렵습니다:

$$\log p(\mathbf{x} \mid \boldsymbol{\theta}) = \log \sum_{\mathbf{z}} p(\mathbf{x}, \mathbf{z} \mid \boldsymbol{\theta})$$

| 예시 | 관측 변수 | 잠재 변수 |
|------|----------|----------|
| 가우시안 혼합 모델 | 데이터 포인트 | 클러스터 소속 |
| Hidden Markov Model | 관측 시퀀스 | 상태 시퀀스 |
| 토픽 모델 | 문서의 단어 | 토픽 할당 |
| 결측 데이터 | 관측된 값 | 결측된 값 |

> **핵심 직관**: 잠재 변수는 "만약 이것을 알 수 있다면 문제가 쉬워질 것"인 숨겨진 정보입니다. EM은 잠재 변수를 "추정하고" 그 추정을 기반으로 파라미터를 학습하는 과정을 반복합니다.

---

## 2. EM 알고리즘의 일반 프레임워크

**E-step (기댓값 단계)**: 현재 파라미터 $\boldsymbol{\theta}^{(t)}$로 잠재 변수의 사후 분포를 계산합니다.

$$Q(\boldsymbol{\theta} \mid \boldsymbol{\theta}^{(t)}) = \mathbb{E}_{\mathbf{z} \mid \mathbf{x}, \boldsymbol{\theta}^{(t)}}[\log p(\mathbf{x}, \mathbf{z} \mid \boldsymbol{\theta})]$$

**M-step (최대화 단계)**: $Q$ 함수를 최대화하여 파라미터를 갱신합니다.

$$\boldsymbol{\theta}^{(t+1)} = \arg\max_{\boldsymbol{\theta}} Q(\boldsymbol{\theta} \mid \boldsymbol{\theta}^{(t)})$$

**EM의 수렴 보장**: 매 반복마다 로그 우도가 증가합니다.

$$\log p(\mathbf{x} \mid \boldsymbol{\theta}^{(t+1)}) \geq \log p(\mathbf{x} \mid \boldsymbol{\theta}^{(t)})$$

증명의 핵심은 **젠센 부등식(Jensen's inequality)**입니다:

$$\log p(\mathbf{x} \mid \boldsymbol{\theta}) = \log \sum_{\mathbf{z}} q(\mathbf{z}) \frac{p(\mathbf{x}, \mathbf{z} \mid \boldsymbol{\theta})}{q(\mathbf{z})} \geq \sum_{\mathbf{z}} q(\mathbf{z}) \log \frac{p(\mathbf{x}, \mathbf{z} \mid \boldsymbol{\theta})}{q(\mathbf{z})}$$

등호 조건: $q(\mathbf{z}) = p(\mathbf{z} \mid \mathbf{x}, \boldsymbol{\theta})$ (E-step의 역할)

> **핵심 직관**: EM은 로그 우도의 하한(ELBO)을 반복적으로 올립니다. E-step은 하한을 로그 우도에 맞닿게 하고, M-step은 하한을 최대화합니다.

---

## 3. 가우시안 혼합 모델 (GMM)

GMM은 데이터가 $K$개의 가우시안 분포의 혼합에서 생성되었다고 가정합니다.

$$p(\mathbf{x}) = \sum_{k=1}^K \pi_k \, \mathcal{N}(\mathbf{x} \mid \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)$$

| 파라미터 | 의미 | 제약 |
|---------|------|------|
| $\pi_k$ | 혼합 비율 | $\sum_k \pi_k = 1$, $\pi_k \geq 0$ |
| $\boldsymbol{\mu}_k$ | 클러스터 $k$의 평균 | 없음 |
| $\boldsymbol{\Sigma}_k$ | 클러스터 $k$의 공분산 | 양의 정부호 |

**잠재 변수** $z_i \in \{1, \ldots, K\}$: 샘플 $\mathbf{x}_i$의 클러스터 소속

**생성 과정**:
1. $z_i \sim \text{Categorical}(\pi_1, \ldots, \pi_K)$
2. $\mathbf{x}_i \mid z_i = k \sim \mathcal{N}(\boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)$

이것은 cm-07에서 다룬 나이브 베이즈의 생성 모델 관점을 직접 확장한 것입니다.

> **핵심 직관**: GMM은 각 클러스터가 "자신만의 정규 분포로 데이터를 생성한다"고 가정합니다. K-Means와 달리 클러스터의 모양(공분산)과 크기(혼합 비율)가 다를 수 있습니다.

---

## 4. GMM의 EM 알고리즘

**E-step**: 각 샘플의 클러스터 소속 확률(responsibility) 계산

$$\gamma_{ik} = \frac{\pi_k \, \mathcal{N}(\mathbf{x}_i \mid \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)}{\sum_{j=1}^K \pi_j \, \mathcal{N}(\mathbf{x}_i \mid \boldsymbol{\mu}_j, \boldsymbol{\Sigma}_j)}$$

**M-step**: 가중 통계량으로 파라미터 갱신

$$N_k = \sum_{i=1}^n \gamma_{ik}$$

$$\boldsymbol{\mu}_k^{\text{new}} = \frac{1}{N_k}\sum_{i=1}^n \gamma_{ik} \mathbf{x}_i$$

$$\boldsymbol{\Sigma}_k^{\text{new}} = \frac{1}{N_k}\sum_{i=1}^n \gamma_{ik}(\mathbf{x}_i - \boldsymbol{\mu}_k^{\text{new}})(\mathbf{x}_i - \boldsymbol{\mu}_k^{\text{new}})^\top$$

$$\pi_k^{\text{new}} = \frac{N_k}{n}$$

```python
from sklearn.mixture import GaussianMixture

gmm = GaussianMixture(
    n_components=3,
    covariance_type='full',   # 'spherical', 'diag', 'tied', 'full'
    n_init=10,                # 다중 초기화
    max_iter=200,
    random_state=42
)
gmm.fit(X_train)

# 소프트 클러스터링: 소속 확률
responsibilities = gmm.predict_proba(X_test)

# 하드 클러스터링
labels = gmm.predict(X_test)
```

> **핵심 직관**: E-step은 "현재 파라미터가 맞다면, 이 데이터는 어떤 클러스터에서 왔을까?"에 답하고, M-step은 "이 소속이 맞다면, 각 클러스터의 파라미터는 무엇일까?"에 답합니다.

---

## 5. GMM vs K-Means

K-Means는 GMM의 특수한 경우로 볼 수 있습니다.

$$\text{K-Means} = \text{GMM with } \boldsymbol{\Sigma}_k = \epsilon I, \quad \epsilon \to 0$$

| 비교 | K-Means | GMM |
|------|---------|-----|
| 소속 | 하드 (0 또는 1) | 소프트 (확률) |
| 클러스터 모양 | 구형 | 타원형 (공분산) |
| 클러스터 크기 | 동일 가정 | 다를 수 있음 ($\pi_k$) |
| 목적 함수 | 클러스터 내 분산 | 로그 우도 |
| 수렴 | 국소 최적 | 국소 최적 |
| 밀도 추정 | 불가 | 가능 |

**공분산 구조 선택**:

| `covariance_type` | 파라미터 수 | 클러스터 모양 |
|-------------------|-----------|-------------|
| `spherical` | $K \cdot d + K$ | 구형, 크기 다름 |
| `diag` | $K \cdot 2d + K$ | 축 정렬 타원 |
| `tied` | $d(d+1)/2 + Kd + K$ | 동일 타원 |
| `full` | $K \cdot d(d+1)/2 + Kd + K$ | 임의 타원 |

> **핵심 직관**: K-Means는 "가장 가까운 중심에 할당"이고, GMM은 "가장 높은 확률의 클러스터에 (소프트) 할당"입니다. 확률적 할당이 클러스터 경계가 모호한 경우 더 적절합니다.

---

## 6. EM의 수렴과 초기화

**EM의 한계**:

- 국소 최적에 수렴 (전역 최적 보장 없음)
- 수렴 속도가 느릴 수 있음 (선형 수렴)
- 초기값에 민감

**초기화 전략**:

| 전략 | 설명 |
|------|------|
| K-Means 초기화 | K-Means 결과로 시작 (sklearn 기본값) |
| 랜덤 초기화 | 여러 번 실행, 최적 선택 (`n_init`) |
| K-Means++ | 분산된 초기 중심 선택 |

**수렴 판단**: 로그 우도의 변화량

$$|\log \mathcal{L}(\boldsymbol{\theta}^{(t+1)}) - \log \mathcal{L}(\boldsymbol{\theta}^{(t)})| < \epsilon$$

**특이성(singularity) 문제**: 하나의 가우시안이 단일 데이터 포인트에 축소되면 $|\boldsymbol{\Sigma}_k| \to 0$으로 우도가 발산합니다. 해결책: 정규화 (`reg_covar` 파라미터)

```python
from sklearn.mixture import GaussianMixture

gmm = GaussianMixture(
    n_components=5,
    n_init=20,              # 20번 실행하여 최적 선택
    init_params='k-means++',
    reg_covar=1e-6,         # 공분산 정규화
    max_iter=300,
    tol=1e-4
)
gmm.fit(X)
print(f"수렴 여부: {gmm.converged_}")
print(f"로그 우도: {gmm.lower_bound_:.2f}")
```

> **핵심 직관**: EM은 "오르막만 걷는" 알고리즘으로, 어디서 출발하느냐에 따라 도착점이 다릅니다. 다중 초기화는 여러 출발점에서 시도하여 가장 높은 곳을 선택하는 전략입니다.

---

## 7. 모델 선택: 클러스터 수 결정

클러스터 수 $K$는 EM이 결정하지 못하는 하이퍼파라미터입니다.

**정보 기준** (cm-12에서 상세히 다룸):

$$\text{AIC} = -2\log \mathcal{L} + 2p$$

$$\text{BIC} = -2\log \mathcal{L} + p \log n$$

여기서 $p$는 파라미터 수, $n$은 데이터 수입니다. BIC는 더 강한 복잡도 벌점을 부여합니다.

| 기준 | $K$ 과소추정 경향 | $K$ 과대추정 경향 |
|------|----------------|----------------|
| AIC | 약함 | 중간 |
| BIC | 중간 | 약함 (일관성) |

```python
import numpy as np
from sklearn.mixture import GaussianMixture

# BIC로 최적 K 선택
bics = []
for k in range(1, 11):
    gmm = GaussianMixture(n_components=k, random_state=42)
    gmm.fit(X)
    bics.append(gmm.bic(X))

optimal_k = np.argmin(bics) + 1
print(f"최적 클러스터 수 (BIC): {optimal_k}")
```

> **핵심 직관**: 클러스터를 늘리면 우도는 항상 증가하지만, 불필요한 복잡도가 추가됩니다. BIC/AIC는 "우도 향상이 복잡도 증가를 정당화하는가?"에 답합니다.

---

## 핵심 정리

- **EM 알고리즘은 잠재 변수가 있는 모델에서 E-step(잠재 변수의 사후 분포 계산)과 M-step(파라미터 최대화)을 반복하여 로그 우도를 단조 증가시킵니다**
- **가우시안 혼합 모델은 데이터가 $K$개의 가우시안 분포의 혼합에서 생성되었다고 가정하며, K-Means의 확률적 일반화로 소프트 클러스터링과 밀도 추정을 제공합니다**
- **GMM의 E-step은 각 샘플의 클러스터 소속 확률(responsibility)을 계산하고, M-step은 가중 통계량으로 평균, 공분산, 혼합 비율을 갱신합니다**
- **EM은 국소 최적에 수렴하므로 다중 초기화가 필수이며, K-Means 초기화가 효과적인 시작점을 제공합니다**
- **클러스터 수 $K$는 BIC/AIC 같은 정보 기준으로 선택하며, 적합도와 모델 복잡도의 균형을 자동으로 맞춥니다**
