# 앙상블: Bagging과 Random Forest

## 왜 앙상블 학습이 필요한가

cm-04에서 확인한 것처럼, 단일 결정 트리는 높은 분산으로 인해 불안정합니다. 앙상블 학습은 여러 모델의 예측을 결합하여 개별 모델보다 더 안정적이고 정확한 예측을 달성합니다. 특히 Bagging은 부트스트랩 샘플링을 통해 분산을 줄이는 전략이며, Random Forest는 이를 결정 트리에 최적화한 가장 성공적인 알고리즘 중 하나입니다.

---

## 1. 앙상블의 수학적 기초

$M$개의 독립적인 모델 $h_1, \ldots, h_M$의 평균을 사용한다고 합시다.

**분산 감소의 원리**: 각 모델의 오차 분산이 $\sigma^2$이고, 모델 간 상관계수가 $\rho$이면:

$$\text{Var}\left(\frac{1}{M}\sum_{m=1}^M h_m\right) = \rho \sigma^2 + \frac{1-\rho}{M}\sigma^2$$

| 조건 | 앙상블 분산 | 의미 |
|------|-----------|------|
| $\rho = 1$ (완전 상관) | $\sigma^2$ | 분산 감소 없음 |
| $\rho = 0$ (독립) | $\sigma^2 / M$ | 최대 분산 감소 |
| $0 < \rho < 1$ (부분 상관) | $\rho\sigma^2 + \frac{(1-\rho)\sigma^2}{M}$ | 부분 감소 |

$M \to \infty$일 때 분산은 $\rho\sigma^2$로 수렴합니다. 따라서 **모델 간 상관을 줄이는 것**이 핵심입니다.

> **핵심 직관**: 앙상블의 효과는 개별 모델의 정확도보다 모델 간의 다양성(비상관성)에 더 크게 의존합니다. 동일한 모델 100개를 평균하면 아무 효과가 없습니다.

---

## 2. 부트스트랩과 Bagging

**부트스트랩(Bootstrap)**: $n$개의 데이터에서 복원 추출로 $n$개의 샘플을 뽑습니다.

각 부트스트랩 샘플에 포함되지 않을 확률:

$$P(\text{미포함}) = \left(1 - \frac{1}{n}\right)^n \approx e^{-1} \approx 0.368$$

따라서 각 부트스트랩 샘플은 원본의 약 **63.2%**의 고유 샘플을 포함합니다.

**Bagging (Bootstrap Aggregating)** 알고리즘:

1. $M$개의 부트스트랩 샘플 $D_1, \ldots, D_M$ 생성
2. 각 $D_m$에서 독립적으로 모델 $h_m$ 학습
3. 예측 시 결합:
   - 분류: 다수결 투표 $\hat{y} = \text{mode}(h_1(\mathbf{x}), \ldots, h_M(\mathbf{x}))$
   - 회귀: 평균 $\hat{y} = \frac{1}{M}\sum_{m=1}^M h_m(\mathbf{x})$

```python
from sklearn.ensemble import BaggingClassifier
from sklearn.tree import DecisionTreeClassifier

bagging = BaggingClassifier(
    estimator=DecisionTreeClassifier(max_depth=None),
    n_estimators=100,
    max_samples=1.0,      # 부트스트랩 샘플 크기 비율
    bootstrap=True,
    random_state=42,
    n_jobs=-1              # 병렬 학습
)
bagging.fit(X_train, y_train)
```

> **핵심 직관**: 부트스트랩은 하나의 데이터셋에서 "가상의 다수 데이터셋"을 생성합니다. 각 모델이 약간 다른 데이터를 보므로 서로 다른 결정 경계를 학습하고, 평균이 안정화됩니다.

---

## 3. Random Forest

Random Forest는 Bagging에 **피처 랜덤 서브스페이스(feature subspace)**를 추가합니다.

**핵심 변형**: 각 분할(split)에서 전체 $d$개 피처 중 $m$개만 무작위로 선택하고, 이 중 최적 분할을 찾습니다.

$$m = \begin{cases} \lfloor \sqrt{d} \rfloor & \text{(분류, 기본값)} \\ \lfloor d/3 \rfloor & \text{(회귀, 기본값)} \end{cases}$$

이 추가적 무작위성이 트리 간 상관 $\rho$를 더욱 줄여줍니다.

| 방법 | 데이터 랜덤화 | 피처 랜덤화 | 트리 상관도 |
|------|------------|-----------|-----------|
| 단일 트리 | 없음 | 없음 | — |
| Bagging | 부트스트랩 | 없음 | 높음 |
| Random Forest | 부트스트랩 | 분할마다 서브셋 | 낮음 |

```python
from sklearn.ensemble import RandomForestClassifier

rf = RandomForestClassifier(
    n_estimators=500,
    max_features='sqrt',    # 분류 기본값: sqrt(d)
    max_depth=None,         # 트리를 최대한 깊게 성장
    min_samples_leaf=1,
    bootstrap=True,
    oob_score=True,         # OOB 평가 활성화
    n_jobs=-1,
    random_state=42
)
rf.fit(X_train, y_train)
```

> **핵심 직관**: Random Forest에서 각 트리는 "다른 피처의 관점"에서 데이터를 봅니다. 강한 피처가 있어도 일부 트리에서는 가려지므로, 약한 피처의 정보도 활용됩니다.

---

## 4. OOB (Out-of-Bag) 평가

각 부트스트랩 샘플에서 제외된 약 36.8%의 데이터가 OOB 샘플입니다.

**OOB 오차 추정**: 각 샘플 $\mathbf{x}_i$에 대해, $\mathbf{x}_i$를 학습에 사용하지 않은 트리들만으로 예측합니다:

$$\hat{y}_i^{\text{OOB}} = \text{mode}\{h_m(\mathbf{x}_i) : i \notin D_m\}$$

$$\text{OOB Error} = \frac{1}{n}\sum_{i=1}^n \mathbb{1}[\hat{y}_i^{\text{OOB}} \neq y_i]$$

| 특성 | OOB 평가 | K-fold CV (cm-12 참조) |
|------|---------|----------------------|
| 추가 계산 | 거의 없음 | $K$배 학습 |
| 편향 | 약간 비관적 | $K$에 따라 다름 |
| 분산 | 낮음 | $K$에 따라 다름 |
| 데이터 사용 | 학습 중 자동 | 별도 분할 필요 |

```python
rf = RandomForestClassifier(n_estimators=500, oob_score=True)
rf.fit(X_train, y_train)
print(f"OOB 정확도: {rf.oob_score_:.4f}")
# OOB 점수는 교차 검증 점수와 매우 유사합니다
```

> **핵심 직관**: OOB 평가는 "공짜로 얻는 교차 검증"입니다. 부트스트랩의 부산물을 활용하므로 추가 학습 비용이 거의 없습니다.

---

## 5. 피처 중요도

Random Forest는 피처 중요도를 자연스럽게 제공합니다.

**불순도 기반 중요도(MDI, Mean Decrease in Impurity)**:

$$\text{Imp}(x_j) = \frac{1}{M}\sum_{m=1}^M \sum_{t \in T_m} \Delta I(t) \cdot \mathbb{1}[v(t) = j]$$

여기서 $\Delta I(t)$는 노드 $t$에서의 불순도 감소량, $v(t)$는 분할에 사용된 피처입니다.

**순열 중요도(Permutation Importance)**: 피처 $x_j$의 값을 무작위로 섞은 후 성능 저하 측정

$$\text{PI}(x_j) = \text{Score}_{\text{원본}} - \text{Score}_{\text{$x_j$ 순열}}$$

| 방법 | 장점 | 단점 |
|------|------|------|
| MDI | 학습 중 자동 계산 | 높은 기수 피처에 편향 |
| Permutation | 편향 적음, 모델 무관 | 추가 계산 필요 |

```python
from sklearn.inspection import permutation_importance

# 불순도 기반 중요도
mdi_importance = rf.feature_importances_

# 순열 중요도 (더 신뢰할 수 있음)
perm_imp = permutation_importance(rf, X_test, y_test, n_repeats=10)
print(f"순열 중요도 평균: {perm_imp.importances_mean}")
```

> **핵심 직관**: 피처 중요도는 "이 피처를 제거하면 성능이 얼마나 떨어지는가?"에 대한 답입니다. 순열 중요도가 MDI보다 실제 중요도를 더 정확히 반영합니다.

---

## 6. Random Forest의 하이퍼파라미터

| 파라미터 | 기본값 | 효과 | 튜닝 방향 |
|---------|-------|------|----------|
| `n_estimators` | 100 | 트리 수 | 많을수록 좋음 (수렴) |
| `max_features` | $\sqrt{d}$ | 분할 시 후보 피처 수 | 줄이면 다양성 증가 |
| `max_depth` | None | 트리 깊이 | 제한하면 과적합 방지 |
| `min_samples_leaf` | 1 | 리프 최소 샘플 | 높이면 과적합 방지 |
| `bootstrap` | True | 부트스트랩 사용 | 거의 항상 True |

**트리 수 수렴**: 일반적으로 100~500개에서 OOB 오차가 수렴합니다.

```python
import numpy as np
from sklearn.ensemble import RandomForestClassifier

# 트리 수에 따른 OOB 오차 수렴 관찰
oob_errors = []
for n in range(10, 510, 10):
    rf = RandomForestClassifier(n_estimators=n, oob_score=True, random_state=42)
    rf.fit(X_train, y_train)
    oob_errors.append(1 - rf.oob_score_)
# oob_errors를 시각화하면 수렴 패턴을 관찰할 수 있습니다
```

> **핵심 직관**: Random Forest는 트리 수를 늘려도 과적합하지 않습니다. 이는 Bagging의 평균화 효과 때문이며, cm-06의 Boosting과 대조적입니다.

---

## 7. Bagging의 한계와 Boosting으로의 전환

Bagging/Random Forest는 분산을 줄이지만 편향은 줄이지 못합니다.

**편향-분산 분해** (cm-12에서 상세히 다룸):

$$\text{MSE} = \text{Bias}^2 + \text{Variance} + \text{Noise}$$

| 전략 | 편향 효과 | 분산 효과 |
|------|----------|----------|
| Bagging | 변화 없음 | 감소 |
| 깊은 트리 사용 | 편향 낮음 | 분산 높음 (Bagging이 해결) |
| Boosting (cm-06) | 감소 | 증가 가능 |

Random Forest는 깊은 트리(낮은 편향, 높은 분산)를 사용하고, Bagging으로 분산을 줄입니다. 반면 cm-06에서 다룰 Boosting은 얕은 트리(높은 편향, 낮은 분산)를 순차적으로 결합하여 편향을 줄입니다.

```python
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier

# Random Forest: 깊은 트리의 평균
rf = RandomForestClassifier(n_estimators=500, max_depth=None)

# Gradient Boosting: 얕은 트리의 순차 결합 (cm-06에서 상세히)
gb = GradientBoostingClassifier(n_estimators=500, max_depth=3)
```

> **핵심 직관**: Random Forest가 "독립적 전문가들의 다수결"이라면, Boosting은 "순차적으로 실수를 교정하는 학습"입니다. 두 접근법은 편향-분산 트레이드오프의 서로 다른 측면을 공략합니다.

---

## 핵심 정리

- **앙상블의 분산 감소 효과는 개별 모델의 상관계수 $\rho$에 의해 결정되며, $\rho$를 줄이는 것이 성능 향상의 핵심입니다**
- **Bagging은 부트스트랩 샘플링으로 다양한 모델을 학습하고 평균하여 분산을 줄이며, 각 부트스트랩 샘플은 원본의 약 63.2%를 포함합니다**
- **Random Forest는 각 분할에서 $\sqrt{d}$개의 랜덤 피처만 고려하여 트리 간 상관을 추가로 줄이며, 트리 수를 늘려도 과적합하지 않습니다**
- **OOB 평가는 부트스트랩의 부산물로 교차 검증에 준하는 성능 추정을 추가 비용 없이 제공합니다**
- **순열 중요도는 피처의 기여도를 모델에 무관하게 측정하며, 불순도 기반 중요도(MDI)보다 높은 기수 피처에 대한 편향이 적습니다**
