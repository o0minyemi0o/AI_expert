# SVM 쌍대 문제와 커널 트릭

## 왜 쌍대 문제와 커널이 필요한가

cm-02에서 다룬 원초(primal) SVM은 입력 공간에서 선형 결정 경계만을 찾습니다. 쌍대(dual) 문제로 변환하면 데이터가 오직 내적을 통해서만 등장하게 되고, 이 내적을 커널 함수로 대체하면 명시적으로 고차원 매핑을 수행하지 않고도 비선형 결정 경계를 구현할 수 있습니다. 이것이 바로 "커널 트릭"이며, 계산 효율과 표현력을 동시에 달성하는 우아한 방법입니다.

---

## 1. 라그랑주 쌍대 문제 유도

cm-02의 원초 문제에서 출발합니다:

$$\min_{\mathbf{w}, b} \frac{1}{2}\|\mathbf{w}\|^2 \quad \text{s.t.} \quad y_i(\mathbf{w}^\top \mathbf{x}_i + b) \geq 1$$

라그랑주 함수:

$$\mathcal{L}(\mathbf{w}, b, \boldsymbol{\alpha}) = \frac{1}{2}\|\mathbf{w}\|^2 - \sum_{i=1}^n \alpha_i[y_i(\mathbf{w}^\top \mathbf{x}_i + b) - 1]$$

**1단계**: $\mathbf{w}$와 $b$에 대해 미분하여 0으로 놓습니다:

$$\frac{\partial \mathcal{L}}{\partial \mathbf{w}} = 0 \implies \mathbf{w} = \sum_{i=1}^n \alpha_i y_i \mathbf{x}_i$$

$$\frac{\partial \mathcal{L}}{\partial b} = 0 \implies \sum_{i=1}^n \alpha_i y_i = 0$$

**2단계**: 이를 대입하면 **쌍대 문제**를 얻습니다:

$$\max_{\boldsymbol{\alpha}} \sum_{i=1}^n \alpha_i - \frac{1}{2}\sum_{i,j} \alpha_i \alpha_j y_i y_j \mathbf{x}_i^\top \mathbf{x}_j$$

$$\text{s.t.} \quad \alpha_i \geq 0, \quad \sum_{i=1}^n \alpha_i y_i = 0$$

> **핵심 직관**: 쌍대 문제에서 데이터는 오직 내적 $\mathbf{x}_i^\top \mathbf{x}_j$의 형태로만 등장합니다. 이것이 커널 트릭의 문을 여는 핵심 관찰입니다.

---

## 2. 강쌍대성과 원초-쌍대 관계

**co-08에서 배운 강쌍대성 정리**에 의해, 원초 문제가 볼록이고 슬레이터 조건이 만족되면:

$$p^* = d^*$$

즉, 원초 최적값과 쌍대 최적값이 일치합니다.

| 비교 항목 | 원초 문제 | 쌍대 문제 |
|-----------|----------|----------|
| 변수 수 | $d + 1$ ($\mathbf{w}$와 $b$) | $n$ ($\boldsymbol{\alpha}$) |
| 유리한 경우 | $n \gg d$ | $d \gg n$ 또는 커널 사용 시 |
| 제약 조건 | $n$개 부등식 | 상자 제약 + 1개 등식 |

**소프트 마진의 쌍대 문제**: $0 \leq \alpha_i \leq C$ 제약이 추가됩니다:

$$\max_{\boldsymbol{\alpha}} \sum_{i=1}^n \alpha_i - \frac{1}{2}\sum_{i,j} \alpha_i \alpha_j y_i y_j \mathbf{x}_i^\top \mathbf{x}_j$$

$$\text{s.t.} \quad 0 \leq \alpha_i \leq C, \quad \sum_{i=1}^n \alpha_i y_i = 0$$

> **핵심 직관**: $\alpha_i = C$인 샘플은 마진 내부에 위치하거나 오분류된 샘플이며, $0 < \alpha_i < C$인 샘플이 마진 경계 위의 서포트 벡터입니다.

---

## 3. 커널 함수와 머서 조건

**커널 함수**는 고차원 특징 공간의 내적을 입력 공간에서 직접 계산합니다:

$$K(\mathbf{x}_i, \mathbf{x}_j) = \phi(\mathbf{x}_i)^\top \phi(\mathbf{x}_j)$$

여기서 $\phi: \mathbb{R}^d \to \mathcal{H}$는 고차원 (잠재적으로 무한차원) 특징 공간으로의 매핑입니다.

**머서 정리(Mercer's theorem)**: 함수 $K(\cdot, \cdot)$가 유효한 커널이 되려면, 모든 유한 데이터셋에 대해 그램 행렬 $\mathbf{K}_{ij} = K(\mathbf{x}_i, \mathbf{x}_j)$가 양의 준정부호(positive semi-definite)여야 합니다.

$$\sum_{i,j} c_i c_j K(\mathbf{x}_i, \mathbf{x}_j) \geq 0, \quad \forall c_i \in \mathbb{R}$$

**la-03에서 다룬 양의 정부호 행렬**의 개념이 여기서 직접 활용됩니다.

> **핵심 직관**: 커널 트릭은 $\phi$를 명시적으로 계산하지 않고도, $\phi$가 존재하는 것처럼 고차원 내적을 계산합니다. 무한차원 공간의 내적도 유한 시간에 계산 가능합니다.

---

## 4. 주요 커널 함수

| 커널 | $K(\mathbf{x}, \mathbf{z})$ | 특징 공간 차원 | 하이퍼파라미터 |
|------|---------------------------|--------------|-------------|
| 선형 | $\mathbf{x}^\top \mathbf{z}$ | $d$ | 없음 |
| 다항식 | $(\gamma \mathbf{x}^\top \mathbf{z} + r)^p$ | $\binom{d+p}{p}$ | $\gamma, r, p$ |
| RBF (가우시안) | $\exp(-\gamma\|\mathbf{x}-\mathbf{z}\|^2)$ | $\infty$ | $\gamma$ |
| 시그모이드 | $\tanh(\gamma \mathbf{x}^\top \mathbf{z} + r)$ | — | $\gamma, r$ |

**다항식 커널의 예시**: $d=2$, $p=2$인 경우

$$K(\mathbf{x}, \mathbf{z}) = (\mathbf{x}^\top \mathbf{z})^2 = (x_1 z_1 + x_2 z_2)^2$$

이는 다음 매핑의 내적과 동치입니다:

$$\phi(\mathbf{x}) = (x_1^2, \sqrt{2}x_1 x_2, x_2^2)^\top$$

**RBF 커널의 특성**:

$$K(\mathbf{x}, \mathbf{z}) = \exp(-\gamma\|\mathbf{x}-\mathbf{z}\|^2)$$

- $\gamma$ 큼: 가까운 점만 영향, 복잡한 경계 (과적합 위험)
- $\gamma$ 작음: 먼 점도 영향, 단순한 경계 (과소적합 위험)

```python
from sklearn.svm import SVC

# 주요 커널별 SVM
svm_linear = SVC(kernel='linear', C=1.0)
svm_poly = SVC(kernel='poly', degree=3, gamma='scale', coef0=1)
svm_rbf = SVC(kernel='rbf', gamma=0.1, C=10)
```

> **핵심 직관**: RBF 커널은 무한차원 특징 공간에 매핑하므로 이론적으로 어떤 결정 경계도 근사할 수 있습니다. 그러나 $\gamma$와 $C$의 조합이 실질적인 복잡도를 제어합니다.

---

## 5. 커널 SVM의 예측과 결정 경계

학습 후 새로운 입력 $\mathbf{x}$에 대한 예측:

$$f(\mathbf{x}) = \sum_{i \in \text{SV}} \alpha_i y_i K(\mathbf{x}_i, \mathbf{x}) + b$$

$$\hat{y} = \text{sign}(f(\mathbf{x}))$$

비선형 결정 경계 $f(\mathbf{x}) = 0$은 입력 공간에서 곡선(2D) 또는 곡면(3D 이상)이 됩니다.

```python
import numpy as np
from sklearn.svm import SVC

# XOR 문제: 선형 분리 불가, 커널 SVM으로 해결
X = np.array([[0,0],[0,1],[1,0],[1,1]])
y = np.array([0, 1, 1, 0])

svm_rbf = SVC(kernel='rbf', gamma=2.0)
svm_rbf.fit(X, y)
print(svm_rbf.predict([[0.5, 0.5]]))  # 경계 근처의 예측
```

**예측 시간 복잡도**: $O(n_{\text{SV}} \cdot d)$ 또는 커널에 따라 $O(n_{\text{SV}} \cdot d')$

> **핵심 직관**: 커널 SVM의 예측은 서포트 벡터와의 커널 값의 가중합으로, 서포트 벡터 수가 예측 속도를 결정합니다.

---

## 6. SMO 알고리즘

**Sequential Minimal Optimization (SMO)**은 SVM의 실용적 학습 알고리즘입니다.

핵심 아이디어: 한 번에 두 개의 $\alpha_i, \alpha_j$만 갱신하여, 등식 제약 $\sum \alpha_i y_i = 0$을 자동으로 만족시킵니다.

| 단계 | 연산 |
|------|------|
| 1. 작업 집합 선택 | KKT 위반이 큰 $\alpha_i, \alpha_j$ 선택 |
| 2. 부분 문제 풀기 | 2변수 QP의 해석적 해 |
| 3. 클리핑 | $[0, C]$ 범위로 투영 |
| 4. 수렴 확인 | KKT 조건 만족 여부 |

```python
from sklearn.svm import SVC

# libsvm은 내부적으로 SMO 변형을 사용합니다
svm = SVC(kernel='rbf', gamma='scale', C=1.0, tol=1e-3, max_iter=-1)
svm.fit(X_train, y_train)
print(f"서포트 벡터 수: {svm.n_support_}")
```

> **핵심 직관**: SMO는 큰 QP 문제를 해석적으로 풀 수 있는 미니 문제로 분해하며, 이것이 SVM을 실용적으로 학습 가능하게 만드는 핵심 알고리즘입니다.

---

## 7. 커널 선택과 하이퍼파라미터 튜닝

실무에서 커널 선택은 데이터의 성질에 따라 달라집니다.

| 상황 | 권장 커널 | 이유 |
|------|----------|------|
| 피처 수 $\gg$ 샘플 수 | 선형 | 이미 고차원, 과적합 방지 |
| 텍스트 분류 | 선형 | 높은 차원, 희소 데이터 |
| 이미지 (저차원) | RBF | 비선형 패턴 포착 |
| 도메인 지식 있음 | 커스텀 커널 | 유사도 함수 직접 설계 |

**하이퍼파라미터 그리드 서치**:

```python
from sklearn.model_selection import GridSearchCV
from sklearn.svm import SVC

param_grid = {
    'C': [0.01, 0.1, 1, 10, 100],
    'gamma': [0.001, 0.01, 0.1, 1, 10],
    'kernel': ['rbf']
}

grid = GridSearchCV(SVC(), param_grid, cv=5, scoring='accuracy')
grid.fit(X_train, y_train)
print(f"최적: C={grid.best_params_['C']}, gamma={grid.best_params_['gamma']}")
```

커널 조합도 가능합니다. 두 커널 $K_1, K_2$에 대해:
- $K_1 + K_2$: 유효한 커널
- $\alpha K_1$ ($\alpha > 0$): 유효한 커널
- $K_1 \cdot K_2$: 유효한 커널

이 성질은 cm-09에서 다룰 가우시안 프로세스의 커널 설계에도 동일하게 적용됩니다.

> **핵심 직관**: 커널 선택은 데이터의 유사도 구조에 대한 가정을 인코딩하는 것입니다. "어떤 데이터 쌍이 비슷한가?"에 대한 답이 커널입니다.

---

## 핵심 정리

- **라그랑주 쌍대 문제에서 데이터는 오직 내적 $\mathbf{x}_i^\top \mathbf{x}_j$로만 등장하며, 이것이 커널 트릭의 수학적 기반입니다**
- **커널 함수 $K(\mathbf{x}, \mathbf{z}) = \phi(\mathbf{x})^\top \phi(\mathbf{z})$는 고차원 특징 공간의 내적을 명시적 매핑 없이 계산하며, 머서 조건(양의 준정부호)을 만족해야 합니다**
- **RBF 커널은 무한차원 특징 공간에 매핑하여 이론적으로 임의의 결정 경계를 표현할 수 있으며, $\gamma$가 결정 경계의 복잡도를 제어합니다**
- **SMO 알고리즘은 쌍대 문제를 해석적으로 풀 수 있는 2변수 부분 문제로 분해하여, SVM의 실용적 학습을 가능하게 합니다**
- **커널 선택은 데이터의 유사도 구조에 대한 사전 가정을 인코딩하며, 교차 검증을 통해 커널과 하이퍼파라미터를 동시에 최적화해야 합니다**
