# 차원 축소: PCA와 커널 PCA

## 왜 차원 축소가 필요한가

cm-08에서 확인한 차원의 저주는 고차원 데이터에서 거리 기반 방법이 실패하는 근본적 이유였습니다. 차원 축소는 데이터의 본질적인 구조를 보존하면서 차원을 줄여, 계산 효율을 높이고 과적합을 방지하며 시각화를 가능하게 합니다. 주성분 분석(PCA)은 분산 최대화 원리에 기반한 가장 기본적이고 강력한 차원 축소 방법이며, 커널 PCA는 cm-03의 커널 트릭을 적용한 비선형 확장입니다.

---

## 1. 분산 최대화 관점

PCA의 목표: 데이터의 분산을 최대한 보존하는 방향을 찾습니다.

$d$차원 데이터 $\mathbf{x} \in \mathbb{R}^d$를 단위 벡터 $\mathbf{u}_1$에 투영했을 때의 분산:

$$\text{Var}(\mathbf{u}_1^\top \mathbf{x}) = \mathbf{u}_1^\top \mathbf{S} \mathbf{u}_1$$

여기서 $\mathbf{S} = \frac{1}{n}\sum_{i=1}^n (\mathbf{x}_i - \bar{\mathbf{x}})(\mathbf{x}_i - \bar{\mathbf{x}})^\top$은 공분산 행렬입니다.

**최적화 문제**:

$$\max_{\mathbf{u}_1} \mathbf{u}_1^\top \mathbf{S} \mathbf{u}_1 \quad \text{s.t.} \quad \|\mathbf{u}_1\| = 1$$

라그랑주 조건에 의해:

$$\mathbf{S} \mathbf{u}_1 = \lambda_1 \mathbf{u}_1$$

**la-03에서 다룬 고유값 문제**입니다. $\mathbf{u}_1$은 $\mathbf{S}$의 최대 고유값에 대응하는 고유벡터입니다.

> **핵심 직관**: 제1 주성분은 데이터의 "가장 큰 변동 방향"입니다. MNIST에서 이 방향은 밝기의 전체적 변화에 대응하는 경우가 많습니다.

---

## 2. 최소 재구성 오차 관점

동치인 관점: 투영 후 재구성 오차를 최소화합니다.

$k$개의 주성분 $\mathbf{U}_k = [\mathbf{u}_1, \ldots, \mathbf{u}_k]$로 투영/재구성:

$$\tilde{\mathbf{x}}_i = \bar{\mathbf{x}} + \mathbf{U}_k \mathbf{U}_k^\top (\mathbf{x}_i - \bar{\mathbf{x}})$$

**재구성 오차**:

$$\frac{1}{n}\sum_{i=1}^n \|\mathbf{x}_i - \tilde{\mathbf{x}}_i\|^2 = \sum_{j=k+1}^d \lambda_j$$

즉, 버려진 고유값의 합이 재구성 오차입니다.

**분산 보존 비율**:

$$\text{EVR}(k) = \frac{\sum_{j=1}^k \lambda_j}{\sum_{j=1}^d \lambda_j}$$

| 목표 | 기준 |
|------|------|
| 시각화 | $k = 2$ 또는 $3$ |
| 분산 95% 보존 | $\text{EVR}(k) \geq 0.95$ |
| 팔꿈치 방법 | $\lambda_k$의 급격한 감소 지점 |

```python
from sklearn.decomposition import PCA
import numpy as np

pca = PCA(n_components=0.95)  # 분산 95% 보존
X_reduced = pca.fit_transform(X)
print(f"원래 차원: {X.shape[1]}")
print(f"축소 차원: {X_reduced.shape[1]}")
print(f"설명된 분산 비율: {pca.explained_variance_ratio_}")
```

> **핵심 직관**: PCA는 "정보 손실을 최소화하면서 차원을 줄이는" 최적 선형 투영입니다. 분산이 큰 방향은 정보가 많고, 분산이 작은 방향은 노이즈에 가깝습니다.

---

## 3. PCA의 계산

**고유값 분해**: $\mathbf{S} = \mathbf{U} \boldsymbol{\Lambda} \mathbf{U}^\top$

**특이값 분해(SVD)**: 실무에서 더 안정적이고 효율적입니다.

중심화된 데이터 행렬 $\mathbf{X} \in \mathbb{R}^{n \times d}$의 SVD:

$$\mathbf{X} = \mathbf{U}_X \boldsymbol{\Sigma} \mathbf{V}^\top$$

공분산 행렬과의 관계:

$$\mathbf{S} = \frac{1}{n}\mathbf{X}^\top \mathbf{X} = \mathbf{V} \frac{\boldsymbol{\Sigma}^2}{n} \mathbf{V}^\top$$

따라서 주성분은 $\mathbf{V}$의 열벡터이고, 고유값은 $\lambda_j = \sigma_j^2 / n$입니다.

| 방법 | 복잡도 | 적합 상황 |
|------|-------|----------|
| 전체 SVD | $O(\min(nd^2, n^2d))$ | 정확한 PCA |
| 절단 SVD | $O(ndk)$ | $k \ll d$ |
| 랜덤화 SVD | $O(nd\log k)$ | 대규모 데이터 |
| 점진적 PCA | $O(ndk)$ | 메모리 제한 |

```python
from sklearn.decomposition import PCA, IncrementalPCA, TruncatedSVD

# 표준 PCA (SVD 기반)
pca = PCA(n_components=50, svd_solver='randomized')

# 희소 데이터 (TF-IDF 등)
tsvd = TruncatedSVD(n_components=50)

# 대규모 데이터 (배치 처리)
ipca = IncrementalPCA(n_components=50, batch_size=500)
```

> **핵심 직관**: PCA의 계산은 결국 **la-03의 SVD**입니다. 랜덤화 SVD는 대규모 행렬에서 상위 $k$개 특이값/벡터를 효율적으로 근사합니다.

---

## 4. PCA의 확률적 해석

PCA는 확률 모델로도 해석할 수 있습니다.

**확률적 PCA (Probabilistic PCA)**:

$$\mathbf{x} = \mathbf{W}\mathbf{z} + \boldsymbol{\mu} + \boldsymbol{\epsilon}$$

여기서 $\mathbf{z} \sim \mathcal{N}(\mathbf{0}, \mathbf{I}_k)$, $\boldsymbol{\epsilon} \sim \mathcal{N}(\mathbf{0}, \sigma^2 \mathbf{I}_d)$

이 모델의 MLE 해는 $\sigma^2 \to 0$일 때 PCA와 동치입니다.

| 장점 | 설명 |
|------|------|
| 결측치 처리 | EM으로 학습 가능 |
| 모델 비교 | 우도 기반 $k$ 선택 가능 |
| 생성 모델 | cm-07, cm-10의 프레임워크 |
| 베이즈 확장 | 자동 차원 결정 |

이는 cm-10에서 다룬 잠재 변수 모델의 연속 잠재 변수 버전입니다. 잠재 변수 $\mathbf{z}$가 데이터의 저차원 표현입니다.

> **핵심 직관**: 확률적 PCA는 "고차원 데이터가 저차원 잠재 변수에 노이즈가 추가되어 생성되었다"는 생성 모델입니다.

---

## 5. 커널 PCA

선형 PCA는 데이터의 선형 구조만 포착합니다. cm-03의 커널 트릭을 PCA에 적용합니다.

**아이디어**: 특징 공간 $\phi(\mathbf{x})$에서 PCA를 수행하되, 내적을 커널로 대체

특징 공간의 공분산 행렬의 고유값 문제를 커널 행렬로 변환합니다:

$$\mathbf{K} \boldsymbol{\alpha} = n\lambda \boldsymbol{\alpha}$$

여기서 $K_{ij} = k(\mathbf{x}_i, \mathbf{x}_j)$, $\boldsymbol{\alpha}$는 쌍대 변수입니다.

투영: $k$번째 주성분 점수

$$z_k(\mathbf{x}) = \sum_{i=1}^n \alpha_k^{(i)} k(\mathbf{x}_i, \mathbf{x})$$

```python
from sklearn.decomposition import KernelPCA

# RBF 커널 PCA
kpca = KernelPCA(
    n_components=2,
    kernel='rbf',
    gamma=0.04,
    fit_inverse_transform=True  # 근사 역변환
)
X_kpca = kpca.fit_transform(X)

# 다항식 커널 PCA
kpca_poly = KernelPCA(n_components=2, kernel='poly', degree=3)
```

| 비교 | 선형 PCA | 커널 PCA |
|------|---------|---------|
| 결정 경계 | 선형 부분공간 | 비선형 매니폴드 |
| 계산 | SVD: $O(nd^2)$ | 커널 행렬: $O(n^2d + n^3)$ |
| 메모리 | $O(nd)$ | $O(n^2)$ |
| 역변환 | 정확 | 근사 |
| 스케일링 | $n$ 또는 $d$ 큰 경우 | $n < 10{,}000$ 권장 |

> **핵심 직관**: 커널 PCA는 "곡면(매니폴드) 위의 데이터를 펼치는" 비선형 차원 축소입니다. 스위스 롤 데이터처럼 선형 PCA가 실패하는 경우에 효과적입니다.

---

## 6. PCA의 한계와 대안

| 한계 | 설명 | 대안 |
|------|------|------|
| 분산 = 중요성 가정 | 분산이 작은 방향이 중요할 수 있음 | 지도 차원 축소 (LDA) |
| 선형 | 비선형 구조 포착 불가 | 커널 PCA, t-SNE, UMAP |
| 전역 구조 | 지역 구조 무시 | LLE, Isomap |
| 직교 제약 | 주성분이 직교해야 함 | ICA (독립 성분 분석) |

**다른 차원 축소 기법과의 비교**:

| 기법 | 유형 | 보존 대상 | 활용 |
|------|------|----------|------|
| PCA | 선형 | 전역 분산 | 전처리, 노이즈 제거 |
| LDA | 선형, 지도 | 클래스 간 분리 | 분류 전처리 |
| t-SNE | 비선형 | 지역 이웃 구조 | 시각화 |
| UMAP | 비선형 | 전역 + 지역 | 시각화, 전처리 |

```python
from sklearn.manifold import TSNE
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis

# t-SNE: 시각화에 특화
tsne = TSNE(n_components=2, perplexity=30, random_state=42)
X_tsne = tsne.fit_transform(X)

# LDA: 클래스 정보를 활용한 차원 축소
lda = LinearDiscriminantAnalysis(n_components=2)
X_lda = lda.fit_transform(X, y)
```

> **핵심 직관**: PCA는 "가장 분산이 큰 방향"을 찾고, LDA는 "클래스를 가장 잘 분리하는 방향"을 찾습니다. t-SNE는 "고차원에서 가까운 점이 저차원에서도 가까운" 매핑을 찾습니다.

---

## 7. PCA의 실무 활용

**전처리로서의 PCA**: 다른 모델의 입력 차원을 줄여 성능과 효율을 개선합니다.

```python
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.svm import SVC

# 전형적인 파이프라인: 스케일링 → PCA → SVM
pipe = Pipeline([
    ('scaler', StandardScaler()),
    ('pca', PCA(n_components=0.95)),
    ('svm', SVC(kernel='rbf'))
])
pipe.fit(X_train, y_train)

# PCA 차원을 하이퍼파라미터로 튜닝
from sklearn.model_selection import GridSearchCV
param_grid = {
    'pca__n_components': [10, 30, 50, 100],
    'svm__C': [0.1, 1, 10]
}
grid = GridSearchCV(pipe, param_grid, cv=5)
```

**화이트닝(Whitening)**: 주성분의 분산을 1로 정규화

$$\mathbf{z}_i = \boldsymbol{\Lambda}^{-1/2} \mathbf{U}^\top (\mathbf{x}_i - \bar{\mathbf{x}})$$

화이트닝 후 공분산 행렬은 단위 행렬이 됩니다. cm-02의 SVM처럼 등방적 스케일링이 필요한 알고리즘에 유용합니다.

> **핵심 직관**: PCA + 화이트닝은 "데이터를 상관 없고 등분산인 좌표계로 변환"합니다. 이는 많은 알고리즘의 성능을 향상시키는 강력한 전처리입니다.

---

## 핵심 정리

- **PCA는 공분산 행렬의 고유벡터 방향으로 투영하여 분산을 최대한 보존하며, $k$개 주성분의 재구성 오차는 버려진 고유값의 합 $\sum_{j=k+1}^d \lambda_j$입니다**
- **PCA의 계산은 SVD로 수행되며, 랜덤화 SVD는 $O(nd\log k)$로 대규모 데이터에서 상위 $k$개 주성분을 효율적으로 근사합니다**
- **확률적 PCA는 잠재 변수 모델로 PCA를 해석하여 결측치 처리와 우도 기반 차원 선택을 가능하게 합니다**
- **커널 PCA는 커널 트릭을 통해 비선형 매니폴드 구조를 포착하지만, $O(n^2)$ 메모리와 $O(n^3)$ 계산이 필요합니다**
- **PCA는 분산 최대화 기반의 비지도 방법이므로, 분류 목적이라면 LDA가, 시각화 목적이라면 t-SNE/UMAP이 더 적합할 수 있습니다**
