# K-최근접 이웃과 거리 기반 학습

## 왜 거리 기반 학습을 이해해야 하는가

K-최근접 이웃(KNN)은 명시적인 모델을 학습하지 않고 예측 시점에 데이터 자체를 참조하는 비모수적 방법입니다. 모델 없이 데이터만으로 결정 경계를 형성하므로, 이론적으로 충분한 데이터가 있으면 어떤 결정 경계도 근사할 수 있습니다. 그러나 고차원에서의 거리가 무의미해지는 "차원의 저주"를 이해하는 것이 핵심이며, 이는 cm-13에서 다룰 차원 축소의 필요성으로 직결됩니다.

---

## 1. KNN 알고리즘

**KNN 분류**: 새로운 입력 $\mathbf{x}$에 대해

1. 학습 데이터에서 $\mathbf{x}$와 가장 가까운 $K$개의 이웃 $\mathcal{N}_K(\mathbf{x})$ 탐색
2. 이웃들의 다수결로 예측: $\hat{y} = \text{mode}\{y_i : \mathbf{x}_i \in \mathcal{N}_K(\mathbf{x})\}$

**KNN 회귀**: 이웃들의 평균값으로 예측

$$\hat{y} = \frac{1}{K}\sum_{\mathbf{x}_i \in \mathcal{N}_K(\mathbf{x})} y_i$$

| $K$ 값 | 결정 경계 | 편향 | 분산 |
|--------|----------|------|------|
| $K = 1$ | 가장 복잡 (보로노이) | 최소 | 최대 |
| $K$ 중간 | 적절히 매끄러움 | 적절 | 적절 |
| $K = n$ | 가장 단순 (다수 클래스) | 최대 | 최소 |

```python
from sklearn.neighbors import KNeighborsClassifier

knn = KNeighborsClassifier(
    n_neighbors=5,
    weights='uniform',     # 'distance'로 가중 가능
    metric='minkowski',
    p=2                    # 유클리드 거리
)
knn.fit(X_train, y_train)
```

> **핵심 직관**: KNN은 "가까운 데이터는 비슷한 레이블을 가진다"는 연속성 가정에 기반합니다. $K=1$이면 가장 가까운 하나의 이웃을 복사하고, $K$가 커지면 더 넓은 이웃의 합의를 구합니다.

---

## 2. 거리 측도

거리 측도의 선택은 KNN의 성능을 결정적으로 좌우합니다.

**Minkowski 거리** ($L_p$ 거리):

$$d_p(\mathbf{x}, \mathbf{z}) = \left(\sum_{j=1}^d |x_j - z_j|^p\right)^{1/p}$$

| $p$ | 이름 | 특성 |
|-----|------|------|
| 1 | 맨해튼 | 희소 데이터에 유리 |
| 2 | 유클리드 | 가장 일반적 |
| $\infty$ | 체비셰프 | $\max_j |x_j - z_j|$ |

**기타 거리 측도**:

| 거리 | 수식 | 적합한 데이터 |
|------|------|-------------|
| 코사인 유사도 | $1 - \frac{\mathbf{x}^\top \mathbf{z}}{\|\mathbf{x}\| \|\mathbf{z}\|}$ | 텍스트 (TF-IDF) |
| 마할라노비스 | $\sqrt{(\mathbf{x}-\mathbf{z})^\top \Sigma^{-1} (\mathbf{x}-\mathbf{z})}$ | 상관된 피처 |
| 해밍 | $\sum_j \mathbb{1}[x_j \neq z_j]$ | 범주형 |

**마할라노비스 거리**는 **la-03에서 다룬 공분산 행렬의 역행렬**을 사용하여 피처 간 상관을 고려합니다.

```python
from sklearn.neighbors import KNeighborsClassifier

# 다양한 거리 측도
knn_euclidean = KNeighborsClassifier(n_neighbors=5, metric='euclidean')
knn_manhattan = KNeighborsClassifier(n_neighbors=5, metric='manhattan')
knn_cosine = KNeighborsClassifier(n_neighbors=5, metric='cosine')
```

> **핵심 직관**: 유클리드 거리는 모든 피처가 동일한 스케일과 중요도를 가진다고 가정합니다. 피처 스케일링이 필수이며, cm-02의 SVM과 마찬가지로 `StandardScaler`를 먼저 적용해야 합니다.

---

## 3. 차원의 저주

고차원에서 거리 기반 방법은 근본적인 한계에 직면합니다.

**현상 1: 거리의 집중(concentration of distances)**

$d$차원 단위 초구(hypercube)에서 균일 분포된 점들 사이의 거리:

$$\frac{d_{\max} - d_{\min}}{d_{\min}} \to 0 \quad \text{as } d \to \infty$$

모든 점 사이의 거리가 거의 같아져, "최근접"의 의미가 사라집니다.

**현상 2: 데이터 희소성**

$K$개의 이웃을 확보하기 위해 필요한 공간의 비율:

$$\frac{K}{n}$$

에 해당하는 초구의 반지름은 $d$가 커질수록 전체 공간의 크기에 가까워집니다.

| 차원 $d$ | 이웃 비율 10%의 초구 반지름 비율 | 의미 |
|----------|------------------------------|------|
| 1 | 0.10 | 전체의 10% |
| 10 | 0.80 | 전체의 80% |
| 100 | 0.98 | 거의 전체 |

```python
import numpy as np

# 고차원에서 거리의 집중 현상 시연
for d in [2, 10, 100, 1000]:
    points = np.random.uniform(0, 1, (1000, d))
    dists = np.sqrt(np.sum((points[0] - points[1:])**2, axis=1))
    ratio = (dists.max() - dists.min()) / dists.min()
    # d가 커질수록 ratio가 0에 가까워짐
```

> **핵심 직관**: 차원이 높아지면 모든 점이 비슷하게 멀어져서 "최근접 이웃"이라는 개념 자체가 무의미해집니다. 이것이 cm-13에서 다룰 PCA 등 차원 축소가 필요한 근본적 이유입니다.

---

## 4. 가중 KNN과 변형

**거리 가중 KNN**: 가까운 이웃에 더 큰 가중치를 부여합니다.

$$\hat{y} = \arg\max_k \sum_{\mathbf{x}_i \in \mathcal{N}_K(\mathbf{x})} w_i \cdot \mathbb{1}[y_i = k]$$

여기서 $w_i = \frac{1}{d(\mathbf{x}, \mathbf{x}_i)^2}$ 또는 커널 함수를 사용합니다.

**커널 가중치 (Nadaraya-Watson 추정)**:

$$\hat{y}(\mathbf{x}) = \frac{\sum_{i=1}^n K_h(\mathbf{x}, \mathbf{x}_i) y_i}{\sum_{i=1}^n K_h(\mathbf{x}, \mathbf{x}_i)}$$

여기서 $K_h$는 대역폭 $h$의 커널 함수입니다. 이는 cm-03에서 다룬 RBF 커널과 관련됩니다.

| 가중 방식 | 장점 | 단점 |
|-----------|------|------|
| 균등 (uniform) | 단순, 해석 용이 | 먼 이웃도 동일 영향 |
| 거리 역수 | 가까운 이웃 강조 | $K$ 선택에 덜 민감 |
| 커널 가중 | 매끄러운 경계 | 대역폭 선택 필요 |

```python
from sklearn.neighbors import KNeighborsClassifier

knn_weighted = KNeighborsClassifier(
    n_neighbors=10,
    weights='distance'  # 거리 역수 가중
)
knn_weighted.fit(X_train, y_train)
```

> **핵심 직관**: 거리 가중 KNN에서 $K$를 크게 설정해도, 먼 이웃의 영향은 자동으로 줄어듭니다. 따라서 $K$ 선택에 덜 민감해집니다.

---

## 5. 효율적인 이웃 탐색

대규모 데이터에서 모든 쌍의 거리를 계산하는 것은 $O(nd)$로 비용이 큽니다.

**공간 분할 자료구조**:

| 자료구조 | 구축 시간 | 질의 시간 (평균) | 적합 조건 |
|---------|----------|----------------|----------|
| 브루트 포스 | $O(1)$ | $O(nd)$ | 소규모 데이터 |
| KD-Tree | $O(dn \log n)$ | $O(d \log n)$ | 저차원 ($d < 20$) |
| Ball Tree | $O(dn \log n)$ | $O(d \log n)$ | 일반 거리 측도 |
| LSH | $O(n)$ | $O(1)$ 근사 | 고차원, 근사 허용 |

**KD-Tree**: 각 노드에서 하나의 차원을 기준으로 공간을 이분합니다. 저차원에서 매우 효율적이지만, $d > 20$이면 브루트 포스와 유사해집니다.

```python
from sklearn.neighbors import KNeighborsClassifier

# 알고리즘 자동 선택
knn_auto = KNeighborsClassifier(n_neighbors=5, algorithm='auto')

# 명시적 알고리즘 선택
knn_kd = KNeighborsClassifier(n_neighbors=5, algorithm='kd_tree')
knn_ball = KNeighborsClassifier(n_neighbors=5, algorithm='ball_tree')
```

> **핵심 직관**: KD-Tree는 저차원에서 이웃 탐색을 $O(\log n)$으로 가속하지만, 차원의 저주에 의해 고차원에서는 효과가 사라집니다.

---

## 6. KNN의 이론적 보장

**Cover-Hart 정리**: $n \to \infty$일 때 1-NN의 오류율 $R_{1\text{NN}}$은:

$$R^* \leq R_{1\text{NN}} \leq 2R^* - \frac{K}{K-1}(R^*)^2 \leq 2R^*$$

여기서 $R^*$는 베이즈 최적 오류율입니다. 즉, 1-NN의 오류율은 최적의 2배를 넘지 않습니다.

**$K$-NN의 보편적 일관성**: $K \to \infty$이고 $K/n \to 0$이면:

$$R_{K\text{NN}} \to R^* \quad \text{as } n \to \infty$$

| 이론적 결과 | 조건 | 의미 |
|------------|------|------|
| Cover-Hart | $n \to \infty$, $K=1$ | $R \leq 2R^*$ |
| Stone | $K \to \infty$, $K/n \to 0$ | $R \to R^*$ (일관성) |
| 수렴 속도 | $d$ 차원 | $O(n^{-2/(d+2)})$ |

수렴 속도 $O(n^{-2/(d+2)})$에서 $d$가 커지면 수렴이 극도로 느려집니다. 이것이 차원의 저주의 이론적 표현입니다.

> **핵심 직관**: KNN은 이론적으로 보편적 일관성을 갖지만, 실질적인 수렴 속도는 차원에 지수적으로 의존합니다. 무한 데이터는 존재하지 않으므로, 차원 축소가 필수입니다.

---

## 7. 실무 가이드와 확장

| 고려 사항 | 권장 |
|-----------|------|
| 피처 스케일링 | 필수 (`StandardScaler` 또는 `MinMaxScaler`) |
| $K$ 선택 | 홀수 (동점 방지), 교차 검증 (cm-12) |
| 결측치 | 거리 계산 전 처리 필요 |
| 불균형 데이터 | 거리 가중 또는 클래스 비율 조정 |
| 고차원 | 차원 축소 후 적용 (cm-13) |

**KNN의 위치**: KNN은 모델 학습이 없어 "게으른 학습기(lazy learner)"라고 불립니다. 전체 학습 데이터를 메모리에 보관해야 하므로, 대규모 데이터에서는 비실용적일 수 있습니다.

```python
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.neighbors import KNeighborsClassifier

# 실무 파이프라인: 스케일링 → 차원 축소 → KNN
pipe = Pipeline([
    ('scaler', StandardScaler()),
    ('pca', PCA(n_components=50)),  # cm-13에서 상세히 다룸
    ('knn', KNeighborsClassifier(n_neighbors=5, weights='distance'))
])
pipe.fit(X_train, y_train)
```

MNIST에서 KNN은 약 97%의 정확도를 달성하며, 이는 단순한 알고리즘 치고 놀라운 성능이지만 예측 시간이 매우 느립니다.

> **핵심 직관**: KNN은 "모든 데이터를 기억하고 필요할 때 참조하는" 방법입니다. 학습은 즉시이지만 예측이 느리며, 이는 모델 기반 방법의 정반대입니다.

---

## 핵심 정리

- **KNN은 명시적 모델 없이 이웃의 다수결/평균으로 예측하며, $K$가 편향-분산 트레이드오프를 제어합니다 ($K$ 작으면 낮은 편향/높은 분산)**
- **거리 측도의 선택(유클리드, 맨해튼, 코사인, 마할라노비스)은 "유사성"의 정의를 결정하며, 피처 스케일링이 필수입니다**
- **차원의 저주에 의해 고차원에서는 모든 점 사이의 거리가 비슷해져 KNN의 판별력이 소실되며, 필요 데이터량이 차원에 지수적으로 증가합니다**
- **KD-Tree와 Ball Tree는 저차원에서 이웃 탐색을 $O(\log n)$으로 가속하지만, 고차원에서는 효과가 사라집니다**
- **Cover-Hart 정리에 의해 1-NN의 오류율은 베이즈 최적의 2배를 넘지 않으며, 적절한 $K$를 선택하면 보편적 일관성을 갖지만 수렴 속도는 차원에 지수적으로 의존합니다**
