# 고전 ML의 현대적 위치

## 왜 딥러닝 시대에 고전 ML이 중요한가

딥러닝이 이미지, 자연어 처리 등에서 혁명을 일으켰지만, 정형(tabular) 데이터에서는 여전히 XGBoost와 LightGBM 같은 그래디언트 부스팅이 최고의 성능을 보입니다. 또한 해석가능성, 소량 데이터, 계산 제약 등의 실무 조건에서 고전 ML은 대체 불가능한 위치를 차지합니다. 이 강의에서는 cm-01부터 cm-13까지 다룬 고전 ML의 전체 지도를 조망하고, 현대적 맥락에서의 역할을 정리합니다.

---

## 1. 정형 데이터에서의 고전 ML 우위

**Kaggle 대회 분석** (2015-2024): 정형 데이터 대회에서 우승 모델의 분포

| 모델 유형 | 우승 비율 (정형 데이터) | 우승 비율 (비정형 데이터) |
|-----------|---------------------|----------------------|
| Gradient Boosting (XGB/LGBM/CatBoost) | ~70% | ~5% |
| 딥러닝 | ~10% | ~85% |
| 앙상블 (혼합) | ~15% | ~10% |
| 기타 고전 ML | ~5% | ~0% |

**왜 정형 데이터에서 딥러닝이 약한가?**

- 정형 데이터는 이미지와 달리 공간적 구조가 없어 CNN의 이점 없음
- 피처 수가 적고 (수십~수백), 피처 간 이질적 (범주형 + 연속형 혼합)
- 트리 기반 모델은 축 정렬 분할이 피처 이질성에 자연스러움
- 결정 트리의 가지치기가 불규칙한 결정 경계에 효과적

```python
# 정형 데이터 경쟁에서의 전형적 파이프라인
import xgboost as xgb
import lightgbm as lgb

# XGBoost
xgb_model = xgb.XGBClassifier(
    n_estimators=1000, max_depth=6, learning_rate=0.05,
    subsample=0.8, colsample_bytree=0.8, reg_alpha=0.1
)

# LightGBM
lgb_model = lgb.LGBMClassifier(
    n_estimators=1000, max_depth=-1, num_leaves=63,
    learning_rate=0.05, subsample=0.8, colsample_bytree=0.8
)
```

> **핵심 직관**: 정형 데이터에서 고전 ML(특히 그래디언트 부스팅)이 딥러닝을 압도하는 현상은 "데이터의 구조적 성질에 맞는 귀납적 편향"의 중요성을 보여줍니다.

---

## 2. XGBoost, LightGBM, CatBoost

cm-06에서 다룬 Gradient Boosting의 현대적 구현들입니다.

**XGBoost의 핵심 혁신**:

정규화된 목적 함수:

$$\mathcal{L}^{(m)} = \sum_{i=1}^n l(y_i, \hat{y}_i^{(m-1)} + f_m(\mathbf{x}_i)) + \Omega(f_m)$$

$$\Omega(f) = \gamma T + \frac{1}{2}\lambda \sum_{j=1}^T w_j^2$$

여기서 $T$는 리프 수, $w_j$는 리프 가중치입니다. 2차 테일러 근사를 사용합니다:

$$\mathcal{L}^{(m)} \approx \sum_{i=1}^n \left[g_i f_m(\mathbf{x}_i) + \frac{1}{2}h_i f_m^2(\mathbf{x}_i)\right] + \Omega(f_m)$$

여기서 $g_i = \partial l / \partial \hat{y}^{(m-1)}$, $h_i = \partial^2 l / \partial (\hat{y}^{(m-1)})^2$

| 비교 | XGBoost | LightGBM | CatBoost |
|------|---------|----------|----------|
| 트리 성장 | Level-wise | Leaf-wise | Oblivious |
| 범주형 처리 | 인코딩 필요 | 내장 지원 | 최적 내장 |
| 속도 | 빠름 | 매우 빠름 | 중간 |
| 메모리 | 중간 | 적음 | 많음 |
| 분산 처리 | Spark/Dask | 내장 | 제한적 |

**LightGBM의 핵심 혁신**:
- **Leaf-wise 성장**: 가장 손실 감소가 큰 리프를 분할 (깊이 제한 대신 리프 수 제한)
- **GOSS**: 큰 기울기 샘플 우선 사용
- **EFB**: 상호 배타적 피처 묶기

```python
import lightgbm as lgb

lgb_model = lgb.LGBMClassifier(
    boosting_type='gbdt',
    num_leaves=31,           # leaf-wise 성장의 핵심 파라미터
    learning_rate=0.05,
    n_estimators=1000,
    min_child_samples=20,
    subsample=0.8,
    colsample_bytree=0.8,
    reg_alpha=0.1,
    reg_lambda=0.1,
    importance_type='gain'
)
```

> **핵심 직관**: XGBoost의 성공은 cm-06의 Gradient Boosting에 2차 근사와 트리 정규화를 결합한 데 있습니다. LightGBM은 계산 효율을, CatBoost는 범주형 피처 처리를 혁신했습니다.

---

## 3. 해석가능성과 설명가능 AI

고전 ML의 큰 장점 중 하나는 해석가능성입니다.

| 해석가능성 수준 | 모델 | 설명 방식 |
|--------------|------|----------|
| 본질적 해석 가능 | 선형 회귀, 결정 트리 | 계수, 분할 규칙 직접 확인 |
| 피처 중요도 | Random Forest, GBDT | cm-05의 MDI, 순열 중요도 |
| 사후 설명 | 모든 모델 | SHAP, LIME |

**SHAP (SHapley Additive exPlanations)**: 게임 이론의 섀플리 값에 기반한 통합적 설명 방법

$$\phi_j = \sum_{S \subseteq F \setminus \{j\}} \frac{|S|!(|F|-|S|-1)!}{|F|!} [f(S \cup \{j\}) - f(S)]$$

각 피처 $j$의 기여도 $\phi_j$는 "이 피처가 없을 때와 있을 때의 예측 차이"의 가중 평균입니다.

```python
import shap

# 트리 기반 모델에 최적화된 SHAP
explainer = shap.TreeExplainer(lgb_model)
shap_values = explainer.shap_values(X_test)

# 전역 피처 중요도
shap.summary_plot(shap_values, X_test)

# 개별 예측 설명
shap.force_plot(explainer.expected_value, shap_values[0], X_test.iloc[0])
```

**LIME (Local Interpretable Model-agnostic Explanations)**: 예측 주변에서 선형 모델로 근사

| 비교 | SHAP | LIME |
|------|------|------|
| 이론적 근거 | 섀플리 값 (유일한 공정 분배) | 국소 선형 근사 |
| 일관성 | 보장됨 | 보장 안 됨 |
| 계산 비용 | 트리 모델: 빠름, 일반: 느림 | 중간 |
| 전역 해석 | 가능 | 불가 (국소만) |

> **핵심 직관**: 의료, 금융, 법률 분야에서는 "왜 이런 결정을 내렸는가?"에 답해야 합니다. 해석가능성은 기술적 요구가 아니라 사회적 요구이며, 고전 ML이 이 요구에 더 잘 대응합니다.

---

## 4. 소량 데이터에서의 고전 ML

딥러닝은 대량의 데이터가 필요하지만, 실무에서는 데이터가 수백~수천 개인 경우가 흔합니다.

| 데이터 크기 | 권장 모델 | 이유 |
|-----------|----------|------|
| $n < 100$ | 로지스틱 회귀, 나이브 베이즈 (cm-07) | 낮은 분산 |
| $100 < n < 1{,}000$ | SVM (cm-02), Random Forest (cm-05) | 적절한 복잡도 |
| $1{,}000 < n < 10{,}000$ | GBDT, 커널 SVM | 비선형 패턴 |
| $n > 10{,}000$ | GBDT, 딥러닝 가능 | 충분한 데이터 |
| $n > 100{,}000$ | 딥러닝, 선형 모델, GBDT | 복잡한 패턴 학습 |

cm-07에서 다룬 Ng & Jordan의 결과를 기억하면, 생성 모델(나이브 베이즈)은 소량 데이터에서 판별 모델보다 빠르게 수렴합니다.

```python
from sklearn.model_selection import learning_curve

# 소량 데이터에서 모델별 학습 곡선 비교
models = {
    'Logistic': LogisticRegression(),
    'SVM-RBF': SVC(kernel='rbf'),
    'RF': RandomForestClassifier(n_estimators=100),
    'GBDT': GradientBoostingClassifier(n_estimators=100)
}

for name, model in models.items():
    sizes, train_s, val_s = learning_curve(model, X, y, cv=5,
        train_sizes=np.linspace(0.1, 1.0, 10))
    # 소량 데이터에서 단순 모델이 복잡한 모델을 이깁니다
```

> **핵심 직관**: 데이터 양에 맞는 모델 복잡도를 선택하는 것이 핵심입니다. cm-12의 편향-분산 트레이드오프에서, 데이터가 적으면 편향이 약간 높더라도 분산이 낮은 모델이 유리합니다.

---

## 5. 고전 ML과 딥러닝의 통합

현대 실무에서는 고전 ML과 딥러닝을 결합하는 경우가 많습니다.

**스태킹(Stacking)**: 여러 모델의 예측을 피처로 사용

```python
from sklearn.ensemble import StackingClassifier

stacking = StackingClassifier(
    estimators=[
        ('rf', RandomForestClassifier(n_estimators=100)),
        ('svm', SVC(kernel='rbf', probability=True)),
        ('lgbm', lgb.LGBMClassifier(n_estimators=200))
    ],
    final_estimator=LogisticRegression(),
    cv=5
)
```

**딥러닝 임베딩 + 고전 ML**:
- 이미지/텍스트 → 딥러닝으로 피처 추출
- 추출된 피처 → GBDT로 최종 분류

| 통합 방식 | 설명 | 활용 예 |
|-----------|------|--------|
| 스태킹 | 다수 모델의 예측을 2차 학습 | Kaggle 우승 솔루션 |
| 피처 추출 + GBDT | 딥러닝 임베딩을 GBDT에 입력 | 멀티모달 데이터 |
| 지식 증류 | 딥러닝 → 해석 가능 모델 | 배포 시 경량화 |
| 규칙 추출 | 블랙박스 → 결정 트리 | 규제 준수 |

> **핵심 직관**: 고전 ML과 딥러닝은 경쟁 관계가 아니라 상호보완 관계입니다. 각자의 강점을 결합하면 단일 모델보다 더 강력한 시스템을 구축할 수 있습니다.

---

## 6. 산업별 고전 ML의 위치

| 산업 | 주요 모델 | 선택 이유 |
|------|----------|----------|
| 금융 (신용 평가) | 로지스틱 회귀, GBDT | 규제, 해석가능성 |
| 의료 (진단) | Random Forest, 베이즈 네트워크 | 불확실성, 소량 데이터 |
| 제조 (품질 관리) | SVM, Isolation Forest | 실시간, 이상 탐지 |
| 마케팅 | GBDT, 클러스터링 | 정형 데이터, 피처 중요도 |
| 자연과학 | GP (cm-09), 선형 모델 | 불확실성, 인과 해석 |
| 추천 시스템 | 행렬 분해, GBDT + 딥러닝 | 희소 데이터 |

**실무에서의 모델 선택 플로우차트**:

1. 데이터가 정형인가? → GBDT 우선 시도
2. 해석가능성이 필요한가? → 선형 모델, 결정 트리
3. 불확실성 정량화가 필요한가? → GP (cm-09), 베이즈 방법
4. 데이터가 적은가? → 단순 모델, 정규화 강하게
5. 계산 자원이 제한적인가? → 선형 모델, 나이브 베이즈

> **핵심 직관**: 모델 선택의 첫 번째 기준은 정확도가 아니라 "문제의 제약 조건"입니다. 해석가능성, 계산 비용, 데이터 양, 배포 환경 등이 정확도보다 중요한 경우가 많습니다.

---

## 7. 전체 과정 복습: cm-01에서 cm-14까지

| 강의 | 핵심 아이디어 | 현대적 활용 |
|------|-------------|-----------|
| cm-01 | 선형 분류, 확률적 해석 | 로지스틱 회귀는 여전히 베이스라인 |
| cm-02 | 마진 최대화 | SVM 이론은 커널 방법의 기초 |
| cm-03 | 커널 트릭 | GP, 커널 PCA 등 다양한 커널 방법 |
| cm-04 | 결정 트리, 정보 이론 | GBDT의 기본 구성 요소 |
| cm-05 | Bagging, Random Forest | 안정적 베이스라인, 피처 중요도 |
| cm-06 | Boosting | XGBoost/LightGBM의 이론적 기반 |
| cm-07 | 나이브 베이즈, 생성 모델 | 텍스트 분류 베이스라인, 생성 AI 기초 |
| cm-08 | KNN, 차원의 저주 | 유사도 기반 검색, 임베딩 공간 |
| cm-09 | 가우시안 프로세스 | 베이즈 최적화, 불확실성 정량화 |
| cm-10 | EM, GMM | 클러스터링, 잠재 변수 모델 |
| cm-11 | 그래프 모델 | 인과 추론, 구조적 모델링 |
| cm-12 | 모델 선택, CV | 모든 ML 프로젝트의 필수 |
| cm-13 | PCA, 커널 PCA | 전처리, 시각화, 노이즈 제거 |
| cm-14 | 현대적 위치 | 실무 적용의 지침 |

```python
# 전체 파이프라인 예시: 현대적 고전 ML
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.model_selection import cross_val_score
import lightgbm as lgb

# 피처 유형별 전처리
preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), numerical_features),
        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)
    ])

# 파이프라인: 전처리 → GBDT
pipe = Pipeline([
    ('preprocess', preprocessor),
    ('model', lgb.LGBMClassifier(n_estimators=500, learning_rate=0.05))
])

# 비편향 성능 추정 (cm-12)
scores = cross_val_score(pipe, X, y, cv=5, scoring='roc_auc')
print(f"ROC-AUC: {scores.mean():.4f} (+/- {scores.std():.4f})")
```

> **핵심 직관**: 고전 ML의 각 알고리즘은 독립적이 아니라, 공통된 수학적 원리(최적화, 확률, 선형대수)로 연결된 하나의 체계입니다. 이 체계를 이해하면 새로운 문제에 적합한 모델을 설계할 수 있습니다.

---

## 핵심 정리

- **정형 데이터에서 XGBoost/LightGBM 같은 그래디언트 부스팅은 딥러닝을 포함한 모든 모델 중 최고의 성능을 보이며, 피처 이질성과 축 정렬 분할이 그 이유입니다**
- **SHAP과 LIME은 모델 해석을 가능하게 하며, 특히 SHAP의 섀플리 값은 이론적으로 유일한 공정한 피처 기여도 분배입니다**
- **소량 데이터($n < 1{,}000$)에서는 낮은 분산의 단순 모델(로지스틱 회귀, SVM, 나이브 베이즈)이 복잡한 모델보다 우수하며, 이는 편향-분산 트레이드오프의 직접적 결과입니다**
- **고전 ML과 딥러닝은 스태킹, 피처 추출, 지식 증류 등을 통해 통합되어 단일 모델보다 강력한 시스템을 구축합니다**
- **모델 선택의 기준은 정확도뿐 아니라 해석가능성, 계산 비용, 데이터 양, 배포 환경 등 문제의 제약 조건에 의해 결정되며, 이것이 고전 ML의 지속적 가치를 보장합니다**
