# 모델 선택과 교차 검증

## 왜 모델 선택이 중요한가

지금까지 다룬 모든 알고리즘에는 하이퍼파라미터가 있습니다: SVM의 $C$와 $\gamma$ (cm-02, cm-03), Random Forest의 트리 수 (cm-05), Gradient Boosting의 학습률 (cm-06). 최적의 하이퍼파라미터를 선택하는 것은 모델의 일반화 성능을 좌우하며, 이를 위해서는 편향-분산 트레이드오프의 이론적 이해와 교차 검증의 올바른 적용이 필수적입니다.

---

## 1. 편향-분산 분해

예측 오차를 세 성분으로 분해할 수 있습니다.

고정된 입력 $\mathbf{x}$에서, 학습 데이터 $D$의 변동에 대해:

$$\mathbb{E}_D[(y - \hat{f}(\mathbf{x}))^2] = \text{Bias}^2[\hat{f}(\mathbf{x})] + \text{Var}[\hat{f}(\mathbf{x})] + \sigma^2$$

| 성분 | 정의 | 의미 |
|------|------|------|
| $\text{Bias}^2$ | $(\mathbb{E}_D[\hat{f}] - f)^2$ | 모델의 체계적 오차 |
| $\text{Var}$ | $\mathbb{E}_D[(\hat{f} - \mathbb{E}_D[\hat{f}])^2]$ | 학습 데이터 변동에 대한 민감도 |
| $\sigma^2$ | 돌이킬 수 없는 노이즈 | 줄일 수 없는 오차 |

**모델 복잡도에 따른 변화**:

| 복잡도 | 편향 | 분산 | 총 오차 |
|--------|------|------|--------|
| 과소적합 | 높음 | 낮음 | 높음 |
| 최적 | 적절 | 적절 | 최소 |
| 과적합 | 낮음 | 높음 | 높음 |

```python
# 다항 회귀에서 편향-분산 시각화
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.pipeline import Pipeline

# 차수 1: 높은 편향, 낮은 분산 (과소적합)
# 차수 15: 낮은 편향, 높은 분산 (과적합)
# 차수 3~5: 최적 균형
```

> **핵심 직관**: 편향-분산 트레이드오프는 "단순함과 정확함 사이의 긴장"입니다. cm-05의 Random Forest는 분산을 줄이고, cm-06의 Boosting은 편향을 줄이는 전략이었습니다.

---

## 2. 학습 곡선과 검증 곡선

**학습 곡선(Learning Curve)**: 학습 데이터 크기 $n$에 따른 오차

- 학습 오차: $n$ 증가 시 증가 (암기 어려움)
- 검증 오차: $n$ 증가 시 감소 (일반화 향상)
- 두 곡선의 갭이 크면 → 분산이 높음 (과적합)
- 두 곡선이 모두 높으면 → 편향이 높음 (과소적합)

**검증 곡선(Validation Curve)**: 하이퍼파라미터 값에 따른 오차

```python
from sklearn.model_selection import learning_curve, validation_curve
from sklearn.svm import SVC

# 학습 곡선
train_sizes, train_scores, val_scores = learning_curve(
    SVC(kernel='rbf', gamma=0.1), X, y, cv=5,
    train_sizes=np.linspace(0.1, 1.0, 10)
)

# 검증 곡선 (C에 대해)
param_range = np.logspace(-3, 3, 7)
train_scores, val_scores = validation_curve(
    SVC(kernel='rbf'), X, y, param_name='C',
    param_range=param_range, cv=5
)
```

> **핵심 직관**: 학습 곡선은 "데이터가 더 필요한가?"에 답하고, 검증 곡선은 "하이퍼파라미터가 적절한가?"에 답합니다.

---

## 3. K-Fold 교차 검증

**홀드아웃의 한계**: 데이터 분할에 따라 결과가 크게 달라질 수 있습니다.

**K-Fold CV**: 데이터를 $K$개 폴드로 나누고, 각 폴드를 한 번씩 검증 세트로 사용합니다.

$$\text{CV}_K = \frac{1}{K}\sum_{k=1}^K L(\hat{f}^{(-k)}, D_k)$$

여기서 $\hat{f}^{(-k)}$는 $k$번째 폴드를 제외하고 학습한 모델입니다.

| $K$ | 학습 데이터 비율 | 편향 | 분산 | 계산 비용 |
|-----|--------------|------|------|----------|
| 2 | 50% | 높음 | 낮음 | 낮음 |
| 5 | 80% | 중간 | 중간 | 중간 |
| 10 | 90% | 낮음 | 약간 높음 | 높음 |
| $n$ (LOO) | $(n-1)/n$ | 최소 | 높음 | 매우 높음 |

**Stratified K-Fold**: 각 폴드에서 클래스 비율을 유지합니다 (분류 문제의 기본값).

```python
from sklearn.model_selection import cross_val_score, StratifiedKFold

cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
scores = cross_val_score(model, X, y, cv=cv, scoring='accuracy')
print(f"CV 점수: {scores.mean():.4f} (+/- {scores.std():.4f})")
```

> **핵심 직관**: K-Fold CV는 "데이터의 모든 부분이 한 번씩 검증에 사용되므로" 홀드아웃보다 안정적인 성능 추정을 제공합니다. $K=5$ 또는 $K=10$이 일반적인 선택입니다.

---

## 4. 교차 검증의 주의사항

**데이터 누출(Data Leakage)**: 검증 데이터의 정보가 학습에 포함되면 성능을 과대추정합니다.

| 올바른 사용 | 잘못된 사용 |
|-----------|-----------|
| 전처리를 CV 루프 안에서 | 전처리를 CV 전에 전체 데이터로 |
| `Pipeline` 사용 | 별도의 스케일링 후 CV |
| 시계열은 시간순 분할 | 시계열에 랜덤 분할 |

```python
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.model_selection import cross_val_score

# 올바른 방법: Pipeline 안에서 전처리
pipe = Pipeline([
    ('scaler', StandardScaler()),
    ('svm', SVC(kernel='rbf', C=1.0))
])
scores = cross_val_score(pipe, X, y, cv=5)  # 각 폴드에서 독립적으로 스케일링

# 잘못된 방법: 전체 데이터로 먼저 스케일링
# scaler = StandardScaler().fit(X)
# X_scaled = scaler.transform(X)
# scores = cross_val_score(SVC(), X_scaled, y, cv=5)  # 데이터 누출!
```

**시계열 교차 검증**: 시간 순서를 보존하는 전방 검증 (Time Series Split)

```python
from sklearn.model_selection import TimeSeriesSplit

tscv = TimeSeriesSplit(n_splits=5)
# 폴드 1: train=[0..t1], test=[t1..t2]
# 폴드 2: train=[0..t2], test=[t2..t3]
# ...
```

> **핵심 직관**: 교차 검증의 목적은 "미래의 데이터에 대한 성능 추정"입니다. 미래의 정보가 과거의 학습에 사용되면 추정이 낙관적으로 편향됩니다.

---

## 5. 하이퍼파라미터 최적화

**그리드 서치**: 모든 조합을 체계적으로 탐색

```python
from sklearn.model_selection import GridSearchCV

param_grid = {
    'svm__C': [0.01, 0.1, 1, 10, 100],
    'svm__gamma': [0.001, 0.01, 0.1, 1]
}

grid = GridSearchCV(pipe, param_grid, cv=5, scoring='accuracy', n_jobs=-1)
grid.fit(X_train, y_train)
print(f"최적 파라미터: {grid.best_params_}")
print(f"최적 CV 점수: {grid.best_score_:.4f}")
```

**랜덤 서치**: 고차원 하이퍼파라미터 공간에서 더 효율적

| 비교 | 그리드 서치 | 랜덤 서치 |
|------|-----------|----------|
| 탐색 방식 | 모든 조합 | 무작위 샘플링 |
| 파라미터 수 증가 시 | 지수적 비용 증가 | 선형 비용 |
| 저차원 부분 공간 | 비효율적 | 효과적으로 커버 |
| 예산 고정 | 격자점 수 제한 | 더 다양한 탐색 |

```python
from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import loguniform, uniform

param_dist = {
    'svm__C': loguniform(1e-3, 1e3),
    'svm__gamma': loguniform(1e-4, 1e1)
}

random_search = RandomizedSearchCV(
    pipe, param_dist, n_iter=100, cv=5, random_state=42
)
```

**베이즈 최적화**: cm-09에서 다룬 GP를 활용한 효율적 탐색

> **핵심 직관**: Bergstra & Bengio (2012)의 결과에 따르면, 랜덤 서치는 같은 예산에서 그리드 서치보다 중요한 하이퍼파라미터를 더 잘 탐색합니다.

---

## 6. 정보 기준: AIC와 BIC

교차 검증 대신 분석적으로 모델 복잡도를 제어하는 방법입니다.

**AIC (Akaike Information Criterion)**:

$$\text{AIC} = -2\log \hat{\mathcal{L}} + 2p$$

**BIC (Bayesian Information Criterion)**:

$$\text{BIC} = -2\log \hat{\mathcal{L}} + p \log n$$

여기서 $\hat{\mathcal{L}}$은 최대 우도, $p$는 파라미터 수, $n$은 데이터 수입니다.

| 비교 | AIC | BIC |
|------|-----|-----|
| 벌점 | $2p$ | $p \log n$ |
| $n > 7$에서 | 덜 엄격 | 더 엄격 |
| 이론적 근거 | KL 발산 최소화 | 베이즈 모델 선택 |
| 일관성 | 없음 (과적합 경향) | 있음 ($n \to \infty$) |
| 효율성 | 예 (예측 최적) | 아니오 |

cm-10에서 GMM의 클러스터 수 선택, cm-11에서 그래프 구조 학습에 BIC가 활용되었습니다.

```python
from sklearn.linear_model import LinearRegression
import numpy as np

# AIC/BIC 직접 계산 (선형 회귀)
def compute_aic_bic(model, X, y):
    n = len(y)
    y_pred = model.predict(X)
    rss = np.sum((y - y_pred)**2)
    p = X.shape[1] + 1  # 피처 수 + 절편

    log_lik = -n/2 * np.log(rss/n) - n/2 * np.log(2*np.pi) - n/2
    aic = -2 * log_lik + 2 * p
    bic = -2 * log_lik + p * np.log(n)
    return aic, bic
```

> **핵심 직관**: AIC는 "예측을 잘 하는 모델", BIC는 "진짜 모델에 가까운 모델"을 선택하는 경향이 있습니다. 예측이 목적이면 AIC, 모델 해석이 목적이면 BIC가 적합합니다.

---

## 7. 중첩 교차 검증

**문제**: 교차 검증으로 하이퍼파라미터를 선택하고, 같은 교차 검증 점수로 일반화 성능을 보고하면 낙관적 편향이 발생합니다.

**중첩(Nested) CV**: 외부 루프로 성능 추정, 내부 루프로 하이퍼파라미터 선택

```
외부 루프 (성능 추정): 5-Fold
  └── 각 외부 폴드에서:
      내부 루프 (하이퍼파라미터 선택): 5-Fold
        └── GridSearchCV로 최적 하이퍼파라미터 선택
      └── 외부 검증 세트에서 성능 측정
```

```python
from sklearn.model_selection import cross_val_score, GridSearchCV

# 내부 CV: 하이퍼파라미터 선택
inner_cv = GridSearchCV(pipe, param_grid, cv=5, scoring='accuracy')

# 외부 CV: 비편향 성능 추정
outer_scores = cross_val_score(inner_cv, X, y, cv=5, scoring='accuracy')
print(f"중첩 CV 점수: {outer_scores.mean():.4f} (+/- {outer_scores.std():.4f})")
```

| 방법 | 용도 | 편향 |
|------|------|------|
| 단순 CV | 하이퍼파라미터 선택 | 성능 추정 시 낙관적 |
| 중첩 CV | 비편향 성능 추정 | 거의 없음 |
| 홀드아웃 테스트 세트 | 최종 성능 보고 | 데이터 분할에 의존 |

> **핵심 직관**: 모델 선택과 모델 평가에 같은 데이터를 사용하면 안 됩니다. 중첩 CV는 이 원칙을 체계적으로 구현합니다.

---

## 핵심 정리

- **편향-분산 분해 $\text{MSE} = \text{Bias}^2 + \text{Var} + \sigma^2$는 모델 복잡도가 증가하면 편향은 줄고 분산은 늘어나는 트레이드오프를 정량화합니다**
- **K-Fold 교차 검증은 데이터를 $K$개 폴드로 나누어 모든 데이터를 검증에 활용하며, $K=5$ 또는 $K=10$이 편향-분산 균형의 일반적 선택입니다**
- **데이터 누출을 방지하기 위해 전처리는 반드시 교차 검증 루프 안에서(Pipeline 사용) 수행해야 하며, 시계열에는 시간순 분할이 필수입니다**
- **AIC는 예측 성능을, BIC는 모델 일관성을 추구하며, BIC의 $p \log n$ 벌점이 AIC의 $2p$보다 큰 모델에 더 강한 벌점을 부여합니다**
- **중첩 교차 검증은 내부 루프로 하이퍼파라미터를 선택하고 외부 루프로 비편향 성능을 추정하여, 모델 선택과 평가를 분리합니다**
