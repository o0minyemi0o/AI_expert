# 확률적 그래프 모델

## 왜 확률적 그래프 모델을 이해해야 하는가

현실의 데이터는 수십에서 수천 개의 변수가 복잡하게 얽혀 있습니다. 확률적 그래프 모델(PGM)은 변수 간의 의존 구조를 그래프로 표현하여, 고차원 결합 분포를 효율적으로 표현하고 추론합니다. cm-07의 나이브 베이즈는 가장 단순한 그래프 모델이었으며, 이 강의에서는 베이즈 네트워크와 마르코프 랜덤 필드라는 두 가지 기본 프레임워크를 통해 조건부 독립 구조가 어떻게 계산과 해석을 가능하게 하는지 다룹니다.

---

## 1. 결합 분포의 분해

$d$개의 이진 변수에 대한 결합 분포를 표현하려면 $2^d - 1$개의 파라미터가 필요합니다. 그래프 모델은 **조건부 독립** 구조를 활용하여 이를 극적으로 줄입니다.

**체인 규칙**:

$$p(x_1, x_2, \ldots, x_d) = p(x_1) \prod_{i=2}^d p(x_i \mid x_1, \ldots, x_{i-1})$$

그래프 모델의 핵심: 각 조건부 분포에서 많은 변수가 불필요합니다.

$$p(x_i \mid x_1, \ldots, x_{i-1}) = p(x_i \mid \text{pa}(x_i))$$

여기서 $\text{pa}(x_i)$는 $x_i$의 **부모 노드**(parent)입니다.

| $d$ | 완전 결합 분포 | 그래프 모델 (부모 최대 2개) |
|-----|-------------|------------------------|
| 10 | 1,023 | ~30 |
| 20 | 1,048,575 | ~60 |
| 100 | $\approx 10^{30}$ | ~300 |

> **핵심 직관**: 그래프 모델은 "어떤 변수가 어떤 변수에 직접 의존하는가?"를 명시적으로 표현합니다. cm-07의 나이브 베이즈는 모든 피처가 클래스에만 의존하는 가장 단순한 구조였습니다.

---

## 2. 베이즈 네트워크 (방향 그래프 모델)

**베이즈 네트워크(Bayesian Network)**는 방향 비순환 그래프(DAG)로 정의됩니다.

결합 분포의 분해:

$$p(\mathbf{x}) = \prod_{i=1}^d p(x_i \mid \text{pa}(x_i))$$

**예시**: 학생 성적 네트워크

```
난이도(D) → 성적(G) ← 지능(I)
                ↓          ↓
           추천서(L)    SAT(S)
```

$$p(D, I, G, S, L) = p(D)\,p(I)\,p(G \mid D, I)\,p(S \mid I)\,p(L \mid G)$$

**조건부 확률 테이블 (CPT)**:

| $D$ | $I$ | $P(G=A)$ | $P(G=B)$ | $P(G=C)$ |
|-----|-----|----------|----------|----------|
| 쉬움 | 높음 | 0.30 | 0.40 | 0.30 |
| 쉬움 | 낮음 | 0.05 | 0.25 | 0.70 |
| 어려움 | 높음 | 0.90 | 0.08 | 0.02 |
| 어려움 | 낮음 | 0.50 | 0.30 | 0.20 |

```python
# pgmpy를 사용한 베이즈 네트워크 구현
# pip install pgmpy
from pgmpy.models import BayesianNetwork
from pgmpy.factors.discrete import TabularCPD

model = BayesianNetwork([('D', 'G'), ('I', 'G'), ('I', 'S'), ('G', 'L')])
# CPD 정의 후 model.add_cpds(...)
```

> **핵심 직관**: 베이즈 네트워크의 화살표는 "인과"가 아니라 "조건부 의존"을 나타냅니다. 그러나 인과 해석이 자연스러운 경우가 많습니다.

---

## 3. 조건부 독립과 d-분리

그래프 구조에서 조건부 독립 관계를 읽어내는 것이 핵심입니다.

**세 가지 기본 구조**:

| 구조 | 그래프 | $A \perp C$? | $A \perp C \mid B$? |
|------|-------|-------------|---------------------|
| 연쇄 (Chain) | $A \to B \to C$ | 아니오 | 예 |
| 분기 (Fork) | $A \leftarrow B \to C$ | 아니오 | 예 |
| 합류 (Collider) | $A \to B \leftarrow C$ | 예 | 아니오 (설명 효과) |

**d-분리(d-separation)**: 노드 집합 $\mathbf{Z}$가 주어졌을 때, $A$에서 $C$로의 모든 경로가 차단되면 $A \perp C \mid \mathbf{Z}$

차단 조건:
- 연쇄/분기: 경로상 노드가 $\mathbf{Z}$에 포함
- 합류: 합류 노드와 그 후손이 $\mathbf{Z}$에 **미포함**

**설명 효과(explaining away)**: $A \to B \leftarrow C$에서 $B$를 관측하면 $A$와 $C$가 조건부 종속이 됩니다. 성적이 좋으면, 과목이 어려울수록 지능이 높을 것이라 추론합니다.

> **핵심 직관**: d-분리는 "정보의 흐름"을 분석합니다. 관측 변수가 정보의 경로를 차단하거나 (연쇄/분기) 개방할 수 있습니다 (합류). 이는 실험 설계와 인과 추론의 기초입니다.

---

## 4. 마르코프 랜덤 필드 (비방향 그래프 모델)

**마르코프 랜덤 필드(MRF)**는 비방향 그래프로 정의됩니다.

결합 분포의 분해 (Hammersley-Clifford 정리):

$$p(\mathbf{x}) = \frac{1}{Z} \prod_{c \in \mathcal{C}} \psi_c(\mathbf{x}_c)$$

여기서 $\psi_c$는 클릭 $c$에 대한 **포텐셜 함수**, $Z$는 **분배 함수(partition function)**:

$$Z = \sum_{\mathbf{x}} \prod_{c \in \mathcal{C}} \psi_c(\mathbf{x}_c)$$

| 비교 | 베이즈 네트워크 (BN) | MRF |
|------|---------------------|-----|
| 그래프 | 방향 (DAG) | 비방향 |
| 분해 | 조건부 확률 $p(x_i \mid \text{pa})$ | 포텐셜 함수 $\psi_c(\mathbf{x}_c)$ |
| 정규화 | 자동 (확률의 곱) | $Z$ 계산 필요 (어려움) |
| 조건부 독립 | d-분리 | 그래프 분리 |
| 대표 응용 | 인과 모델, 의료 진단 | 이미지 분할, 물리 시스템 |

**마르코프 성질**: $x_i$는 이웃이 주어지면 나머지와 조건부 독립

$$p(x_i \mid \mathbf{x}_{-i}) = p(x_i \mid \text{ne}(x_i))$$

> **핵심 직관**: MRF는 "이웃끼리 비슷한 경향"을 자연스럽게 표현합니다. 이미지에서 인접 픽셀은 비슷한 값을 가지며, 이를 포텐셜 함수로 인코딩합니다.

---

## 5. 추론: 정확 추론과 근사 추론

주어진 증거(관측값)에서 질의 변수의 사후 분포를 계산하는 것이 추론입니다.

**정확 추론**:

| 알고리즘 | 적용 구조 | 복잡도 |
|---------|----------|-------|
| 변수 소거 | 일반 | $O(\exp(\text{treewidth}))$ |
| 신뢰 전파 (BP) | 트리 | $O(n \cdot K^2)$ |
| Junction Tree | 일반 | $O(\exp(\text{treewidth}))$ |

**근사 추론** (treewidth가 큰 경우):

| 알고리즘 | 유형 | 특징 |
|---------|------|------|
| Loopy BP | 결정적 | 수렴 보장 없음, 실무에서 효과적 |
| 변분 추론 | 결정적 | 하한 최적화 (cm-10의 EM과 유사) |
| 깁스 샘플링 | 확률적 | MCMC, 정확하지만 느림 |

```python
# 변수 소거 예시 (개념적)
# P(I | L=good) = sum_D,G,S P(D)P(I)P(G|D,I)P(S|I)P(L=good|G)
# 효율적 순서: D를 먼저 소거, 그다음 G, S

# pgmpy를 사용한 정확 추론
from pgmpy.inference import VariableElimination
inference = VariableElimination(model)
# result = inference.query(['I'], evidence={'L': 1})
```

> **핵심 직관**: 변수 소거의 핵심은 "합의 곱을 곱의 합으로" 변환하는 것입니다. 소거 순서에 따라 중간 인수의 크기가 달라지며, 최적 순서 찾기는 NP-hard입니다.

---

## 6. 구조 학습과 파라미터 학습

**파라미터 학습** (구조가 주어졌을 때):

| 데이터 상태 | 방법 |
|-----------|------|
| 완전 관측 | MLE (CPT 빈도 추정) |
| 잠재 변수 있음 | EM 알고리즘 (cm-10) |
| 소량 데이터 | MAP / 베이즈 추정 |

**구조 학습** (구조를 데이터에서 학습):

| 접근법 | 방법 | 장점 | 단점 |
|--------|------|------|------|
| 제약 기반 | PC 알고리즘 | 독립성 검정 기반 | 검정 오류에 민감 |
| 점수 기반 | BIC + 탐색 | 전역 최적에 가까움 | 탐색 공간 방대 |
| 하이브리드 | MMHC | 두 접근의 장점 결합 | 구현 복잡 |

```python
# 구조 학습 (pgmpy)
from pgmpy.estimators import HillClimbSearch, BicScore

hc = HillClimbSearch(data)
best_model = hc.estimate(scoring_method=BicScore(data))
# cm-12의 BIC가 구조 선택 기준으로 활용됩니다
```

> **핵심 직관**: 구조 학습은 "변수 간 의존 관계를 데이터에서 발견하는" 것입니다. 이는 상관관계를 넘어 조건부 독립 구조를 파악하는 것으로, 인과 발견의 첫 단계가 될 수 있습니다.

---

## 7. 그래프 모델의 응용과 확장

| 응용 | 모델 | 핵심 구조 |
|------|------|----------|
| 의료 진단 | BN | 증상 ← 질병 → 검사 결과 |
| 스팸 필터 | 나이브 베이즈 (cm-07) | 단어 ← 스팸 여부 |
| 음성 인식 | HMM | 관측 ← 은닉 상태 체인 |
| 이미지 분할 | MRF / CRF | 픽셀 격자 구조 |
| 자연어 처리 | CRF | 토큰 시퀀스 |

**조건부 랜덤 필드(CRF)**: MRF의 판별 버전

$$p(\mathbf{y} \mid \mathbf{x}) = \frac{1}{Z(\mathbf{x})} \prod_{c} \psi_c(\mathbf{y}_c, \mathbf{x})$$

cm-07에서 다룬 판별 vs 생성 모델의 구분이 그래프 모델에도 적용됩니다:
- 생성 모델: HMM (결합 분포 $p(\mathbf{x}, \mathbf{y})$)
- 판별 모델: CRF (조건부 분포 $p(\mathbf{y} \mid \mathbf{x})$)

```python
# CRF for 시퀀스 레이블링 (개념적)
# sklearn_crfsuite 사용 가능
# import sklearn_crfsuite
# crf = sklearn_crfsuite.CRF(algorithm='lbfgs', max_iterations=100)
```

> **핵심 직관**: 그래프 모델은 문제의 구조적 지식을 직접 모델에 반영할 수 있는 프레임워크입니다. 도메인 전문가의 지식을 그래프 구조로 인코딩하면, 적은 데이터로도 효과적인 추론이 가능합니다.

---

## 핵심 정리

- **확률적 그래프 모델은 변수 간의 조건부 독립 구조를 그래프로 표현하여, $2^d$ 규모의 결합 분포를 소수의 국소 조건부 분포로 분해합니다**
- **베이즈 네트워크(방향 그래프)는 DAG 구조에서 $p(\mathbf{x}) = \prod_i p(x_i \mid \text{pa}(x_i))$로 분해되며, d-분리로 조건부 독립을 판별합니다**
- **마르코프 랜덤 필드(비방향 그래프)는 클릭의 포텐셜 함수로 정의되며, 분배 함수 $Z$의 계산이 주요 난제입니다**
- **정확 추론(변수 소거, 신뢰 전파)은 트리폭이 작을 때 효율적이며, 큰 그래프에서는 변분 추론이나 MCMC 같은 근사 추론이 필요합니다**
- **구조 학습은 데이터에서 변수 간 조건부 독립 관계를 발견하는 것으로, BIC 같은 정보 기준(cm-12)이 모델 복잡도를 제어합니다**
