# 선형 분류와 결정 경계

## 왜 선형 분류를 이해해야 하는가

선형 분류기는 모든 분류 알고리즘의 출발점이며, 결정 경계의 기하학적 의미를 이해하면 비선형 모델로의 확장이 자연스럽습니다. 퍼셉트론에서 로지스틱 회귀까지의 발전 과정은 "분류를 확률적으로 해석한다"는 핵심 아이디어를 담고 있습니다. 이 강의에서는 선형 결정 경계가 어떻게 형성되고, 확률적 해석이 왜 중요한지를 다룹니다.

---

## 1. 선형 분류기의 기본 구조

선형 분류기는 입력 벡터 $\mathbf{x} \in \mathbb{R}^d$를 가중치 벡터 $\mathbf{w}$와 편향 $b$를 이용하여 분류합니다.

$$f(\mathbf{x}) = \mathbf{w}^\top \mathbf{x} + b$$

이 함수의 부호에 따라 클래스를 결정합니다:

$$\hat{y} = \text{sign}(f(\mathbf{x}))$$

| 구성 요소 | 역할 | 기하학적 의미 |
|-----------|------|-------------|
| $\mathbf{w}$ | 가중치 벡터 | 결정 경계의 법선 벡터 |
| $b$ | 편향 | 결정 경계의 원점으로부터의 이동 |
| $f(\mathbf{x}) = 0$ | 결정 경계 | $d$차원 공간의 초평면 |

> **핵심 직관**: 결정 경계 $\mathbf{w}^\top \mathbf{x} + b = 0$은 $\mathbf{w}$에 수직인 초평면이며, 점 $\mathbf{x}$에서 경계까지의 거리는 $\frac{|f(\mathbf{x})|}{\|\mathbf{w}\|}$입니다.

```python
import numpy as np

# 2차원 선형 분류기
w = np.array([2.0, -1.0])
b = -3.0
x = np.array([3.0, 1.0])
decision = np.dot(w, x) + b  # 2*3 + (-1)*1 + (-3) = 2 > 0 → 클래스 +1
```

---

## 2. 퍼셉트론 알고리즘

퍼셉트론은 가장 단순한 선형 분류기로, 오분류된 샘플에 대해 가중치를 반복적으로 갱신합니다.

**갱신 규칙**: 샘플 $(\mathbf{x}_i, y_i)$가 오분류되었을 때

$$\mathbf{w} \leftarrow \mathbf{w} + \eta \, y_i \, \mathbf{x}_i, \quad b \leftarrow b + \eta \, y_i$$

여기서 $\eta > 0$은 학습률입니다.

**퍼셉트론 수렴 정리**: 데이터가 선형 분리 가능하면, 퍼셉트론 알고리즘은 유한 번의 갱신 후 반드시 수렴합니다. 갱신 횟수의 상한은 다음과 같습니다:

$$t \leq \frac{\|\mathbf{w}^*\|^2 \cdot \max_i \|\mathbf{x}_i\|^2}{\gamma^2}$$

여기서 $\gamma$는 마진(margin)입니다.

```python
from sklearn.linear_model import Perceptron

clf = Perceptron(eta0=0.1, max_iter=1000)
clf.fit(X_train, y_train)
# 퍼셉트론은 선형 분리 불가능한 데이터에서 수렴을 보장하지 않습니다
```

> **핵심 직관**: 퍼셉트론은 오분류 샘플의 방향으로 결정 경계를 회전시킵니다. 그러나 해가 유일하지 않으며, 선형 분리 불가능 시 발산합니다.

---

## 3. 로지스틱 회귀의 확률적 해석

퍼셉트론의 한계를 극복하기 위해, 로지스틱 회귀는 시그모이드 함수를 통해 출력을 확률로 변환합니다.

$$P(y=1 \mid \mathbf{x}) = \sigma(\mathbf{w}^\top \mathbf{x} + b) = \frac{1}{1 + e^{-(\mathbf{w}^\top \mathbf{x} + b)}}$$

시그모이드 함수 $\sigma(z)$의 핵심 성질:

- $\sigma(z) \in (0, 1)$: 출력이 항상 유효한 확률
- $\sigma(-z) = 1 - \sigma(z)$: 대칭성
- $\sigma'(z) = \sigma(z)(1 - \sigma(z))$: 깔끔한 미분

**로그 오즈(log-odds)**: 로지스틱 회귀는 로그 오즈가 입력의 선형 함수라는 가정입니다.

$$\log \frac{P(y=1 \mid \mathbf{x})}{P(y=0 \mid \mathbf{x})} = \mathbf{w}^\top \mathbf{x} + b$$

> **핵심 직관**: 로지스틱 회귀는 "결정 경계에서 멀수록 더 확신한다"를 수학적으로 정량화합니다. 스팸 필터에서 $P(\text{spam}) = 0.95$와 $P(\text{spam}) = 0.51$은 같은 예측이지만 확신도가 다릅니다.

---

## 4. 최대 우도 추정과 교차 엔트로피 손실

로지스틱 회귀의 파라미터는 최대 우도 추정(MLE)으로 학습합니다.

$n$개의 독립 샘플 $\{(\mathbf{x}_i, y_i)\}$에 대한 우도 함수:

$$L(\mathbf{w}, b) = \prod_{i=1}^{n} p_i^{y_i}(1-p_i)^{1-y_i}$$

여기서 $p_i = \sigma(\mathbf{w}^\top \mathbf{x}_i + b)$입니다. 음의 로그 우도를 최소화하면 **이진 교차 엔트로피 손실**을 얻습니다:

$$\mathcal{L} = -\frac{1}{n}\sum_{i=1}^{n}\left[y_i \log p_i + (1-y_i)\log(1-p_i)\right]$$

이 손실 함수는 **co-08에서 배운 볼록 함수**이므로, 경사 하강법으로 전역 최적해에 수렴할 수 있습니다.

```python
from sklearn.linear_model import LogisticRegression

clf = LogisticRegression(penalty='l2', C=1.0, solver='lbfgs')
clf.fit(X_train, y_train)
probs = clf.predict_proba(X_test)  # 확률값 반환
```

| 손실 함수 | 수식 | 특성 |
|-----------|------|------|
| 0-1 손실 | $\mathbb{1}[\hat{y} \neq y]$ | 미분 불가, NP-hard |
| 힌지 손실 | $\max(0, 1 - yf(\mathbf{x}))$ | cm-02에서 상세히 다룸 |
| 교차 엔트로피 | $-y\log p - (1-y)\log(1-p)$ | 볼록, 확률적 해석 |

> **핵심 직관**: 교차 엔트로피 손실은 올바른 클래스의 확률이 낮을수록 큰 벌점을 부여합니다. $p \to 0$일 때 $-\log p \to \infty$이므로, 확신을 가지고 틀리면 매우 큰 손실을 받습니다.

---

## 5. 결정 경계의 기하학

결정 경계 $f(\mathbf{x}) = 0$의 기하학적 성질을 깊이 이해해 봅시다.

**2클래스 선형 결정 경계**:
- $d$차원 공간에서 $(d-1)$차원 초평면
- 점 $\mathbf{x}_0$에서 결정 경계까지의 부호 있는 거리:

$$d(\mathbf{x}_0) = \frac{\mathbf{w}^\top \mathbf{x}_0 + b}{\|\mathbf{w}\|}$$

**다중 클래스 확장**: $K$개 클래스에 대해

| 전략 | 분류기 수 | 장점 | 단점 |
|------|----------|------|------|
| OvR (One-vs-Rest) | $K$ | 간단, 확률 해석 가능 | 클래스 불균형 |
| OvO (One-vs-One) | $\binom{K}{2}$ | 작은 학습 데이터 | 분류기 수 많음 |
| Softmax | 1 | 직접 다중 클래스 확률 | 모든 클래스 동시 학습 필요 |

**소프트맥스(Softmax) 회귀**: 로지스틱 회귀의 다중 클래스 일반화

$$P(y=k \mid \mathbf{x}) = \frac{e^{\mathbf{w}_k^\top \mathbf{x} + b_k}}{\sum_{j=1}^{K} e^{\mathbf{w}_j^\top \mathbf{x} + b_j}}$$

> **핵심 직관**: 소프트맥스의 결정 경계는 클래스 $k$와 $l$ 사이에서 $(\mathbf{w}_k - \mathbf{w}_l)^\top \mathbf{x} + (b_k - b_l) = 0$으로, 여전히 선형입니다.

---

## 6. 정규화와 일반화

과적합을 방지하기 위해 정규화 항을 추가합니다.

$$\mathcal{L}_{\text{reg}} = \mathcal{L} + \lambda \, \Omega(\mathbf{w})$$

| 정규화 | $\Omega(\mathbf{w})$ | 효과 | sklearn 파라미터 |
|--------|---------------------|------|-----------------|
| L2 (Ridge) | $\frac{1}{2}\|\mathbf{w}\|_2^2$ | 가중치를 0 근처로 축소 | `penalty='l2'` |
| L1 (Lasso) | $\|\mathbf{w}\|_1$ | 희소 해, 피처 선택 | `penalty='l1'` |
| Elastic Net | $\alpha\|\mathbf{w}\|_1 + (1-\alpha)\frac{1}{2}\|\mathbf{w}\|_2^2$ | L1 + L2 결합 | `penalty='elasticnet'` |

**la-03에서 다룬 고유값 분석**과 연결하면, L2 정규화는 공분산 행렬의 작은 고유값에 대응하는 방향의 가중치를 특히 강하게 억제합니다.

```python
from sklearn.linear_model import LogisticRegression

# C는 정규화 강도의 역수 (C가 작을수록 강한 정규화)
clf_l1 = LogisticRegression(penalty='l1', C=0.1, solver='saga')
clf_l2 = LogisticRegression(penalty='l2', C=0.1, solver='lbfgs')
```

> **핵심 직관**: 정규화는 결정 경계를 "더 단순하게" 만듭니다. L1은 불필요한 피처의 가중치를 정확히 0으로 만들어 자동 피처 선택을 수행합니다.

---

## 7. 퍼셉트론에서 로지스틱 회귀까지: 비교와 통합

| 속성 | 퍼셉트론 | 로지스틱 회귀 |
|------|---------|-------------|
| 출력 | $\{-1, +1\}$ | $P(y=1 \mid \mathbf{x}) \in (0,1)$ |
| 손실 함수 | 오분류 벌점 | 교차 엔트로피 |
| 수렴 보장 | 선형 분리 가능 시만 | 항상 (볼록 최적화) |
| 확률 해석 | 불가 | 가능 |
| 마진 최대화 | 아니오 | 암묵적 (정규화 시) |

MNIST에서 로지스틱 회귀는 약 92%의 정확도를 달성하며, 이는 선형 모델의 한계를 보여줍니다. 이를 극복하기 위해 cm-02에서 SVM의 마진 최대화, cm-03에서 커널 트릭을 통한 비선형 결정 경계를 배웁니다.

> **핵심 직관**: 퍼셉트론은 "틀리지 않는 경계"를 찾고, 로지스틱 회귀는 "가장 그럴듯한 경계"를 찾습니다. 이 차이가 확률적 모델링의 핵심입니다.

---

## 핵심 정리

- **선형 분류기의 결정 경계는 가중치 벡터 $\mathbf{w}$에 수직인 초평면이며, 점에서 경계까지의 거리가 예측의 확신도를 결정합니다**
- **퍼셉트론은 오분류 기반 갱신으로 선형 분리 가능 데이터에서만 수렴을 보장하는 가장 기본적인 분류 알고리즘입니다**
- **로지스틱 회귀는 시그모이드 함수를 통해 출력을 확률로 변환하며, 로그 오즈가 입력의 선형 함수라는 가정에 기반합니다**
- **교차 엔트로피 손실은 최대 우도 추정에서 자연스럽게 유도되며, 볼록 함수이므로 전역 최적해를 보장합니다**
- **L1/L2 정규화는 결정 경계의 복잡도를 제어하여 일반화 성능을 향상시키며, L1은 자동 피처 선택 효과를 제공합니다**
