# 나이브 베이즈와 생성 모델

## 왜 생성 모델을 이해해야 하는가

지금까지 다룬 SVM과 로지스틱 회귀는 결정 경계 $P(y \mid \mathbf{x})$를 직접 학습하는 판별 모델입니다. 생성 모델은 반대 방향에서 접근하여 데이터의 생성 과정 $P(\mathbf{x} \mid y)$를 모델링합니다. 나이브 베이즈는 가장 단순한 생성 모델이지만, 조건부 독립 가정 덕분에 고차원 데이터에서도 놀랍도록 효과적이며, 텍스트 분류의 기본 베이스라인으로 여전히 활용됩니다.

---

## 1. 베이즈 정리와 분류

**베이즈 정리**를 분류에 적용합니다:

$$P(y=k \mid \mathbf{x}) = \frac{P(\mathbf{x} \mid y=k) \, P(y=k)}{P(\mathbf{x})}$$

| 항 | 이름 | 의미 |
|----|------|------|
| $P(y=k \mid \mathbf{x})$ | 사후 확률 | 관측 후 클래스 확률 |
| $P(\mathbf{x} \mid y=k)$ | 우도 | 클래스 $k$에서 $\mathbf{x}$가 관측될 확률 |
| $P(y=k)$ | 사전 확률 | 클래스의 기본 확률 |
| $P(\mathbf{x})$ | 증거 | 정규화 상수 (분류에 불필요) |

**MAP (Maximum A Posteriori) 분류**:

$$\hat{y} = \arg\max_k P(y=k \mid \mathbf{x}) = \arg\max_k P(\mathbf{x} \mid y=k) \, P(y=k)$$

분모 $P(\mathbf{x})$는 모든 클래스에 공통이므로 분류 결정에 영향을 주지 않습니다.

> **핵심 직관**: 베이즈 분류기는 "이 데이터가 어떤 클래스에서 생성되었을 가능성이 가장 높은가?"를 묻습니다. 스팸 필터에서 "무료", "당첨" 같은 단어가 스팸 메일에서 더 자주 나타나면 $P(\mathbf{x} \mid \text{spam})$이 높아집니다.

---

## 2. 나이브 베이즈 가정

문제: $\mathbf{x} = (x_1, \ldots, x_d)$에 대해 $P(\mathbf{x} \mid y=k)$를 추정하려면 지수적으로 많은 파라미터가 필요합니다.

**나이브(조건부 독립) 가정**: 클래스가 주어지면 피처들이 독립

$$P(\mathbf{x} \mid y=k) = \prod_{j=1}^d P(x_j \mid y=k)$$

파라미터 수 비교:

| 모델 | 파라미터 수 (이진 피처, $K$ 클래스) |
|------|--------------------------------|
| 완전 결합 분포 | $K \cdot (2^d - 1)$ |
| 나이브 베이즈 | $K \cdot d$ |

$d = 1000$일 때 $2^{1000}$ vs $1000K$: 나이브 가정이 추정 가능성을 극적으로 향상시킵니다.

```python
# 나이브 베이즈의 분류 결정
# log P(y=k|x) = log P(y=k) + sum_j log P(x_j|y=k) + const
import numpy as np

def nb_predict(X, log_prior, log_likelihood):
    # log_prior: (K,), log_likelihood: (K, d)
    log_posterior = log_prior + X @ log_likelihood.T
    return np.argmax(log_posterior, axis=1)
```

> **핵심 직관**: 나이브 가정은 거의 항상 틀리지만, 분류 결정에는 사후 확률의 **순서**만 필요하므로 확률의 정확한 값이 틀려도 분류 성능은 좋을 수 있습니다.

---

## 3. 나이브 베이즈의 세 가지 변형

### 3.1 Gaussian Naive Bayes

연속 피처에 대해 클래스 조건부 분포를 정규 분포로 가정합니다:

$$P(x_j \mid y=k) = \frac{1}{\sqrt{2\pi\sigma_{kj}^2}} \exp\left(-\frac{(x_j - \mu_{kj})^2}{2\sigma_{kj}^2}\right)$$

파라미터: 각 클래스-피처 쌍의 평균 $\mu_{kj}$와 분산 $\sigma_{kj}^2$

### 3.2 Multinomial Naive Bayes

단어 빈도(bag-of-words) 같은 이산 카운트 데이터에 적합합니다:

$$P(\mathbf{x} \mid y=k) \propto \prod_{j=1}^d \theta_{kj}^{x_j}$$

여기서 $\theta_{kj} = P(\text{단어 } j \mid \text{클래스 } k)$

### 3.3 Bernoulli Naive Bayes

이진 피처(단어 유무)에 적합합니다:

$$P(\mathbf{x} \mid y=k) = \prod_{j=1}^d \theta_{kj}^{x_j}(1-\theta_{kj})^{1-x_j}$$

| 변형 | 피처 유형 | 대표 응용 |
|------|----------|----------|
| Gaussian | 연속 | 센서 데이터 |
| Multinomial | 카운트 | 문서 분류 (TF) |
| Bernoulli | 이진 | 문서 분류 (유무) |

```python
from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB

gnb = GaussianNB()
mnb = MultinomialNB(alpha=1.0)   # 라플라스 스무딩
bnb = BernoulliNB(alpha=1.0)
```

> **핵심 직관**: 피처의 성질에 맞는 변형을 선택하는 것이 중요합니다. 텍스트 분류에서 Multinomial NB는 단어 빈도를 반영하고, Bernoulli NB는 단어의 존재 여부만 고려합니다.

---

## 4. 라플라스 스무딩

학습 데이터에서 한 번도 등장하지 않은 피처가 있으면 $P(x_j \mid y=k) = 0$이 되어, 전체 우도가 0이 됩니다.

**라플라스 스무딩 (Additive Smoothing)**:

$$\hat{\theta}_{kj} = \frac{N_{kj} + \alpha}{N_k + \alpha \cdot V}$$

여기서 $N_{kj}$는 클래스 $k$에서 피처 $j$의 빈도, $V$는 전체 피처 수, $\alpha$는 스무딩 파라미터입니다.

| $\alpha$ | 효과 |
|----------|------|
| 0 | 스무딩 없음 (MLE) |
| 1 | 라플라스 스무딩 (가장 일반적) |
| $< 1$ | 약한 스무딩 |
| $\to \infty$ | 균등 분포에 수렴 |

> **핵심 직관**: 스무딩은 "아직 보지 못한 것이 불가능한 것은 아니다"라는 원리를 수학적으로 구현합니다. 이는 베이즈 추론에서 사전 분포(디리클레 분포)를 적용하는 것과 동일합니다.

---

## 5. 판별 모델 vs 생성 모델

| 비교 | 판별 모델 | 생성 모델 |
|------|----------|----------|
| 학습 대상 | $P(y \mid \mathbf{x})$ 직접 | $P(\mathbf{x} \mid y)$와 $P(y)$ |
| 대표 알고리즘 | 로지스틱 회귀, SVM | 나이브 베이즈, LDA, GMM |
| 장점 | 분류에 직접 최적화 | 결측치 처리, 데이터 생성 |
| 단점 | 데이터 분포 모델링 불가 | 모델 오명세에 민감 |
| 수렴 속도 | 점근적으로 우수 | 소량 데이터에서 유리 |

**Ng & Jordan (2002)의 결과**:

- 판별 모델(로지스틱 회귀)은 점근적으로 더 낮은 오류율에 수렴
- 생성 모델(나이브 베이즈)은 적은 데이터에서 더 빨리 수렴
- 교차점: 약 $n \approx \log d$ 부근

```python
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import MultinomialNB

# 텍스트 분류: 소량 데이터에서는 NB가 유리할 수 있음
nb = MultinomialNB(alpha=1.0)
lr = LogisticRegression(penalty='l2', C=1.0)

# 20 Newsgroups에서 NB는 빠르고 합리적인 베이스라인
```

> **핵심 직관**: 판별 모델은 "어떤 클래스인가?"만 답하고, 생성 모델은 "데이터가 어떻게 생겼는가?"까지 답합니다. cm-10에서 다룰 EM 알고리즘과 GMM은 이 생성 모델의 확장입니다.

---

## 6. 나이브 베이즈의 기하학적 해석

나이브 베이즈의 결정 경계를 분석해 봅시다. 이진 분류에서:

$$\log \frac{P(y=1 \mid \mathbf{x})}{P(y=0 \mid \mathbf{x})} = \log \frac{P(y=1)}{P(y=0)} + \sum_{j=1}^d \log \frac{P(x_j \mid y=1)}{P(x_j \mid y=0)}$$

Gaussian NB의 경우, $\sigma_{0j} = \sigma_{1j}$이면 이 로그 비는 $\mathbf{x}$의 **선형 함수**가 됩니다:

$$= \mathbf{w}^\top \mathbf{x} + b$$

따라서 동분산 Gaussian NB는 cm-01의 **로지스틱 회귀와 동일한 결정 경계**를 만듭니다. 차이는 파라미터 추정 방법입니다:

| 모델 | 추정 방법 | 결정 경계 |
|------|----------|----------|
| 로지스틱 회귀 | 조건부 우도 최대화 | 선형 |
| 동분산 Gaussian NB | 결합 우도 최대화 | 선형 (동일) |
| 이분산 Gaussian NB | 결합 우도 최대화 | 이차 |

> **핵심 직관**: 나이브 베이즈와 로지스틱 회귀는 같은 결정 경계를 다른 원리로 학습합니다. "생성적 vs 판별적"은 결정 경계의 형태가 아니라 학습 철학의 차이입니다.

---

## 7. 실무 활용과 한계

**나이브 베이즈가 잘 작동하는 경우**:
- 고차원 희소 데이터 (텍스트, 유전자)
- 학습 데이터가 적을 때
- 빠른 학습/예측이 필요할 때
- 확률 보정(calibration)이 불필요할 때

**한계와 해결책**:

| 한계 | 원인 | 해결책 |
|------|------|--------|
| 확률 추정 부정확 | 독립 가정 위반 | CalibratedClassifierCV |
| 상관 피처 무시 | 조건부 독립 가정 | cm-11 그래프 모델 |
| 연속 피처 분포 가정 | Gaussian 가정 | 커널 밀도 추정 |

```python
from sklearn.naive_bayes import MultinomialNB
from sklearn.calibration import CalibratedClassifierCV

# 확률 보정으로 나이브 베이즈의 확률 추정 개선
nb = MultinomialNB()
calibrated_nb = CalibratedClassifierCV(nb, cv=5, method='isotonic')
calibrated_nb.fit(X_train, y_train)
probs = calibrated_nb.predict_proba(X_test)
```

> **핵심 직관**: 나이브 베이즈는 "잘못된 가정으로 올바른 결정을 내리는" 모델입니다. 확률의 절대값은 부정확하지만, 클래스 간의 상대적 순서는 놀랍도록 정확합니다.

---

## 핵심 정리

- **나이브 베이즈는 베이즈 정리와 조건부 독립 가정을 결합하여, $O(Kd)$개의 파라미터만으로 고차원 데이터를 분류합니다**
- **Gaussian, Multinomial, Bernoulli의 세 가지 변형은 피처의 성질(연속, 카운트, 이진)에 따라 적합한 우도 함수를 제공합니다**
- **라플라스 스무딩($\alpha$)은 학습 데이터에 없는 피처-클래스 조합의 확률이 0이 되는 것을 방지하며, 디리클레 사전 분포와 동치입니다**
- **판별 모델은 점근적으로 우수하지만, 생성 모델(나이브 베이즈)은 소량 데이터에서 더 빠르게 수렴합니다**
- **동분산 Gaussian NB는 로지스틱 회귀와 동일한 선형 결정 경계를 만들며, 두 모델의 차이는 결정 경계의 형태가 아니라 파라미터 추정 원리에 있습니다**
