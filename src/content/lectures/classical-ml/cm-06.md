# 앙상블: Boosting 이론

## 왜 Boosting이 필요한가

cm-05에서 다룬 Bagging은 분산을 줄이지만 편향에는 영향을 주지 않습니다. Boosting은 반대 전략으로, 약한 학습기(weak learner)를 순차적으로 결합하여 편향을 줄이고 강한 학습기를 만듭니다. AdaBoost에서 시작하여 Gradient Boosting으로 일반화되는 이론적 흐름을 이해하면, cm-14에서 다룰 XGBoost와 LightGBM의 설계 원리가 자연스럽게 따라옵니다.

---

## 1. 약한 학습기에서 강한 학습기로

**약한 학습기(Weak Learner)**: 랜덤 추측보다 약간만 나은 분류기 (오류율 $< 0.5$)

Boosting의 핵심 질문: "약간만 나은 분류기를 여러 개 결합하면 임의로 정확한 분류기를 만들 수 있는가?"

**Schapire의 정리 (1990)**: 약한 학습기가 존재하면, 이를 결합하여 임의로 낮은 오류율의 강한 학습기를 구성할 수 있습니다.

| 개념 | Bagging (cm-05) | Boosting |
|------|----------------|----------|
| 기본 학습기 | 강한 학습기 (깊은 트리) | 약한 학습기 (얕은 트리) |
| 학습 방식 | 독립, 병렬 | 순차, 적응적 |
| 주요 효과 | 분산 감소 | 편향 감소 |
| 가중치 | 동일 | 성능에 비례 |

> **핵심 직관**: Boosting은 "틀린 문제를 더 많이 공부하는" 학습 전략입니다. 이전 모델이 틀린 샘플에 집중하여 다음 모델을 학습합니다.

---

## 2. AdaBoost 알고리즘

**Adaptive Boosting (AdaBoost)**: 오분류 샘플의 가중치를 높여 다음 학습기가 이에 집중하도록 합니다.

**알고리즘**:

1. 초기 가중치: $w_i^{(1)} = \frac{1}{n}$
2. $m = 1, \ldots, M$에 대해:
   - 가중 데이터로 약한 학습기 $h_m$ 학습
   - 가중 오류율 계산: $\epsilon_m = \sum_{i=1}^n w_i^{(m)} \mathbb{1}[h_m(\mathbf{x}_i) \neq y_i]$
   - 학습기 가중치: $\alpha_m = \frac{1}{2}\ln\frac{1 - \epsilon_m}{\epsilon_m}$
   - 샘플 가중치 갱신: $w_i^{(m+1)} = w_i^{(m)} \exp(-\alpha_m y_i h_m(\mathbf{x}_i))$, 정규화
3. 최종 예측: $H(\mathbf{x}) = \text{sign}\left(\sum_{m=1}^M \alpha_m h_m(\mathbf{x})\right)$

| $\epsilon_m$ | $\alpha_m$ | 의미 |
|--------------|-----------|------|
| 0.0 | $+\infty$ | 완벽한 분류기 |
| 0.1 | 1.10 | 매우 좋은 분류기, 높은 가중치 |
| 0.3 | 0.42 | 적당한 분류기 |
| 0.5 | 0.0 | 랜덤 추측, 가중치 0 |
| 0.5+ | 음수 | 반전하면 유용, 음의 가중치 |

```python
from sklearn.ensemble import AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier

ada = AdaBoostClassifier(
    estimator=DecisionTreeClassifier(max_depth=1),  # 결정 스텀프
    n_estimators=200,
    learning_rate=1.0,
    algorithm='SAMME'
)
ada.fit(X_train, y_train)
```

> **핵심 직관**: $\alpha_m$은 학습기의 "발언권"입니다. 오류율이 낮은 학습기가 최종 결정에서 더 큰 영향력을 갖습니다.

---

## 3. AdaBoost와 지수 손실

AdaBoost는 **지수 손실(exponential loss)**의 전방 단계적 가법 모델(Forward Stagewise Additive Model)과 동치임이 증명되었습니다.

**지수 손실**:

$$L(y, f(\mathbf{x})) = \exp(-y f(\mathbf{x}))$$

**가법 모델**: $F(\mathbf{x}) = \sum_{m=1}^M \alpha_m h_m(\mathbf{x})$

$m$번째 단계에서 $(\alpha_m, h_m)$을 결정:

$$(\alpha_m, h_m) = \arg\min_{\alpha, h} \sum_{i=1}^n \exp\left(-y_i\left[F_{m-1}(\mathbf{x}_i) + \alpha h(\mathbf{x}_i)\right]\right)$$

이를 전개하면:

$$= \arg\min_{\alpha, h} \sum_{i=1}^n w_i^{(m)} \exp(-\alpha y_i h(\mathbf{x}_i))$$

여기서 $w_i^{(m)} = \exp(-y_i F_{m-1}(\mathbf{x}_i))$이 정확히 AdaBoost의 샘플 가중치입니다.

| 손실 함수 | 이상점 민감도 | 대응 알고리즘 |
|-----------|-------------|-------------|
| 지수 손실 | 매우 높음 | AdaBoost |
| 로그 손실 | 중간 | LogitBoost |
| 힌지 손실 | 낮음 | cm-02 SVM |

> **핵심 직관**: AdaBoost의 샘플 재가중은 지수 손실의 경사 하강에서 자연스럽게 유도됩니다. 그러나 지수 손실은 이상점에 매우 민감하여, 노이즈가 많은 데이터에서 성능이 저하됩니다.

---

## 4. Gradient Boosting

Gradient Boosting은 AdaBoost를 임의의 미분 가능한 손실 함수로 일반화합니다.

**핵심 아이디어**: 함수 공간에서의 경사 하강법

$$F_m(\mathbf{x}) = F_{m-1}(\mathbf{x}) + \eta \cdot h_m(\mathbf{x})$$

여기서 $h_m$은 **음의 기울기(negative gradient, 유사 잔차)**를 근사합니다:

$$r_i^{(m)} = -\frac{\partial L(y_i, F_{m-1}(\mathbf{x}_i))}{\partial F_{m-1}(\mathbf{x}_i)}$$

**손실 함수별 유사 잔차**:

| 손실 | $L(y, F)$ | 유사 잔차 $r_i$ |
|------|----------|----------------|
| 제곱 오차 | $\frac{1}{2}(y-F)^2$ | $y_i - F_{m-1}(\mathbf{x}_i)$ |
| 절대 오차 | $|y-F|$ | $\text{sign}(y_i - F_{m-1}(\mathbf{x}_i))$ |
| 이항 편차 | $\log(1+e^{-2yF})$ | $\frac{2y_i}{1+e^{2y_i F_{m-1}}}$ |
| Huber | 혼합 | 조건부 |

**Gradient Boosting 알고리즘**:

1. 초기화: $F_0(\mathbf{x}) = \arg\min_c \sum_{i=1}^n L(y_i, c)$
2. $m = 1, \ldots, M$에 대해:
   - 유사 잔차 계산: $r_i^{(m)} = -\partial L / \partial F_{m-1}$
   - $h_m$을 유사 잔차에 적합
   - 라인 서치: $\gamma_m = \arg\min_\gamma \sum_i L(y_i, F_{m-1}(\mathbf{x}_i) + \gamma h_m(\mathbf{x}_i))$
   - 갱신: $F_m = F_{m-1} + \eta \gamma_m h_m$

```python
from sklearn.ensemble import GradientBoostingClassifier

gb = GradientBoostingClassifier(
    n_estimators=300,
    max_depth=3,           # 얕은 트리 (약한 학습기)
    learning_rate=0.1,     # 학습률 (shrinkage)
    subsample=0.8,         # 확률적 GB
    loss='log_loss'
)
gb.fit(X_train, y_train)
```

> **핵심 직관**: Gradient Boosting은 "함수 공간에서의 경사 하강법"입니다. 각 트리가 현재 모델의 잔차(오차)를 학습하므로, 순차적으로 오차가 줄어듭니다.

---

## 5. 학습률과 정규화

**학습률(Shrinkage)** $\eta \in (0, 1]$:

$$F_m = F_{m-1} + \eta \cdot h_m$$

| $\eta$ | 트리 수 | 학습 시간 | 일반화 |
|--------|--------|----------|-------|
| 1.0 | 적게 필요 | 빠름 | 과적합 위험 |
| 0.1 | 중간 | 중간 | 양호 |
| 0.01 | 많이 필요 | 느림 | 우수 |

경험 법칙: 낮은 학습률 + 많은 트리 > 높은 학습률 + 적은 트리

**추가 정규화 기법**:

- **확률적 Gradient Boosting (Stochastic GB)**: 각 단계에서 데이터의 일부만 사용 (`subsample < 1.0`)
- **트리 복잡도 제한**: `max_depth`, `min_samples_leaf`
- **조기 종료(Early Stopping)**: 검증 손실이 더 이상 감소하지 않을 때 중단

```python
from sklearn.ensemble import GradientBoostingClassifier

gb = GradientBoostingClassifier(
    n_estimators=1000,
    learning_rate=0.05,
    max_depth=4,
    subsample=0.8,
    validation_fraction=0.15,
    n_iter_no_change=10,     # 조기 종료
    tol=1e-4
)
gb.fit(X_train, y_train)
print(f"실제 사용된 트리 수: {gb.n_estimators_}")
```

> **핵심 직관**: 학습률은 "한 걸음의 크기"입니다. **co-08에서 배운 경사 하강법의 스텝 사이즈**와 동일한 역할을 하며, 작은 걸음이 더 정확한 최적해에 도달합니다.

---

## 6. Boosting의 학습 이론

**학습 오차 상한 (AdaBoost)**:

$$\text{Train Error} \leq \exp\left(-2\sum_{m=1}^M \gamma_m^2\right)$$

여기서 $\gamma_m = \frac{1}{2} - \epsilon_m$은 "에지(edge)"입니다. 각 학습기의 에지가 양수면, 학습 오차는 지수적으로 감소합니다.

**마진 이론**: AdaBoost는 학습 오차가 0이 된 후에도 테스트 성능이 계속 향상되는 현상이 관찰됩니다. 이는 마진 분포가 개선되기 때문입니다.

$$\text{margin}(\mathbf{x}_i, y_i) = \frac{y_i \sum_m \alpha_m h_m(\mathbf{x}_i)}{\sum_m |\alpha_m|}$$

| 현상 | 설명 |
|------|------|
| 학습 오차 0 후에도 개선 | 마진 분포가 넓어짐 |
| 과적합 저항성 | 마진 최대화 효과 |
| 이상점 민감성 | 지수 손실의 한계 |

> **핵심 직관**: Boosting이 트리를 추가해도 과적합이 느리게 발생하는 이유는, cm-02의 SVM처럼 마진을 간접적으로 최대화하기 때문입니다.

---

## 7. AdaBoost vs Gradient Boosting 비교

| 비교 항목 | AdaBoost | Gradient Boosting |
|-----------|---------|-------------------|
| 손실 함수 | 지수 손실 (고정) | 임의 미분 가능 손실 |
| 잔차 학습 | 샘플 재가중 | 음의 기울기 적합 |
| 이상점 민감도 | 높음 | 손실 선택으로 제어 |
| 분류/회귀 | 주로 분류 | 둘 다 자연스러움 |
| 확장성 | 제한적 | XGBoost, LightGBM |

Gradient Boosting의 유연성이 실무에서 더 선호되며, cm-14에서 다룰 XGBoost, LightGBM, CatBoost는 모두 Gradient Boosting의 최적화된 구현입니다.

```python
# Gradient Boosting이 더 유연한 예: Huber 손실로 이상점에 강건
from sklearn.ensemble import GradientBoostingRegressor

gb_huber = GradientBoostingRegressor(
    loss='huber',           # 이상점에 강건한 손실
    alpha=0.95,             # Huber 전환점
    n_estimators=300,
    max_depth=3,
    learning_rate=0.1
)
```

> **핵심 직관**: AdaBoost는 Gradient Boosting의 특수한 경우(지수 손실)입니다. Gradient Boosting의 프레임워크는 문제에 맞는 손실 함수를 자유롭게 선택할 수 있어 훨씬 유연합니다.

---

## 핵심 정리

- **Boosting은 약한 학습기를 순차적으로 결합하여 편향을 줄이는 전략이며, Bagging(분산 감소)과 상보적 관계에 있습니다**
- **AdaBoost는 오분류 샘플의 가중치를 지수적으로 증가시키고, 학습기의 발언권 $\alpha_m$을 오류율에 반비례하게 부여합니다**
- **AdaBoost는 지수 손실에 대한 전방 단계적 가법 모델과 동치이며, 이 관점에서 Gradient Boosting으로 일반화됩니다**
- **Gradient Boosting은 함수 공간에서의 경사 하강법으로, 각 트리가 이전 모델의 음의 기울기(유사 잔차)를 근사하여 순차적으로 오차를 줄입니다**
- **낮은 학습률($\eta$)과 충분한 트리 수의 조합이 최적 성능을 내며, 조기 종료와 확률적 서브샘플링이 과적합을 효과적으로 방지합니다**
