# 가우시안 프로세스

## 왜 가우시안 프로세스를 배워야 하는가

지금까지 다룬 모델들은 점 추정(point estimate) 예측만 제공합니다. 가우시안 프로세스(GP)는 예측에 대한 불확실성을 자연스럽게 정량화하는 베이즈 비모수 방법입니다. GP는 함수 자체에 대한 확률 분포를 정의하며, cm-03에서 다룬 커널 함수가 함수의 사전 지식을 인코딩하는 핵심 도구가 됩니다. 의료 진단이나 자율 주행처럼 불확실성 인식이 중요한 응용에서 GP의 가치가 빛납니다.

---

## 1. 함수에 대한 확률 분포

가우시안 프로세스는 함수 공간에서의 확률 분포입니다.

**정의**: $f(\mathbf{x})$가 가우시안 프로세스라 함은, 유한 개의 입력 $\{\mathbf{x}_1, \ldots, \mathbf{x}_n\}$에 대한 함수값 $(f(\mathbf{x}_1), \ldots, f(\mathbf{x}_n))$이 항상 다변량 정규 분포를 따르는 것입니다.

$$f \sim \mathcal{GP}(m(\mathbf{x}), k(\mathbf{x}, \mathbf{x}'))$$

| 구성 요소 | 정의 | 역할 |
|-----------|------|------|
| $m(\mathbf{x}) = \mathbb{E}[f(\mathbf{x})]$ | 평균 함수 | 함수의 사전 추세 |
| $k(\mathbf{x}, \mathbf{x}') = \text{Cov}(f(\mathbf{x}), f(\mathbf{x}'))$ | 커널(공분산) 함수 | 함수의 매끄러움/주기성 |

통상적으로 $m(\mathbf{x}) = 0$으로 설정합니다 (데이터로 학습).

```python
import numpy as np

# GP 사전 분포에서 함수 샘플링
def rbf_kernel(X1, X2, l=1.0, sigma_f=1.0):
    sq_dist = np.sum(X1**2, 1).reshape(-1,1) + np.sum(X2**2, 1) - 2*X1@X2.T
    return sigma_f**2 * np.exp(-0.5 / l**2 * sq_dist)

X_test = np.linspace(-5, 5, 100).reshape(-1, 1)
K = rbf_kernel(X_test, X_test)
samples = np.random.multivariate_normal(np.zeros(100), K, size=5)
# 각 sample은 GP 사전 분포에서 뽑은 하나의 함수
```

> **핵심 직관**: GP는 "가능한 함수들의 앙상블"입니다. 데이터를 관측하기 전에는 무한히 많은 함수가 가능하고, 데이터를 관측하면 일관된 함수들만 남습니다.

---

## 2. GP 회귀: 사전에서 사후로

관측 데이터 $\mathbf{X}, \mathbf{y}$가 주어지면, 새로운 입력 $\mathbf{X}_*$에 대한 **사후 분포**를 계산합니다.

노이즈 모델: $y = f(\mathbf{x}) + \epsilon$, $\epsilon \sim \mathcal{N}(0, \sigma_n^2)$

결합 분포:

$$\begin{pmatrix} \mathbf{y} \\ \mathbf{f}_* \end{pmatrix} \sim \mathcal{N}\left(\mathbf{0}, \begin{pmatrix} K(\mathbf{X}, \mathbf{X}) + \sigma_n^2 I & K(\mathbf{X}, \mathbf{X}_*) \\ K(\mathbf{X}_*, \mathbf{X}) & K(\mathbf{X}_*, \mathbf{X}_*) \end{pmatrix}\right)$$

**사후 분포** (조건부 정규 분포):

$$\mathbf{f}_* \mid \mathbf{X}, \mathbf{y}, \mathbf{X}_* \sim \mathcal{N}(\boldsymbol{\mu}_*, \boldsymbol{\Sigma}_*)$$

$$\boldsymbol{\mu}_* = K(\mathbf{X}_*, \mathbf{X})[K(\mathbf{X}, \mathbf{X}) + \sigma_n^2 I]^{-1}\mathbf{y}$$

$$\boldsymbol{\Sigma}_* = K(\mathbf{X}_*, \mathbf{X}_*) - K(\mathbf{X}_*, \mathbf{X})[K(\mathbf{X}, \mathbf{X}) + \sigma_n^2 I]^{-1}K(\mathbf{X}, \mathbf{X}_*)$$

> **핵심 직관**: 사후 평균 $\boldsymbol{\mu}_*$는 관측값의 가중합이며 (cm-08의 커널 가중 KNN과 유사), 사후 분산 $\boldsymbol{\Sigma}_*$는 데이터에서 먼 곳에서 커지고 가까운 곳에서 줄어듭니다.

---

## 3. 커널 함수: 함수의 사전 지식

커널 함수는 "함수가 어떤 성질을 가져야 하는가"를 인코딩합니다.

| 커널 | $k(\mathbf{x}, \mathbf{x}')$ | 성질 |
|------|---------------------------|------|
| RBF (SE) | $\sigma_f^2 \exp\left(-\frac{\|\mathbf{x}-\mathbf{x}'\|^2}{2l^2}\right)$ | 무한히 미분 가능, 매끄러움 |
| Matern-3/2 | $\sigma_f^2\left(1+\frac{\sqrt{3}r}{l}\right)e^{-\sqrt{3}r/l}$ | 1회 미분 가능 |
| Matern-5/2 | $\sigma_f^2\left(1+\frac{\sqrt{5}r}{l}+\frac{5r^2}{3l^2}\right)e^{-\sqrt{5}r/l}$ | 2회 미분 가능 |
| 주기 (Periodic) | $\sigma_f^2 \exp\left(-\frac{2\sin^2(\pi r/p)}{l^2}\right)$ | 주기적 패턴 |
| 선형 | $\sigma_f^2 \mathbf{x}^\top \mathbf{x}'$ | 선형 함수 |

여기서 $r = \|\mathbf{x} - \mathbf{x}'\|$, $l$은 길이 스케일, $\sigma_f^2$는 신호 분산입니다.

**cm-03에서 다룬 커널 조합 법칙**이 동일하게 적용됩니다:

- $k_1 + k_2$: 두 성분의 합 (추세 + 주기)
- $k_1 \times k_2$: 상호작용 (주기의 진폭 변화)
- 새 유효 커널 생성

```python
from sklearn.gaussian_process.kernels import RBF, Matern, WhiteKernel

# 커널 조합: RBF + 노이즈
kernel = 1.0 * RBF(length_scale=1.0) + WhiteKernel(noise_level=0.1)

# Matern 커널 (물리 시스템에 더 현실적)
kernel_matern = 1.0 * Matern(length_scale=1.0, nu=2.5)
```

> **핵심 직관**: 길이 스케일 $l$은 "입력이 얼마나 변해야 출력이 크게 변하는가"를 결정합니다. $l$이 크면 매끄러운 함수, 작으면 급격히 변하는 함수를 사전에 선호합니다.

---

## 4. 하이퍼파라미터 학습: 주변 우도

커널의 하이퍼파라미터 $\boldsymbol{\theta} = (l, \sigma_f, \sigma_n)$을 **주변 우도(marginal likelihood)** 최대화로 학습합니다.

$$\log p(\mathbf{y} \mid \mathbf{X}, \boldsymbol{\theta}) = -\frac{1}{2}\mathbf{y}^\top K_y^{-1}\mathbf{y} - \frac{1}{2}\log|K_y| - \frac{n}{2}\log 2\pi$$

여기서 $K_y = K(\mathbf{X}, \mathbf{X}) + \sigma_n^2 I$

| 항 | 역할 |
|----|------|
| $-\frac{1}{2}\mathbf{y}^\top K_y^{-1}\mathbf{y}$ | 데이터 적합도 (fit) |
| $-\frac{1}{2}\log|K_y|$ | 모델 복잡도 벌점 |
| $-\frac{n}{2}\log 2\pi$ | 정규화 상수 |

이 주변 우도는 자동으로 **오컴의 면도날**을 구현합니다. 너무 단순한 모델은 데이터를 설명하지 못하고, 너무 복잡한 모델은 복잡도 벌점을 받습니다.

```python
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, WhiteKernel

kernel = 1.0 * RBF(length_scale=1.0) + WhiteKernel(noise_level=0.1)
gpr = GaussianProcessRegressor(
    kernel=kernel,
    n_restarts_optimizer=10,  # 다중 초기점
    normalize_y=True
)
gpr.fit(X_train, y_train)
print(f"학습된 커널: {gpr.kernel_}")
```

> **핵심 직관**: 주변 우도는 cm-12에서 다룰 BIC와 유사하게 모델 적합도와 복잡도의 균형을 자동으로 맞춥니다. 교차 검증 없이도 하이퍼파라미터를 선택할 수 있습니다.

---

## 5. 예측 불확실성

GP의 가장 큰 강점은 예측 불확실성의 자연스러운 정량화입니다.

새로운 입력 $\mathbf{x}_*$에 대해:

$$f(\mathbf{x}_*) \sim \mathcal{N}(\mu_*, \sigma_*^2)$$

**신뢰 구간**: $\mu_* \pm 2\sigma_*$는 약 95% 신뢰 구간

```python
from sklearn.gaussian_process import GaussianProcessRegressor

gpr = GaussianProcessRegressor(kernel=kernel)
gpr.fit(X_train, y_train)

# 예측 평균과 표준편차
mu, sigma = gpr.predict(X_test, return_std=True)
# mu: 예측값, sigma: 불확실성

# 95% 신뢰 구간
upper = mu + 1.96 * sigma
lower = mu - 1.96 * sigma
```

**불확실성의 두 가지 원천**:

| 유형 | 원천 | GP에서의 표현 |
|------|------|-------------|
| 인식적 (epistemic) | 데이터 부족 | $\sigma_*^2$ (데이터 멀면 큼) |
| 우연적 (aleatoric) | 관측 노이즈 | $\sigma_n^2$ (고정) |

> **핵심 직관**: GP는 "모르는 것을 안다"는 것을 수학적으로 표현합니다. 데이터가 없는 영역에서는 불확실성이 크고, 데이터가 밀집한 곳에서는 확신을 가집니다.

---

## 6. 계산 복잡도와 확장

GP의 주요 병목은 역행렬 계산입니다.

| 연산 | 복잡도 |
|------|-------|
| 학습 ($K_y^{-1}$ 계산) | $O(n^3)$ |
| 예측 (평균) | $O(n)$ per query |
| 예측 (분산) | $O(n^2)$ per query |
| 메모리 | $O(n^2)$ |

$n > 10{,}000$이면 정확한 GP는 비실용적입니다. **근사 방법**:

| 방법 | 아이디어 | 복잡도 |
|------|---------|-------|
| 유도점 (Inducing points) | $m \ll n$개 유도점으로 근사 | $O(nm^2)$ |
| FITC | 유도점 기반 희소 GP | $O(nm^2)$ |
| 랜덤 피처 | 커널을 랜덤 피처로 근사 | $O(nm_r)$ |
| GPyTorch | GPU 가속, CG 기반 | 대규모 가능 |

```python
# sklearn의 GP는 소규모에 적합
from sklearn.gaussian_process import GaussianProcessRegressor

# 대규모 데이터: GPyTorch 또는 GPflow 사용 권장
# import gpytorch  # 유도점 기반 확장 GP
```

> **핵심 직관**: GP의 $O(n^3)$ 비용은 "정확한 불확실성의 대가"입니다. 근사 방법은 이 대가를 줄이지만 불확실성 추정의 정확도를 어느 정도 희생합니다.

---

## 7. GP의 응용과 베이즈 최적화

GP는 예측 자체뿐만 아니라 다른 알고리즘의 핵심 구성 요소로 활용됩니다.

**베이즈 최적화**: 평가 비용이 큰 블랙박스 함수의 최적화

1. GP로 목적 함수의 대리 모델(surrogate) 구축
2. **획득 함수(acquisition function)**로 다음 평가 지점 선택
3. 평가 후 GP 갱신, 반복

| 획득 함수 | 전략 |
|-----------|------|
| EI (Expected Improvement) | 현재 최적값 대비 기대 개선 |
| UCB (Upper Confidence Bound) | $\mu + \kappa\sigma$ 최대화 (탐색-활용) |
| PI (Probability of Improvement) | 개선 확률 최대화 |

cm-12에서 다룰 하이퍼파라미터 튜닝에 베이즈 최적화가 그리드 서치보다 효율적입니다.

```python
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import Matern

# 베이즈 최적화의 핵심 루프 (개념적)
gpr = GaussianProcessRegressor(kernel=Matern(nu=2.5))

# 1. 초기 데이터로 GP 학습
gpr.fit(X_observed, y_observed)

# 2. 예측 평균/분산으로 획득 함수 계산
mu, sigma = gpr.predict(X_candidates, return_std=True)
ei = expected_improvement(mu, sigma, y_best=y_observed.min())

# 3. 획득 함수 최대화 → 다음 평가 지점
x_next = X_candidates[np.argmax(ei)]
```

> **핵심 직관**: 베이즈 최적화에서 GP의 불확실성은 "아직 탐색하지 않은 영역"을 식별합니다. 불확실성이 큰 곳을 탐색하면 효율적으로 최적해를 찾을 수 있습니다.

---

## 핵심 정리

- **가우시안 프로세스는 함수에 대한 확률 분포로, 유한 개의 점에서의 함수값이 항상 다변량 정규 분포를 따르며, 평균 함수와 커널 함수로 정의됩니다**
- **GP 회귀의 사후 분포는 해석적으로 계산되며, 사후 평균은 관측값의 커널 가중합이고 사후 분산은 데이터에서 먼 곳에서 커집니다**
- **커널 함수는 함수의 사전 지식(매끄러움, 주기성 등)을 인코딩하며, 길이 스케일 $l$이 함수의 변동 폭을 결정합니다**
- **주변 우도는 데이터 적합도와 모델 복잡도의 균형을 자동으로 맞추어, 교차 검증 없이 하이퍼파라미터를 학습할 수 있게 합니다**
- **GP의 $O(n^3)$ 계산 비용은 대규모 데이터에서의 한계이지만, 유도점 기반 근사와 베이즈 최적화 등의 응용에서 예측 불확실성의 가치가 비용을 정당화합니다**
