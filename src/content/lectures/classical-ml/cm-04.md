# 결정 트리와 정보 이론

## 왜 결정 트리를 배워야 하는가

결정 트리는 인간이 직관적으로 이해할 수 있는 몇 안 되는 모델 중 하나이며, 비선형 결정 경계를 피처 스케일링 없이 자동으로 학습합니다. 또한 정보 이론의 핵심 개념인 엔트로피와 정보 이득이 트리 분할 기준으로 자연스럽게 연결됩니다. 이 강의에서 배우는 결정 트리는 cm-05와 cm-06에서 다룰 앙상블 방법의 기본 구성 요소(base learner)가 됩니다.

---

## 1. 결정 트리의 구조

결정 트리는 재귀적으로 피처 공간을 분할하여 축 정렬(axis-aligned) 결정 경계를 만듭니다.

| 용어 | 정의 |
|------|------|
| 루트 노드 | 전체 데이터셋, 최상위 분할 |
| 내부 노드 | 피처 $x_j$와 임계값 $t$에 대한 분할: $x_j \leq t$ |
| 리프 노드 | 최종 예측 (분류: 다수결, 회귀: 평균) |
| 깊이 | 루트에서 리프까지의 최대 경로 길이 |

**결정 규칙의 기하학**: 각 분할은 하나의 피처 축에 수직인 초평면이므로, 결정 영역은 초직사각형(hyperrectangle)의 합집합입니다.

```python
from sklearn.tree import DecisionTreeClassifier

tree = DecisionTreeClassifier(max_depth=5, criterion='entropy')
tree.fit(X_train, y_train)

# 트리 구조 확인
print(f"노드 수: {tree.tree_.node_count}")
print(f"피처 중요도: {tree.feature_importances_}")
```

> **핵심 직관**: 결정 트리는 cm-01의 선형 분류기와 달리, 피처 공간을 축 방향 직사각형으로 분할합니다. 이 단순한 분할이 깊이가 깊어지면 매우 복잡한 경계를 표현할 수 있습니다.

---

## 2. 엔트로피와 정보량

**정보량(self-information)**: 확률 $p$인 사건의 정보량은

$$I(p) = -\log_2 p$$

확률이 낮을수록 정보량이 큽니다. $p=1$이면 정보량은 0입니다.

**엔트로피(entropy)**: 확률 분포 $\mathbf{p} = (p_1, \ldots, p_K)$의 불확실성 측도

$$H(\mathbf{p}) = -\sum_{k=1}^{K} p_k \log_2 p_k$$

| 분포 | 엔트로피 | 의미 |
|------|---------|------|
| $(1, 0)$ | 0 | 완전 확실 |
| $(0.5, 0.5)$ | 1 bit | 최대 불확실 (이진) |
| $(1/K, \ldots, 1/K)$ | $\log_2 K$ | 최대 불확실 ($K$-클래스) |
| $(0.9, 0.1)$ | 0.469 bit | 대체로 확실 |

> **핵심 직관**: 엔트로피는 "예측이 얼마나 불확실한가"를 측정합니다. 노드의 엔트로피가 0이면 해당 노드의 모든 샘플이 같은 클래스에 속합니다.

---

## 3. 정보 이득과 분할 기준

**정보 이득(Information Gain)**: 피처 $A$로 분할했을 때 엔트로피의 감소량

$$\text{IG}(S, A) = H(S) - \sum_{v \in \text{values}(A)} \frac{|S_v|}{|S|} H(S_v)$$

여기서 $S$는 부모 노드의 데이터, $S_v$는 분할 후 자식 노드의 데이터입니다.

**연속 피처의 분할**: 피처 $x_j$의 모든 가능한 임계값 $t$에 대해 정보 이득을 계산하고, 최대 이득을 주는 $(j, t)$ 쌍을 선택합니다.

```python
import numpy as np

def entropy(y):
    _, counts = np.unique(y, return_counts=True)
    probs = counts / len(y)
    return -np.sum(probs * np.log2(probs + 1e-10))

def information_gain(y, left_idx, right_idx):
    H_parent = entropy(y)
    n = len(y)
    H_children = (len(left_idx)/n * entropy(y[left_idx]) +
                  len(right_idx)/n * entropy(y[right_idx]))
    return H_parent - H_children
```

**ID3 / C4.5 / CART 비교**:

| 알고리즘 | 분할 기준 | 분할 방식 | 피처 유형 |
|----------|----------|----------|----------|
| ID3 | 정보 이득 | 다중 분할 | 범주형만 |
| C4.5 | 이득 비율 | 다중 분할 | 범주형 + 연속 |
| CART | Gini 불순도 | 이진 분할 | 범주형 + 연속 |

> **핵심 직관**: 정보 이득은 "이 분할이 클래스 예측의 불확실성을 얼마나 줄이는가"를 측정합니다. 스팸 필터에서 "무료"라는 단어 유무로 분할하면 엔트로피가 크게 감소합니다.

---

## 4. Gini 불순도

sklearn의 CART 알고리즘은 기본적으로 **Gini 불순도**를 사용합니다:

$$\text{Gini}(\mathbf{p}) = 1 - \sum_{k=1}^K p_k^2 = \sum_{k \neq l} p_k p_l$$

Gini 불순도의 직관적 해석: 두 개의 샘플을 무작위로 뽑아 클래스 분포에 따라 독립적으로 레이블을 매기면, 서로 다른 레이블이 될 확률입니다.

**엔트로피 vs Gini 비교** (이진 분류, $p$는 양성 클래스 비율):

| $p$ | 엔트로피 | Gini |
|-----|---------|------|
| 0.0 | 0 | 0 |
| 0.1 | 0.469 | 0.180 |
| 0.3 | 0.881 | 0.420 |
| 0.5 | 1.000 | 0.500 |

두 지표는 실제 성능 차이가 거의 없지만, Gini가 계산이 더 빠릅니다 ($\log$ 연산 불필요).

```python
from sklearn.tree import DecisionTreeClassifier

tree_gini = DecisionTreeClassifier(criterion='gini')
tree_entropy = DecisionTreeClassifier(criterion='entropy')
# 실무에서 두 기준의 성능 차이는 미미합니다
```

> **핵심 직관**: Gini 불순도와 엔트로피는 모두 불확실성의 측도이며, $p$의 2차 근사로 보면 Gini $\approx$ 엔트로피의 스케일된 버전입니다.

---

## 5. 가지치기(Pruning)

가지치기 없는 결정 트리는 학습 데이터를 완벽히 암기하여 과적합됩니다.

**사전 가지치기(Pre-pruning)**: 트리 성장 중 제한

| 파라미터 | 효과 |
|---------|------|
| `max_depth` | 트리 최대 깊이 제한 |
| `min_samples_split` | 분할에 필요한 최소 샘플 수 |
| `min_samples_leaf` | 리프에 필요한 최소 샘플 수 |
| `max_leaf_nodes` | 리프 노드의 최대 수 |
| `min_impurity_decrease` | 최소 불순도 감소 요구 |

**사후 가지치기(Post-pruning)**: 완전히 성장한 트리를 축소

**비용 복잡도 가지치기(Cost-Complexity Pruning)**: 하위 트리 $T_t$를 리프로 대체할 때의 비용:

$$R_\alpha(T) = R(T) + \alpha |T|$$

여기서 $R(T)$는 오분류율, $|T|$는 리프 수, $\alpha$는 복잡도 벌점입니다.

```python
from sklearn.tree import DecisionTreeClassifier

# 비용 복잡도 가지치기 경로 계산
clf = DecisionTreeClassifier(random_state=42)
clf.fit(X_train, y_train)
path = clf.cost_complexity_pruning_path(X_train, y_train)
alphas = path.ccp_alphas

# 최적 alpha를 교차 검증으로 선택 (cm-12에서 상세히 다룸)
from sklearn.model_selection import cross_val_score
scores = [cross_val_score(
    DecisionTreeClassifier(ccp_alpha=a), X_train, y_train, cv=5
).mean() for a in alphas]
```

> **핵심 직관**: 과적합 없는 결정 트리란 존재하지 않습니다. 가지치기는 편향-분산 트레이드오프에서 적절한 지점을 선택하는 것입니다.

---

## 6. 회귀 트리

결정 트리는 회귀 문제에도 적용됩니다. 분할 기준으로 **분산 감소(variance reduction)**를 사용합니다:

$$\text{VR}(S, A) = \text{Var}(S) - \sum_{v} \frac{|S_v|}{|S|} \text{Var}(S_v)$$

각 리프 노드의 예측값은 해당 영역의 **평균값**입니다:

$$\hat{y}(\mathbf{x}) = \frac{1}{|R_m|}\sum_{\mathbf{x}_i \in R_m} y_i, \quad \mathbf{x} \in R_m$$

```python
from sklearn.tree import DecisionTreeRegressor

reg_tree = DecisionTreeRegressor(max_depth=4, min_samples_leaf=10)
reg_tree.fit(X_train, y_train)
# 회귀 트리의 예측은 구간 상수 함수 (계단 함수)
```

> **핵심 직관**: 회귀 트리는 피처 공간을 직사각형으로 나누고, 각 영역에서 상수를 예측합니다. 깊이가 깊어질수록 계단 함수가 연속 함수에 근사합니다.

---

## 7. 결정 트리의 장단점과 확장

| 장점 | 단점 |
|------|------|
| 해석 가능성 높음 | 높은 분산 (과적합) |
| 피처 스케일링 불필요 | 축 정렬 경계만 가능 |
| 범주형/연속 피처 혼합 처리 | 불안정 (데이터 변화에 민감) |
| 결측치 처리 가능 (C4.5) | 최적 트리 찾기는 NP-완전 |

결정 트리의 높은 분산은 치명적 약점입니다. 학습 데이터가 약간만 바뀌어도 전혀 다른 트리가 생성될 수 있습니다. 이 문제를 해결하기 위해:

- cm-05: **Bagging/Random Forest** — 다수의 트리를 평균하여 분산 감소
- cm-06: **Boosting** — 약한 트리를 순차적으로 결합하여 편향 감소

> **핵심 직관**: 단일 결정 트리는 "불안정한 전문가"입니다. 앙상블은 불안정한 전문가 여럿의 의견을 종합하여 안정적이고 정확한 예측을 만듭니다.

---

## 핵심 정리

- **결정 트리는 피처 공간을 축 정렬 초직사각형으로 재귀 분할하며, 정보 이득 또는 Gini 불순도를 기준으로 최적 분할을 선택합니다**
- **엔트로피 $H = -\sum p_k \log_2 p_k$는 분포의 불확실성을 측정하며, 정보 이득은 분할에 의한 엔트로피 감소량입니다**
- **Gini 불순도 $1 - \sum p_k^2$는 엔트로피의 실용적 대안이며, 계산이 빠르고 실제 성능 차이는 미미합니다**
- **가지치기는 결정 트리의 과적합을 방지하는 필수 기법이며, 비용 복잡도 가지치기($R_\alpha = R + \alpha|T|$)가 대표적입니다**
- **단일 결정 트리의 높은 분산은 앙상블(Bagging, Boosting)의 기본 학습기로 활용될 때 강점으로 전환되며, 이것이 Random Forest와 Gradient Boosting의 출발점입니다**
