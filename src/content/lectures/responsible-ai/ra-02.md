# 알고리즘 공정성

## 왜 공정성이 중요한가

AI 시스템이 채용, 대출, 형사 사법 등에 적용될 때, 특정 집단에 불리한 결정을 체계적으로 내린다면 이는 사회적 신뢰를 훼손하고 법적 문제를 야기합니다. 2016년 COMPAS 재범 예측 시스템이 흑인 피고인에게 높은 위험도를 과잉 예측한 사례는, 공정성에 대한 명확한 정의와 측정 없이는 "좋은 성능"이 곧 "좋은 AI"가 아님을 보여줍니다. 이 강의에서는 공정성의 수학적 정의들과 그 한계를 다룹니다.

> **핵심 직관**: 공정성은 단일한 정의가 아닙니다. 어떤 공정성 정의를 선택할지는 기술적 문제가 아니라 사회적, 맥락적 판단의 문제입니다.

## 1. 공정성의 다양한 정의

### 1.1 그룹 공정성 (Group Fairness)

보호 속성(예: 성별, 인종)으로 정의된 집단 간 결과의 통계적 균등을 요구합니다.

**인구 통계적 동등성 (Demographic Parity)**

$$P(\hat{Y}=1 \mid A=0) = P(\hat{Y}=1 \mid A=1)$$

보호 속성 $A$와 무관하게 긍정적 예측 비율이 동일해야 합니다.

**균등 기회 (Equalized Odds)**

$$P(\hat{Y}=1 \mid A=a, Y=y) = P(\hat{Y}=1 \mid A=b, Y=y), \quad \forall y \in \{0,1\}$$

실제 레이블 $Y$를 조건으로 할 때, 보호 속성에 따른 예측 차이가 없어야 합니다.

**기회 균등 (Equal Opportunity)**

균등 기회의 완화된 버전으로, 양성 클래스($Y=1$)에 대해서만 균등을 요구합니다.

$$P(\hat{Y}=1 \mid A=0, Y=1) = P(\hat{Y}=1 \mid A=1, Y=1)$$

### 1.2 개인 공정성 (Individual Fairness)

유사한 개인은 유사한 결과를 받아야 한다는 직관을 형식화합니다.

$$d_{\text{outcome}}(f(x_i), f(x_j)) \leq L \cdot d_{\text{input}}(x_i, x_j)$$

여기서 $d_{\text{input}}$은 "관련 속성" 기준 유사도를 나타내며, 이 거리 함수의 정의 자체가 도메인 전문 지식을 요구합니다.

| 공정성 유형 | 단위 | 장점 | 한계 |
|------------|------|------|------|
| 인구 통계적 동등성 | 집단 | 직관적, 측정 용이 | 자격 차이 무시 |
| 균등 기회 | 집단 | 실제 성과 반영 | 레이블 신뢰도 필요 |
| 기회 균등 | 집단 | 양성에 집중, 실용적 | 음성 결과 무시 |
| 개인 공정성 | 개인 | 세밀한 공정성 | 거리 함수 정의 어려움 |
| 반사실적 공정성 | 개인 | 인과적 해석 가능 | 반사실 세계 가정 필요 |

## 2. 불가능 정리 (Impossibility Theorem)

> **핵심 직관**: Chouldechova(2017)와 Kleinberg et al.(2016)의 불가능 정리는 기저율(base rate)이 다른 집단에서 복수의 공정성 조건을 동시에 만족시키는 것이 수학적으로 불가능함을 증명합니다. 이는 공정성 "선택"이 필연적임을 의미합니다.

### 수학적 정리

보호 속성 $A$에 대해 기저율이 다를 때 ($P(Y=1|A=0) \neq P(Y=1|A=1)$), 다음 세 조건을 동시에 만족하는 것은 불가능합니다:

1. **예측 동등성**: $P(Y=1|\hat{Y}=1, A=0) = P(Y=1|\hat{Y}=1, A=1)$ (PPV 균등)
2. **균등 기회**: $P(\hat{Y}=1|Y=1, A=0) = P(\hat{Y}=1|Y=1, A=1)$ (TPR 균등)
3. **거짓 양성 균등**: $P(\hat{Y}=1|Y=0, A=0) = P(\hat{Y}=1|Y=0, A=1)$ (FPR 균등)

```
┌───────────────────────────────────────────────┐
│          공정성 정의 선택 의사결정               │
├───────────────────────────────────────────────┤
│                                               │
│  기저율이 집단 간 동일한가?                     │
│     ├── YES → 세 조건 동시 충족 가능            │
│     └── NO  → 어떤 조건을 우선할 것인가?        │
│              ├── 결과 균등 중시                  │
│              │   → 인구 통계적 동등성            │
│              ├── 적격자 보호 중시                │
│              │   → 기회 균등                     │
│              └── 정밀도 균등 중시                │
│                  → 예측 동등성                   │
│                                               │
│  ※ 선택은 도메인과 이해관계자에 의해 결정        │
│    (ra-10 참조)                                │
└───────────────────────────────────────────────┘
```

## 3. 공정성 측정 지표

### 3.1 주요 지표 정의

| 지표 | 수식 | 의미 |
|------|------|------|
| Statistical Parity Difference (SPD) | $P(\hat{Y}=1|A=1) - P(\hat{Y}=1|A=0)$ | 집단 간 양성 예측 비율 차이 |
| Disparate Impact Ratio (DIR) | $\frac{P(\hat{Y}=1|A=0)}{P(\hat{Y}=1|A=1)}$ | 4/5 규칙: DIR < 0.8이면 불균형 |
| Equal Opportunity Difference (EOD) | $TPR_{A=1} - TPR_{A=0}$ | 참양성률 차이 |
| Average Odds Difference (AOD) | $\frac{(FPR_{A=1}-FPR_{A=0})+(TPR_{A=1}-TPR_{A=0})}{2}$ | 균등 기회 위반 정도 |

### 3.2 Fairlearn을 이용한 측정

```python
from fairlearn.metrics import (
    demographic_parity_difference,
    equalized_odds_difference,
    MetricFrame
)
from sklearn.metrics import accuracy_score, precision_score, recall_score
import numpy as np

# 시나리오: 채용 AI의 공정성 측정
y_true = np.array([1, 0, 1, 1, 0, 1, 0, 0, 1, 1])
y_pred = np.array([1, 0, 1, 0, 0, 1, 1, 0, 0, 1])
sensitive = np.array([0, 0, 0, 0, 0, 1, 1, 1, 1, 1])  # 0: 남성, 1: 여성

# 그룹별 성능 비교
metric_frame = MetricFrame(
    metrics={"accuracy": accuracy_score, "recall": recall_score},
    y_true=y_true,
    y_pred=y_pred,
    sensitive_features=sensitive
)
print(metric_frame.by_group)

# 공정성 지표 계산
spd = demographic_parity_difference(y_true, y_pred, sensitive_features=sensitive)
eod = equalized_odds_difference(y_true, y_pred, sensitive_features=sensitive)
print(f"Statistical Parity Difference: {spd:.3f}")
print(f"Equalized Odds Difference: {eod:.3f}")
```

## 4. 교차성과 다중 속성

실제 시나리오에서 보호 속성은 하나가 아닙니다. 성별과 인종이 교차하는 집단(예: 흑인 여성)은 각 속성 단독 분석에서는 드러나지 않는 차별을 겪을 수 있습니다. 이를 **교차 공정성(intersectional fairness)**이라 합니다.

### 시나리오: 신용평가 모델의 교차 분석

한 은행의 신용평가 모델이 성별별, 인종별로는 공정성 기준을 통과하지만, "젊은 여성" 하위 집단에서 승인율이 현저히 낮은 경우가 발생합니다. 이는 단일 속성 분석만으로는 발견할 수 없으며, 교차 분석이 필수적입니다.

```python
# 교차 공정성 분석
metric_frame_intersect = MetricFrame(
    metrics={"selection_rate": lambda y_t, y_p: np.mean(y_p)},
    y_true=y_true,
    y_pred=y_pred,
    sensitive_features={"gender": sensitive, "age_group": age_group}
)
print(metric_frame_intersect.by_group)
# 교차 집단별 선택률을 확인하여 숨겨진 차별 탐지
```

## 5. 공정성과 성능의 트레이드오프

공정성 제약을 적용하면 전체 정확도가 일부 하락할 수 있습니다. 이는 모델 선택(ms-02)과 하이퍼파라미터 튜닝(ms-05)에서 공정성 지표를 함께 고려해야 함을 의미합니다.

> **핵심 직관**: 공정성-정확도 트레이드오프는 항상 존재하는 것이 아닙니다. 편향이 노이즈인 경우, 편향 제거가 오히려 성능을 향상시킬 수 있습니다(ra-03 참조).

| 완화 단계 | 기법 | 정확도 영향 | 공정성 개선 |
|-----------|------|------------|------------|
| 전처리 | 리샘플링, 리웨이팅 | 낮음 | 중간 |
| 학습 중 | 제약 최적화, 적대적 학습 | 중간 | 높음 |
| 후처리 | 임계값 조정 | 낮음~중간 | 중간~높음 |

이러한 완화 기법의 구체적 구현은 ra-03에서 상세히 다룹니다.

## 핵심 정리

- 공정성은 인구 통계적 동등성, 균등 기회, 기회 균등, 개인 공정성 등 다양한 정의가 존재하며, 각각 서로 다른 가치를 반영합니다
- 불가능 정리에 의해 기저율이 다른 집단에서 복수의 공정성 조건을 동시에 만족시키는 것은 수학적으로 불가능합니다
- SPD, DIR, EOD, AOD 등의 지표로 공정성을 정량적으로 측정하며, Fairlearn 같은 라이브러리를 활용합니다
- 교차 공정성 분석은 단일 보호 속성 분석에서 드러나지 않는 차별을 탐지하는 데 필수적입니다
- 공정성과 성능의 트레이드오프는 도메인 맥락에 따라 이해관계자와 함께 결정해야 하며, 편향 제거가 성능을 향상시키는 경우도 존재합니다
