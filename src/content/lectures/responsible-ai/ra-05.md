# 사후 설명 기법

## 왜 사후 설명이 중요한가

ra-04에서 해석 가능한 모델의 장점을 확인했습니다. 그러나 이미지 분류(dl-06), 자연어 처리(le-01), 복잡한 비정형 데이터 분석 등에서는 블랙박스 모델이 압도적 성능을 보입니다. 이런 모델의 의사결정을 사후적으로 설명하는 기법이 필요합니다. 사후 설명은 모델 디버깅, 신뢰 구축, 규제 준수(ra-08), 공정성 감사(ra-02)에 핵심적 역할을 합니다.

> **핵심 직관**: 사후 설명은 "모델이 실제로 어떻게 작동하는가"가 아니라 "모델의 행동을 사람이 이해할 수 있는 형태로 근사하는 것"입니다. 설명의 충실도(fidelity)와 단순성(simplicity) 사이에는 항상 트레이드오프가 존재합니다.

## 1. 설명 기법의 분류

| 기준 | 분류 | 예시 |
|------|------|------|
| 범위 | 글로벌 / 로컬 | 전체 모델 동작 vs 개별 예측 설명 |
| 모델 의존성 | 모델-불가지론 / 모델-특정 | SHAP(불가지론) vs 어텐션(특정) |
| 설명 형태 | 피처 중요도 / 규칙 / 예시 / 반사실 | 수치, IF-THEN, 유사 사례, 가정 |

```
┌─────────────────────────────────────────────────────┐
│              설명 기법 분류 체계                      │
├─────────────────────────────────────────────────────┤
│                                                     │
│              ┌─── 글로벌 ───┐                       │
│              │              │                       │
│         순열 중요도    부분 의존 도표                 │
│         (PFI)         (PDP)                         │
│                                                     │
│              ┌─── 로컬 ─────┐                       │
│              │              │                       │
│           SHAP           LIME                       │
│         (Shapley)    (로컬 대리)                     │
│                                                     │
│              ┌─── 예시 기반 ─┐                       │
│              │              │                       │
│         반사실적 설명    프로토타입                    │
│         (Counterfactual) (Prototype)                │
│                                                     │
│              ┌─── 개념 기반 ─┐                       │
│              │              │                       │
│           TCAV          ConceptSHAP                  │
│                                                     │
└─────────────────────────────────────────────────────┘
```

## 2. SHAP (SHapley Additive exPlanations)

### 2.1 이론적 기초

SHAP은 게임이론의 Shapley 값을 기반으로, 각 피처의 예측에 대한 기여를 공정하게 배분합니다.

$$\phi_j = \sum_{S \subseteq N \setminus \{j\}} \frac{|S|!(|N|-|S|-1)!}{|N|!} \left[ f(S \cup \{j\}) - f(S) \right]$$

Shapley 값은 다음 네 가지 공리를 유일하게 만족합니다:
- **효율성**: 모든 피처 기여의 합 = 예측값 - 기대값
- **대칭성**: 동일한 기여를 하는 피처는 동일한 Shapley 값
- **더미**: 기여 없는 피처의 Shapley 값 = 0
- **가산성**: 독립 모델의 Shapley 값은 합산 가능

### 2.2 SHAP 구현

```python
import shap
import xgboost as xgb

# 모델 학습
model = xgb.XGBClassifier(n_estimators=200, max_depth=6)
model.fit(X_train, y_train)

# TreeSHAP (트리 기반 모델 전용, O(TLD) 시간)
explainer = shap.TreeExplainer(model)
shap_values = explainer.shap_values(X_test)

# 글로벌 피처 중요도 (평균 |SHAP|)
shap.summary_plot(shap_values, X_test, feature_names=feature_names)

# 로컬 설명: 개별 예측의 피처 기여
shap.force_plot(explainer.expected_value, shap_values[0], X_test.iloc[0])

# 상호작용 효과 분석
shap_interaction = explainer.shap_interaction_values(X_test[:100])
shap.summary_plot(shap_interaction, X_test[:100])
```

### 시나리오: 보험 청구 사기 탐지 설명

보험 회사의 사기 탐지 모델이 특정 청구를 "사기 의심"으로 분류했습니다. 심사원에게 다음과 같은 SHAP 설명을 제공합니다:

- `claim_amount = $50,000` → SHAP: +0.35 (사기 방향으로 강한 기여)
- `days_since_policy = 15` → SHAP: +0.28 (가입 직후 청구)
- `previous_claims = 0` → SHAP: -0.12 (사기 의심 감소)
- 기저 확률: 0.05 → 최종 예측: 0.56

## 3. LIME (Local Interpretable Model-agnostic Explanations)

### 3.1 원리

LIME은 설명하려는 인스턴스 주변에서 해석 가능한 대리 모델(주로 선형)을 학습합니다.

$$\xi(x) = \arg\min_{g \in G} \mathcal{L}(f, g, \pi_x) + \Omega(g)$$

- $f$: 블랙박스 모델
- $g$: 해석 가능한 대리 모델
- $\pi_x$: $x$ 주변의 근접성 커널
- $\Omega(g)$: 대리 모델의 복잡도 제약

```python
import lime
import lime.lime_tabular

# LIME 설명기 생성
lime_explainer = lime.lime_tabular.LimeTabularExplainer(
    training_data=X_train.values,
    feature_names=feature_names,
    class_names=["정상", "사기"],
    mode="classification"
)

# 개별 예측 설명
explanation = lime_explainer.explain_instance(
    X_test.iloc[0].values,
    model.predict_proba,
    num_features=10,
    num_samples=5000  # 섭동 샘플 수
)
explanation.show_in_notebook()
```

### 3.2 SHAP vs LIME 비교

| 기준 | SHAP | LIME |
|------|------|------|
| 이론적 기반 | Shapley 값 (게임이론) | 로컬 대리 모델 |
| 일관성 보장 | 4가지 공리 만족 | 보장 없음 |
| 계산 비용 | TreeSHAP: 효율적 / KernelSHAP: 높음 | 섭동 기반: 중간 |
| 글로벌 설명 | 자연스러운 집계 | 불가 |
| 안정성 | 결정적 (TreeSHAP) | 확률적 (샘플링 의존) |
| 상호작용 | 상호작용 값 제공 | 제한적 |

> **핵심 직관**: SHAP은 이론적으로 우수하지만 계산 비용이 높을 수 있습니다. LIME은 빠르지만 안정성이 낮습니다. 실무에서는 TreeSHAP(트리 모델)이나 GradientSHAP(신경망)으로 효율성과 이론적 엄밀성을 모두 확보할 수 있습니다.

## 4. 피처 중요도 기법

### 4.1 순열 중요도 (Permutation Feature Importance)

```python
from sklearn.inspection import permutation_importance

# 피처 순열 후 성능 하락 측정
perm_result = permutation_importance(
    model, X_test, y_test,
    n_repeats=30,
    random_state=42,
    scoring="roc_auc"
)

for i in perm_result.importances_mean.argsort()[::-1][:10]:
    print(f"{feature_names[i]}: {perm_result.importances_mean[i]:.4f} "
          f"(+/- {perm_result.importances_std[i]:.4f})")
```

## 5. 반사실적 설명 (Counterfactual Explanations)

"예측을 바꾸려면 입력을 어떻게 바꿔야 하는가?"에 답합니다.

$$\text{argmin}_{x'} \; d(x, x') \quad \text{s.t.} \quad f(x') \neq f(x)$$

```python
import dice_ml

# DiCE (Diverse Counterfactual Explanations)
data_dice = dice_ml.Data(
    dataframe=df,
    continuous_features=["income", "age", "credit_score"],
    outcome_name="approved"
)
model_dice = dice_ml.Model(model=model, backend="sklearn")
exp = dice_ml.Dice(data_dice, model_dice, method="random")

# 반사실적 설명 생성: "거절"을 "승인"으로 바꾸려면?
counterfactuals = exp.generate_counterfactuals(
    query_instances=X_test.iloc[:1],
    total_CFs=3,          # 3가지 대안 제시
    desired_class="opposite"
)
counterfactuals.visualize_as_dataframe()
# 예: income을 $35K→$45K로 올리거나, credit_score를 650→720으로 올리면 승인
```

### 시나리오: 대출 거절 사유 설명

고객이 대출 거절 사유를 요청합니다. 반사실적 설명으로 "승인을 받으려면 어떤 조건이 필요한가"를 구체적으로 제시합니다. 이는 EU AI Act의 설명 의무(ra-01)를 충족하며, 고객의 실행 가능한 피드백(actionable feedback)을 제공합니다.

## 6. 개념 기반 설명 (Concept-based Explanations)

### 6.1 TCAV (Testing with Concept Activation Vectors)

피처 수준이 아닌 인간이 이해하는 개념 수준에서 설명을 제공합니다. 예를 들어 "이 모델은 줄무늬 패턴 개념에 얼마나 민감한가?"를 측정합니다 (dl-06 CNN 중간층 활성화 참조).

```python
# TCAV 개념적 사용 예시
# 1. 개념 데이터셋 준비 (예: "줄무늬" 이미지 30장)
# 2. 활성화 벡터(CAV) 학습: 개념 vs 비개념 방향
# 3. 방향 미분으로 개념 민감도 점수 계산

# 결과 해석:
# "얼룩말 분류에서 줄무늬 개념의 TCAV 점수: 0.92"
# → 모델이 줄무늬에 강하게 의존 (기대와 일치)
# "얼룩말 분류에서 배경색 개념의 TCAV 점수: 0.78"
# → 배경색에도 의존 (스퓨리어스 상관, 주의 필요)
```

## 7. 설명 기법 선택 가이드

```
┌──────────────────────────────────────────────────────┐
│            설명 기법 선택 의사결정                     │
├──────────────────────────────────────────────────────┤
│                                                      │
│  설명의 목적은?                                       │
│     ├── 모델 디버깅 → 글로벌 SHAP + PFI              │
│     ├── 개별 예측 해명 → 로컬 SHAP 또는 LIME          │
│     ├── 실행 가능한 조언 → 반사실적 설명 (DiCE)       │
│     ├── 고수준 개념 검증 → TCAV                       │
│     └── 규제 준수 문서화 → SHAP + 반사실적 (ra-08)    │
│                                                      │
│  모델 유형은?                                         │
│     ├── 트리 기반 → TreeSHAP (효율적)                 │
│     ├── 신경망 → GradientSHAP, DeepLIFT              │
│     └── 임의 모델 → KernelSHAP, LIME                 │
│                                                      │
└──────────────────────────────────────────────────────┘
```

> **핵심 직관**: 하나의 설명 기법만 사용하지 마십시오. SHAP으로 피처 기여를 파악하고, 반사실적 설명으로 실행 가능한 조언을 제공하며, TCAV로 고수준 개념을 검증하는 다층적 설명이 가장 효과적입니다.

## 핵심 정리

- SHAP은 게임이론 기반의 Shapley 값으로 피처 기여를 공정하게 배분하며, 효율성/대칭성/더미/가산성의 4가지 공리를 유일하게 만족합니다
- LIME은 로컬 대리 모델을 통해 빠르게 설명을 생성하지만, 샘플링에 의존하여 안정성이 SHAP보다 낮습니다
- 반사실적 설명은 "예측을 바꾸려면 무엇을 변경해야 하는가"라는 실행 가능한 피드백을 제공합니다
- TCAV 등 개념 기반 설명은 피처가 아닌 인간이 이해하는 고수준 개념으로 모델 행동을 해석합니다
- 설명의 목적(디버깅, 해명, 조언, 검증, 규제)에 따라 적절한 기법을 조합하여 다층적 설명을 제공해야 합니다
