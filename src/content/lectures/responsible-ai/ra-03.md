# 편향의 원천과 완화

## 왜 편향 완화가 중요한가

ra-02에서 공정성의 정의와 측정 지표를 학습했습니다. 그러나 공정성 위반을 "감지"하는 것만으로는 부족합니다. 편향이 어디서 발생하고, 어떻게 전파되며, 어떤 기법으로 완화할 수 있는지를 체계적으로 이해해야 합니다. 편향은 데이터 수집부터 모델 배포까지 ML 파이프라인(mo-03)의 모든 단계에서 유입될 수 있으며, 단일 지점의 수정만으로는 해결되지 않습니다.

> **핵심 직관**: 편향은 "나쁜 데이터" 하나의 문제가 아닙니다. 데이터 수집, 레이블링, 피처 선택, 알고리즘 설계, 평가, 배포 후 피드백 루프 등 전체 파이프라인에 걸쳐 발생합니다.

## 1. 편향의 원천 분류

### 1.1 데이터 편향

| 편향 유형 | 설명 | 예시 |
|-----------|------|------|
| 선택 편향 (Selection Bias) | 표본이 모집단을 대표하지 못함 | 온라인 설문 → 디지털 접근성 높은 집단 과대표현 |
| 역사적 편향 (Historical Bias) | 과거의 차별이 데이터에 반영 | 과거 채용 데이터에 성별 편향 내재 |
| 측정 편향 (Measurement Bias) | 피처/레이블이 실제를 정확히 반영하지 못함 | 체포 데이터를 범죄율의 프록시로 사용 |
| 집계 편향 (Aggregation Bias) | 이질적 집단을 단일 모델로 처리 | 전 인종에 동일한 의료 예측 모델 적용 |
| 표현 편향 (Representation Bias) | 특정 하위 집단의 데이터 부족 | 소수 인종의 얼굴 이미지 부족 |

### 1.2 알고리즘 편향

- **학습 목표 편향**: 최적화 목표가 공정성을 반영하지 못함 (정확도만 최적화 → 다수 집단 편향)
- **귀납 편향**: 모델 구조 자체가 특정 패턴을 선호 (dl-02)
- **과적합 편향**: 소수 집단 데이터 부족 시 소수 집단에 대한 일반화 실패 (pt-08)

### 1.3 평가 및 배포 편향

- **벤치마크 편향**: 평가 데이터셋이 다양성을 반영하지 못함
- **피드백 루프 편향**: 모델 예측이 향후 데이터 생성에 영향 (예: 범죄 예측 → 특정 지역 순찰 강화 → 더 많은 체포 → 편향 강화)

```
┌────────────────────────────────────────────────────────┐
│              편향의 전파 경로                            │
├────────────────────────────────────────────────────────┤
│                                                        │
│  [실세계] → [데이터수집] → [전처리] → [모델학습]         │
│    ↑         역사적편향    선택편향    알고리즘편향       │
│    │         표현편향      측정편향    귀납편향           │
│    │                                                   │
│    │       → [평가] → [배포] → [피드백] ─────┐          │
│    │         벤치마크   사용편향              │          │
│    │         편향                             │          │
│    └─────────────────────────────────────────┘          │
│              피드백 루프 (편향 증폭)                     │
└────────────────────────────────────────────────────────┘
```

## 2. 전처리 편향 완화 기법

데이터 수준에서 편향을 완화하는 기법으로, 모델에 독립적이라는 장점이 있습니다.

### 2.1 리웨이팅 (Reweighting)

각 (보호속성, 레이블) 조합에 가중치를 부여하여 데이터 분포를 균형화합니다.

```python
from aif360.datasets import BinaryLabelDataset
from aif360.algorithms.preprocessing import Reweighing

# 시나리오: 대출 승인 데이터에서 성별 편향 완화
dataset = BinaryLabelDataset(
    df=loan_df,
    label_names=["approved"],
    protected_attribute_names=["gender"]
)

rw = Reweighing(
    unprivileged_groups=[{"gender": 0}],
    privileged_groups=[{"gender": 1}]
)
dataset_rw = rw.fit_transform(dataset)
# dataset_rw.instance_weights에 조정된 가중치 반영
```

### 2.2 디스패리트 임팩트 제거기 (Disparate Impact Remover)

피처 값을 보호 속성과의 상관관계를 줄이는 방향으로 변환합니다.

```python
from aif360.algorithms.preprocessing import DisparateImpactRemover

di = DisparateImpactRemover(repair_level=1.0)  # 0~1, 수정 강도
dataset_repaired = di.fit_transform(dataset)
```

## 3. 학습 중 편향 완화 기법

### 3.1 제약 최적화 (Constrained Optimization)

공정성 제약을 최적화 문제에 직접 포함합니다. Fairlearn의 `ExponentiatedGradient`가 대표적입니다.

$$\min_{\theta} \mathcal{L}(\theta) \quad \text{s.t.} \quad |g_a(\theta)| \leq \epsilon, \quad \forall a$$

여기서 $g_a(\theta)$는 그룹 $a$에 대한 공정성 위반 정도입니다.

```python
from fairlearn.reductions import ExponentiatedGradient, DemographicParity
from sklearn.linear_model import LogisticRegression

estimator = LogisticRegression(max_iter=1000)
constraint = DemographicParity()

mitigator = ExponentiatedGradient(estimator, constraint)
mitigator.fit(X_train, y_train, sensitive_features=sensitive_train)

y_pred_fair = mitigator.predict(X_test)
```

### 3.2 적대적 탈편향 (Adversarial Debiasing)

보호 속성을 예측하지 못하도록 적대적 네트워크를 학습에 결합합니다 (dl-08 GAN 구조 참조).

```python
from aif360.algorithms.inprocessing import AdversarialDebiasing
import tensorflow as tf

sess = tf.compat.v1.Session()
adv_model = AdversarialDebiasing(
    privileged_groups=[{"gender": 1}],
    unprivileged_groups=[{"gender": 0}],
    scope_name="adversarial_debiasing",
    debias=True,
    sess=sess
)
adv_model.fit(train_dataset)
pred_dataset = adv_model.predict(test_dataset)
```

> **핵심 직관**: 적대적 탈편향은 모델이 "유용한 정보는 학습하되, 보호 속성에 관한 정보는 잊도록" 유도합니다. 이는 표현 학습(dl-05)에서 정보 병목 원리와 유사합니다.

## 4. 후처리 편향 완화 기법

### 4.1 임계값 조정 (Threshold Adjustment)

그룹별로 서로 다른 결정 임계값을 적용하여 공정성을 확보합니다.

```python
from fairlearn.postprocessing import ThresholdOptimizer

postprocessor = ThresholdOptimizer(
    estimator=base_model,
    constraints="equalized_odds",
    objective="accuracy_score"
)
postprocessor.fit(X_train, y_train, sensitive_features=sensitive_train)
y_pred_adjusted = postprocessor.predict(X_test, sensitive_features=sensitive_test)
```

### 4.2 캘리브레이션 기반 조정

확률 출력의 캘리브레이션(pt-06)을 그룹별로 수행하여 예측 확률의 신뢰도를 균등화합니다.

## 5. 기법 비교 및 선택 가이드

| 기준 | 전처리 | 학습 중 | 후처리 |
|------|--------|---------|--------|
| 모델 독립성 | 높음 | 낮음 | 높음 |
| 성능 영향 | 낮음 | 중간 | 낮음~중간 |
| 공정성 개선 | 중간 | 높음 | 중간~높음 |
| 구현 복잡도 | 낮음 | 높음 | 낮음 |
| 적용 시점 | 데이터 준비 | 모델 학습 | 모델 배포 |
| 적합한 상황 | 데이터 편향이 명확 | 강한 공정성 보장 필요 | 기존 모델 수정 불가 |

```
┌────────────────────────────────────────────────┐
│        편향 완화 기법 선택 플로우                │
├────────────────────────────────────────────────┤
│                                                │
│  모델을 수정할 수 있는가?                       │
│     ├── NO  → 후처리 기법                       │
│     │        (ThresholdOptimizer)               │
│     └── YES → 편향의 주 원천은?                 │
│              ├── 데이터 → 전처리 기법            │
│              │   (Reweighing, DI Remover)       │
│              ├── 알고리즘 → 학습 중 기법         │
│              │   (ExponentiatedGradient)        │
│              └── 불확실 → 전처리 + 학습 중 결합  │
│                                                │
└────────────────────────────────────────────────┘
```

### 시나리오: 이력서 심사 AI의 편향 완화

한 기업의 이력서 심사 AI가 여성 지원자의 서류 통과율이 남성 대비 60%임을 발견했습니다 (DIR = 0.6 < 0.8). 원인 분석 결과 과거 채용 데이터에 역사적 편향이 내재되어 있었습니다. 다음 단계로 접근합니다:

1. **전처리**: 리웨이팅으로 성별-채용결과 조합의 가중치 균형화
2. **학습 중**: ExponentiatedGradient로 인구 통계적 동등성 제약 적용
3. **후처리**: ThresholdOptimizer로 성별별 임계값 보정
4. **모니터링**: 배포 후 DIR을 지속 추적 (mo-07), 피드백 루프 감시

> **핵심 직관**: 편향 완화는 일회성이 아닌 지속적 프로세스입니다. 모델 배포 후에도 데이터 분포 변화(mo-06 데이터 드리프트)로 인해 새로운 편향이 발생할 수 있으므로, 실시간 공정성 모니터링이 필수적입니다.

## 핵심 정리

- 편향은 데이터(선택/역사적/측정/표현 편향), 알고리즘(학습 목표/귀납/과적합 편향), 평가/배포(벤치마크/피드백 루프 편향)의 세 차원에서 발생합니다
- 전처리 기법(리웨이팅, DI 제거기)은 모델 독립적이며 데이터 수준에서 편향을 교정합니다
- 학습 중 기법(제약 최적화, 적대적 탈편향)은 공정성 제약을 모델 학습에 직접 통합하여 강한 보장을 제공합니다
- 후처리 기법(임계값 조정)은 기존 모델을 수정하지 않고도 공정성을 개선할 수 있어 실무 적용이 용이합니다
- 편향 완화는 일회성이 아닌 지속적 프로세스이며, 배포 후 피드백 루프에 의한 편향 증폭을 모니터링해야 합니다
