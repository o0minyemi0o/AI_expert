# 실전 책임 AI 구축

## 왜 실전 구축이 중요한가

ra-01~ra-09에서 책임있는 AI의 원칙, 공정성, 편향 완화, 해석 가능성, 설명 기법, 프라이버시, 강건성, 거버넌스, LLM 윤리를 학습했습니다. 그러나 개별 기법의 이해만으로는 충분하지 않습니다. 실제 조직에서 RAI를 구축하려면 체크리스트, 영향 평가, 이해관계자 참여, 지속적 모니터링을 하나의 통합된 프로세스로 운영해야 합니다. 이 강의는 이론을 실무로 전환하는 구체적 방법론을 다룹니다.

> **핵심 직관**: 책임있는 AI는 기술적 체크박스가 아니라 조직 문화입니다. 도구와 프로세스는 필요 조건이지만, 모든 팀원이 "이 모델이 누군가에게 해를 끼칠 수 있는가?"를 자연스럽게 질문하는 문화가 충분 조건입니다.

## 1. RAI 체크리스트

### 1.1 단계별 체크리스트

ML 생명주기(mo-01)의 각 단계에 RAI 점검을 내장합니다.

| 단계 | 점검 항목 | 도구/기법 | 관련 강의 |
|------|-----------|-----------|-----------|
| 기획 | 윤리적 위험 식별, 이해관계자 매핑 | 영향 평가 (아래 2절) | ra-01 |
| 데이터 | 데이터 편향 감사, 대표성 검증 | Aequitas, AIF360 | ra-03 |
| 모델링 | 해석 가능 모델 우선 검토, 공정성 제약 | Fairlearn, InterpretML | ra-02, ra-04 |
| 평가 | 하위 집단별 성능, 공정성 지표, 강건성 | 감사 파이프라인 | ra-02, ra-07 |
| 문서화 | Model Card, 데이터 시트, 한계 명시 | Model Card Toolkit | ra-08 |
| 배포 | 인간 감독 설계, 롤백 계획 | MLOps 파이프라인 | mo-08 |
| 모니터링 | 공정성 드리프트, 성능 모니터링 | 대시보드 | mo-06, mo-07 |

### 1.2 자동화된 RAI 게이트

```python
from dataclasses import dataclass
from typing import Dict, List, Optional
from enum import Enum

class GateStatus(Enum):
    PASS = "pass"
    WARN = "warn"
    FAIL = "fail"

@dataclass
class RAIGate:
    """ML 파이프라인의 RAI 게이트 (mo-03 CI/CD 통합)"""
    name: str
    thresholds: Dict[str, float]

    def evaluate(self, metrics: Dict[str, float]) -> GateStatus:
        for metric, threshold in self.thresholds.items():
            if metric not in metrics:
                return GateStatus.FAIL
            if abs(metrics[metric]) > threshold:
                return GateStatus.FAIL
        return GateStatus.PASS

class RAIPipeline:
    def __init__(self):
        self.gates = [
            RAIGate("fairness", {
                "spd": 0.1,       # |SPD| < 0.1
                "eod": 0.1,       # |EOD| < 0.1
                "dir_min": 0.8    # DIR > 0.8
            }),
            RAIGate("performance_parity", {
                "accuracy_gap": 0.05  # 그룹 간 정확도 차이 < 5%
            }),
            RAIGate("robustness", {
                "pgd_accuracy_drop": 0.15  # PGD 공격 시 정확도 하락 < 15%
            }),
            RAIGate("privacy", {
                "membership_inference_auc": 0.6  # 멤버십 추론 AUC < 0.6
            })
        ]

    def run_all_gates(self, metrics: Dict[str, float]) -> Dict[str, GateStatus]:
        results = {}
        for gate in self.gates:
            results[gate.name] = gate.evaluate(metrics)
        return results

    def can_deploy(self, results: Dict[str, GateStatus]) -> bool:
        return all(s != GateStatus.FAIL for s in results.values())

# 사용 예시
pipeline = RAIPipeline()
metrics = {
    "spd": -0.03, "eod": -0.02, "dir_min": 0.85,
    "accuracy_gap": 0.03,
    "pgd_accuracy_drop": 0.12,
    "membership_inference_auc": 0.55
}
results = pipeline.run_all_gates(metrics)
print(f"배포 가능 여부: {pipeline.can_deploy(results)}")
# 배포 가능 여부: True
```

## 2. 영향 평가 (Impact Assessment)

### 2.1 알고리즘 영향 평가(AIA) 프로세스

```
┌──────────────────────────────────────────────────────────┐
│            알고리즘 영향 평가 (AIA) 프로세스               │
├──────────────────────────────────────────────────────────┤
│                                                          │
│  [1단계] 시스템 설명                                      │
│     ├── 목적, 입력/출력, 의사결정 유형                     │
│     ├── 자동화 수준 (보조/자문/자동)                       │
│     └── 영향 받는 인구 규모와 특성                         │
│     ↓                                                    │
│  [2단계] 리스크 식별                                      │
│     ├── 누가 이 시스템으로 인해 피해를 받을 수 있는가?     │
│     ├── 어떤 유형의 피해가 가능한가? (차별, 프라이버시 등) │
│     ├── 피해의 규모와 비가역성은?                          │
│     └── 기존 불평등을 강화하는가?                          │
│     ↓                                                    │
│  [3단계] 완화 계획                                        │
│     ├── 기술적 완화: ra-02~ra-07의 기법 적용               │
│     ├── 절차적 완화: 인간 감독, 이의 제기 절차             │
│     └── 조직적 완화: 교육, 거버넌스 (ra-08)               │
│     ↓                                                    │
│  [4단계] 이해관계자 검토                                   │
│     └── 영향 받는 당사자의 의견 수렴 (아래 3절)            │
│     ↓                                                    │
│  [5단계] 결정 및 문서화                                    │
│     ├── 진행/수정/중단 결정                               │
│     └── 공개적 영향 평가 보고서                            │
│                                                          │
└──────────────────────────────────────────────────────────┘
```

### 2.2 영향 평가 템플릿

```python
@dataclass
class AlgorithmicImpactAssessment:
    system_name: str
    description: str
    decision_type: str  # "assistive", "advisory", "automated"
    affected_population: str
    estimated_scale: int

    # 리스크 식별
    potential_harms: List[str] = None
    vulnerable_groups: List[str] = None
    reversibility: str = ""  # "easy", "difficult", "impossible"
    existing_inequality_risk: str = ""

    # 완화 계획
    technical_mitigations: List[str] = None
    procedural_mitigations: List[str] = None
    monitoring_plan: str = ""

    # 이해관계자
    stakeholders_consulted: List[str] = None
    stakeholder_feedback: str = ""

    # 결정
    decision: str = ""  # "proceed", "modify", "halt"
    conditions: List[str] = None

# 시나리오: 공공 주택 배분 AI 영향 평가
housing_aia = AlgorithmicImpactAssessment(
    system_name="HousingAllocator",
    description="공공 주택 대기자 우선순위 결정 AI",
    decision_type="advisory",
    affected_population="저소득 가구, 노숙자, 이민자",
    estimated_scale=50000,
    potential_harms=[
        "소수 인종 가구의 체계적 불이익",
        "장애인 가구의 접근성 미고려",
        "디지털 리터러시 낮은 집단의 이의 제기 어려움"
    ],
    vulnerable_groups=["노숙자", "이민자", "장애인", "노인"],
    reversibility="difficult",
    technical_mitigations=[
        "공정성 제약 적용 (ra-02)",
        "해석 가능 모델 사용 (ra-04)",
        "개인별 설명 제공 (ra-05)"
    ],
    procedural_mitigations=[
        "모든 결정에 인간 심사관 검토",
        "이의 제기 절차 마련 (다국어, 오프라인 포함)",
        "정기적 외부 감사"
    ],
    decision="proceed",
    conditions=["6개월 시범 운영 후 재평가", "분기별 공정성 보고서 공개"]
)
```

## 3. 이해관계자 참여

### 3.1 이해관계자 매핑

| 이해관계자 | 역할 | 관심사 | 참여 방법 |
|-----------|------|--------|-----------|
| 영향 받는 개인 | 시스템의 대상 | 공정성, 투명성, 이의 제기 | 포커스 그룹, 설문 |
| 도메인 전문가 | 전문 지식 제공 | 정확성, 적절성 | 자문 위원회 |
| 시민 사회 | 공익 대변 | 인권, 차별, 사회적 영향 | 공개 의견 수렴 |
| 규제 기관 | 법적 준수 확인 | 규제 준수, 투명성 | 보고, 감사 |
| 개발팀 | 기술 구현 | 실행 가능성, 성능 | 내부 검토 |
| 경영진 | 의사결정 | 비즈니스 가치, 리스크 | 보고서 승인 |

### 3.2 참여적 설계 원칙

> **핵심 직관**: "Nothing about us without us" — 영향 받는 당사자가 AI 시스템의 설계와 평가에 직접 참여해야 합니다. 기술팀만으로 "무엇이 공정한가"를 결정할 수 없습니다. 공정성의 정의 선택(ra-02)은 본질적으로 사회적 판단입니다.

```
┌──────────────────────────────────────────────────┐
│         이해관계자 참여 수준                       │
├──────────────────────────────────────────────────┤
│                                                  │
│  [정보 제공] ← 최소 수준                          │
│     AI 사용 사실 고지                             │
│     ↓                                            │
│  [의견 수렴]                                      │
│     설문, 공개 의견 수렴                          │
│     ↓                                            │
│  [협의]                                           │
│     포커스 그룹, 자문 위원회                      │
│     ↓                                            │
│  [공동 설계]                                      │
│     워크숍, 참여적 디자인                         │
│     ↓                                            │
│  [권한 위임] ← 최고 수준                          │
│     이해관계자가 의사결정권 보유                   │
│                                                  │
└──────────────────────────────────────────────────┘
```

## 4. 지속적 모니터링

### 4.1 RAI 모니터링 대시보드

```python
from datetime import datetime, timedelta
import numpy as np

class RAIMonitor:
    """배포 후 RAI 지표 지속적 모니터링 (mo-07 통합)"""

    def __init__(self, model_name: str, alert_thresholds: dict):
        self.model_name = model_name
        self.thresholds = alert_thresholds
        self.history = []

    def log_metrics(self, metrics: dict):
        """실시간 지표 기록"""
        entry = {
            "timestamp": datetime.now().isoformat(),
            "metrics": metrics
        }
        self.history.append(entry)
        self._check_alerts(metrics)

    def _check_alerts(self, metrics: dict):
        """임계값 위반 시 알림"""
        for metric, value in metrics.items():
            if metric in self.thresholds:
                threshold = self.thresholds[metric]
                if abs(value) > threshold:
                    self._send_alert(metric, value, threshold)

    def _send_alert(self, metric: str, value: float, threshold: float):
        print(f"[ALERT] {self.model_name}: {metric}={value:.4f} "
              f"(threshold: {threshold})")

    def detect_fairness_drift(self, window_days: int = 30) -> dict:
        """시간에 따른 공정성 지표 드리프트 탐지"""
        recent = [h for h in self.history
                  if datetime.fromisoformat(h["timestamp"])
                  > datetime.now() - timedelta(days=window_days)]

        if len(recent) < 2:
            return {"drift_detected": False}

        first_half = recent[:len(recent)//2]
        second_half = recent[len(recent)//2:]

        for metric in ["spd", "eod"]:
            first_avg = np.mean([h["metrics"].get(metric, 0) for h in first_half])
            second_avg = np.mean([h["metrics"].get(metric, 0) for h in second_half])
            drift = abs(second_avg - first_avg)
            if drift > 0.05:
                return {
                    "drift_detected": True,
                    "metric": metric,
                    "drift_magnitude": drift,
                    "recommendation": "공정성 지표 악화 탐지. 데이터 분포 변화 확인 필요 (mo-06)"
                }
        return {"drift_detected": False}

# 사용 예시
monitor = RAIMonitor(
    model_name="LoanApprovalModel_v2.1",
    alert_thresholds={
        "spd": 0.1,
        "eod": 0.1,
        "accuracy_gap": 0.05
    }
)

# 일일 모니터링 (mo-07 스케줄러와 통합)
daily_metrics = {"spd": -0.04, "eod": -0.03, "accuracy_gap": 0.02}
monitor.log_metrics(daily_metrics)
```

### 시나리오: 실시간 공정성 모니터링에서 편향 발견

채용 AI 시스템의 RAI 모니터링 대시보드에서 최근 2주간 여성 지원자의 서류 통과율이 점진적으로 하락하는 패턴이 감지되었습니다.

1. **탐지**: 공정성 드리프트 감지 알림 발생 (SPD: -0.03 → -0.12)
2. **진단**: 최근 채용 공고가 특정 기술 직군에 편중 → 과거 해당 직군의 성별 불균형 데이터가 영향
3. **대응**: 임시로 후처리 임계값 조정 (ra-03) + 데이터 리밸런싱 계획 수립
4. **보고**: 이해관계자에게 사건 보고서 발행, 개선 조치 및 일정 공유
5. **예방**: 직군별 공정성 모니터링 세분화, 드리프트 임계값 하향 조정

## 5. 통합 RAI 로드맵

```
┌──────────────────────────────────────────────────────────┐
│                 조직 RAI 구축 로드맵                       │
├──────────────────────────────────────────────────────────┤
│                                                          │
│  [Q1] 기반 구축                                           │
│  ├── RAI 원칙 수립 (ra-01)                                │
│  ├── 팀 교육 (ra-01~ra-09 커리큘럼)                       │
│  └── 기존 모델 인벤토리 작성                               │
│                                                          │
│  [Q2] 프로세스 도입                                       │
│  ├── 영향 평가 제도화 (2절)                                │
│  ├── Model Card 표준 수립 (ra-08)                          │
│  └── 공정성 테스트 의무화 (ra-02)                           │
│                                                          │
│  [Q3] 기술 통합                                           │
│  ├── RAI 게이트를 CI/CD에 내장 (1절, mo-03)                │
│  ├── 모니터링 대시보드 구축 (4절)                           │
│  └── 감사 파이프라인 자동화 (ra-08)                         │
│                                                          │
│  [Q4] 성숙도 향상                                         │
│  ├── 외부 감사 도입                                        │
│  ├── 이해관계자 참여 프로그램 운영 (3절)                    │
│  └── RAI 성과 지표(KPI) 수립 및 보고                       │
│                                                          │
│  [지속] 반복 개선                                          │
│  ├── 분기별 RAI 리뷰                                       │
│  ├── 신기술/규제 변화 반영                                  │
│  └── 사례 공유 및 외부 협력                                 │
│                                                          │
└──────────────────────────────────────────────────────────┘
```

### 시나리오: 스타트업의 RAI 도입

20명 규모의 AI 스타트업이 헬스케어 AI 제품을 개발합니다. 대기업 수준의 거버넌스는 비현실적이지만, 최소한의 RAI 프로세스가 필요합니다.

- **최소 실행 세트**: 영향 평가 간소화 버전 + Model Card + 공정성 테스트 1개 + 기본 모니터링
- **핵심 역할**: 기존 ML 엔지니어 1명에게 RAI 챔피언 역할 부여
- **점진적 확장**: 제품 성장과 함께 거버넌스 수준을 단계적으로 향상

> **핵심 직관**: RAI는 완벽을 추구하면 시작할 수 없습니다. "아무것도 하지 않는 것"보다 "불완전하지만 시작하는 것"이 훨씬 낫습니다. 최소 실행 가능한 RAI(Minimum Viable RAI)부터 시작하여 점진적으로 성숙도를 높여가는 것이 현실적 전략입니다.

## 6. RAI 과정 전체 요약

이 과정에서 다룬 10개 강의의 흐름을 정리합니다.

| 강의 | 핵심 질문 | 핵심 도구/기법 |
|------|-----------|---------------|
| ra-01 | 왜 RAI가 필요한가? | 프레임워크, 위험 분류 |
| ra-02 | 무엇이 공정한가? | 공정성 정의, Fairlearn |
| ra-03 | 편향은 어디서 오는가? | AIF360, 전/중/후처리 기법 |
| ra-04 | 모델 자체를 투명하게? | EBM, 규칙 목록 |
| ra-05 | 블랙박스를 어떻게 설명? | SHAP, LIME, DiCE |
| ra-06 | 개인정보를 어떻게 보호? | DP-SGD, 연합 학습 |
| ra-07 | 공격에 어떻게 견딜까? | 적대적 학습, 랜덤 스무딩 |
| ra-08 | 어떻게 체계적으로 관리? | Model Card, 감사 |
| ra-09 | LLM의 특수한 위험은? | 레드팀, RLHF 한계 |
| ra-10 | 실전에서 어떻게 구축? | 체크리스트, AIA, 모니터링 |

## 핵심 정리

- RAI 체크리스트는 ML 생명주기의 모든 단계(기획~모니터링)에 공정성, 설명 가능성, 프라이버시, 강건성 점검을 내장합니다
- 알고리즘 영향 평가(AIA)는 시스템 설명 → 리스크 식별 → 완화 계획 → 이해관계자 검토 → 결정/문서화의 5단계로 수행됩니다
- 이해관계자 참여는 정보 제공 → 의견 수렴 → 협의 → 공동 설계 → 권한 위임의 스펙트럼에서 적절한 수준을 선택해야 합니다
- 지속적 모니터링은 배포 후 공정성 드리프트, 성능 저하, 편향 증폭을 실시간으로 탐지하고 대응하는 시스템입니다
- RAI 구축은 "최소 실행 가능한 RAI"부터 시작하여 점진적으로 성숙도를 높여가는 실용적 접근이 효과적입니다
