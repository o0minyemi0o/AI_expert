# 적대적 강건성

## 왜 적대적 강건성이 중요한가

딥러닝 모델은 인간이 인지하지 못하는 미세한 입력 변형에도 완전히 잘못된 예측을 할 수 있습니다. 자율주행 차량의 표지판 인식, 의료 영상 진단, 악성코드 탐지 등 안전이 중요한 시스템에서 이러한 취약점은 심각한 위험을 초래합니다. 적대적 강건성은 이러한 의도적 또는 자연적 입력 변형에 대해 모델이 안정적으로 작동하도록 보장하는 영역입니다.

> **핵심 직관**: 적대적 예제는 모델의 "버그"가 아니라 고차원 공간에서의 선형성(linearity)에서 비롯되는 본질적 취약점입니다. 입력 공간의 각 차원에 아주 작은 변형을 주더라도, 차원 수가 많으면 총 변형 효과가 출력을 크게 바꿀 수 있습니다 (dl-02).

## 1. 적대적 예제의 이해

### 1.1 정의

입력 $x$에 작은 섭동 $\delta$를 추가하여 모델의 예측을 오류로 유도합니다:

$$x_{\text{adv}} = x + \delta, \quad \|\delta\|_p \leq \epsilon, \quad f(x_{\text{adv}}) \neq f(x)$$

여기서 $\|\delta\|_p$는 $L_p$ 노름으로 섭동의 크기를 제한합니다.

| 노름 | 의미 | 시각적 효과 |
|------|------|------------|
| $L_0$ | 변경된 픽셀 수 | 소수 픽셀만 변경 |
| $L_2$ | 유클리드 거리 | 전체에 작은 노이즈 |
| $L_\infty$ | 최대 변경량 | 모든 픽셀에 균일 제한 |

### 1.2 공격의 분류

```
┌──────────────────────────────────────────────────────────┐
│                적대적 공격 분류 체계                       │
├──────────────────────────────────────────────────────────┤
│                                                          │
│  공격자 지식 기준                                         │
│  ├── 화이트박스: 모델 구조/파라미터 완전 접근              │
│  │   (FGSM, PGD, C&W)                                   │
│  ├── 블랙박스: 모델 출력만 접근 가능                      │
│  │   (전이 공격, 쿼리 기반 공격)                          │
│  └── 그레이박스: 부분 정보 접근                           │
│                                                          │
│  공격 목표 기준                                           │
│  ├── 비표적(Untargeted): 아무 오분류 유도                 │
│  └── 표적(Targeted): 특정 클래스로 오분류 유도            │
│                                                          │
│  적용 환경                                                │
│  ├── 디지털: 입력 데이터 직접 수정                        │
│  └── 물리적: 실제 환경에서의 공격 (스티커, 패치)           │
│                                                          │
└──────────────────────────────────────────────────────────┘
```

## 2. 주요 공격 기법

### 2.1 FGSM (Fast Gradient Sign Method)

손실 함수의 그래디언트 부호 방향으로 한 번의 스텝을 취합니다:

$$x_{\text{adv}} = x + \epsilon \cdot \text{sign}(\nabla_x \mathcal{L}(\theta, x, y))$$

```python
import torch
import torch.nn.functional as F

def fgsm_attack(model, x, y, epsilon=0.03):
    """FGSM 공격 구현"""
    x_adv = x.clone().detach().requires_grad_(True)

    # 순전파 및 손실 계산
    output = model(x_adv)
    loss = F.cross_entropy(output, y)

    # 입력에 대한 그래디언트 계산
    loss.backward()

    # 그래디언트 부호 방향으로 섭동
    perturbation = epsilon * x_adv.grad.sign()
    x_adv = x_adv + perturbation
    x_adv = torch.clamp(x_adv, 0, 1)  # 유효 범위 유지

    return x_adv

# 시나리오: MNIST 분류기 공격
x_adv = fgsm_attack(model, images, labels, epsilon=0.1)
pred_clean = model(images).argmax(dim=1)
pred_adv = model(x_adv).argmax(dim=1)
# 정상: 정확도 99% → FGSM 공격 후: 정확도 30%
```

### 2.2 PGD (Projected Gradient Descent)

FGSM을 반복 적용하되, 매 스텝마다 $\epsilon$-볼 내로 투영합니다:

$$x^{t+1} = \Pi_{x + S} \left( x^t + \alpha \cdot \text{sign}(\nabla_x \mathcal{L}(\theta, x^t, y)) \right)$$

여기서 $\Pi$는 $L_\infty$ 노름 $\epsilon$-볼로의 투영, $\alpha$는 스텝 크기입니다.

```python
def pgd_attack(model, x, y, epsilon=0.03, alpha=0.007, num_steps=20):
    """PGD 공격 구현 - 가장 강력한 1차 공격"""
    x_adv = x.clone().detach()
    # 랜덤 초기화 (로컬 최솟값 회피)
    x_adv = x_adv + torch.empty_like(x_adv).uniform_(-epsilon, epsilon)
    x_adv = torch.clamp(x_adv, 0, 1)

    for _ in range(num_steps):
        x_adv.requires_grad_(True)
        output = model(x_adv)
        loss = F.cross_entropy(output, y)
        loss.backward()

        # 그래디언트 스텝
        x_adv = x_adv.detach() + alpha * x_adv.grad.sign()
        # epsilon-볼 내로 투영
        delta = torch.clamp(x_adv - x, min=-epsilon, max=epsilon)
        x_adv = torch.clamp(x + delta, 0, 1)

    return x_adv
```

### 2.3 공격 기법 비교

| 기법 | 반복 | 강도 | 계산 비용 | 특징 |
|------|------|------|-----------|------|
| FGSM | 1회 | 중간 | 낮음 | 빠른 평가용 |
| PGD | 다회 | 높음 | 중간 | 강건성 벤치마크 표준 |
| C&W | 다회 | 매우 높음 | 높음 | 최소 섭동 탐색 |
| AutoAttack | 앙상블 | 최고 | 높음 | 다중 공격 자동 조합 |

## 3. 방어 전략

### 3.1 적대적 학습 (Adversarial Training)

가장 효과적이고 검증된 방어 기법입니다. 적대적 예제를 학습 데이터에 포함시켜 모델을 강건하게 만듭니다.

$$\min_\theta \mathbb{E}_{(x,y)} \left[ \max_{\|\delta\|_\infty \leq \epsilon} \mathcal{L}(\theta, x+\delta, y) \right]$$

이는 내부 최대화(공격)와 외부 최소화(방어)의 min-max 최적화 문제입니다.

```python
import torchattacks

def adversarial_training(model, train_loader, optimizer, epsilon=8/255, epochs=50):
    """PGD 기반 적대적 학습"""
    attack = torchattacks.PGD(model, eps=epsilon, alpha=2/255, steps=7)

    for epoch in range(epochs):
        for images, labels in train_loader:
            # 적대적 예제 생성
            adv_images = attack(images, labels)

            # 적대적 예제에 대한 손실 최소화
            outputs = model(adv_images)
            loss = F.cross_entropy(outputs, labels)

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

        # 평가: 정상 / 적대적 정확도 함께 추적
        clean_acc = evaluate(model, test_loader)
        robust_acc = evaluate_adversarial(model, test_loader, attack)
        print(f"Epoch {epoch}: Clean={clean_acc:.1%}, Robust={robust_acc:.1%}")
```

> **핵심 직관**: 적대적 학습은 정상 정확도를 일부 희생합니다 (보통 2-5%). 이것이 "강건성-정확도 트레이드오프"이며, 이론적으로 불가피한 것으로 알려져 있습니다(Tsipras et al., 2019). 모델 선택(ms-02) 시 이 트레이드오프를 고려해야 합니다.

### 3.2 방어 기법 비교

| 방어 기법 | 효과 | 비용 | 정확도 영향 | 성숙도 |
|-----------|------|------|------------|--------|
| 적대적 학습 | 높음 | 학습 3~10배 | 2~5% 하락 | 높음 |
| 입력 전처리 | 낮음 | 낮음 | 1~2% 하락 | 우회 가능 |
| 모델 앙상블 | 중간 | 높음 | 유지/소폭 하락 | 중간 |
| 인증된 방어 | 보장됨 | 매우 높음 | 5~15% 하락 | 초기 |
| 랜덤 스무딩 | 높음 | 중간 | 3~8% 하락 | 중간 |

## 4. 인증된 강건성 (Certified Robustness)

### 4.1 랜덤 스무딩 (Randomized Smoothing)

입력에 가우시안 노이즈를 추가한 여러 버전의 예측을 집계하여 인증 반경을 제공합니다.

$$g(x) = \arg\max_c P(f(x + \eta) = c), \quad \eta \sim \mathcal{N}(0, \sigma^2 I)$$

인증 반경 $R$: $g$의 예측이 $L_2$ 노름 $R$ 이내의 모든 섭동에 대해 변하지 않음을 보장합니다.

$$R = \frac{\sigma}{2} \left( \Phi^{-1}(\underline{p_A}) - \Phi^{-1}(\overline{p_B}) \right)$$

```python
# 랜덤 스무딩 인증 예시 (개념적)
from scipy.stats import norm

def certify(model, x, sigma=0.25, n_samples=1000, alpha=0.001):
    """랜덤 스무딩으로 인증 반경 계산"""
    # 노이즈 추가된 입력에 대한 예측 집계
    counts = {}
    for _ in range(n_samples):
        noisy_x = x + torch.randn_like(x) * sigma
        pred = model(noisy_x).argmax().item()
        counts[pred] = counts.get(pred, 0) + 1

    top_class = max(counts, key=counts.get)
    p_lower = proportion_confint(counts[top_class], n_samples, alpha, method="beta")[0]

    if p_lower > 0.5:
        radius = sigma * norm.ppf(p_lower)
        return top_class, radius  # 인증 성공
    return top_class, 0.0  # 인증 불가 (ABSTAIN)
```

### 시나리오: 자율주행의 적대적 강건성

자율주행 차량의 표지판 인식 시스템에 적대적 패치(스티커)를 부착하여 STOP 표지판을 속도 제한 표지판으로 오인하게 할 수 있습니다.

1. **위험 평가**: 물리적 공격 가능성이 높은 고위험 시스템 (ra-01)
2. **적대적 학습**: PGD 기반 학습으로 기본 강건성 확보
3. **인증된 방어**: 랜덤 스무딩으로 최소 강건성 반경 보장
4. **다중 센서**: 카메라 + LiDAR + 레이더 앙상블로 단일 모달리티 공격 완화
5. **모니터링**: 입력 이상 탐지 모듈 (mo-07)로 적대적 입력 실시간 감지

### 시나리오: 악성코드 탐지의 회피 공격

악성코드 작성자가 탐지 모델을 회피하도록 코드를 변형합니다. 기능은 유지하면서 피처를 조작하는 이른바 "기능 보존 변환(functionality-preserving transformations)"이 핵심 위협입니다.

## 5. 평가 프로토콜

```
┌──────────────────────────────────────────────────────┐
│           적대적 강건성 평가 프로토콜                  │
├──────────────────────────────────────────────────────┤
│                                                      │
│  1. 기본 평가                                         │
│     ├── 정상 정확도 (Clean Accuracy)                  │
│     └── 적대적 정확도 (Robust Accuracy)               │
│          └── FGSM, PGD-20, PGD-100, AutoAttack       │
│                                                      │
│  2. 적응적 공격 (Adaptive Attack) ← 핵심!            │
│     └── 방어 메커니즘을 아는 공격자 가정               │
│         (Tramer et al., 2020)                        │
│                                                      │
│  3. 인증 평가                                         │
│     └── 인증 반경 내 보장 정확도                      │
│                                                      │
│  4. 보고서 작성                                       │
│     └── 모델 카드에 강건성 지표 기록 (ra-08)          │
│                                                      │
└──────────────────────────────────────────────────────┘
```

> **핵심 직관**: 강건성 평가에서 가장 중요한 원칙은 "적응적 공격(adaptive attack)"입니다. 방어 메커니즘을 모르는 공격자를 가정한 평가는 의미가 없습니다. 공격자가 방어 전략을 완전히 알고 있다고 가정해야 합니다.

## 핵심 정리

- 적대적 예제는 인간이 감지할 수 없는 미세한 입력 변형으로 모델을 오류에 빠뜨리며, 이는 고차원 공간에서의 선형성에서 비롯됩니다
- FGSM은 단일 그래디언트 스텝, PGD는 반복적 투영 그래디언트로 더 강력한 공격을 수행하며, PGD가 강건성 벤치마크의 표준입니다
- 적대적 학습(min-max 최적화)이 가장 효과적이고 검증된 방어이며, 정상 정확도 2~5% 하락이라는 불가피한 트레이드오프가 존재합니다
- 랜덤 스무딩 기반 인증된 강건성은 수학적으로 보장된 방어 반경을 제공하지만 정확도 비용이 크며, 고위험 시스템에 적합합니다
- 강건성 평가는 반드시 방어 메커니즘을 아는 적응적 공격자를 가정해야 하며, 결과를 모델 카드(ra-08)에 투명하게 기록해야 합니다
