# 책임있는 AI의 프레임워크

## 왜 책임있는 AI가 중요한가

AI 시스템이 채용, 대출, 형사 사법 등 고위험 의사결정에 광범위하게 적용되면서, 기술적 성능만으로는 충분하지 않습니다. 모델이 편향된 결정을 내리거나(ra-02), 설명 불가능한 예측을 하거나(ra-05), 프라이버시를 침해할 때(ra-06) 발생하는 사회적 비용은 막대합니다. 책임있는 AI(Responsible AI, RAI)는 이러한 위험을 체계적으로 식별하고 관리하기 위한 프레임워크입니다.

> **핵심 직관**: 책임있는 AI는 "AI를 만들지 말아야 한다"가 아니라, "AI를 올바르게 만드는 방법"에 관한 것입니다. 성능 최적화(ms-01)와 윤리적 고려는 상충이 아닌 공존의 문제입니다.

## 1. AI 윤리의 핵심 원칙

전 세계 주요 기관들이 제시하는 AI 윤리 원칙은 공통적으로 다음 요소를 포함합니다.

| 원칙 | 설명 | 관련 강의 |
|------|------|-----------|
| 공정성(Fairness) | 특정 집단에 대한 차별 방지 | ra-02, ra-03 |
| 투명성(Transparency) | 의사결정 과정의 설명 가능성 | ra-04, ra-05 |
| 프라이버시(Privacy) | 개인정보 보호와 데이터 주권 | ra-06 |
| 안전성(Safety) | 적대적 공격에 대한 강건성 | ra-07 |
| 책무성(Accountability) | 거버넌스와 감사 체계 | ra-08 |
| 포용성(Inclusiveness) | 다양한 이해관계자 참여 | ra-10 |

> **핵심 직관**: 이 여섯 가지 원칙은 독립적이 아니라 상호 연결되어 있습니다. 공정성을 확보하려면 투명성이 필요하고, 투명성은 책무성의 전제 조건입니다.

## 2. 글로벌 규제 동향

### 2.1 EU AI Act

EU AI Act는 세계 최초의 포괄적 AI 규제 법안으로, 위험 기반 접근법을 채택합니다.

```
┌─────────────────────────────────────────────┐
│            EU AI Act 위험 분류               │
├─────────────────────────────────────────────┤
│                                             │
│  [금지] 사회적 점수 매기기, 실시간 생체인식  │
│    ↓                                        │
│  [고위험] 채용, 신용평가, 사법, 의료         │
│    ↓  → 적합성 평가, 데이터 거버넌스 필수    │
│  [제한위험] 챗봇, 딥페이크                   │
│    ↓  → 투명성 의무 (AI 사용 고지)           │
│  [최소위험] 스팸 필터, 게임 AI               │
│       → 별도 규제 없음                       │
│                                             │
└─────────────────────────────────────────────┘
```

### 2.2 주요국 규제 비교

| 항목 | EU AI Act | 미국 AI BoR | 한국 AI 기본법 |
|------|-----------|-------------|----------------|
| 접근법 | 위험 기반 규제 | 원칙 기반 가이드 | 위험 기반 + 진흥 |
| 법적 구속력 | 강제 | 자발적 | 강제 (예정) |
| 고위험 분류 | 명시적 목록 | 권리 중심 | 영향평가 기반 |
| 제재 | 매출의 6%까지 | 없음 | 과태료 |
| 시행 시기 | 2024-2026 단계적 | 2022 발표 | 단계적 시행 |

## 3. 위험 분류 체계

AI 시스템의 위험을 체계적으로 분류하면 적절한 거버넌스 수준을 결정할 수 있습니다.

```python
from dataclasses import dataclass
from enum import Enum
from typing import List

class RiskLevel(Enum):
    MINIMAL = 1
    LIMITED = 2
    HIGH = 3
    UNACCEPTABLE = 4

@dataclass
class AIRiskAssessment:
    system_name: str
    domain: str
    affected_population: int
    reversibility: bool  # 결정을 되돌릴 수 있는가
    autonomy_level: float  # 0~1, 인간 개입 정도

    def classify_risk(self) -> RiskLevel:
        score = 0
        if self.affected_population > 100_000:
            score += 2
        if not self.reversibility:
            score += 2
        if self.autonomy_level > 0.8:
            score += 1
        if self.domain in ["hiring", "credit", "justice", "healthcare"]:
            score += 2

        if score >= 5:
            return RiskLevel.HIGH
        elif score >= 3:
            return RiskLevel.LIMITED
        return RiskLevel.MINIMAL

# 시나리오: 채용 AI 시스템 위험 평가
hiring_ai = AIRiskAssessment(
    system_name="ResumeRanker",
    domain="hiring",
    affected_population=500_000,
    reversibility=False,
    autonomy_level=0.9
)
print(f"위험 등급: {hiring_ai.classify_risk()}")  # RiskLevel.HIGH
```

## 4. 조직 체계와 역할

### 4.1 RAI 거버넌스 구조

```
┌─────────────────────────────────────────────────┐
│                  경영진/이사회                    │
│            (RAI 정책 승인, 자원 배분)             │
├─────────────────────────────────────────────────┤
│              RAI 위원회 / 윤리위원회              │
│     (정책 수립, 고위험 프로젝트 심의, 감사)        │
├──────────┬──────────┬──────────┬────────────────┤
│ 데이터팀  │  ML팀    │ 법무팀   │  제품팀         │
│ 데이터    │ 모델     │ 규제     │  사용자         │
│ 편향 검사 │ 공정성   │ 준수     │  영향 평가      │
│ (ra-03)  │ 테스트   │ 확인     │  (ra-10)       │
│          │ (ra-02)  │ (ra-08)  │                │
└──────────┴──────────┴──────────┴────────────────┘
```

### 시나리오: 금융 AI 시스템의 RAI 적용

대형 은행이 대출 심사 AI를 도입하려 합니다. RAI 프레임워크에 따른 절차는 다음과 같습니다.

1. **영향 평가**: 시스템이 영향을 미치는 인구 규모와 의사결정의 비가역성 분석
2. **위험 분류**: 금융 도메인 + 대규모 영향 → 고위험으로 분류
3. **공정성 검증**: 인종, 성별, 연령에 따른 승인율 차이 분석 (ra-02)
4. **설명 가능성**: 거절 사유를 개인별로 제공할 수 있는 설명 모듈 탑재 (ra-05)
5. **모니터링**: 배포 후 공정성 지표 실시간 추적 (mo-07)
6. **거버넌스**: 정기 감사, 모델 카드 작성, 규제 보고 (ra-08)

## 5. RAI 성숙도 모델

조직의 RAI 도입 수준을 5단계로 구분할 수 있습니다.

| 단계 | 이름 | 특징 |
|------|------|------|
| 1 | 인식(Awareness) | 윤리적 위험 인식, 교육 시작 |
| 2 | 정책(Policy) | 원칙 수립, 가이드라인 문서화 |
| 3 | 프로세스(Process) | 체크리스트, 영향 평가 제도화 |
| 4 | 통합(Integration) | MLOps 파이프라인에 RAI 내장 (mo-03) |
| 5 | 최적화(Optimization) | 자동 모니터링, 지속적 개선, 외부 감사 |

> **핵심 직관**: 대부분의 조직은 1~2단계에 머물러 있습니다. 3단계 이상으로 진입하려면 기술적 도구(ra-02~ra-07)와 조직적 체계(ra-08, ra-10)가 함께 갖추어져야 합니다.

## 6. RAI 프레임워크의 실무 적용

```python
# RAI 체크리스트 자동화 예시
class RAIChecklist:
    def __init__(self, project_name: str):
        self.project_name = project_name
        self.checks = {
            "data_bias_audit": False,       # ra-03
            "fairness_metrics": False,       # ra-02
            "explainability": False,         # ra-04, ra-05
            "privacy_assessment": False,     # ra-06
            "robustness_test": False,        # ra-07
            "model_card": False,             # ra-08
            "stakeholder_review": False,     # ra-10
        }

    def gate_check(self) -> bool:
        """배포 전 게이트 검사: 모든 항목 통과 필요"""
        passed = all(self.checks.values())
        if not passed:
            failed = [k for k, v in self.checks.items() if not v]
            print(f"[BLOCKED] 미통과 항목: {failed}")
        return passed
```

## 핵심 정리

- 책임있는 AI는 공정성, 투명성, 프라이버시, 안전성, 책무성, 포용성의 여섯 가지 핵심 원칙으로 구성됩니다
- EU AI Act는 위험 기반 접근법을 채택하여 AI 시스템을 4단계(금지/고위험/제한/최소)로 분류합니다
- 위험 분류는 영향 범위, 비가역성, 자율성 수준, 도메인 민감도를 종합적으로 고려해야 합니다
- RAI 거버넌스는 경영진 → 윤리위원회 → 실무팀의 다층 구조로 운영되며, 각 팀의 역할이 명확히 정의되어야 합니다
- 조직의 RAI 성숙도는 인식 → 정책 → 프로세스 → 통합 → 최적화의 5단계로 발전하며, MLOps 파이프라인과의 통합이 핵심입니다
