# 프라이버시 보존 ML

## 왜 프라이버시 보존이 중요한가

머신러닝 모델은 학습 데이터의 정보를 기억합니다. 이는 멤버십 추론 공격(학습 데이터에 포함 여부 판별), 모델 역전 공격(입력 데이터 복원), 속성 추론 공격(민감 속성 추론) 등을 통해 개인정보가 유출될 수 있음을 의미합니다. GDPR, 한국 개인정보보호법 등 법규는 데이터 최소화와 목적 제한을 요구하며, 이를 기술적으로 보장하는 프라이버시 보존 ML 기법이 필수적입니다.

> **핵심 직관**: 단순히 데이터를 익명화하는 것으로는 부족합니다. ML 모델 자체가 학습 데이터의 "기억"을 보유하며, 이 기억에서 개인정보를 추출하는 다양한 공격이 존재합니다. 특히 LLM은 학습 데이터를 그대로 출력할 수 있습니다(le-08, ra-09).

## 1. 프라이버시 공격의 유형

| 공격 유형 | 목표 | 공격자 정보 | 위험도 |
|-----------|------|------------|--------|
| 멤버십 추론 | 특정 데이터의 학습 포함 여부 | 모델 출력 | 높음 |
| 모델 역전 | 학습 데이터 복원 | 모델 출력 + 보조 정보 | 매우 높음 |
| 속성 추론 | 민감 속성 추론 | 모델 출력 | 중간 |
| 데이터 추출 | 학습 데이터 그대로 출력 | 프롬프트 (LLM) | 매우 높음 |
| 모델 탈취 | 모델 복제 | 쿼리 접근 | 중간 |

```
┌─────────────────────────────────────────────────────┐
│              프라이버시 공격 표면                     │
├─────────────────────────────────────────────────────┤
│                                                     │
│  [학습 데이터] ──→ [모델 학습] ──→ [모델 배포]       │
│       │               │               │             │
│    데이터 유출       과적합          API 접근         │
│    부적절 공유      기억화           쿼리 공격        │
│       │               │               │             │
│       ▼               ▼               ▼             │
│    직접 유출       멤버십 추론     모델 역전          │
│                    속성 추론      데이터 추출         │
│                                  모델 탈취           │
│                                                     │
│  방어: 차분 프라이버시 / 연합 학습 / 동형 암호        │
└─────────────────────────────────────────────────────┘
```

## 2. 차분 프라이버시 (Differential Privacy)

### 2.1 정의

메커니즘 $\mathcal{M}$이 $(\epsilon, \delta)$-차분 프라이버시를 만족하려면, 하나의 레코드만 다른 인접 데이터셋 $D$와 $D'$에 대해:

$$P[\mathcal{M}(D) \in S] \leq e^{\epsilon} \cdot P[\mathcal{M}(D') \in S] + \delta$$

- $\epsilon$ (프라이버시 예산): 작을수록 강한 보호. 실무에서 1~10 범위
- $\delta$: 보호 실패 확률. 보통 $1/n^2$ 이하로 설정

### 2.2 DP-SGD (Differentially Private Stochastic Gradient Descent)

```python
import torch
from opacus import PrivacyEngine

# 기본 모델 정의
model = torch.nn.Sequential(
    torch.nn.Linear(784, 256),
    torch.nn.ReLU(),
    torch.nn.Linear(256, 10)
)

optimizer = torch.optim.SGD(model.parameters(), lr=0.1)
train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64)

# Opacus로 DP-SGD 적용
privacy_engine = PrivacyEngine()
model, optimizer, train_loader = privacy_engine.make_private_with_epsilon(
    module=model,
    optimizer=optimizer,
    data_loader=train_loader,
    epochs=10,
    target_epsilon=3.0,     # 프라이버시 예산
    target_delta=1e-5,      # 실패 확률
    max_grad_norm=1.0       # 그래디언트 클리핑 상한
)

# 학습 루프
for epoch in range(10):
    for batch in train_loader:
        optimizer.zero_grad()
        loss = criterion(model(batch[0]), batch[1])
        loss.backward()
        optimizer.step()

    # 현재까지 소모된 프라이버시 예산 확인
    epsilon = privacy_engine.get_epsilon(delta=1e-5)
    print(f"Epoch {epoch}: ε = {epsilon:.2f}")
```

### 2.3 프라이버시-정확도 트레이드오프

| $\epsilon$ | 프라이버시 보호 | 정확도 영향 | 적합한 용도 |
|------------|----------------|------------|------------|
| 0.1 ~ 1 | 매우 강함 | 큰 하락 | 민감 의료/금융 |
| 1 ~ 5 | 강함 | 중간 하락 | 일반 개인정보 |
| 5 ~ 10 | 보통 | 작은 하락 | 통계 분석 |
| > 10 | 약함 | 거의 없음 | 형식적 준수 |

> **핵심 직관**: DP-SGD에서 프라이버시 보호의 핵심은 그래디언트 클리핑 + 노이즈 추가입니다. 클리핑은 개별 데이터 포인트의 영향을 제한하고, 노이즈는 어떤 개인이 포함되었는지를 불확실하게 만듭니다.

## 3. 연합 학습 (Federated Learning)

데이터를 중앙에 모으지 않고 분산된 상태에서 모델을 학습합니다.

### 3.1 FedAvg 알고리즘

```
┌──────────────────────────────────────────────────┐
│             연합 학습 (FedAvg) 프로세스            │
├──────────────────────────────────────────────────┤
│                                                  │
│  [중앙 서버]                                      │
│     │  1. 글로벌 모델 $w_t$ 배포                  │
│     ▼                                            │
│  [클라이언트 1]  [클라이언트 2]  [클라이언트 3]    │
│  로컬 데이터로   로컬 데이터로   로컬 데이터로     │
│  E 에폭 학습    E 에폭 학습    E 에폭 학습        │
│  $w_1^{t+1}$   $w_2^{t+1}$   $w_3^{t+1}$       │
│     │              │              │               │
│     └──────────────┼──────────────┘               │
│                    ▼                              │
│  [중앙 서버]                                      │
│  2. 모델 파라미터 집계                             │
│  $w_{t+1} = \sum_k \frac{n_k}{n} w_k^{t+1}$     │
│  3. 다음 라운드 반복                              │
│                                                  │
└──────────────────────────────────────────────────┘
```

```python
# 연합 학습 의사 코드 (Flower 프레임워크 기반)
import flwr as fl

# 클라이언트 정의
class HospitalClient(fl.client.NumPyClient):
    def __init__(self, model, train_data, val_data):
        self.model = model
        self.train_data = train_data
        self.val_data = val_data

    def get_parameters(self, config):
        return [p.detach().numpy() for p in self.model.parameters()]

    def fit(self, parameters, config):
        # 글로벌 파라미터로 로컬 모델 업데이트
        set_parameters(self.model, parameters)
        # 로컬 데이터로 학습 (데이터는 병원 밖으로 나가지 않음)
        train(self.model, self.train_data, epochs=5)
        return self.get_parameters(config), len(self.train_data), {}

    def evaluate(self, parameters, config):
        set_parameters(self.model, parameters)
        loss, accuracy = evaluate(self.model, self.val_data)
        return loss, len(self.val_data), {"accuracy": accuracy}

# 서버 실행
fl.server.start_server(
    server_address="0.0.0.0:8080",
    config=fl.server.ServerConfig(num_rounds=50)
)
```

### 시나리오: 다수 병원의 의료 AI 협업

5개 대학병원이 희귀 질환 진단 모델을 공동 개발하려 합니다. 환자 데이터를 공유할 수 없으므로 연합 학습을 적용합니다. 각 병원은 로컬 데이터로 학습하고 모델 업데이트만 서버에 전송합니다. DP를 결합하면 업데이트 자체에서도 개인정보를 추론할 수 없습니다.

## 4. 동형 암호 (Homomorphic Encryption)

암호화된 데이터에 대해 직접 연산을 수행할 수 있는 기법입니다.

$$E(a) \oplus E(b) = E(a + b), \quad E(a) \otimes E(b) = E(a \cdot b)$$

| 유형 | 지원 연산 | 성능 오버헤드 | 적합한 용도 |
|------|-----------|-------------|------------|
| 부분 동형 (PHE) | 덧셈 또는 곱셈 | 10~100x | 집계 연산 |
| 다소 동형 (SHE) | 제한된 덧셈+곱셈 | 100~1000x | 간단한 추론 |
| 완전 동형 (FHE) | 임의 연산 | 1000~10000x | 복잡한 추론 |

```python
# 간단한 동형 암호 추론 예시 (개념적)
# TenSEAL 라이브러리 사용
import tenseal as ts

# 암호화 컨텍스트 생성
context = ts.context(
    ts.SCHEME_TYPE.CKKS,
    poly_modulus_degree=8192,
    coeff_mod_bit_sizes=[60, 40, 40, 60]
)
context.global_scale = 2**40

# 데이터 암호화
encrypted_input = ts.ckks_vector(context, patient_features.tolist())

# 암호화된 상태에서 선형 예측
encrypted_result = encrypted_input.dot(model_weights) + model_bias

# 결과 복호화 (데이터 소유자만 가능)
prediction = encrypted_result.decrypt()
```

## 5. 프라이버시 보존 기법 비교

| 기준 | 차분 프라이버시 | 연합 학습 | 동형 암호 |
|------|---------------|-----------|-----------|
| 보호 대상 | 학습 과정 | 데이터 위치 | 데이터 내용 |
| 정확도 영향 | 중간~큼 | 작음 | 없음 |
| 계산 비용 | 낮음 | 중간 (통신) | 매우 높음 |
| 이론적 보장 | 수학적 증명 | 제한적 | 수학적 증명 |
| 성숙도 | 높음 | 높음 | 초기 |
| 결합 가능성 | FL + DP 가능 | DP, HE와 결합 | FL + HE 가능 |

> **핵심 직관**: 세 기법은 상호 배타적이 아닙니다. 연합 학습으로 데이터를 분산 유지하고, DP로 모델 업데이트를 보호하며, 동형 암호로 추론 시 입력을 보호하는 다층 방어가 가능합니다.

## 6. 프라이버시 공격과 방어

### 시나리오: 멤버십 추론 공격 방어

공격자가 모델의 출력 확률 분포를 분석하여 특정 데이터가 학습에 포함되었는지 추론합니다. 학습 데이터에 대한 모델의 과신(overconfidence)이 주요 단서입니다.

```python
# 멤버십 추론 공격 시뮬레이션
def membership_inference_attack(model, member_data, non_member_data):
    """학습 데이터 vs 비학습 데이터의 예측 확신도 비교"""
    member_conf = model.predict_proba(member_data).max(axis=1)
    non_member_conf = model.predict_proba(non_member_data).max(axis=1)

    # 공격 정확도: 확신도 기반 분류
    threshold = 0.5 * (member_conf.mean() + non_member_conf.mean())
    attack_acc = 0.5 * (
        (member_conf > threshold).mean() +
        (non_member_conf <= threshold).mean()
    )
    return attack_acc  # 0.5 = 무작위, >0.5 = 프라이버시 유출

# 방어: DP-SGD 적용 후 공격 정확도 비교
# 일반 모델: attack_acc ≈ 0.72
# DP 모델 (ε=3): attack_acc ≈ 0.53 (거의 무작위 수준)
```

## 핵심 정리

- ML 모델은 멤버십 추론, 모델 역전, 데이터 추출 등 다양한 프라이버시 공격에 취약하며, 단순 익명화로는 방어가 불충분합니다
- 차분 프라이버시(DP)는 수학적으로 증명 가능한 프라이버시 보장을 제공하며, DP-SGD를 통해 딥러닝에 적용됩니다
- 연합 학습은 데이터를 분산 유지하면서 협업적 모델 학습을 가능하게 하며, 의료/금융 등 데이터 공유가 제한된 도메인에 적합합니다
- 동형 암호는 암호화된 데이터에 직접 연산을 수행하지만 계산 비용이 매우 높아, 현재는 간단한 추론 작업에 제한됩니다
- 세 기법은 상호 보완적이며, FL + DP + HE의 다층 방어가 가장 강력한 프라이버시 보호를 제공합니다
