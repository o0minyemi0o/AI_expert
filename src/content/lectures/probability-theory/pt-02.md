# 결합 분포와 조건부 확률

## 왜 결합 분포와 조건부 확률을 배워야 하는가

현실의 데이터는 단일 변수로 존재하지 않습니다. 이미지의 픽셀은 서로 상관되어 있고, 텍스트의 단어는 문맥에 따라 확률이 달라지며, 특성(feature)과 레이블(label)은 함께 관찰됩니다. **결합 분포(joint distribution)**는 여러 확률 변수의 관계를 기술하고, **조건부 확률(conditional probability)**은 관측된 정보가 주어졌을 때의 믿음을 업데이트합니다.

이번 강의에서는 결합 분포와 주변 분포의 관계, 베이즈 정리, 그리고 독립성의 개념을 ML 관점에서 다룹니다.

---

## 1. 결합 분포

두 확률 변수 $X, Y$의 **결합 분포(joint distribution)**는 두 변수가 동시에 취하는 값의 확률을 기술합니다.

**이산의 경우 (결합 PMF):**

$$
p_{X,Y}(x, y) = P(X = x, Y = y)
$$

**연속의 경우 (결합 PDF):**

$$
f_{X,Y}(x, y) \geq 0, \quad \int_{-\infty}^{\infty}\int_{-\infty}^{\infty} f_{X,Y}(x, y)\,dx\,dy = 1
$$

$n$개의 확률 변수 $X_1, \ldots, X_n$으로 확장하면, 결합 분포 $p(x_1, \ldots, x_n)$은 **데이터 생성 과정(DGP)**의 완전한 기술이 됩니다.

> **핵심 직관**: ML에서 결합 분포 $p(\mathbf{x}, y)$를 알면 모든 것을 할 수 있습니다 — 분류($p(y|\mathbf{x})$), 생성($p(\mathbf{x})$), 이상 탐지($p(\mathbf{x})$가 낮은 영역) 모두 결합 분포로부터 도출됩니다.

---

## 2. 주변 분포 (Marginalization)

결합 분포에서 하나의 변수를 "합산(또는 적분)하여 제거"하면 나머지 변수의 **주변 분포(marginal distribution)**를 얻습니다.

**이산:**

$$
p_X(x) = \sum_{y} p_{X,Y}(x, y)
$$

**연속:**

$$
f_X(x) = \int_{-\infty}^{\infty} f_{X,Y}(x, y)\,dy
$$

```
주변화(Marginalization) 시각화
───────────────────────────────────
       y
       ▲
       │  ┌─────────────┐
       │  │  p(x, y)    │ → ∫ dy → p(x)  [아래쪽 축으로 사영]
       │  │   결합분포    │
       │  └─────────────┘
       └──────────────────► x
              ↓ ∫ dx
            p(y)  [왼쪽 축으로 사영]
───────────────────────────────────
```

> **핵심 직관**: 주변화는 "관심 없는 변수를 적분으로 지운다"는 뜻입니다. 베이즈 추론에서 파라미터를 적분으로 제거하는 것(pt-07에서 다룰 예정)이 바로 이 연산입니다.

---

## 3. 조건부 확률

사건 $B$가 관측되었을 때 사건 $A$의 **조건부 확률**은:

$$
P(A | B) = \frac{P(A \cap B)}{P(B)}, \quad P(B) > 0
$$

확률 변수로 확장하면:

**이산:**

$$
p_{Y|X}(y|x) = \frac{p_{X,Y}(x, y)}{p_X(x)}
$$

**연속:**

$$
f_{Y|X}(y|x) = \frac{f_{X,Y}(x, y)}{f_X(x)}
$$

결합 = 조건부 $\times$ 주변이라는 관계가 핵심입니다:

$$
p(x, y) = p(y|x)\,p(x) = p(x|y)\,p(y)
$$

---

## 4. 베이즈 정리

조건부 확률의 정의로부터 직접 유도됩니다:

$$
p(y|x) = \frac{p(x|y)\,p(y)}{p(x)} = \frac{p(x|y)\,p(y)}{\sum_{y'} p(x|y')\,p(y')}
$$

각 요소의 이름을 정리하면:

| 항 | 이름 | ML 대응 |
|----|------|---------|
| $p(y\|x)$ | 사후 확률 (posterior) | 모델의 예측 |
| $p(x\|y)$ | 우도 (likelihood) | 클래스별 데이터 생성 모델 |
| $p(y)$ | 사전 확률 (prior) | 클래스 비율, 사전 지식 |
| $p(x)$ | 증거 (evidence) | 정규화 상수 |

> **핵심 직관**: 베이즈 정리는 "관측 전 믿음(prior)을 데이터(likelihood)로 업데이트하여 관측 후 믿음(posterior)을 얻는" 학습의 수학적 공식입니다. 나이브 베이즈 분류기, 베이즈 최적화, VAE 등 ML 전반에 등장합니다.

---

## 5. 독립성과 조건부 독립성

### 독립성

$X$와 $Y$가 **독립(independent)**이면:

$$
p(x, y) = p(x)\,p(y) \quad \forall x, y
$$

동치 조건: $p(y|x) = p(y)$, 즉 $X$를 알아도 $Y$에 대한 정보가 없습니다.

### 조건부 독립성

$Z$가 주어졌을 때 $X$와 $Y$가 **조건부 독립(conditionally independent)**이면:

$$
p(x, y | z) = p(x|z)\,p(y|z) \quad \forall x, y, z
$$

이를 $X \perp Y \mid Z$로 표기합니다.

| 개념 | 표기 | 의미 |
|------|------|------|
| 독립 | $X \perp Y$ | $p(x,y) = p(x)p(y)$ |
| 조건부 독립 | $X \perp Y \mid Z$ | $p(x,y\|z) = p(x\|z)p(y\|z)$ |

**주의**: 독립이면 조건부 독립이 되는 것도 아니고, 조건부 독립이면 독립이 되는 것도 아닙니다.

---

## 6. ML에서의 조건부 독립성

조건부 독립 가정은 고차원 결합 분포를 다루기 쉽게 만드는 핵심 도구입니다.

**나이브 베이즈(Naive Bayes):**

$$
p(\mathbf{x}|y) = \prod_{j=1}^{d} p(x_j | y) \quad \text{(특성들이 클래스 주어질 때 조건부 독립)}
$$

**마르코프 체인:**

$$
p(x_t | x_1, \ldots, x_{t-1}) = p(x_t | x_{t-1}) \quad \text{(과거 전체가 아닌 직전 상태에만 의존)}
$$

**그래프 모델(Bayesian Network):**

각 노드가 부모 노드에만 조건부 의존하므로, 결합 분포가 인수분해됩니다:

$$
p(x_1, \ldots, x_n) = \prod_{i=1}^{n} p(x_i | \text{parents}(x_i))
$$

---

## 7. Python으로 확인하기

```python
import numpy as np

# --- 결합 분포 테이블 (이산) ---
# X: 공부 시간 {적음=0, 많음=1}, Y: 합격 여부 {불합격=0, 합격=1}
joint = np.array([[0.30, 0.10],   # X=0: P(X=0,Y=0), P(X=0,Y=1)
                  [0.05, 0.55]])  # X=1: P(X=1,Y=0), P(X=1,Y=1)

print("결합 분포 합:", joint.sum())  # 1.0

# 주변 분포
p_X = joint.sum(axis=1)  # [0.40, 0.60]
p_Y = joint.sum(axis=0)  # [0.35, 0.65]
print("P(X):", p_X)
print("P(Y):", p_Y)

# 조건부 분포: P(Y|X=1)
p_Y_given_X1 = joint[1, :] / p_X[1]
print("P(Y|X=1):", p_Y_given_X1)  # [0.083, 0.917]

# 독립성 검정: p(x,y) == p(x)*p(y)?
independent = np.allclose(joint, np.outer(p_X, p_Y))
print("X, Y 독립?", independent)  # False

# --- 베이즈 정리 검증 ---
# P(X=1|Y=1) = P(Y=1|X=1) * P(X=1) / P(Y=1)
p_X1_given_Y1 = p_Y_given_X1[1] * p_X[1] / p_Y[1]
print(f"P(X=1|Y=1) = {p_X1_given_Y1:.4f}")  # 0.8462
print(f"검증 (결합/주변): {joint[1,1] / p_Y[1]:.4f}")  # 동일
```

---

## 핵심 정리

1. **결합 분포** $p(x, y)$는 여러 확률 변수의 완전한 확률적 기술이며, ML에서 데이터 생성 과정을 모델링하는 기초이다.
2. **주변화**는 관심 없는 변수를 합산/적분으로 제거하는 연산으로, 결합에서 주변으로의 유일한 경로이다.
3. **베이즈 정리** $p(y|x) \propto p(x|y)\,p(y)$는 관측 데이터로 사전 믿음을 업데이트하는 학습의 수학적 원리이다.
4. **조건부 독립** $X \perp Y \mid Z$은 고차원 결합 분포를 인수분해하여 계산 가능하게 만드는 핵심 가정이다.
5. 나이브 베이즈, 마르코프 체인, 베이즈 네트워크 등 ML의 주요 모델들은 모두 **조건부 독립 구조**에 기반한다.
