# 최대 엔트로피와 정보 기하

## 왜 최대 엔트로피와 정보 기하를 배워야 하는가

pt-10에서 "평균과 분산이 고정일 때 최대 엔트로피 분포는 가우시안"이라고 언급했습니다. 이것은 우연이 아니라 **최대 엔트로피 원리(Maximum Entropy Principle)**의 결과이며, 지수족이 왜 자연스러운 분포족인지를 설명합니다. 또한 확률 분포를 기하학적 공간의 점으로 보면, **Fisher 정보 행렬**이 이 공간의 "곡률"을 정의하고, 이를 활용한 **자연 경사법(natural gradient)**이 더 효율적인 학습을 가능하게 합니다.

이번 강의에서는 최대 엔트로피 원리, Fisher 정보 행렬, 그리고 자연 경사법의 기초를 다룹니다.

---

## 1. 최대 엔트로피 원리

**주어진 제약 조건을 만족하면서 가장 불확실한(엔트로피가 최대인) 분포를 선택하라.**

이것은 "알려진 것 이외에 가정을 최소화한다"는 철학입니다.

제약 조건 $\mathbb{E}[f_k(X)] = c_k$ ($k = 1, \ldots, m$)와 $\sum_x p(x) = 1$ 하에서:

$$
\max_p H(p) = -\sum_x p(x) \log p(x)
$$

라그랑주 승수법(co-08에서 다뤘던)을 적용하면:

$$
p^*(x) = \frac{1}{Z} \exp\!\left(\sum_{k=1}^{m} \lambda_k f_k(x)\right)
$$

여기서 $Z = \sum_x \exp(\sum_k \lambda_k f_k(x))$는 분할 함수입니다.

> **핵심 직관**: 최대 엔트로피 해는 항상 **지수족** 형태입니다! 제약 함수 $f_k$가 충분 통계량, 라그랑주 승수 $\lambda_k$가 자연 파라미터에 대응합니다. pt-08의 지수족이 "가장 자연스러운" 이유가 바로 이것입니다.

---

## 2. 최대 엔트로피 분포의 예

| 제약 조건 | 최대 엔트로피 분포 |
|-----------|-------------------|
| 없음 (유한 지지 $\{1,\ldots,K\}$) | 균등 분포 (Uniform) |
| $\mathbb{E}[X] = \mu$ ($X \geq 0$) | 지수 분포 (Exponential) |
| $\mathbb{E}[X] = \mu$, $\text{Var}(X) = \sigma^2$ | **가우시안** (Gaussian) |
| $\sum_k \pi_k = 1$ | Categorical (균등) |

### 가우시안의 유도

제약: $\int f(x)dx = 1$, $\int xf(x)dx = \mu$, $\int (x-\mu)^2 f(x)dx = \sigma^2$

라그랑주:

$$
\mathcal{L}[f] = -\int f\log f\,dx + \lambda_0\!\left(\int f\,dx - 1\right) + \lambda_1\!\left(\int xf\,dx - \mu\right) + \lambda_2\!\left(\int (x-\mu)^2 f\,dx - \sigma^2\right)
$$

변분법으로 풀면:

$$
f^*(x) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\!\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)
$$

> **핵심 직관**: 가우시안을 사용하는 것은 "평균과 분산만 알고, 나머지는 모른다"는 최소 가정에 해당합니다. 이것이 회귀 문제에서 가우시안 노이즈를 가정하는 이론적 근거입니다.

---

## 3. Fisher 정보 행렬

파라미터 $\boldsymbol{\theta}$로 인덱싱된 분포족 $\{p(x|\boldsymbol{\theta})\}$에 대해, **Fisher 정보 행렬**:

$$
\mathcal{I}(\boldsymbol{\theta})_{ij} = \mathbb{E}_{X \sim p(\cdot|\boldsymbol{\theta})}\!\left[\frac{\partial \log p(X|\boldsymbol{\theta})}{\partial \theta_i} \cdot \frac{\partial \log p(X|\boldsymbol{\theta})}{\partial \theta_j}\right]
$$

동치 표현 (정칙 조건 하):

$$
\mathcal{I}(\boldsymbol{\theta})_{ij} = -\mathbb{E}\!\left[\frac{\partial^2 \log p(X|\boldsymbol{\theta})}{\partial \theta_i \partial \theta_j}\right]
$$

### Fisher 정보의 성질

- 양의 반정치 행렬 (la-01의 성질)
- 로그 우도의 곡률(curvature)을 측정
- Cramer-Rao 하한: 비편향 추정량 $\hat{\theta}$에 대해 $\text{Var}(\hat{\theta}) \geq \mathcal{I}(\theta)^{-1}$
- pt-05에서 본 MLE의 점근적 분산이 $\mathcal{I}(\theta)^{-1}$

> **핵심 직관**: Fisher 정보는 "파라미터를 약간 바꿨을 때 분포가 얼마나 변하는가"의 척도입니다. 정보가 큰 방향은 분포가 민감하게 변하므로, 그 방향의 파라미터를 정밀하게 추정할 수 있습니다.

---

## 4. 정보 기하학의 관점

확률 분포의 공간을 **리만 다양체(Riemannian manifold)**로 봅니다. Fisher 정보 행렬 $\mathcal{I}(\boldsymbol{\theta})$가 이 다양체의 **계량 텐서(metric tensor)**가 됩니다.

두 가까운 분포 사이의 "거리":

$$
ds^2 = \sum_{ij} \mathcal{I}(\boldsymbol{\theta})_{ij}\,d\theta_i\,d\theta_j = d\boldsymbol{\theta}^T \mathcal{I}(\boldsymbol{\theta})\,d\boldsymbol{\theta}
$$

이 거리는 KL 발산의 2차 근사와 일치합니다:

$$
D_{\text{KL}}(p_{\boldsymbol{\theta}} \| p_{\boldsymbol{\theta}+d\boldsymbol{\theta}}) \approx \frac{1}{2} d\boldsymbol{\theta}^T \mathcal{I}(\boldsymbol{\theta})\,d\boldsymbol{\theta}
$$

```
정보 기하학적 관점
──────────────────────────────────────────
유클리드 공간 (보통 경사법)    통계 다양체 (자연 경사법)
   계량 = I (항등행렬)           계량 = F(θ) (Fisher 행렬)

   θ₁                          θ₁
    ↑                           ↑
    │→→→→                       │╲
    │    ↓   (유클리드 최단)      │  ╲   (측지선)
    │    ↓                       │    ╲
    └─────► θ₂                  └──────► θ₂
──────────────────────────────────────────
```

---

## 5. 자연 경사법 (Natural Gradient)

보통 경사법은 **유클리드 공간**에서의 최급강하 방향입니다:

$$
\boldsymbol{\theta}_{t+1} = \boldsymbol{\theta}_t - \eta \nabla_{\boldsymbol{\theta}} L
$$

그러나 파라미터 공간은 유클리드 공간이 아니라 **통계 다양체**입니다. 자연 경사법은 Fisher 행렬을 계량으로 사용합니다:

$$
\boldsymbol{\theta}_{t+1} = \boldsymbol{\theta}_t - \eta \mathcal{I}(\boldsymbol{\theta}_t)^{-1} \nabla_{\boldsymbol{\theta}} L
$$

co-05에서 다뤘던 Newton's method에서 Hessian 대신 Fisher 행렬을 쓴 것과 형태가 같습니다.

### 장점

- **파라미터화 불변**: $\theta$를 재파라미터화해도 업데이트 방향이 동일
- 분포 공간에서의 "진짜 최급강하 방향"
- 실전에서는 KFAC, TNGD 등의 근사 방법 사용

> **핵심 직관**: 보통 경사법은 "파라미터 공간에서 가장 빠른 방향"이고, 자연 경사법은 "분포 공간에서 가장 빠른 방향"입니다. 파라미터의 스케일에 영향받지 않으므로, 특히 신경망에서 학습 효율이 높습니다.

---

## 6. 지수족에서의 Fisher 정보

pt-08의 지수족 $p(x|\boldsymbol{\eta}) = h(x)\exp(\boldsymbol{\eta}^T\mathbf{T}(x) - A(\boldsymbol{\eta}))$에서:

$$
\mathcal{I}(\boldsymbol{\eta}) = \nabla^2 A(\boldsymbol{\eta}) = \text{Cov}(\mathbf{T}(X))
$$

로그 분할 함수의 Hessian이 Fisher 정보 행렬입니다! 이것은 $A$가 볼록 함수(pt-08에서 언급)라는 사실과 일관됩니다.

---

## 7. Python으로 확인하기

```python
import numpy as np
from scipy import stats

# --- 최대 엔트로피 검증: 분산 고정 시 가우시안의 엔트로피가 최대 ---
sigma = 2.0
n_samples = 100000

# 가우시안의 미분 엔트로피 (해석적)
h_gaussian = 0.5 * np.log(2 * np.pi * np.e * sigma**2)
print(f"가우시안 미분 엔트로피: {h_gaussian:.4f} nats")

# 같은 분산을 가진 라플라스 분포
b_laplace = sigma / np.sqrt(2)  # Laplace의 분산 = 2b²
h_laplace = np.log(2 * b_laplace * np.e)
print(f"라플라스 미분 엔트로피: {h_laplace:.4f} nats")
print(f"가우시안이 더 큰 엔트로피? {h_gaussian > h_laplace}")

# --- Fisher 정보 행렬: Bernoulli ---
def fisher_bernoulli(theta):
    """Bernoulli의 Fisher 정보"""
    return 1 / (theta * (1 - theta))

# 수치적 검증
theta = 0.3
eps = 1e-5
n_mc = 200000

# 스코어 함수의 분산으로 Fisher 정보 추정
scores = []
for _ in range(n_mc):
    x = np.random.binomial(1, theta)
    score = x / theta - (1 - x) / (1 - theta)
    scores.append(score)

fisher_empirical = np.var(scores)
fisher_analytic = fisher_bernoulli(theta)
print(f"\nFisher 정보 (Bernoulli, θ={theta}):")
print(f"  해석적: {fisher_analytic:.4f}")
print(f"  경험적: {fisher_empirical:.4f}")

# --- 자연 경사 vs 보통 경사 ---
# 간단한 예: Bernoulli 파라미터의 MLE
true_theta = 0.7
data = np.random.binomial(1, true_theta, size=30)

# 보통 경사법
theta_std = 0.5
lr = 0.01
for step in range(100):
    grad = np.sum(data / theta_std - (1 - data) / (1 - theta_std)) / len(data)
    theta_std = np.clip(theta_std + lr * grad, 0.01, 0.99)

# 자연 경사법
theta_nat = 0.5
for step in range(100):
    grad = np.sum(data / theta_nat - (1 - data) / (1 - theta_nat)) / len(data)
    fisher = fisher_bernoulli(theta_nat)
    natural_grad = grad / fisher  # F⁻¹ ∇
    theta_nat = np.clip(theta_nat + lr * natural_grad, 0.01, 0.99)

print(f"\nMLE (참값 θ={true_theta}):")
print(f"  보통 경사법: θ = {theta_std:.4f}")
print(f"  자연 경사법: θ = {theta_nat:.4f}")
print(f"  직접 MLE:  θ = {data.mean():.4f}")
```

---

## 핵심 정리

1. **최대 엔트로피 원리**는 주어진 제약 하에서 가장 적은 가정을 하는 분포를 선택하며, 그 해는 항상 **지수족** 형태이다.
2. 평균·분산 제약 하의 최대 엔트로피 분포는 **가우시안**이며, 이것이 회귀에서 가우시안 가정의 이론적 근거이다.
3. **Fisher 정보 행렬** $\mathcal{I}(\boldsymbol{\theta})$는 로그 우도의 곡률이며, 추정량의 분산 하한(Cramer-Rao)과 MLE의 점근적 분산을 결정한다.
4. Fisher 정보는 확률 분포 공간의 **리만 계량**으로, KL 발산의 국소적 이차 근사에 해당한다.
5. **자연 경사법**은 Fisher 행렬을 사용하여 분포 공간에서의 최급강하 방향을 구하며, 파라미터화에 불변인 학습을 가능하게 한다.
