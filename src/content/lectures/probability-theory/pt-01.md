# 확률 공간과 확률 변수

## 왜 확률 공간을 배워야 하는가

머신러닝은 본질적으로 **불확실성 아래서의 의사결정**입니다. 데이터는 확률적으로 생성되고, 모델의 예측에는 항상 불확실성이 따르며, 학습 자체가 확률 분포의 파라미터를 추정하는 과정입니다. 이 모든 것을 수학적으로 엄밀하게 다루려면, 확률의 기초인 **확률 공간(probability space)**과 **확률 변수(random variable)**를 정확히 이해해야 합니다.

이번 강의에서는 표본 공간과 사건의 개념부터 시작해, 콜모고로프의 확률 공리, 확률 변수의 엄밀한 정의, 그리고 CDF·PMF·PDF의 관계까지 다룹니다.

---

## 1. 표본 공간과 사건

**표본 공간(sample space)** $\Omega$는 실험에서 나올 수 있는 모든 결과의 집합입니다.

| 실험 | 표본 공간 $\Omega$ |
|------|---------------------|
| 동전 던지기 | $\{H, T\}$ |
| 주사위 던지기 | $\{1, 2, 3, 4, 5, 6\}$ |
| 신경망 출력 (회귀) | $\mathbb{R}$ |
| 이미지 생성 | $[0, 1]^{H \times W \times 3}$ |

**사건(event)** $A$는 표본 공간의 부분집합입니다. "주사위가 짝수"라는 사건은 $A = \{2, 4, 6\} \subseteq \Omega$입니다.

> **핵심 직관**: ML에서 표본 공간은 "데이터가 살 수 있는 모든 가능한 공간"이고, 사건은 "우리가 관심 있는 조건"입니다. 분류기의 정확도란 결국 "올바른 예측"이라는 사건의 확률을 최대화하는 것입니다.

---

## 2. 시그마 대수

표본 공간이 무한할 때, 모든 부분집합에 확률을 부여하면 수학적 모순이 발생합니다. 이를 해결하기 위해 **시그마 대수(sigma-algebra, $\sigma$-algebra)** $\mathcal{F}$를 도입합니다.

$\mathcal{F} \subseteq 2^{\Omega}$는 다음 세 조건을 만족하는 사건들의 모임입니다:

1. $\Omega \in \mathcal{F}$ (전체 집합 포함)
2. $A \in \mathcal{F} \Rightarrow A^c \in \mathcal{F}$ (여사건에 닫힘)
3. $A_1, A_2, \ldots \in \mathcal{F} \Rightarrow \bigcup_{i=1}^{\infty} A_i \in \mathcal{F}$ (가산 합집합에 닫힘)

$\mathbb{R}$ 위에서 가장 흔히 사용하는 시그마 대수는 **보렐 시그마 대수(Borel $\sigma$-algebra)** $\mathcal{B}(\mathbb{R})$로, 모든 열린 구간으로부터 생성됩니다.

> **핵심 직관**: 시그마 대수는 "확률을 물어볼 수 있는 질문의 집합"입니다. ML에서 대부분의 경우 보렐 시그마 대수를 사용하므로, 실용적으로는 "구간의 확률을 물어볼 수 있다"고 이해하면 충분합니다.

---

## 3. 확률 측도와 콜모고로프 공리

**확률 측도(probability measure)** $P: \mathcal{F} \to [0, 1]$는 다음 세 공리를 만족합니다:

$$
\begin{aligned}
&\textbf{(K1)} \quad P(A) \geq 0 \quad \forall A \in \mathcal{F} \\
&\textbf{(K2)} \quad P(\Omega) = 1 \\
&\textbf{(K3)} \quad A_1, A_2, \ldots \text{가 서로소이면} \quad P\!\left(\bigcup_{i=1}^{\infty} A_i\right) = \sum_{i=1}^{\infty} P(A_i)
\end{aligned}
$$

이 세 공리로부터 우리가 아는 모든 확률의 성질이 도출됩니다:

- $P(\emptyset) = 0$
- $P(A^c) = 1 - P(A)$
- $A \subseteq B \Rightarrow P(A) \leq P(B)$
- $P(A \cup B) = P(A) + P(B) - P(A \cap B)$

세 요소 $(\Omega, \mathcal{F}, P)$를 묶어 **확률 공간(probability space)**이라 합니다.

---

## 4. 확률 변수

**확률 변수(random variable)** $X$는 표본 공간에서 실수로의 **가측 함수(measurable function)**입니다:

$$
X: \Omega \to \mathbb{R}, \quad \{X \leq x\} := \{\omega \in \Omega : X(\omega) \leq x\} \in \mathcal{F} \quad \forall x \in \mathbb{R}
$$

확률 변수는 "숫자가 아니라 함수"라는 점이 핵심입니다. 주사위 실험에서 $X(\omega) = \omega$로 정의하면, $X$는 1부터 6 사이의 값을 취하는 확률 변수가 됩니다.

> **핵심 직관**: 확률 변수는 "실험 결과를 숫자로 번역하는 규칙"입니다. ML에서 데이터 $\mathbf{x}$, 레이블 $y$, 모델 파라미터 $\boldsymbol{\theta}$(베이즈 관점) 모두 확률 변수입니다.

---

## 5. CDF, PMF, PDF의 관계

확률 변수의 분포를 기술하는 세 가지 방법이 있습니다.

### 누적 분포 함수 (CDF)

모든 확률 변수에 대해 정의됩니다:

$$
F_X(x) = P(X \leq x)
$$

CDF의 성질: 단조 비감소, $\lim_{x \to -\infty} F_X(x) = 0$, $\lim_{x \to +\infty} F_X(x) = 1$, 우연속.

### 확률 질량 함수 (PMF)

이산 확률 변수에 대해 정의됩니다:

$$
p_X(x) = P(X = x)
$$

### 확률 밀도 함수 (PDF)

연속 확률 변수에 대해, CDF가 미분 가능할 때:

$$
f_X(x) = \frac{dF_X(x)}{dx}, \quad P(a \leq X \leq b) = \int_a^b f_X(x)\,dx
$$

```
CDF, PMF, PDF 관계도
─────────────────────────────────────────
        이산 확률 변수         연속 확률 변수
        ─────────────         ─────────────
PMF:    p(x) = P(X=x)        (존재하지 않음)
PDF:    (존재하지 않음)        f(x) = dF/dx
CDF:    F(x) = Σ p(k)         F(x) = ∫f(t)dt
              k≤x                    -∞ to x
─────────────────────────────────────────
```

> **핵심 직관**: PDF 값 $f(x)$는 확률이 **아닙니다** — 1을 초과할 수도 있습니다. 확률은 반드시 구간에 대해서만 정의됩니다. 분류 모델의 소프트맥스 출력은 PMF이고, 정규화 흐름(normalizing flow)의 출력은 PDF입니다.

---

## 6. Python으로 확인하기

```python
import numpy as np
from scipy import stats
import matplotlib.pyplot as plt

# --- 이산 확률 변수: 주사위 ---
die = stats.randint(1, 7)  # {1,2,3,4,5,6} 균등
x_disc = np.arange(1, 7)

print("PMF:", die.pmf(x_disc))           # [1/6, 1/6, ..., 1/6]
print("CDF at 3:", die.cdf(3))           # 0.5
print("P(2 ≤ X ≤ 4):", die.cdf(4) - die.cdf(1))  # 0.5

# --- 연속 확률 변수: 표준 정규 ---
normal = stats.norm(loc=0, scale=1)
x_cont = np.linspace(-4, 4, 200)

pdf_vals = normal.pdf(x_cont)
cdf_vals = normal.cdf(x_cont)

print(f"PDF at 0: {normal.pdf(0):.4f}")   # 0.3989 (> 0이지만 확률 아님)
print(f"P(-1 ≤ X ≤ 1): {normal.cdf(1) - normal.cdf(-1):.4f}")  # 0.6827

# --- 경험적 CDF 확인 ---
samples = normal.rvs(size=10000, random_state=42)
ecdf = np.mean(samples <= 1.0)
print(f"경험적 P(X ≤ 1): {ecdf:.4f}")    # ≈ 0.8413
print(f"이론적 P(X ≤ 1): {normal.cdf(1):.4f}")
```

---

## 7. ML에서의 연결

| 확률론 개념 | ML 대응 |
|-------------|---------|
| 확률 공간 $(\Omega, \mathcal{F}, P)$ | 데이터 생성 과정 (DGP) |
| 확률 변수 $X$ | 특성(feature), 레이블(label) |
| PMF $p(x)$ | 분류 모델의 소프트맥스 출력 |
| PDF $f(x)$ | 생성 모델의 밀도 추정 |
| CDF $F(x)$ | 분위수 회귀 (quantile regression) |

---

## 핵심 정리

1. **확률 공간** $(\Omega, \mathcal{F}, P)$는 표본 공간·시그마 대수·확률 측도의 삼중 구조이며, 확률론의 모든 논의의 출발점이다.
2. **시그마 대수**는 "확률을 물어볼 수 있는 사건의 모임"으로, 무한 표본 공간에서 수학적 일관성을 보장한다.
3. **확률 변수**는 숫자가 아니라 $\Omega \to \mathbb{R}$인 가측 함수이며, ML의 모든 데이터와 파라미터를 수학적으로 표현한다.
4. **CDF**는 모든 확률 변수에 대해 존재하며, **PMF**(이산)와 **PDF**(연속)는 각각 특수한 경우에 정의된다.
5. PDF 값은 확률이 아니라 **확률 밀도**이므로 1을 초과할 수 있으며, 확률은 반드시 구간의 적분으로 구한다.
