# 집중 부등식

## 왜 집중 부등식을 배워야 하는가

pt-05에서 대수의 법칙과 CLT가 "결국 수렴한다"와 "정규분포로 수렴한다"를 말해 주었습니다. 하지만 ML에서는 **유한한 데이터**로 작업하므로, "얼마나 빨리" 그리고 "얼마나 확실하게" 수렴하는지 정량적으로 알아야 합니다. 이것이 **집중 부등식(concentration inequalities)**의 역할입니다.

집중 부등식은 일반화 이론, PAC 학습, 모델 선택의 수학적 기초를 제공합니다. 이번 강의에서는 Markov에서 시작해 Chebyshev, Chernoff, Hoeffding까지 점점 날카로운 부등식을 도출합니다.

---

## 1. Markov 부등식

가장 약하지만 가장 일반적인 부등식입니다. $X \geq 0$이면:

$$
P(X \geq a) \leq \frac{\mathbb{E}[X]}{a}, \quad a > 0
$$

**증명:**

$$
\mathbb{E}[X] = \int_0^{\infty} x\,f(x)\,dx \geq \int_a^{\infty} x\,f(x)\,dx \geq a \int_a^{\infty} f(x)\,dx = a\,P(X \geq a)
$$

> **핵심 직관**: Markov 부등식은 "기댓값만 알면 꼬리 확률의 상한을 잡을 수 있다"는 것입니다. 매우 약하지만, 다른 부등식을 유도하는 출발점이 됩니다.

---

## 2. Chebyshev 부등식

$X$의 평균이 $\mu$이고 분산이 $\sigma^2$이면:

$$
P(|X - \mu| \geq k\sigma) \leq \frac{1}{k^2}
$$

동치 표현:

$$
P(|X - \mu| \geq t) \leq \frac{\sigma^2}{t^2}
$$

**유도**: $(X - \mu)^2$에 Markov 부등식을 적용하면 됩니다.

| $k$ (표준편차 수) | Chebyshev 상한 | 정규분포 실제 확률 |
|:-:|:-:|:-:|
| 1 | 100% | 31.7% |
| 2 | 25% | 4.6% |
| 3 | 11.1% | 0.3% |

Chebyshev는 분포를 특정하지 않기에 보수적이지만, **모든 분포**에 적용 가능합니다.

---

## 3. Chernoff Bound

**지수 모멘트**를 활용하여 훨씬 날카로운 꼬리 확률을 얻습니다.

임의의 $t > 0$에 대해:

$$
P(X \geq a) = P(e^{tX} \geq e^{ta}) \leq \frac{\mathbb{E}[e^{tX}]}{e^{ta}}
$$

$t$를 최적화하면 가장 날카로운 bound를 얻습니다:

$$
P(X \geq a) \leq \inf_{t > 0} \frac{M_X(t)}{e^{ta}}
$$

여기서 $M_X(t) = \mathbb{E}[e^{tX}]$는 **적률 생성 함수(moment generating function, MGF)**입니다.

> **핵심 직관**: Chernoff bound가 강력한 이유는 꼬리 확률이 $1/t^2$이 아닌 **지수적으로** 감소하기 때문입니다. 이 지수적 감소가 ML의 일반화 bound에서 $O(e^{-n\epsilon^2})$ 형태로 나타납니다.

---

## 4. Hoeffding 부등식

$X_1, \ldots, X_n$이 독립이고 $a_i \leq X_i \leq b_i$이면 (유계):

$$
P\!\left(\bar{X}_n - \mu \geq t\right) \leq \exp\!\left(-\frac{2n^2 t^2}{\sum_{i=1}^{n}(b_i - a_i)^2}\right)
$$

특히 모든 $X_i \in [a, b]$이면:

$$
P\!\left(|\bar{X}_n - \mu| \geq t\right) \leq 2\exp\!\left(-\frac{2nt^2}{(b-a)^2}\right)
$$

이것을 **신뢰도** $\delta$로 뒤집으면: $1 - \delta$ 확률로

$$
|\bar{X}_n - \mu| \leq (b-a)\sqrt{\frac{\ln(2/\delta)}{2n}}
$$

> **핵심 직관**: Hoeffding은 "표본 크기 $n$이 커질수록 표본 평균이 참값에서 벗어날 확률이 **지수적으로** 줄어든다"고 말합니다. $n$에 대한 $O(1/\sqrt{n})$ 수렴률은 ML의 표본 복잡도(sample complexity) 분석의 핵심입니다.

---

## 5. 부등식 비교

```
부등식의 날카로움 비교 (꼬리 감소 속도)
────────────────────────────────────────
Markov:    P(X ≥ a) ≤ C/a          (다항 감소, 1차)
Chebyshev: P(|X-μ|≥t) ≤ C/t²      (다항 감소, 2차)
Chernoff:  P(X ≥ a) ≤ C·e^{-ct}   (지수 감소)
Hoeffding: P(|X̄-μ|≥t) ≤ 2e^{-cnt²} (지수 감소, 합에 특화)
────────────────────────────────────────
정보를 많이 사용할수록 → bound가 날카로워짐
(기댓값 < 분산 < MGF < 유계+독립)
```

| 부등식 | 필요 조건 | 꼬리 감소 | ML 응용 |
|--------|-----------|-----------|---------|
| Markov | $X \geq 0$ | $O(1/a)$ | 기본 도구 |
| Chebyshev | $\sigma^2 < \infty$ | $O(1/t^2)$ | WLLN 증명 |
| Chernoff | MGF 존재 | $O(e^{-ct})$ | 일반 꼬리 확률 |
| Hoeffding | 독립, 유계 | $O(e^{-cnt^2})$ | 일반화 bound |

---

## 6. ML에서의 응용: 일반화 bound

유한 가설 공간 $\mathcal{H}$에서, 각 가설 $h$의 경험적 위험 $\hat{R}(h)$와 참 위험 $R(h)$에 Hoeffding을 적용한 뒤 union bound를 씌우면:

$$
P\!\left(\sup_{h \in \mathcal{H}} |R(h) - \hat{R}(h)| \geq \epsilon\right) \leq 2|\mathcal{H}|\exp(-2n\epsilon^2)
$$

이를 $\delta$로 놓고 정리하면, 확률 $1-\delta$ 이상으로:

$$
R(h) \leq \hat{R}(h) + \sqrt{\frac{\ln|\mathcal{H}| + \ln(2/\delta)}{2n}}
$$

이것이 **PAC(Probably Approximately Correct) learning** bound의 기본 형태입니다.

---

## 7. Python으로 확인하기

```python
import numpy as np

np.random.seed(42)

# --- Hoeffding 부등식 검증 ---
n = 100         # 표본 크기
a, b = 0, 1     # [a, b] 구간
mu = 0.5        # Uniform(0,1)의 평균
n_trials = 100000

violations = []
t_values = np.linspace(0.01, 0.3, 30)

for t in t_values:
    count = 0
    for _ in range(n_trials):
        sample_mean = np.random.uniform(a, b, n).mean()
        if abs(sample_mean - mu) >= t:
            count += 1
    empirical_prob = count / n_trials
    hoeffding_bound = 2 * np.exp(-2 * n * t**2 / (b - a)**2)
    violations.append(empirical_prob <= hoeffding_bound + 1e-10)
    if t in [0.05, 0.10, 0.15, 0.20]:
        print(f"t={t:.2f}: 실제 P={empirical_prob:.5f}, "
              f"Hoeffding={hoeffding_bound:.5f}, "
              f"Chebyshev={1/(12*n*t**2):.5f}")

print(f"\nHoeffding bound 만족 비율: {sum(violations)/len(violations)*100:.0f}%")

# --- 표본 크기 vs 오차 bound ---
delta = 0.05
for n in [100, 1000, 10000, 100000]:
    eps = np.sqrt(np.log(2 / delta) / (2 * n))
    print(f"n={n:>6d}: 95% 신뢰구간 반폭 = {eps:.5f}")
```

---

## 핵심 정리

1. **Markov 부등식**은 기댓값만으로 꼬리 확률을 제한하며, 모든 다른 부등식의 출발점이다.
2. **Chebyshev 부등식**은 분산 정보를 추가하여 $O(1/t^2)$ 꼬리 감소를 달성하고, WLLN을 증명하는 도구이다.
3. **Chernoff bound**는 지수 모멘트를 활용하여 꼬리 확률을 **지수적으로** 감소시킨다.
4. **Hoeffding 부등식**은 유계 독립 확률 변수의 합에 대해 $O(e^{-nt^2})$ bound를 주며, PAC learning의 기초이다.
5. 일반화 bound $R(h) \leq \hat{R}(h) + O(1/\sqrt{n})$은 Hoeffding + union bound로 유도되며, 표본 복잡도를 결정한다.
