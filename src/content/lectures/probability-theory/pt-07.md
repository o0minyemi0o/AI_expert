# 베이즈 추론

## 왜 베이즈 추론을 배워야 하는가

ML에서 모델 파라미터 $\theta$를 추정하는 방법은 크게 세 가지입니다: 최대 우도 추정(MLE), 최대 사후 확률 추정(MAP), 그리고 **완전 베이즈 추론(full Bayesian inference)**. MLE가 "가장 그럴듯한 하나의 값"을 찾는다면, 베이즈 추론은 "모든 가능한 값에 대한 불확실성"을 유지합니다. 과적합을 자연스럽게 방지하고, 불확실성을 정량화하며, 데이터가 적을 때도 사전 지식을 활용할 수 있습니다.

이번 강의에서는 사전 분포에서 사후 분포까지의 베이즈 프레임워크를 체계적으로 다룹니다.

---

## 1. 베이즈 프레임워크

데이터 $\mathcal{D} = \{x_1, \ldots, x_n\}$이 주어졌을 때, 파라미터 $\theta$에 대한 **사후 분포**:

$$
\underbrace{p(\theta | \mathcal{D})}_{\text{사후}} = \frac{\overbrace{p(\mathcal{D} | \theta)}^{\text{우도}} \cdot \overbrace{p(\theta)}^{\text{사전}}}{\underbrace{p(\mathcal{D})}_{\text{증거}}} = \frac{p(\mathcal{D} | \theta)\,p(\theta)}{\int p(\mathcal{D} | \theta')\,p(\theta')\,d\theta'}
$$

```
베이즈 추론의 흐름
──────────────────────────────────────────
사전 분포     데이터(우도)     사후 분포
  p(θ)    ×   p(D|θ)    ∝    p(θ|D)

[넓은 분포]  →  [데이터 관측]  →  [좁은 분포]
     │              │               │
  사전 지식    관측된 증거      업데이트된 믿음
──────────────────────────────────────────
```

> **핵심 직관**: 베이즈 추론은 "모른다"는 상태(사전 분포)에서 시작해, 데이터를 관측할 때마다 "조금 더 안다"(사후 분포)로 업데이트하는 학습 과정입니다. 데이터가 무한히 많아지면 사전 분포의 영향은 사라지고, MLE에 수렴합니다.

---

## 2. 우도 함수

i.i.d. 데이터에 대한 **우도 함수(likelihood function)**:

$$
\mathcal{L}(\theta; \mathcal{D}) = p(\mathcal{D} | \theta) = \prod_{i=1}^{n} p(x_i | \theta)
$$

**로그 우도(log-likelihood)**:

$$
\ell(\theta) = \log \mathcal{L}(\theta) = \sum_{i=1}^{n} \log p(x_i | \theta)
$$

로그를 취하는 이유: 곱이 합으로 바뀌어 계산이 편해지고, 수치적으로 안정적입니다.

---

## 3. 세 가지 추정 방법 비교

### 3.1 최대 우도 추정 (MLE)

$$
\hat{\theta}_{\text{MLE}} = \arg\max_\theta \prod_{i=1}^{n} p(x_i | \theta)
$$

사전 분포를 사용하지 않고, 데이터만으로 가장 그럴듯한 $\theta$를 찾습니다.

### 3.2 최대 사후 확률 추정 (MAP)

$$
\hat{\theta}_{\text{MAP}} = \arg\max_\theta \; p(\mathcal{D} | \theta)\,p(\theta)
$$

사전 분포를 **정규화 항**으로 활용합니다. 가우시안 사전 $p(\theta) \propto e^{-\lambda\|\theta\|^2}$를 쓰면, L2 정규화(co-08에서 다뤘던 Ridge 회귀)와 동일합니다.

### 3.3 완전 베이즈 (Full Bayesian)

$$
p(x_{\text{new}} | \mathcal{D}) = \int p(x_{\text{new}} | \theta)\,p(\theta | \mathcal{D})\,d\theta
$$

$\theta$를 적분으로 제거(주변화)하여, 모든 가능한 모델의 가중 평균 예측을 합니다.

| 방법 | 추정 대상 | 사전 분포 | 불확실성 | 과적합 |
|------|-----------|-----------|----------|--------|
| MLE | 점 추정 $\hat{\theta}$ | 사용 안 함 | 없음 | 취약 |
| MAP | 점 추정 $\hat{\theta}$ | 정규화로 사용 | 없음 | 부분 방지 |
| Full Bayesian | 분포 $p(\theta\|\mathcal{D})$ | 완전히 사용 | 정량화 | 자연 방지 |

> **핵심 직관**: MLE는 $p(\theta)$를 균등 분포로 놓은 MAP의 특수 경우입니다. MAP는 사후 분포의 최빈값(mode)만 취한 완전 베이즈의 근사입니다. 데이터가 많으면 세 방법 모두 같은 결과에 수렴합니다.

---

## 4. 예시: 동전 던지기

동전을 $n$번 던져 $k$번 앞면이 나왔을 때, 앞면 확률 $\theta$를 추정합니다.

**우도**: $p(\mathcal{D}|\theta) = \theta^k(1-\theta)^{n-k}$

**MLE**: $\hat{\theta}_{\text{MLE}} = k/n$

**MAP with Beta prior**: $p(\theta) = \text{Beta}(\alpha, \beta)$이면

$$
\hat{\theta}_{\text{MAP}} = \frac{k + \alpha - 1}{n + \alpha + \beta - 2}
$$

**Full Bayesian**: $p(\theta|\mathcal{D}) = \text{Beta}(\alpha + k, \beta + n - k)$ (pt-08에서 상세히 다룹니다)

$$
\mathbb{E}[\theta | \mathcal{D}] = \frac{\alpha + k}{\alpha + \beta + n}
$$

> **핵심 직관**: 10번 던져 10번 앞면이 나왔다고 $\theta = 1$이라고 할까요? MLE는 그렇지만, 베이즈 추정은 사전 분포 덕분에 더 합리적인 값을 줍니다. 이것이 **라플라스 스무딩(Laplace smoothing)**의 원리이기도 합니다.

---

## 5. 순차적 베이즈 업데이트

베이즈 추론의 아름다운 성질: **이전의 사후 분포가 다음의 사전 분포**가 됩니다.

데이터가 $D_1, D_2, \ldots$로 순차적으로 도착하면:

$$
p(\theta | D_1, D_2) \propto p(D_2 | \theta) \cdot \underbrace{p(\theta | D_1)}_{\text{이전 사후 = 현재 사전}}
$$

한 번에 모든 데이터를 본 것과 순차적으로 업데이트한 결과가 동일합니다.

---

## 6. Python으로 확인하기

```python
import numpy as np
from scipy import stats

# --- 동전 던지기 베이즈 추론 ---
# 사전: Beta(2, 2) (약간 대칭적 사전)
alpha_prior, beta_prior = 2, 2

# 데이터: 10번 던져 7번 앞면
n, k = 10, 7

# MLE
theta_mle = k / n
print(f"MLE:  θ = {theta_mle:.4f}")

# MAP
theta_map = (k + alpha_prior - 1) / (n + alpha_prior + beta_prior - 2)
print(f"MAP:  θ = {theta_map:.4f}")

# Full Bayesian 사후 분포
alpha_post = alpha_prior + k
beta_post = beta_prior + (n - k)
posterior = stats.beta(alpha_post, beta_post)
print(f"사후 평균: θ = {posterior.mean():.4f}")
print(f"사후 95% CI: [{posterior.ppf(0.025):.4f}, {posterior.ppf(0.975):.4f}]")

# --- 순차적 업데이트 검증 ---
# 한번에 업데이트
data_all = [1]*7 + [0]*3
alpha_all = alpha_prior + sum(data_all)
beta_all = beta_prior + len(data_all) - sum(data_all)

# 순차적 업데이트
a, b = alpha_prior, beta_prior
for x in data_all:
    a += x
    b += (1 - x)

print(f"\n일괄 업데이트:  Beta({alpha_all}, {beta_all})")
print(f"순차적 업데이트: Beta({a}, {b})")
print(f"동일? {alpha_all == a and beta_all == b}")

# --- 데이터가 많아지면 MLE에 수렴 ---
true_theta = 0.6
for n in [10, 100, 1000, 10000]:
    k = np.random.binomial(n, true_theta)
    mle = k / n
    bayes_mean = (alpha_prior + k) / (alpha_prior + beta_prior + n)
    print(f"n={n:5d}: MLE={mle:.4f}, Bayes={bayes_mean:.4f}, 차이={abs(mle-bayes_mean):.5f}")
```

---

## 핵심 정리

1. **베이즈 정리** $p(\theta|\mathcal{D}) \propto p(\mathcal{D}|\theta)\,p(\theta)$는 사전 지식과 데이터를 결합하는 최적의 프레임워크이다.
2. **MLE**는 사전 분포를 균등으로 놓은 MAP의 특수 경우이며, MAP는 사후 분포의 최빈값만 취한 베이즈의 근사이다.
3. **완전 베이즈 추론**은 $\theta$를 적분으로 제거하여 불확실성을 정량화하고 과적합을 자연스럽게 방지한다.
4. **순차적 베이즈 업데이트**는 이전 사후를 현재 사전으로 사용하며, 온라인 학습과 자연스럽게 연결된다.
5. 데이터가 충분히 많아지면 사전 분포의 영향은 사라지고, MLE·MAP·베이즈 추정이 모두 같은 값으로 **수렴**한다.
