# 충분 통계량

## 왜 충분 통계량을 배워야 하는가

$n$개의 데이터 $x_1, \ldots, x_n$으로 파라미터 $\theta$를 추정할 때, 반드시 모든 원시 데이터를 보관해야 할까요? **충분 통계량(sufficient statistic)**은 "파라미터 추정에 필요한 정보를 **손실 없이** 요약하는 함수"입니다. pt-08에서 지수족의 $\mathbf{T}(x)$가 충분 통계량임을 봤는데, 이번 강의에서는 충분성의 엄밀한 정의, 인수분해 정리, 그리고 추정량 개선의 이론인 Rao-Blackwell 정리까지 다룹니다.

ML에서 데이터 압축, 피처 추출, 온라인 학습 알고리즘의 수학적 기초가 바로 충분 통계량입니다.

---

## 1. 충분성의 정의

통계량 $T(\mathbf{X})$가 파라미터 $\theta$에 대해 **충분(sufficient)**하다는 것은:

$$
p(\mathbf{X} | T(\mathbf{X}) = t, \theta) = p(\mathbf{X} | T(\mathbf{X}) = t) \quad \forall \theta
$$

즉, $T$의 값이 주어지면, 원래 데이터 $\mathbf{X}$의 조건부 분포가 $\theta$에 의존하지 않습니다.

> **핵심 직관**: 충분 통계량은 "데이터에서 $\theta$에 관한 정보를 **모두** 짜낸 요약"입니다. $T$를 알면, 원래 데이터의 나머지 부분은 $\theta$에 대해 아무런 추가 정보를 주지 않습니다.

---

## 2. Fisher-Neyman 인수분해 정리

충분성을 직접 검증하는 것은 어렵습니다. **인수분해 정리(factorization theorem)**가 실용적인 판별 도구를 제공합니다.

$T(\mathbf{X})$가 $\theta$에 대해 충분할 필요충분조건:

$$
p(\mathbf{x}|\theta) = g(T(\mathbf{x}), \theta) \cdot h(\mathbf{x})
$$

여기서:
- $g$: $T$와 $\theta$에만 의존하는 함수
- $h$: $\theta$에 의존하지 않는 함수 (데이터에만 의존)

### 예시들

| 분포 | 우도의 인수분해 | 충분 통계량 $T$ |
|------|----------------|----------------|
| $\text{Bern}(\theta)$ | $\theta^{\sum x_i}(1-\theta)^{n-\sum x_i} \cdot 1$ | $\sum_{i=1}^n x_i$ |
| $\mathcal{N}(\mu, \sigma^2_{\text{known}})$ | $\exp\!\left(-\frac{n(\bar{x}-\mu)^2}{2\sigma^2}\right) \cdot \exp\!\left(-\frac{\sum(x_i-\bar{x})^2}{2\sigma^2}\right)$ | $\bar{X} = \frac{1}{n}\sum x_i$ |
| $\mathcal{N}(\mu, \sigma^2)$ (둘 다 미지) | — | $\left(\sum x_i, \sum x_i^2\right)$ |
| $\text{Pois}(\lambda)$ | $\frac{\lambda^{\sum x_i} e^{-n\lambda}}{\prod x_i!}$ | $\sum_{i=1}^n x_i$ |

> **핵심 직관**: 인수분해 정리는 "우도 함수에서 $\theta$와 결합하여 나타나는 데이터의 함수"를 찾으면 그것이 충분 통계량이라고 말합니다. 지수족에서 이 역할을 하는 것이 바로 $\mathbf{T}(\mathbf{x})$입니다.

---

## 3. 지수족과 충분 통계량

pt-08에서 배운 지수족의 일반 형태를 다시 보면:

$$
p(\mathbf{x}|\boldsymbol{\eta}) = h(\mathbf{x}) \exp\!\left(\boldsymbol{\eta}^T \mathbf{T}(\mathbf{x}) - A(\boldsymbol{\eta})\right)
$$

인수분해: $g(\mathbf{T}, \boldsymbol{\eta}) = \exp(\boldsymbol{\eta}^T \mathbf{T} - A(\boldsymbol{\eta}))$, $h(\mathbf{x}) = h(\mathbf{x})$

따라서 $\mathbf{T}(\mathbf{x})$는 자동으로 충분 통계량입니다.

i.i.d. 데이터 $x_1, \ldots, x_n$에 대한 충분 통계량:

$$
\mathbf{T}_n = \sum_{i=1}^{n} \mathbf{T}(x_i)
$$

이것이 온라인 학습에서 "합계만 유지하면 된다"는 것의 수학적 근거입니다.

---

## 4. 최소 충분 통계량

충분 통계량은 유일하지 않습니다 — 극단적으로, 원시 데이터 자체가 항상 충분 통계량입니다. **최소 충분 통계량(minimal sufficient statistic)**은 가능한 한 가장 많이 데이터를 압축한 충분 통계량입니다.

$T$가 최소 충분 ⟺ 모든 충분 통계량 $T'$에 대해 $T = f(T')$인 함수 $f$가 존재.

```
데이터 압축 정도
─────────────────────────────────────────────────
원시 데이터     충분 통계량     최소 충분 통계량
(x₁,...,xₙ)  ⊇  T'(x)    ⊇     T(x)
  정보 보존         정보 보존        최대 압축
─────────────────────────────────────────────────
```

**판별법**: $T$가 최소 충분 ⟺ $\frac{p(\mathbf{x}|\theta)}{p(\mathbf{y}|\theta)}$가 $\theta$에 무관 ⟺ $T(\mathbf{x}) = T(\mathbf{y})$

---

## 5. Rao-Blackwell 정리

충분 통계량은 단순히 데이터 압축뿐 아니라, **추정량을 개선**하는 데도 사용됩니다.

$\tilde{\theta}$가 $\theta$의 비편향 추정량이고, $T$가 충분 통계량이면:

$$
\hat{\theta} = \mathbb{E}[\tilde{\theta} | T]
$$

이 "라오-블랙웰화된" 추정량은:

1. 여전히 **비편향**이다: $\mathbb{E}[\hat{\theta}] = \mathbb{E}[\tilde{\theta}] = \theta$
2. **분산이 줄어든다**: $\text{Var}(\hat{\theta}) \leq \text{Var}(\tilde{\theta})$
3. 등호는 $\tilde{\theta}$가 이미 $T$의 함수일 때만 성립

> **핵심 직관**: Rao-Blackwell 정리는 "좋은 추정량은 충분 통계량의 함수여야 한다"고 말합니다. 불필요한 정보(노이즈)를 조건부 기댓값으로 평균내면 분산이 줄어듭니다. pt-03에서 배운 전체 분산의 법칙이 여기서 핵심 도구로 쓰입니다.

**증명 스케치** (전체 분산의 법칙):

$$
\text{Var}(\tilde{\theta}) = \mathbb{E}[\text{Var}(\tilde{\theta}|T)] + \text{Var}(\mathbb{E}[\tilde{\theta}|T]) = \underbrace{\mathbb{E}[\text{Var}(\tilde{\theta}|T)]}_{\geq 0} + \text{Var}(\hat{\theta})
$$

따라서 $\text{Var}(\hat{\theta}) \leq \text{Var}(\tilde{\theta})$.

---

## 6. ML에서의 응용

| 개념 | ML 응용 |
|------|---------|
| 충분 통계량 | 온라인 학습에서 데이터 요약 유지 (예: 실행 평균/분산) |
| 최소 충분 통계량 | 피처 추출 — 예측에 필요한 최소 정보 |
| Rao-Blackwell | 몬테카를로 추정의 분산 감소 기법 |
| 지수족 + 충분 통계량 | 배치 정규화(BatchNorm)가 미니배치의 평균/분산만 사용하는 이론적 근거 |

---

## 7. Python으로 확인하기

```python
import numpy as np

np.random.seed(42)

# --- 충분 통계량의 정보 보존 검증 ---
# Bernoulli(θ)에서 충분 통계량 T = Σxᵢ
true_theta = 0.6
n = 20
n_experiments = 50000

# 방법 1: 원시 데이터로 MLE
mle_raw = []
# 방법 2: 충분 통계량만으로 MLE
mle_suff = []

for _ in range(n_experiments):
    data = np.random.binomial(1, true_theta, size=n)
    mle_raw.append(data.mean())
    mle_suff.append(data.sum() / n)  # T = Σxᵢ만 사용

print("원시 데이터 MLE와 충분 통계량 MLE 동일?",
      np.allclose(mle_raw, mle_suff))  # True

# --- Rao-Blackwell 정리 검증 ---
# θ = P(X=1)을 추정, 충분 통계량 T = Σxᵢ
# 나쁜 추정량: θ̃ = X₁ (첫 번째 관측만 사용)
# Rao-Blackwell: θ̂ = E[X₁|T] = T/n

theta_tilde = []  # X₁
theta_hat = []    # T/n (Rao-Blackwell)

for _ in range(n_experiments):
    data = np.random.binomial(1, true_theta, size=n)
    theta_tilde.append(data[0])
    theta_hat.append(data.sum() / n)

print(f"\n추정량 비교:")
print(f"θ̃ = X₁:   편향={np.mean(theta_tilde)-true_theta:.4f}, "
      f"분산={np.var(theta_tilde):.4f}")
print(f"θ̂ = T/n:  편향={np.mean(theta_hat)-true_theta:.4f}, "
      f"분산={np.var(theta_hat):.4f}")
print(f"분산 감소율: {(1 - np.var(theta_hat)/np.var(theta_tilde))*100:.1f}%")

# --- 온라인 충분 통계량 업데이트 ---
# Gaussian의 충분 통계량: (Σxᵢ, Σxᵢ², n)
running_sum = 0.0
running_sum_sq = 0.0
stream = np.random.normal(5.0, 2.0, size=1000)

for i, x in enumerate(stream, 1):
    running_sum += x
    running_sum_sq += x**2
    if i in [10, 100, 1000]:
        mu_hat = running_sum / i
        var_hat = running_sum_sq / i - mu_hat**2
        print(f"n={i:4d}: μ̂={mu_hat:.3f}, σ̂²={var_hat:.3f}")
```

---

## 핵심 정리

1. **충분 통계량** $T(\mathbf{X})$는 데이터에서 파라미터 $\theta$에 관한 정보를 손실 없이 요약하며, $p(\mathbf{X}|T,\theta) = p(\mathbf{X}|T)$를 만족한다.
2. **Fisher-Neyman 인수분해 정리**: $p(\mathbf{x}|\theta) = g(T(\mathbf{x}),\theta)\cdot h(\mathbf{x})$이면 $T$는 충분 통계량이다.
3. **지수족**에서 $\mathbf{T}(\mathbf{x})$는 자동으로 충분 통계량이며, i.i.d. 데이터의 충분 통계량은 $\sum \mathbf{T}(x_i)$이다.
4. **Rao-Blackwell 정리**: 비편향 추정량을 충분 통계량에 대해 조건부 기댓값을 취하면, 편향을 유지하면서 분산이 감소한다.
5. 충분 통계량은 온라인 학습에서 **실행 합계만 유지**하면 되는 이론적 근거이며, 데이터 압축과 효율적 추정의 수학적 기초이다.
