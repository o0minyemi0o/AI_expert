# 주요 확률 분포

## 왜 주요 확률 분포를 배워야 하는가

확률 분포는 ML 모델의 **빌딩 블록**입니다. 분류기의 출력은 Categorical 분포이고, 회귀의 잔차는 Gaussian을 가정하며, 베이즈 추론의 사전 분포로 Beta나 Gamma를 선택합니다. 각 분포의 성질과 파라미터를 정확히 알아야, 모델을 설계하고 결과를 해석할 수 있습니다.

이번 강의에서는 ML에서 빈번히 등장하는 이산 분포 4가지와 연속 분포 5가지를 체계적으로 정리합니다.

---

## 1. 이산 분포

### 1.1 Bernoulli 분포

하나의 이진 시행에서의 성공/실패:

$$
X \sim \text{Bern}(p), \quad p(x) = p^x (1-p)^{1-x}, \quad x \in \{0, 1\}
$$

$$
\mathbb{E}[X] = p, \quad \text{Var}(X) = p(1-p)
$$

**ML 활용**: 이진 분류의 레이블, 로지스틱 회귀의 출력.

### 1.2 Binomial 분포

$n$번의 독립 Bernoulli 시행에서 성공 횟수:

$$
X \sim \text{Bin}(n, p), \quad p(k) = \binom{n}{k} p^k (1-p)^{n-k}
$$

$$
\mathbb{E}[X] = np, \quad \text{Var}(X) = np(1-p)
$$

### 1.3 Poisson 분포

단위 시간/공간에서 발생하는 사건의 횟수:

$$
X \sim \text{Pois}(\lambda), \quad p(k) = \frac{\lambda^k e^{-\lambda}}{k!}, \quad k = 0, 1, 2, \ldots
$$

$$
\mathbb{E}[X] = \lambda, \quad \text{Var}(X) = \lambda
$$

**ML 활용**: 카운트 데이터 모델링, Poisson 회귀.

### 1.4 Categorical 분포

$K$개 클래스 중 하나를 선택:

$$
X \sim \text{Cat}(\boldsymbol{\pi}), \quad p(x=k) = \pi_k, \quad \sum_{k=1}^{K} \pi_k = 1
$$

**ML 활용**: 다중 클래스 분류에서 소프트맥스의 출력이 $\boldsymbol{\pi}$입니다.

> **핵심 직관**: 소프트맥스 함수가 Categorical 분포의 파라미터를 생성한다고 이해하면, 분류 모델의 출력이 "확률"인 이유가 명확해집니다. 교차 엔트로피 손실(pt-10에서 상세히 다룸)은 이 Categorical 분포의 음의 로그 우도입니다.

---

## 2. 연속 분포

### 2.1 Gaussian (정규) 분포

$$
X \sim \mathcal{N}(\mu, \sigma^2), \quad f(x) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\!\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)
$$

$$
\mathbb{E}[X] = \mu, \quad \text{Var}(X) = \sigma^2
$$

**다변량 확장:**

$$
\mathbf{X} \sim \mathcal{N}(\boldsymbol{\mu}, \boldsymbol{\Sigma}), \quad f(\mathbf{x}) = \frac{1}{(2\pi)^{d/2}|\boldsymbol{\Sigma}|^{1/2}} \exp\!\left(-\frac{1}{2}(\mathbf{x}-\boldsymbol{\mu})^T \boldsymbol{\Sigma}^{-1}(\mathbf{x}-\boldsymbol{\mu})\right)
$$

la-01에서 배운 고유값 분해로 $\boldsymbol{\Sigma}$를 분해하면, 분포의 주축 방향과 퍼짐을 직관적으로 이해할 수 있습니다.

> **핵심 직관**: 가우시안이 ML에서 가장 많이 쓰이는 이유는 중심극한정리(pt-05), 최대 엔트로피 원리(pt-12), 그리고 계산의 편리함(켤레 사전분포, pt-08) 때문입니다.

### 2.2 Uniform 분포

$$
X \sim \text{Unif}(a, b), \quad f(x) = \frac{1}{b-a}, \quad a \leq x \leq b
$$

$$
\mathbb{E}[X] = \frac{a+b}{2}, \quad \text{Var}(X) = \frac{(b-a)^2}{12}
$$

### 2.3 Exponential 분포

사건 간 대기 시간:

$$
X \sim \text{Exp}(\lambda), \quad f(x) = \lambda e^{-\lambda x}, \quad x \geq 0
$$

$$
\mathbb{E}[X] = \frac{1}{\lambda}, \quad \text{Var}(X) = \frac{1}{\lambda^2}
$$

무기억(memoryless) 성질: $P(X > s + t | X > s) = P(X > t)$.

### 2.4 Gamma 분포

$$
X \sim \text{Gamma}(\alpha, \beta), \quad f(x) = \frac{\beta^\alpha}{\Gamma(\alpha)} x^{\alpha-1} e^{-\beta x}, \quad x > 0
$$

$$
\mathbb{E}[X] = \frac{\alpha}{\beta}, \quad \text{Var}(X) = \frac{\alpha}{\beta^2}
$$

**ML 활용**: 정밀도(precision)의 사전 분포, 대기 시간 모델링.

### 2.5 Beta 분포

$$
X \sim \text{Beta}(\alpha, \beta), \quad f(x) = \frac{x^{\alpha-1}(1-x)^{\beta-1}}{B(\alpha, \beta)}, \quad 0 \leq x \leq 1
$$

$$
\mathbb{E}[X] = \frac{\alpha}{\alpha+\beta}, \quad \text{Var}(X) = \frac{\alpha\beta}{(\alpha+\beta)^2(\alpha+\beta+1)}
$$

**ML 활용**: Bernoulli 파라미터 $p$의 사전 분포 (pt-08의 켤레 사전분포).

> **핵심 직관**: Beta 분포는 "확률의 확률"입니다. $\alpha$는 "성공을 관측한 횟수 + 1", $\beta$는 "실패를 관측한 횟수 + 1"로 해석하면, 베이즈 업데이트가 단순히 $\alpha, \beta$에 관측 횟수를 더하는 것이 됩니다.

---

## 3. 분포 요약 표

| 분포 | 지지 | $\mathbb{E}[X]$ | $\text{Var}(X)$ | ML 활용 |
|------|------|------------------|------------------|---------|
| Bernoulli($p$) | $\{0,1\}$ | $p$ | $p(1-p)$ | 이진 분류 |
| Binomial($n,p$) | $\{0,\ldots,n\}$ | $np$ | $np(1-p)$ | 성공 횟수 |
| Poisson($\lambda$) | $\{0,1,\ldots\}$ | $\lambda$ | $\lambda$ | 카운트 데이터 |
| Categorical($\boldsymbol{\pi}$) | $\{1,\ldots,K\}$ | — | — | 다중 분류 |
| Gaussian($\mu,\sigma^2$) | $\mathbb{R}$ | $\mu$ | $\sigma^2$ | 회귀, 생성 모델 |
| Uniform($a,b$) | $[a,b]$ | $\frac{a+b}{2}$ | $\frac{(b-a)^2}{12}$ | 초기화, 사전분포 |
| Exponential($\lambda$) | $[0,\infty)$ | $1/\lambda$ | $1/\lambda^2$ | 대기 시간 |
| Gamma($\alpha,\beta$) | $(0,\infty)$ | $\alpha/\beta$ | $\alpha/\beta^2$ | 정밀도 사전분포 |
| Beta($\alpha,\beta$) | $[0,1]$ | $\frac{\alpha}{\alpha+\beta}$ | — | 확률의 사전분포 |

---

## 4. Python으로 확인하기

```python
import numpy as np
from scipy import stats

# --- 각 분포의 기댓값/분산 검증 ---
distributions = {
    'Bernoulli(0.7)':   stats.bernoulli(0.7),
    'Binomial(10,0.3)': stats.binom(10, 0.3),
    'Poisson(5)':       stats.poisson(5),
    'Gaussian(3,4)':    stats.norm(3, 2),       # σ=2, σ²=4
    'Exp(2)':           stats.expon(scale=0.5),  # scale=1/λ
    'Gamma(3,2)':       stats.gamma(3, scale=0.5),
    'Beta(2,5)':        stats.beta(2, 5),
}

for name, dist in distributions.items():
    samples = dist.rvs(size=100000, random_state=42)
    print(f"{name:20s}  이론 E={dist.mean():.3f} 표본 E={samples.mean():.3f}  "
          f"이론 V={dist.var():.3f} 표본 V={samples.var():.3f}")

# --- 다변량 가우시안 ---
mu = np.array([1.0, 2.0])
Sigma = np.array([[2.0, 0.8],
                  [0.8, 1.0]])
data = np.random.multivariate_normal(mu, Sigma, size=10000)
print(f"\n다변량 정규: 표본 평균 = {data.mean(axis=0).round(3)}")
print(f"표본 공분산 =\n{np.cov(data.T).round(3)}")
```

---

## 핵심 정리

1. **Bernoulli/Categorical**은 분류 모델의 출력 분포이며, 교차 엔트로피 손실은 이 분포의 음의 로그 우도이다.
2. **Gaussian**은 중심극한정리, 최대 엔트로피, 계산 편의성 덕분에 ML에서 가장 널리 사용되는 분포이다.
3. **Beta/Gamma**는 베이즈 추론에서 Bernoulli/Gaussian의 파라미터에 대한 사전 분포로 활용된다.
4. 다변량 정규분포의 **공분산 행렬** $\boldsymbol{\Sigma}$는 분포의 기하학적 모양을 결정하며, 고유값 분해가 PCA로 연결된다.
5. 각 분포의 **지지(support)**, 기댓값, 분산을 외우는 것보다, **어떤 상황에서 어떤 분포를 선택하는지**가 ML 실전에서 중요하다.
