# 대수의 법칙과 중심극한정리

## 왜 대수의 법칙과 중심극한정리를 배워야 하는가

ML에서 우리는 유한한 데이터로 전체 모집단에 대한 결론을 내립니다. 이것이 수학적으로 정당화되는 이유가 바로 **대수의 법칙(Law of Large Numbers)**과 **중심극한정리(Central Limit Theorem)**입니다. 대수의 법칙은 "충분히 많은 데이터를 모으면 표본 평균이 참값에 수렴한다"고 보장하고, 중심극한정리는 "표본 평균의 분포가 정규분포에 가까워진다"고 알려줍니다.

이번 강의에서는 이 두 정리의 정확한 진술, 수렴의 종류, 그리고 ML에서의 의미를 다룹니다.

---

## 1. 수렴의 종류

확률 변수 열 $\{X_n\}$이 $X$로 수렴하는 방식은 여러 가지가 있습니다. 강한 순서대로:

### 1.1 거의 확실한 수렴 (Almost Sure Convergence)

$$
X_n \xrightarrow{\text{a.s.}} X \quad \Leftrightarrow \quad P\!\left(\lim_{n \to \infty} X_n = X\right) = 1
$$

### 1.2 확률 수렴 (Convergence in Probability)

$$
X_n \xrightarrow{p} X \quad \Leftrightarrow \quad \forall \epsilon > 0, \; P(|X_n - X| > \epsilon) \to 0
$$

### 1.3 분포 수렴 (Convergence in Distribution)

$$
X_n \xrightarrow{d} X \quad \Leftrightarrow \quad F_{X_n}(x) \to F_X(x) \quad \text{모든 연속점 } x
$$

```
수렴 모드들의 관계 (화살표 = 함의)
──────────────────────────────────
 거의 확실한 수렴 ──→ 확률 수렴 ──→ 분포 수렴
   (a.s.)              (p)           (d)
                       ↑
                  Lᵖ 수렴 (평균 수렴)
──────────────────────────────────
역방향은 일반적으로 성립하지 않음
(단, 상수로의 분포 수렴 = 확률 수렴)
```

> **핵심 직관**: 거의 확실한 수렴은 "개별 경로가 수렴한다", 확률 수렴은 "벗어날 확률이 줄어든다", 분포 수렴은 "CDF의 모양이 닮아간다"입니다. ML에서는 대부분 확률 수렴 또는 분포 수렴을 다룹니다.

---

## 2. 약 대수의 법칙 (WLLN)

$X_1, X_2, \ldots$가 i.i.d.이고 $\mathbb{E}[X_i] = \mu$, $\text{Var}(X_i) = \sigma^2 < \infty$이면:

$$
\bar{X}_n = \frac{1}{n}\sum_{i=1}^{n} X_i \xrightarrow{p} \mu
$$

**증명 스케치** (Chebyshev 부등식 활용):

$$
P(|\bar{X}_n - \mu| \geq \epsilon) \leq \frac{\text{Var}(\bar{X}_n)}{\epsilon^2} = \frac{\sigma^2}{n\epsilon^2} \to 0
$$

> **핵심 직관**: 경험적 위험(empirical risk) $\frac{1}{n}\sum \ell(f_\theta(x_i), y_i)$이 참 위험(true risk) $\mathbb{E}[\ell]$에 수렴한다는 것을 보장합니다. 이것이 **ERM(경험적 위험 최소화)**이 작동하는 근본적 이유입니다.

---

## 3. 강 대수의 법칙 (SLLN)

$X_1, X_2, \ldots$가 i.i.d.이고 $\mathbb{E}[|X_i|] < \infty$이면:

$$
\bar{X}_n \xrightarrow{\text{a.s.}} \mu
$$

SLLN은 유한 분산 조건 없이도 성립하므로, WLLN보다 강한 결과입니다.

| | WLLN | SLLN |
|---|------|------|
| 수렴 방식 | 확률 수렴 | 거의 확실한 수렴 |
| 조건 | $\sigma^2 < \infty$ | $\mathbb{E}[\|X\|] < \infty$ |
| 강도 | 약함 | 강함 |
| 증명 난이도 | 쉬움 (Chebyshev) | 어려움 |

---

## 4. 중심극한정리 (CLT)

$X_1, X_2, \ldots$가 i.i.d.이고 $\mathbb{E}[X_i] = \mu$, $\text{Var}(X_i) = \sigma^2 \in (0, \infty)$이면:

$$
\frac{\bar{X}_n - \mu}{\sigma / \sqrt{n}} \xrightarrow{d} \mathcal{N}(0, 1)
$$

동치 표현:

$$
\sqrt{n}(\bar{X}_n - \mu) \xrightarrow{d} \mathcal{N}(0, \sigma^2)
$$

또는 근사적으로:

$$
\bar{X}_n \approx \mathcal{N}\!\left(\mu, \frac{\sigma^2}{n}\right) \quad \text{for large } n
$$

> **핵심 직관**: CLT는 "원래 분포가 무엇이든, 표본 평균은 정규분포를 따른다"고 말합니다. 이것이 정규분포가 자연 과학과 ML에서 편재하는 근본적 이유입니다. 미니배치 경사하강법에서 그래디언트 평균이 안정적인 것도 CLT 덕분입니다.

---

## 5. 다변량 CLT

$\mathbf{X}_i \in \mathbb{R}^d$가 i.i.d.이고 $\mathbb{E}[\mathbf{X}_i] = \boldsymbol{\mu}$, $\text{Cov}(\mathbf{X}_i) = \boldsymbol{\Sigma}$이면:

$$
\sqrt{n}(\bar{\mathbf{X}}_n - \boldsymbol{\mu}) \xrightarrow{d} \mathcal{N}(\mathbf{0}, \boldsymbol{\Sigma})
$$

이 결과는 MLE의 점근적 정규성과 직접 연결됩니다: 최대 우도 추정량 $\hat{\theta}_{\text{MLE}}$는 적절한 정칙 조건 하에서

$$
\sqrt{n}(\hat{\theta}_{\text{MLE}} - \theta_0) \xrightarrow{d} \mathcal{N}\!\left(\mathbf{0}, \mathcal{I}(\theta_0)^{-1}\right)
$$

여기서 $\mathcal{I}(\theta_0)$는 Fisher 정보 행렬(pt-12에서 다룹니다)입니다.

---

## 6. Python으로 확인하기

```python
import numpy as np
from scipy import stats

np.random.seed(42)

# --- LLN 시각적 확인 ---
# 지수분포 (비대칭, 비정규)
lam = 2.0
true_mean = 1 / lam
N = 10000
samples = np.random.exponential(scale=1/lam, size=N)
running_avg = np.cumsum(samples) / np.arange(1, N + 1)

print(f"n=100:   표본 평균 = {running_avg[99]:.4f}")
print(f"n=1000:  표본 평균 = {running_avg[999]:.4f}")
print(f"n=10000: 표본 평균 = {running_avg[9999]:.4f}")
print(f"참 평균 = {true_mean:.4f}")

# --- CLT 확인: 표본 평균의 분포 ---
n_samples = 1000   # 표본 크기
n_repeats = 10000  # 반복 횟수

sample_means = np.array([
    np.random.exponential(scale=1/lam, size=n_samples).mean()
    for _ in range(n_repeats)
])

# 표준화
z = (sample_means - true_mean) / (1/lam / np.sqrt(n_samples))

# 정규성 검정 (Shapiro-Wilk)
_, p_value = stats.shapiro(z[:5000])  # 5000개 제한
print(f"\nCLT 검증 - Shapiro-Wilk p-value: {p_value:.4f}")
print(f"표준화 평균: {z.mean():.4f} (≈0), 표준편차: {z.std():.4f} (≈1)")

# --- 다양한 n에서의 CLT 수렴 속도 ---
for n in [5, 30, 100, 1000]:
    means = [np.random.exponential(1/lam, n).mean() for _ in range(5000)]
    _, p_val = stats.shapiro(means[:5000])
    print(f"n={n:4d}: Shapiro p={p_val:.4f}, skewness={stats.skew(means):.4f}")
```

---

## 7. ML에서의 응용

| 정리 | ML 응용 |
|------|---------|
| WLLN | 경험적 위험 → 참 위험 수렴 (ERM의 정당화) |
| SLLN | 몬테카를로 적분의 수렴 보장 |
| CLT | 미니배치 그래디언트의 근사 정규성 |
| CLT | MLE의 점근적 정규성과 신뢰 구간 구성 |
| CLT | 부트스트랩(bootstrap)의 이론적 기초 |

---

## 핵심 정리

1. **대수의 법칙**은 표본 평균 $\bar{X}_n$이 모평균 $\mu$로 수렴함을 보장하며, 경험적 위험 최소화(ERM)의 이론적 기반이다.
2. **중심극한정리**는 원래 분포와 무관하게 표본 평균이 $\mathcal{N}(\mu, \sigma^2/n)$에 수렴하며, 정규분포의 편재성을 설명한다.
3. 수렴의 강도는 거의 확실한 수렴 > 확률 수렴 > 분포 수렴 순이며, 각각 다른 문맥에서 사용된다.
4. MLE의 **점근적 정규성**은 CLT의 직접적 귀결이며, Fisher 정보 행렬이 공분산을 결정한다.
5. 미니배치 SGD의 그래디언트가 안정적인 이유, 몬테카를로 방법이 수렴하는 이유 모두 LLN과 CLT에 기반한다.
