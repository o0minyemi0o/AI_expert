# 엔트로피와 정보량

## 왜 엔트로피를 배워야 하는가

분류 모델을 학습시킬 때 사용하는 **교차 엔트로피 손실(cross-entropy loss)**의 수학적 기원이 바로 Shannon의 정보 이론입니다. 엔트로피는 확률 분포의 "불확실성" 또는 "정보량"을 정량화하며, ML의 손실 함수 설계, 결정 트리의 분할 기준, 생성 모델의 목적 함수 등 곳곳에 등장합니다.

이번 강의에서는 엔트로피의 정의와 직관, 교차 엔트로피가 분류 손실이 되는 이유, 그리고 조건부 엔트로피와 체인 룰까지 다룹니다.

---

## 1. 자기 정보량 (Self-Information)

사건 $x$가 확률 $p(x)$로 발생할 때, 그 사건의 **정보량(information content)**:

$$
I(x) = -\log_2 p(x) \quad \text{(단위: 비트)}
$$

또는 자연로그를 사용하면 단위가 **냇(nats)**이 됩니다: $I(x) = -\ln p(x)$

| $p(x)$ | $I(x)$ (bits) | 직관 |
|:-------:|:--------------:|------|
| 1 | 0 | 확실한 사건 → 정보 없음 |
| 0.5 | 1 | 동전 던지기 → 1비트 |
| 0.25 | 2 | 4지선다 → 2비트 |
| 0.01 | 6.64 | 희귀 사건 → 높은 정보량 |

> **핵심 직관**: 정보량은 "놀라움의 정도"입니다. 확실한 사건은 놀라움이 없고(정보 = 0), 드문 사건은 많은 정보를 전달합니다. 이것은 통신에서 "드문 메시지일수록 더 많은 비트가 필요하다"는 직관과 일치합니다.

---

## 2. Shannon 엔트로피

확률 분포 $p$의 **엔트로피(entropy)**는 자기 정보량의 기댓값입니다:

**이산:**

$$
H(X) = -\sum_{x} p(x) \log p(x) = \mathbb{E}_{X \sim p}[-\log p(X)]
$$

**연속 (미분 엔트로피):**

$$
h(X) = -\int f(x) \log f(x)\,dx
$$

### 엔트로피의 성질

1. $H(X) \geq 0$ (이산), 미분 엔트로피는 음수 가능
2. $H(X) = 0$ ⟺ $X$가 결정적 (하나의 값만 가능)
3. 이산에서 최대 엔트로피: $H(X) \leq \log |\mathcal{X}|$ (균등 분포일 때 등호)
4. 연속에서 평균·분산이 고정일 때 최대 엔트로피: Gaussian (pt-12에서 증명)

> **핵심 직관**: 엔트로피는 "분포의 불확실성"을 측정합니다. 결정 트리가 엔트로피가 가장 크게 감소하는 특성으로 분할하는 이유는, 불확실성을 최대한 줄이는 것이 최적의 질문이기 때문입니다.

---

## 3. 교차 엔트로피 (Cross-Entropy)

분포 $p$ 하에서 분포 $q$의 코드를 사용할 때 필요한 평균 비트 수:

$$
H(p, q) = -\sum_{x} p(x) \log q(x) = \mathbb{E}_{X \sim p}[-\log q(X)]
$$

핵심 관계:

$$
H(p, q) = H(p) + D_{\text{KL}}(p \| q) \geq H(p)
$$

여기서 $D_{\text{KL}}$은 KL 발산(pt-11에서 다룸)입니다. 등호 조건: $p = q$.

### 분류 손실로서의 교차 엔트로피

데이터의 참 분포 $p$와 모델의 예측 분포 $q_\theta$에 대해:

$$
\text{CE Loss} = H(p, q_\theta) = -\sum_{k=1}^{K} p(y=k) \log q_\theta(y=k|\mathbf{x})
$$

원-핫 인코딩 ($p(y=c) = 1$, 나머지 0)이면:

$$
\text{CE Loss} = -\log q_\theta(y=c|\mathbf{x})
$$

이것이 바로 **음의 로그 우도(negative log-likelihood)**입니다.

```
교차 엔트로피 손실의 구조
──────────────────────────────────────────
H(p, q) = H(p)      +   D_KL(p ∥ q)
           ─────         ──────────────
          데이터 고유     모델이 줄여야 할
          불확실성 (상수)    부분 (≥ 0)

→ q를 학습시켜 H(p,q)를 최소화하면,
  D_KL(p∥q)가 최소화되어 q → p가 됨
──────────────────────────────────────────
```

> **핵심 직관**: 교차 엔트로피를 최소화하는 것은 KL 발산을 최소화하는 것과 동일합니다(참 분포 $p$의 엔트로피 $H(p)$는 상수이므로). 따라서 교차 엔트로피 손실은 "모델의 예측 분포를 참 분포에 가깝게 만드는" 최적의 손실 함수입니다.

---

## 4. 결합 엔트로피

두 확률 변수의 결합 분포에 대한 엔트로피:

$$
H(X, Y) = -\sum_{x, y} p(x, y) \log p(x, y)
$$

부등식:

$$
\max(H(X), H(Y)) \leq H(X, Y) \leq H(X) + H(Y)
$$

등호($H(X,Y) = H(X) + H(Y)$)는 $X$와 $Y$가 독립일 때 성립합니다.

---

## 5. 조건부 엔트로피

$Y$가 주어졌을 때 $X$의 남은 불확실성:

$$
H(X|Y) = -\sum_{x,y} p(x,y) \log p(x|y) = \mathbb{E}_{Y}\!\left[H(X|Y=y)\right]
$$

성질:
- $H(X|Y) \leq H(X)$ (정보는 불확실성을 줄인다)
- $H(X|Y) = 0$ ⟺ $X$가 $Y$에 의해 결정

---

## 6. 엔트로피 체인 룰

$$
H(X, Y) = H(X) + H(Y|X) = H(Y) + H(X|Y)
$$

일반화:

$$
H(X_1, \ldots, X_n) = \sum_{i=1}^{n} H(X_i | X_1, \ldots, X_{i-1})
$$

```
엔트로피 벤 다이어그램
──────────────────────────────────
  ┌─────────────────────────────┐
  │         H(X, Y)             │
  │  ┌──────────┬──────────┐    │
  │  │  H(X|Y)  │ I(X;Y)  │    │
  │  │          │          │    │
  │  │          │  H(Y|X)  │    │
  │  └──────────┴──────────┘    │
  │                             │
  │  H(X) = H(X|Y) + I(X;Y)    │
  │  H(Y) = H(Y|X) + I(X;Y)    │
  └─────────────────────────────┘
  I(X;Y) = 상호 정보량 (pt-11에서 다룸)
──────────────────────────────────
```

---

## 7. Python으로 확인하기

```python
import numpy as np
from scipy import stats

# --- Shannon 엔트로피 ---
def entropy(p):
    """이산 분포의 엔트로피 (nats)"""
    p = p[p > 0]  # 0 log 0 = 0 처리
    return -np.sum(p * np.log(p))

# 균등 분포 vs 편향 분포
p_uniform = np.array([0.25, 0.25, 0.25, 0.25])
p_skewed = np.array([0.9, 0.05, 0.03, 0.02])
p_certain = np.array([1.0, 0.0, 0.0, 0.0])

print(f"균등 분포 엔트로피:   {entropy(p_uniform):.4f} nats ({entropy(p_uniform)/np.log(2):.4f} bits)")
print(f"편향 분포 엔트로피:   {entropy(p_skewed):.4f} nats")
print(f"확정적 분포 엔트로피: {entropy(p_certain):.4f} nats")
print(f"최대 엔트로피 (log 4): {np.log(4):.4f} nats")

# --- 교차 엔트로피와 분류 손실 ---
def cross_entropy(p, q):
    """교차 엔트로피 H(p, q)"""
    mask = p > 0
    return -np.sum(p[mask] * np.log(q[mask]))

# 참 분포 (원-핫), 모델 예측
p_true = np.array([0, 1, 0])  # 클래스 2가 정답
q_good = np.array([0.05, 0.90, 0.05])  # 좋은 모델
q_bad = np.array([0.30, 0.40, 0.30])   # 나쁜 모델

print(f"\n좋은 모델 CE 손실: {cross_entropy(p_true, q_good):.4f}")
print(f"나쁜 모델 CE 손실: {cross_entropy(p_true, q_bad):.4f}")
print(f"원-핫이면 CE = -log(q_c): {-np.log(0.90):.4f}, {-np.log(0.40):.4f}")

# --- 조건부 엔트로피와 체인 룰 ---
# 결합 분포
p_xy = np.array([[0.1, 0.2],
                 [0.3, 0.4]])
p_x = p_xy.sum(axis=1)
p_y = p_xy.sum(axis=0)

H_XY = entropy(p_xy.flatten())
H_X = entropy(p_x)
H_Y = entropy(p_y)
H_Y_given_X = H_XY - H_X   # 체인 룰

print(f"\nH(X,Y) = {H_XY:.4f}")
print(f"H(X) = {H_X:.4f}")
print(f"H(Y) = {H_Y:.4f}")
print(f"H(Y|X) = H(X,Y) - H(X) = {H_Y_given_X:.4f}")
print(f"H(X) + H(Y) = {H_X + H_Y:.4f} ≥ H(X,Y) = {H_XY:.4f}")
```

---

## 핵심 정리

1. **Shannon 엔트로피** $H(X) = -\sum p(x)\log p(x)$는 확률 분포의 불확실성을 정량화하며, 균등 분포일 때 최대이다.
2. **교차 엔트로피** $H(p,q) = -\sum p(x)\log q(x)$는 분류 손실의 수학적 기초이며, 최소화하면 KL 발산이 최소화된다.
3. **조건부 엔트로피** $H(X|Y) \leq H(X)$는 "조건부 정보는 불확실성을 줄인다"는 원리를 표현한다.
4. **체인 룰** $H(X,Y) = H(X) + H(Y|X)$는 결합 엔트로피를 조건부 엔트로피의 합으로 분해한다.
5. 결정 트리의 정보 이득, 분류 모델의 CE 손실, 생성 모델의 ELBO 등 ML의 핵심 알고리즘들은 모두 **엔트로피의 최소/최대화**에 기반한다.
