# 기댓값과 분산

## 왜 기댓값과 분산을 배워야 하는가

확률 분포를 완전히 기술하려면 무한한 정보가 필요하지만, 실전에서는 몇 가지 **요약 통계량**으로 분포의 핵심을 파악합니다. **기댓값(expectation)**은 분포의 중심을, **분산(variance)**은 퍼짐의 정도를, **공분산(covariance)**은 변수 간 관계를 알려줍니다.

ML에서 손실 함수의 기댓값을 최소화하는 것이 학습이고, 분산을 줄이는 것이 안정적 학습의 핵심이며, la-01에서 다뤘던 벡터 공간의 내적 구조가 공분산 행렬을 통해 확률로 확장됩니다.

---

## 1. 기댓값의 정의

확률 변수 $X$의 **기댓값(expectation)**은 가능한 값의 확률 가중 평균입니다.

**이산:**

$$
\mathbb{E}[X] = \sum_{x} x\,p(x)
$$

**연속:**

$$
\mathbb{E}[X] = \int_{-\infty}^{\infty} x\,f(x)\,dx
$$

함수 $g(X)$의 기댓값 (**LOTUS — Law of the Unconscious Statistician**):

$$
\mathbb{E}[g(X)] = \int_{-\infty}^{\infty} g(x)\,f(x)\,dx
$$

LOTUS 덕분에 $g(X)$의 분포를 구하지 않아도 기댓값을 계산할 수 있습니다.

> **핵심 직관**: ML의 손실 함수 $L(\theta)$는 사실 $\mathbb{E}_{(\mathbf{x},y)\sim p_{\text{data}}}[\ell(f_\theta(\mathbf{x}), y)]$입니다. 학습 데이터의 평균 손실은 이 기댓값의 **몬테카를로 근사**입니다.

---

## 2. 기댓값의 성질

| 성질 | 수식 | 비고 |
|------|------|------|
| 선형성 | $\mathbb{E}[aX + bY] = a\mathbb{E}[X] + b\mathbb{E}[Y]$ | **항상** 성립 (독립 불필요) |
| 상수 | $\mathbb{E}[c] = c$ | |
| 독립 시 곱 | $X \perp Y \Rightarrow \mathbb{E}[XY] = \mathbb{E}[X]\mathbb{E}[Y]$ | 독립일 때만 |
| 단조성 | $X \geq 0 \Rightarrow \mathbb{E}[X] \geq 0$ | |

기댓값의 **선형성**은 확률론에서 가장 강력한 도구입니다. 어떤 복잡한 확률 변수의 기댓값이라도, 더 단순한 확률 변수의 합으로 분해하면 쉽게 구할 수 있습니다.

---

## 3. 분산과 표준편차

**분산(variance)**은 확률 변수가 기댓값으로부터 얼마나 떨어져 있는지의 평균적 척도입니다:

$$
\text{Var}(X) = \mathbb{E}\!\left[(X - \mathbb{E}[X])^2\right] = \mathbb{E}[X^2] - (\mathbb{E}[X])^2
$$

분산의 성질:

$$
\begin{aligned}
\text{Var}(aX + b) &= a^2\,\text{Var}(X) \\
\text{Var}(X + Y) &= \text{Var}(X) + \text{Var}(Y) + 2\,\text{Cov}(X, Y) \\
&= \text{Var}(X) + \text{Var}(Y) \quad \text{(독립 시)}
\end{aligned}
$$

> **핵심 직관**: ML에서 **편향-분산 분해(bias-variance decomposition)**는 모델의 예측 오차를 $\text{Error} = \text{Bias}^2 + \text{Variance} + \text{Noise}$로 나눕니다. 분산이 크면 과적합, 편향이 크면 과소적합입니다.

---

## 4. 공분산과 상관계수

두 확률 변수의 선형 관계를 측정합니다.

**공분산:**

$$
\text{Cov}(X, Y) = \mathbb{E}\!\left[(X - \mathbb{E}[X])(Y - \mathbb{E}[Y])\right] = \mathbb{E}[XY] - \mathbb{E}[X]\mathbb{E}[Y]
$$

**상관계수 (Pearson):**

$$
\rho(X, Y) = \frac{\text{Cov}(X, Y)}{\sqrt{\text{Var}(X)\,\text{Var}(Y)}}, \quad -1 \leq \rho \leq 1
$$

**공분산 행렬:** $\mathbf{X} = (X_1, \ldots, X_d)^T$에 대해

$$
\boldsymbol{\Sigma} = \text{Cov}(\mathbf{X}) = \mathbb{E}\!\left[(\mathbf{X} - \boldsymbol{\mu})(\mathbf{X} - \boldsymbol{\mu})^T\right] \in \mathbb{R}^{d \times d}
$$

la-01에서 배운 대칭 행렬의 성질에 의해, 공분산 행렬은 항상 **양의 반정치(positive semi-definite)**입니다.

> **핵심 직관**: 공분산 행렬 $\boldsymbol{\Sigma}$는 데이터 분포의 "모양"을 결정합니다. la-05에서 다뤘던 고유값 분해를 공분산 행렬에 적용하면, 그것이 바로 **PCA(주성분 분석)**입니다.

---

## 5. 조건부 기댓값과 전체 기댓값의 법칙

**조건부 기댓값**은 $Y$가 주어졌을 때 $X$의 기댓값입니다:

$$
\mathbb{E}[X | Y = y] = \int x\,f_{X|Y}(x|y)\,dx
$$

$\mathbb{E}[X | Y]$는 $Y$의 함수이므로 그 자체가 확률 변수입니다.

**전체 기댓값의 법칙 (Law of Total Expectation, 반복 기댓값의 법칙):**

$$
\mathbb{E}[X] = \mathbb{E}_Y\!\left[\mathbb{E}[X | Y]\right]
$$

**전체 분산의 법칙 (Law of Total Variance):**

$$
\text{Var}(X) = \mathbb{E}\!\left[\text{Var}(X|Y)\right] + \text{Var}\!\left(\mathbb{E}[X|Y]\right)
$$

```
전체 분산의 분해
──────────────────────────────────────────
Var(X) = E[Var(X|Y)]  +  Var(E[X|Y])
           ──────────     ─────────────
           그룹 내 분산    그룹 간 분산
           (설명 못한 부분)  (Y가 설명한 부분)
──────────────────────────────────────────
```

---

## 6. Python으로 확인하기

```python
import numpy as np

np.random.seed(42)

# --- 기댓값과 분산의 수치 검증 ---
# 지수분포: E[X] = 1/λ, Var(X) = 1/λ²
lam = 2.0
samples = np.random.exponential(scale=1/lam, size=100000)

print(f"E[X] 이론: {1/lam:.4f}, 표본: {samples.mean():.4f}")
print(f"Var(X) 이론: {1/lam**2:.4f}, 표본: {samples.var():.4f}")

# --- 기댓값의 선형성 ---
X = np.random.normal(3, 1, size=100000)
Y = np.random.normal(5, 2, size=100000)
print(f"E[2X + 3Y] 이론: {2*3 + 3*5}, 표본: {(2*X + 3*Y).mean():.4f}")

# --- 공분산 행렬 ---
mean = [0, 0]
cov_true = [[1.0, 0.8],
            [0.8, 1.0]]
data = np.random.multivariate_normal(mean, cov_true, size=10000)
cov_est = np.cov(data.T)
print("추정 공분산 행렬:")
print(np.round(cov_est, 3))

# --- 전체 기댓값의 법칙 ---
# Y ~ Bernoulli(0.3), X|Y=0 ~ N(0,1), X|Y=1 ~ N(5,1)
Y = np.random.binomial(1, 0.3, size=100000)
X = np.where(Y == 0, np.random.normal(0, 1, 100000),
                      np.random.normal(5, 1, 100000))

E_X_given_Y0 = X[Y == 0].mean()
E_X_given_Y1 = X[Y == 1].mean()
E_X_total = 0.7 * E_X_given_Y0 + 0.3 * E_X_given_Y1
print(f"E[X] 전체 기댓값의 법칙: {E_X_total:.4f}, 직접 계산: {X.mean():.4f}")
```

---

## 핵심 정리

1. **기댓값의 선형성** $\mathbb{E}[aX+bY] = a\mathbb{E}[X]+b\mathbb{E}[Y]$는 독립과 무관하게 항상 성립하며, 확률론의 가장 강력한 도구이다.
2. **분산** $\text{Var}(X) = \mathbb{E}[X^2] - (\mathbb{E}[X])^2$은 분포의 퍼짐을 측정하며, 편향-분산 분해의 핵심 구성 요소이다.
3. **공분산 행렬** $\boldsymbol{\Sigma}$는 양의 반정치 대칭 행렬로, 다변량 분포의 모양을 결정하고 PCA의 수학적 기초이다.
4. **전체 기댓값의 법칙** $\mathbb{E}[X] = \mathbb{E}[\mathbb{E}[X|Y]]$은 복잡한 기댓값을 조건부로 분해하는 핵심 기법이다.
5. **전체 분산의 법칙**은 분산을 "그룹 내"와 "그룹 간"으로 분해하며, ANOVA와 혼합 모델의 수학적 기초이다.
