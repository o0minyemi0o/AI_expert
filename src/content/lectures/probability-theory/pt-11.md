# KL 발산과 상호 정보량

## 왜 KL 발산과 상호 정보량을 배워야 하는가

pt-10에서 엔트로피와 교차 엔트로피를 다뤘습니다. 이제 두 분포 사이의 "거리"를 측정하는 **KL 발산(Kullback-Leibler divergence)**과, 두 변수 간의 "정보 공유량"을 측정하는 **상호 정보량(mutual information)**을 다룹니다.

KL 발산은 VAE의 ELBO, GAN의 목적 함수, 지식 증류(knowledge distillation)의 핵심이며, 상호 정보량은 특성 선택과 표현 학습의 이론적 기초입니다. 또한 이들을 포괄하는 **f-divergence** 프레임워크를 소개합니다.

---

## 1. KL 발산의 정의

분포 $p$에서 분포 $q$로의 **KL 발산**:

$$
D_{\text{KL}}(p \| q) = \sum_{x} p(x) \log \frac{p(x)}{q(x)} = \mathbb{E}_{X \sim p}\!\left[\log \frac{p(X)}{q(X)}\right]
$$

연속의 경우:

$$
D_{\text{KL}}(p \| q) = \int p(x) \log \frac{p(x)}{q(x)}\,dx
$$

### 핵심 성질

| 성질 | 설명 |
|------|------|
| 비음수 | $D_{\text{KL}}(p \\\| q) \geq 0$ (Gibbs' inequality) |
| 등호 조건 | $D_{\text{KL}}(p \\\| q) = 0 \Leftrightarrow p = q$ |
| **비대칭** | $D_{\text{KL}}(p \\\| q) \neq D_{\text{KL}}(q \\\| p)$ 일반적 |
| 삼각 부등식 불성립 | 엄밀한 거리(metric)가 아님 |

> **핵심 직관**: KL 발산은 "분포 $q$가 분포 $p$를 얼마나 잘못 설명하는가"의 척도입니다. 거리 함수(metric)가 아니므로 "KL 거리"라 부르지 않고 "KL 발산(divergence)"이라 합니다.

---

## 2. KL 발산의 비대칭성과 두 방향

비대칭성이 ML에서 중요한 의미를 가집니다:

### Forward KL: $D_{\text{KL}}(p \| q)$

$q$가 0인 곳에서 $p$가 양수이면 발산 → $q$는 $p$의 지지(support)를 **모두 커버**해야 함

→ **모드 커버링 (mode-covering)** 행동: $q$가 $p$보다 넓어짐

### Reverse KL: $D_{\text{KL}}(q \| p)$

$p$가 0인 곳에서 $q$가 양수이면 발산 → $q$는 $p$가 0인 곳을 **피해야** 함

→ **모드 탐색 (mode-seeking)** 행동: $q$가 $p$의 한 모드에 집중

```
Forward KL vs Reverse KL (다봉(bimodal) p에 대해)
──────────────────────────────────────────────────
       p(x)           Forward KL          Reverse KL
                     min D_KL(p∥q)       min D_KL(q∥p)

     ┌─┐   ┌─┐     ┌───────────┐        ┌─┐
     │ │   │ │     │    q(x)   │        │q│
     │ │   │ │     │  넓게 커버  │        │ │ 한 모드에 집중
    ─┘ └───┘ └─   ─┘           └─      ─┘ └───────────
──────────────────────────────────────────────────
```

> **핵심 직관**: VAE는 reverse KL을 사용하여 사후 분포를 근사합니다 ($D_{\text{KL}}(q_\phi \| p(\mathbf{z}|\mathbf{x}))$). 그래서 VAE의 생성 샘플이 다양성이 부족한 경향이 있습니다.

---

## 3. 교차 엔트로피와의 관계

pt-10에서 본 관계를 다시 확인합니다:

$$
H(p, q) = H(p) + D_{\text{KL}}(p \| q)
$$

$H(p)$는 상수이므로:

$$
\arg\min_q H(p, q) = \arg\min_q D_{\text{KL}}(p \| q)
$$

교차 엔트로피 최소화 = Forward KL 최소화.

---

## 4. 상호 정보량 (Mutual Information)

두 확률 변수 $X$와 $Y$가 공유하는 정보량:

$$
I(X; Y) = D_{\text{KL}}\!\left(p(x, y) \| p(x)p(y)\right) = \sum_{x, y} p(x, y) \log \frac{p(x, y)}{p(x)p(y)}
$$

동치 표현들:

$$
\begin{aligned}
I(X; Y) &= H(X) - H(X|Y) \\
        &= H(Y) - H(Y|X) \\
        &= H(X) + H(Y) - H(X, Y)
\end{aligned}
$$

성질:
- $I(X; Y) \geq 0$ (KL 발산이므로)
- $I(X; Y) = 0 \Leftrightarrow X \perp Y$
- **대칭**: $I(X; Y) = I(Y; X)$

> **핵심 직관**: 상호 정보량은 "$X$를 알면 $Y$의 불확실성이 얼마나 줄어드는가"입니다. 특성 선택에서 $I(X_j; Y)$가 큰 특성이 예측에 유용한 특성입니다. 상관계수와 달리 **비선형 관계**도 포착합니다.

---

## 5. f-divergence 프레임워크

KL 발산은 더 일반적인 **f-divergence**의 특수 경우입니다:

$$
D_f(p \| q) = \sum_{x} q(x)\,f\!\left(\frac{p(x)}{q(x)}\right)
$$

여기서 $f$는 볼록 함수이고 $f(1) = 0$입니다.

| f-divergence | $f(t)$ | 용도 |
|:---:|:---:|------|
| KL (forward) | $t \log t$ | VAE, 교차 엔트로피 |
| Reverse KL | $-\log t$ | 변분 추론 |
| Jensen-Shannon | $\frac{1}{2}(t\log t - (t+1)\log\frac{t+1}{2})$ | 원조 GAN |
| Hellinger | $(\sqrt{t}-1)^2$ | 강건한 추정 |
| $\chi^2$ | $(t-1)^2$ | 적합도 검정 |
| Total Variation | $\frac{1}{2}|t-1|$ | 분포 차이의 상한 |

### Jensen-Shannon Divergence

대칭적 KL 발산:

$$
\text{JSD}(p \| q) = \frac{1}{2}D_{\text{KL}}\!\left(p \| m\right) + \frac{1}{2}D_{\text{KL}}\!\left(q \| m\right), \quad m = \frac{p + q}{2}
$$

성질: 대칭, 유계 ($0 \leq \text{JSD} \leq \log 2$), $\sqrt{\text{JSD}}$는 metric.

> **핵심 직관**: 원조 GAN의 판별기는 Jensen-Shannon divergence를 최대화하는 것으로 해석됩니다. 다양한 GAN 변형(WGAN, f-GAN 등)은 서로 다른 divergence를 최소화합니다.

---

## 6. 가우시안 분포의 KL 발산

두 다변량 가우시안 사이의 KL 발산 (닫힌 형태):

$$
D_{\text{KL}}\!\left(\mathcal{N}(\boldsymbol{\mu}_1, \boldsymbol{\Sigma}_1) \| \mathcal{N}(\boldsymbol{\mu}_0, \boldsymbol{\Sigma}_0)\right) = \frac{1}{2}\left[\text{tr}(\boldsymbol{\Sigma}_0^{-1}\boldsymbol{\Sigma}_1) + (\boldsymbol{\mu}_0 - \boldsymbol{\mu}_1)^T\boldsymbol{\Sigma}_0^{-1}(\boldsymbol{\mu}_0 - \boldsymbol{\mu}_1) - d + \log\frac{|\boldsymbol{\Sigma}_0|}{|\boldsymbol{\Sigma}_1|}\right]
$$

특히 $\boldsymbol{\mu}_0 = \mathbf{0}$, $\boldsymbol{\Sigma}_0 = \mathbf{I}$이면 (VAE의 정규화 항):

$$
D_{\text{KL}}\!\left(\mathcal{N}(\boldsymbol{\mu}, \boldsymbol{\Sigma}) \| \mathcal{N}(\mathbf{0}, \mathbf{I})\right) = \frac{1}{2}\left[\text{tr}(\boldsymbol{\Sigma}) + \boldsymbol{\mu}^T\boldsymbol{\mu} - d - \log|\boldsymbol{\Sigma}|\right]
$$

la-01에서 배운 행렬식과 대각합(trace)이 여기서 직접 활용됩니다.

---

## 7. Python으로 확인하기

```python
import numpy as np
from scipy import stats
from scipy.special import rel_entr  # p*log(p/q)

# --- KL 발산 ---
p = np.array([0.4, 0.3, 0.2, 0.1])
q = np.array([0.25, 0.25, 0.25, 0.25])

kl_pq = np.sum(rel_entr(p, q))
kl_qp = np.sum(rel_entr(q, p))

print(f"D_KL(p || q) = {kl_pq:.4f}")
print(f"D_KL(q || p) = {kl_qp:.4f}")
print(f"비대칭: {abs(kl_pq - kl_qp):.4f}")

# --- 교차 엔트로피 = H(p) + D_KL(p||q) ---
H_p = -np.sum(p * np.log(p))
CE_pq = -np.sum(p * np.log(q))
print(f"\nH(p) = {H_p:.4f}")
print(f"H(p,q) = {CE_pq:.4f}")
print(f"H(p) + D_KL(p||q) = {H_p + kl_pq:.4f}")  # CE와 동일

# --- 상호 정보량 ---
p_xy = np.array([[0.1, 0.2],
                 [0.3, 0.4]])
p_x = p_xy.sum(axis=1, keepdims=True)
p_y = p_xy.sum(axis=0, keepdims=True)

MI = np.sum(p_xy * np.log(p_xy / (p_x * p_y)))
H_X = -np.sum(p_x * np.log(p_x))
H_Y = -np.sum(p_y * np.log(p_y))
H_XY = -np.sum(p_xy * np.log(p_xy))

print(f"\nI(X;Y) = {MI:.4f}")
print(f"H(X) - H(X|Y) = {H_X - (H_XY - H_Y):.4f}")  # 동일

# --- 가우시안 KL (VAE 정규화 항) ---
d = 10
mu = np.random.randn(d)
log_var = np.random.randn(d)  # log(σ²)
sigma_sq = np.exp(log_var)

kl_vae = 0.5 * np.sum(sigma_sq + mu**2 - 1 - log_var)
print(f"\nVAE KL (대각 Σ): {kl_vae:.4f}")

# scipy로 검증
kl_scipy = stats.entropy(
    stats.multivariate_normal(mu, np.diag(sigma_sq)).pdf(
        np.random.multivariate_normal(mu, np.diag(sigma_sq), 100000)),
    stats.multivariate_normal(np.zeros(d), np.eye(d)).pdf(
        np.random.multivariate_normal(mu, np.diag(sigma_sq), 100000))
)
# 참고: scipy.stats.entropy는 이산 분포용이므로 위 KL 공식이 정확함
```

---

## 핵심 정리

1. **KL 발산** $D_{\text{KL}}(p\|q) = \mathbb{E}_p[\log(p/q)] \geq 0$은 두 분포의 차이를 측정하지만, 비대칭이라 거리 함수가 아니다.
2. Forward KL은 **모드 커버링**, Reverse KL은 **모드 탐색** 행동을 유도하며, 이 차이가 VAE vs GAN의 생성 특성을 결정한다.
3. **상호 정보량** $I(X;Y) = H(X) - H(X|Y)$는 비선형 의존성까지 포착하는 변수 간 관계의 척도이다.
4. **f-divergence**는 KL·JSD·Hellinger 등을 통합하는 프레임워크이며, 각 GAN 변형이 서로 다른 f-divergence를 최소화한다.
5. 가우시안 KL의 닫힌 형태 $\frac{1}{2}[\text{tr}(\boldsymbol{\Sigma}) + \|\boldsymbol{\mu}\|^2 - d - \log|\boldsymbol{\Sigma}|]$는 VAE의 정규화 항으로 직접 사용된다.
