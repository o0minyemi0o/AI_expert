# 통계적 모델과 추정량

## 왜 통계적 모델을 세우는가

데이터만으로는 현상을 체계적으로 이해할 수 없습니다. 통계적 모델은 데이터 생성 과정을 수학적으로 기술하여, 관측값 뒤에 숨은 모수(parameter)를 추정하고 미래를 예측할 수 있게 해줍니다. 모델의 품질은 곧 추정량(estimator)의 성질에 의해 결정됩니다.

---

## 1. 통계적 모델의 정의

통계적 모델은 관측 데이터 $X_1, X_2, \dots, X_n$의 확률 분포를 지정하는 분포족(family of distributions)입니다.

$$\mathcal{P} = \{ P_\theta : \theta \in \Theta \}$$

여기서 $\Theta$는 **모수 공간(parameter space)**이라 합니다.

| 구분 | 모수적 모델 | 비모수적 모델 |
|------|------------|--------------|
| 모수 공간 | 유한 차원 ($\Theta \subseteq \mathbb{R}^d$) | 무한 차원 |
| 예시 | $N(\mu, \sigma^2)$ | 커널 밀도 추정 |
| 장점 | 해석 용이, 효율적 추정 | 유연성, 모델 오류 감소 |
| 단점 | 모델 오류 가능 | 수렴 속도 느림 |

> **핵심 직관**: 모수적 모델은 "데이터가 어떤 수학적 가족에서 왔다"고 가정하는 것입니다. 가정이 맞으면 강력하고, 틀리면 위험합니다.

```python
import numpy as np
from scipy.stats import norm

# 모수적 모델 예시: 정규 분포 가정
data = np.random.normal(loc=5, scale=2, size=100)
mu_hat, sigma_hat = np.mean(data), np.std(data, ddof=1)
print(f"추정된 모수: mu={mu_hat:.2f}, sigma={sigma_hat:.2f}")
```

---

## 2. 추정량과 추정값

**추정량(estimator)**은 데이터의 함수 $\hat{\theta} = g(X_1, \dots, X_n)$이며, **추정값(estimate)**은 관측된 데이터를 대입한 구체적인 수치입니다.

$$\hat{\theta}_n = g(x_1, x_2, \dots, x_n)$$

동전 100번 던져 앞면 60번이면, $\hat{p} = 0.6$이 추정값입니다.

> **핵심 직관**: 추정량은 "레시피"이고, 추정값은 "요리 결과"입니다. 좋은 레시피(추정량)는 어떤 재료(데이터)를 넣어도 안정적인 결과를 냅니다.

---

## 3. 불편성 (Unbiasedness)

추정량 $\hat{\theta}$가 **불편(unbiased)**하다는 것은 기대값이 참값과 일치한다는 뜻입니다.

$$E_\theta[\hat{\theta}] = \theta, \quad \forall \theta \in \Theta$$

**편향(bias)**은 다음과 같이 정의됩니다.

$$\text{Bias}(\hat{\theta}) = E_\theta[\hat{\theta}] - \theta$$

| 추정량 | 불편성 여부 | 설명 |
|--------|-----------|------|
| $\bar{X} = \frac{1}{n}\sum X_i$ | 불편 | $E[\bar{X}] = \mu$ |
| $S^2 = \frac{1}{n-1}\sum(X_i - \bar{X})^2$ | 불편 | $E[S^2] = \sigma^2$ |
| $\hat{\sigma}^2 = \frac{1}{n}\sum(X_i - \bar{X})^2$ | 편향 | $E[\hat{\sigma}^2] = \frac{n-1}{n}\sigma^2$ |

> **핵심 직관**: 불편성은 "평균적으로 과녁의 중심을 맞추는 것"입니다. 하지만 불편성만으로 좋은 추정량이라 할 수는 없습니다.

```python
# 불편 vs 편향 분산 추정량
samples = np.random.normal(0, 1, size=(10000, 30))
biased_var = np.var(samples, axis=1)       # 1/n
unbiased_var = np.var(samples, axis=1, ddof=1)  # 1/(n-1)
print(f"편향 추정량 평균: {biased_var.mean():.4f}")
print(f"불편 추정량 평균: {unbiased_var.mean():.4f}")
```

---

## 4. 일치성 (Consistency)

추정량 $\hat{\theta}_n$이 **일치(consistent)**하다는 것은 표본 크기가 커질수록 참값에 수렴한다는 뜻입니다.

$$\hat{\theta}_n \xrightarrow{P} \theta \quad \text{as } n \to \infty$$

즉, 임의의 $\epsilon > 0$에 대해

$$\lim_{n \to \infty} P(|\hat{\theta}_n - \theta| > \epsilon) = 0$$

일치성을 보이는 가장 흔한 방법은 **MSE 일치성**입니다.

$$\text{MSE}(\hat{\theta}_n) = \text{Bias}^2(\hat{\theta}_n) + \text{Var}(\hat{\theta}_n) \to 0$$

> **핵심 직관**: 일치성은 "데이터가 충분히 많으면 결국 참값을 찾아간다"는 최소한의 보장입니다. 불편성이 없더라도 일치성은 가질 수 있습니다.

```python
# 일치성 시뮬레이션: 표본 크기 증가에 따른 수렴
true_mu = 5.0
ns = [10, 50, 100, 500, 1000, 5000]
for n in ns:
    estimates = [np.mean(np.random.normal(true_mu, 2, n)) for _ in range(1000)]
    print(f"n={n:5d}: 평균 추정값={np.mean(estimates):.4f}, 분산={np.var(estimates):.6f}")
```

---

## 5. 효율성 (Efficiency)

추정량의 효율성은 **분산이 얼마나 작은가**로 측정합니다. Cramer-Rao 하한은 불편 추정량이 달성할 수 있는 최소 분산을 제시합니다.

$$\text{Var}_\theta(\hat{\theta}) \geq \frac{1}{n \cdot I(\theta)}$$

여기서 $I(\theta)$는 **피셔 정보(Fisher information)**입니다.

$$I(\theta) = -E\left[\frac{\partial^2}{\partial \theta^2} \log f(X; \theta)\right]$$

**상대 효율(relative efficiency)**은 두 추정량의 분산 비입니다.

$$\text{ARE}(\hat{\theta}_1, \hat{\theta}_2) = \frac{\text{Var}(\hat{\theta}_2)}{\text{Var}(\hat{\theta}_1)}$$

| 분포 | 추정량 | 분산 | CRLB 달성 |
|------|--------|------|----------|
| $N(\mu, \sigma^2)$ | $\bar{X}$ | $\sigma^2/n$ | 달성 |
| $\text{Exp}(\lambda)$ | $1/\bar{X}$ | $\lambda^2/n$ (점근) | 점근적 달성 |
| $U(0, \theta)$ | $X_{(n)}$ | — | CRLB 적용 불가 |

> **핵심 직관**: 효율적 추정량은 같은 양의 데이터에서 최대한 많은 정보를 "짜내는" 추정량입니다. si-02에서 다룰 MLE는 점근적으로 효율적입니다.

```python
from scipy.stats import norm

# 피셔 정보 계산 예시: 정규 분포의 평균
# I(mu) = 1/sigma^2
sigma = 2.0
n = 100
fisher_info = 1 / sigma**2
crlb = 1 / (n * fisher_info)
sample_var = sigma**2 / n
print(f"CRLB: {crlb:.4f}, 표본 평균의 분산: {sample_var:.4f}")
print(f"표본 평균은 CRLB를 달성합니다: {np.isclose(crlb, sample_var)}")
```

---

## 6. 평균 제곱 오차와 편향-분산 트레이드오프

추정량의 전체 성능은 **MSE(Mean Squared Error)**로 평가합니다.

$$\text{MSE}(\hat{\theta}) = E[(\hat{\theta} - \theta)^2] = \text{Bias}^2(\hat{\theta}) + \text{Var}(\hat{\theta})$$

이 분해는 pt-02에서 다룬 베이즈 정리와 함께 통계적 의사결정의 기반을 형성합니다.

| 추정량 유형 | 편향 | 분산 | MSE |
|------------|------|------|-----|
| 불편 추정량 | 0 | 높을 수 있음 | 분산과 동일 |
| 축소 추정량 (James-Stein) | 존재 | 낮음 | 종종 더 낮음 |
| 정규화된 추정량 | 존재 | 낮음 | 트레이드오프 |

> **핵심 직관**: 약간의 편향을 허용하면 분산을 크게 줄여 전체 오차(MSE)를 낮출 수 있습니다. 이것이 cm-12에서 배운 정규화의 통계적 근거입니다.

```python
# 편향-분산 트레이드오프 시뮬레이션
true_theta = 3.0
n_sim = 5000
n_sample = 10

# 불편 추정량: 표본 평균
unbiased = [np.mean(np.random.normal(true_theta, 1, n_sample)) for _ in range(n_sim)]
# 축소 추정량: 0 방향으로 축소
shrink = [0.8 * np.mean(np.random.normal(true_theta, 1, n_sample)) for _ in range(n_sim)]

mse_unbiased = np.mean([(x - true_theta)**2 for x in unbiased])
mse_shrink = np.mean([(x - true_theta)**2 for x in shrink])
print(f"불편 MSE: {mse_unbiased:.4f}, 축소 MSE: {mse_shrink:.4f}")
```

---

## 7. 손실 함수와 위험 함수

추정의 품질을 일반적으로 측정하려면 **손실 함수(loss function)** $L(\theta, \hat{\theta})$를 정의합니다.

$$R(\theta, \hat{\theta}) = E_\theta[L(\theta, \hat{\theta})]$$

이를 **위험 함수(risk function)**라 합니다.

| 손실 함수 | 정의 | 최적 추정량 |
|-----------|------|-----------|
| 제곱 손실 | $(\theta - \hat{\theta})^2$ | 사후 평균 |
| 절대 손실 | $\|\theta - \hat{\theta}\|$ | 사후 중앙값 |
| 0-1 손실 | $I(\hat{\theta} \neq \theta)$ | 사후 최빈값 (MAP) |

> **핵심 직관**: 손실 함수의 선택은 "어떤 종류의 오차가 더 심각한가"를 반영합니다. si-03에서 다룰 베이즈 추정에서는 사전 분포와 결합하여 최적 추정량을 유도합니다.

---

## 핵심 정리

- **통계적 모델은 데이터의 확률 분포를 모수화하는 분포족 $\mathcal{P} = \{P_\theta\}$이며, 모수적/비모수적으로 나뉩니다**
- **불편성은 추정량의 기대값이 참값과 일치하는 성질이지만, 그 자체로 좋은 추정량을 보장하지 않습니다**
- **일치성은 표본 크기가 커질수록 추정량이 참값에 확률 수렴하는 최소한의 요구 조건입니다**
- **효율성은 Cramer-Rao 하한(CRLB)으로 측정하며, 피셔 정보가 추정 가능한 정보의 상한을 결정합니다**
- **MSE = 편향² + 분산 분해는 편향과 분산 사이의 트레이드오프를 명시하며, 정규화와 축소 추정의 이론적 근거입니다**
