# ML에서의 통계적 추론

## 왜 ML에 통계적 추론이 필요한가

기계학습 모델의 예측값만으로는 충분하지 않습니다. "이 예측이 얼마나 확실한가?", "A 모델이 B 모델보다 정말 나은가?", "이 A/B 테스트 결과를 신뢰할 수 있는가?" 등의 질문에 답하려면 통계적 추론이 필수입니다. 이 강의에서는 si-01부터 si-11까지 다룬 이론을 ML 실무에 적용하는 방법을 다룹니다.

---

## 1. 예측의 불확실성 정량화

ML 모델의 불확실성은 세 가지로 분해됩니다.

| 불확실성 유형 | 원인 | 감소 방법 |
|-------------|------|---------|
| 인식론적 (Epistemic) | 데이터 부족, 모델 미지 | 더 많은 데이터 |
| 우연적 (Aleatoric) | 고유한 노이즈 | 감소 불가 |
| 모델 (Model) | 모델 오특정 | 더 나은 모델 |

$$\text{Var}(\hat{Y}) = \underbrace{\text{Var}(E[\hat{Y}|\theta])}_{\text{인식론적}} + \underbrace{E[\text{Var}(\hat{Y}|\theta)]}_{\text{우연적}}$$

> **핵심 직관**: 인식론적 불확실성은 "모르기 때문에" 발생하며 데이터로 줄일 수 있고, 우연적 불확실성은 "세상이 원래 불확실하기 때문에" 발생하며 줄일 수 없습니다. si-03의 베이즈 추정이 인식론적 불확실성을 자연스럽게 포착합니다.

```python
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import make_classification

# 앙상블 기반 불확실성 추정
X, y = make_classification(n_samples=500, n_features=20, random_state=42)
X_train, X_test = X[:400], X[400:]
y_train, y_test = y[:400], y[400:]

rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)

# 각 트리의 예측을 통한 불확실성 추정
tree_preds = np.array([tree.predict_proba(X_test)[:, 1] for tree in rf.estimators_])
mean_pred = tree_preds.mean(axis=0)
epistemic_unc = tree_preds.var(axis=0)  # 트리 간 불일치

print(f"예측 평균: {mean_pred[:5]}")
print(f"인식론적 불확실성: {epistemic_unc[:5]}")
print(f"가장 불확실한 예측 인덱스: {np.argmax(epistemic_unc)}")
```

---

## 2. 확률 교정 (Probability Calibration)

**교정(calibration)**은 모델이 출력하는 확률이 실제 빈도와 일치하는지를 측정합니다.

$$P(Y = 1 | \hat{p}(x) = p) = p \quad \text{(완벽한 교정)}$$

| 교정 방법 | 아이디어 | 장점 |
|-----------|---------|------|
| Platt Scaling | 로지스틱 변환 | 단순, 이진 분류 |
| Temperature Scaling | $\text{softmax}(z/T)$ | 다중 클래스, 1개 모수 |
| Isotonic Regression | 비모수 단조 변환 | 유연, 데이터 적응 |
| Beta Calibration | 베타 분포 기반 | 비대칭 왜곡 보정 |

**교정 오류 측도**:

$$\text{ECE} = \sum_{b=1}^{B} \frac{n_b}{N} |\text{acc}(b) - \text{conf}(b)|$$

> **핵심 직관**: 신경망은 종종 과신(overconfident)합니다. "90% 확신"이라고 해도 실제 정확도가 70%일 수 있습니다. 교정은 예측 확률을 신뢰할 수 있게 만드는 후처리입니다.

```python
from sklearn.calibration import calibration_curve, CalibratedClassifierCV
from sklearn.linear_model import LogisticRegression

# 교정 전후 비교
from sklearn.svm import SVC

svm = SVC(probability=True, random_state=42)
svm.fit(X_train, y_train)

# 교정 전
prob_uncal = svm.predict_proba(X_test)[:, 1]
fraction_pos_uncal, mean_pred_uncal = calibration_curve(y_test, prob_uncal, n_bins=5)

# Platt Scaling으로 교정
cal_svm = CalibratedClassifierCV(svm, method='sigmoid', cv=5)
cal_svm.fit(X_train, y_train)
prob_cal = cal_svm.predict_proba(X_test)[:, 1]
fraction_pos_cal, mean_pred_cal = calibration_curve(y_test, prob_cal, n_bins=5)

print("교정 전:", list(zip(mean_pred_uncal.round(3), fraction_pos_uncal.round(3))))
print("교정 후:", list(zip(mean_pred_cal.round(3), fraction_pos_cal.round(3))))
```

---

## 3. 등각 예측 (Conformal Prediction)

**등각 예측**은 모델에 구애받지 않는(model-agnostic) 방법으로, 유한 표본에서 정확한 커버리지를 보장하는 예측 집합을 구성합니다.

$$P(Y_{n+1} \in C(X_{n+1})) \geq 1 - \alpha$$

**분할 등각 예측 (Split Conformal)**:

1. 데이터를 학습/교정으로 분할
2. 교정 데이터에서 비순응 점수(nonconformity score) 계산: $s_i = |Y_i - \hat{f}(X_i)|$
3. $\hat{q}$를 $s_i$들의 $\lceil(1-\alpha)(n_{\text{cal}}+1)\rceil / n_{\text{cal}}$ 분위수로 설정
4. 예측 집합: $C(X_{\text{new}}) = [\hat{f}(X_{\text{new}}) - \hat{q}, \hat{f}(X_{\text{new}}) + \hat{q}]$

| 특징 | si-07의 CI | 등각 예측 |
|------|-----------|---------|
| 커버 대상 | 모수 $\theta$ | 미래 관측 $Y$ |
| 분포 가정 | 필요 (또는 점근) | 교환 가능성만 |
| 유한 표본 보장 | 점근적 | 정확히 $\geq 1-\alpha$ |
| 모델 의존 | 검정/추정 방법에 의존 | 모델 무관 |

> **핵심 직관**: 등각 예측은 "어떤 ML 모델이든, 교정 데이터가 있으면, 확률적으로 보장된 예측 구간을 줄 수 있다"는 강력한 결과입니다. 분포 가정이나 점근 이론이 필요 없습니다.

```python
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.datasets import make_regression

# 분할 등각 예측
X, y = make_regression(n_samples=1000, n_features=10, noise=10, random_state=42)
X_train, X_cal, X_test = X[:600], X[600:800], X[800:]
y_train, y_cal, y_test = y[:600], y[600:800], y[800:]

model = GradientBoostingRegressor(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# 교정 단계
cal_preds = model.predict(X_cal)
scores = np.abs(y_cal - cal_preds)
alpha = 0.1
q_hat = np.quantile(scores, np.ceil((1-alpha)*(len(y_cal)+1)) / len(y_cal))

# 예측 집합
test_preds = model.predict(X_test)
lower = test_preds - q_hat
upper = test_preds + q_hat

coverage = np.mean((y_test >= lower) & (y_test <= upper))
avg_width = np.mean(upper - lower)
print(f"목표 커버리지: {1-alpha:.2f}")
print(f"실제 커버리지: {coverage:.4f}")
print(f"평균 구간 폭: {avg_width:.2f}")
```

---

## 4. A/B 테스트의 통계적 기초

A/B 테스트는 si-05의 가설 검정을 온라인 실험에 적용한 것입니다.

**기본 프레임워크**:
- $H_0$: 처리 효과 없음 ($\mu_A = \mu_B$)
- $H_1$: 처리 효과 있음 ($\mu_A \neq \mu_B$)

| 설계 요소 | 통계적 근거 | 참조 |
|-----------|-----------|------|
| 표본 크기 결정 | si-05의 검정력 분석 | 효과 크기, $\alpha$, $\beta$ |
| 분산 감소 | CUPED (공변량 보정) | 사전 데이터 활용 |
| 다중 변형 | si-11의 FDR 통제 | Bonferroni/BH |
| 조기 종료 | 순차 분석 | Alpha spending |
| 이질적 효과 | 하위 그룹 분석 | 다중 검정 주의 |

> **핵심 직관**: A/B 테스트는 "통계적으로 엄밀한 실험"입니다. si-05의 유의 수준, 검정력, 표본 크기 설계가 직접 적용되며, 여러 변형을 동시에 테스트하면 si-11의 다중 검정 문제가 발생합니다.

```python
from scipy.stats import ttest_ind, norm

# A/B 테스트 시뮬레이션
np.random.seed(42)

# 표본 크기 계산
effect_size = 0.05  # 전환율 차이 5%p
baseline_rate = 0.10
alpha = 0.05
power = 0.80

p_bar = baseline_rate + effect_size / 2
se = np.sqrt(2 * p_bar * (1 - p_bar))
n_per_group = int(np.ceil(((norm.ppf(1-alpha/2) + norm.ppf(power)) * se / effect_size)**2))
print(f"그룹당 필요 표본: {n_per_group}")

# A/B 테스트 실행
n = n_per_group
group_a = np.random.binomial(1, 0.10, n)  # 대조군: 10% 전환
group_b = np.random.binomial(1, 0.15, n)  # 처리군: 15% 전환

rate_a, rate_b = group_a.mean(), group_b.mean()
diff = rate_b - rate_a
se_diff = np.sqrt(rate_a*(1-rate_a)/n + rate_b*(1-rate_b)/n)
z_stat = diff / se_diff
p_val = 2 * (1 - norm.cdf(abs(z_stat)))

print(f"전환율: A={rate_a:.4f}, B={rate_b:.4f}")
print(f"차이: {diff:.4f} (SE={se_diff:.4f})")
print(f"z-stat={z_stat:.4f}, p-value={p_val:.4f}")
print(f"95% CI for diff: ({diff - 1.96*se_diff:.4f}, {diff + 1.96*se_diff:.4f})")
```

---

## 5. 순차 검정과 조기 종료

고정 표본 검정 대신, 데이터가 축적되면서 결론을 내릴 수 있는 **순차 분석(sequential analysis)**:

| 방법 | 설명 | 장점 |
|------|------|------|
| Alpha spending | 유의 수준을 중간 분석에 분배 | FWER 통제 |
| 순차 확률비 검정 (SPRT) | 우도비 모니터링 | 빠른 결론 |
| 항상 유효한 CI (anytime-valid) | 언제든 유효한 CI | 유연한 모니터링 |
| 베이즈 순차 검정 | 사후 확률 모니터링 | 자연스러운 해석 |

**O'Brien-Fleming Alpha Spending**:

중간 분석 시점 $t_1, t_2, \dots, t_K$에서 누적 유의 수준 $\alpha(t_k)$를 사용합니다.

$$\alpha(t) = 2 - 2\Phi\left(\frac{z_{\alpha/2}}{\sqrt{t}}\right)$$

> **핵심 직관**: 데이터를 반복적으로 "들여다보면" 유의 수준이 팽창합니다. 매일 p-value를 확인하면서 유의해지면 멈추는 것은 거짓 발견률을 크게 높입니다. 순차 분석은 이를 통제합니다.

```python
# 반복 검정의 위험성 시뮬레이션
np.random.seed(42)
n_sim = 10000
n_max = 1000
check_points = [100, 200, 300, 500, 1000]

false_positives_naive = 0
false_positives_corrected = 0
alpha = 0.05

# O'Brien-Fleming 임계값 (간이 계산)
of_thresholds = [norm.ppf(1 - alpha/2 * np.sqrt(cp/n_max)) for cp in check_points]

for _ in range(n_sim):
    data_a = np.random.normal(0, 1, n_max)
    data_b = np.random.normal(0, 1, n_max)  # H0 참 (차이 없음)

    # 순진한 접근: 어떤 시점에서든 p < 0.05이면 기각
    for cp in check_points:
        _, p = ttest_ind(data_a[:cp], data_b[:cp])
        if p < alpha:
            false_positives_naive += 1
            break

    # O'Brien-Fleming 보정
    for cp, thresh in zip(check_points, of_thresholds):
        _, p = ttest_ind(data_a[:cp], data_b[:cp])
        z_observed = norm.ppf(1 - p/2)
        if z_observed > thresh:
            false_positives_corrected += 1
            break

print(f"순진한 반복 검정 거짓 양성률: {false_positives_naive/n_sim:.4f}")
print(f"O'Brien-Fleming 거짓 양성률: {false_positives_corrected/n_sim:.4f}")
```

---

## 6. 모델 비교의 통계적 검정

"모델 A가 모델 B보다 좋은가?"에 대한 엄밀한 검정:

| 방법 | 설명 | 주의 |
|------|------|------|
| Paired t-test | 교차 검증 폴드별 차이 검정 | 폴드 간 의존성 |
| McNemar's test | 분류 오류 패턴 비교 | 이진 결과 |
| 5x2 CV test | 5회 반복 2-fold CV | Dietterich (1998) |
| 부트스트랩 검정 | 성능 차이의 부트스트랩 분포 | si-07의 부트스트랩 |
| 베이즈 검정 | $P(\text{A > B}\|데이터)$ 직접 계산 | si-03의 베이즈 추정 |

> **핵심 직관**: 교차 검증의 정확도 차이가 0.5%p이면 유의한가요? 답은 분산에 달려 있습니다. cm-12에서 배운 교차 검증의 분산 추정은 과소 추정되는 경향이 있으므로, 보정된 검정을 사용해야 합니다.

```python
from sklearn.model_selection import cross_val_score
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from scipy.stats import ttest_rel

# 모델 비교: Paired t-test on CV scores
X, y = make_classification(n_samples=300, n_features=20, random_state=42)

svm_scores = cross_val_score(SVC(random_state=42), X, y, cv=10)
dt_scores = cross_val_score(DecisionTreeClassifier(random_state=42), X, y, cv=10)

diff = svm_scores - dt_scores
stat, p = ttest_rel(svm_scores, dt_scores)
print(f"SVM 평균: {svm_scores.mean():.4f}, DT 평균: {dt_scores.mean():.4f}")
print(f"차이 평균: {diff.mean():.4f} +/- {diff.std():.4f}")
print(f"Paired t-test: t={stat:.4f}, p={p:.4f}")
```

---

## 7. 통합 프레임워크: ML 파이프라인에서의 추론

전체 ML 워크플로에서 통계적 추론이 개입하는 지점:

| 단계 | 통계적 추론 도구 | 참조 |
|------|---------------|------|
| 탐색적 분석 | 다중 검정 (FDR) | si-11 |
| 특성 선택 | 가설 검정, 신뢰 구간 | si-05, si-07 |
| 모델 학습 | MLE, 베이즈 추정 | si-02, si-03 |
| 하이퍼파라미터 | 다중 비교 보정 | si-11 |
| 모델 평가 | 부트스트랩 CI | si-07 |
| 예측 불확실성 | 등각 예측, 베이즈 | si-03 |
| 배포 후 | A/B 테스트, 순차 검정 | si-05 |
| 공정성 검증 | 하위 그룹 검정, FDR | si-11 |

```python
# 종합 예시: 모델 성능의 부트스트랩 CI
from sklearn.metrics import accuracy_score

# 테스트 세트에서의 부트스트랩 CI
X_train, X_test = X[:200], X[200:]
y_train, y_test = y[:200], y[200:]

model = SVC(random_state=42).fit(X_train, y_train)
y_pred = model.predict(X_test)
base_accuracy = accuracy_score(y_test, y_pred)

# 부트스트랩
n_boot = 10000
boot_accs = []
for _ in range(n_boot):
    idx = np.random.choice(len(y_test), len(y_test), replace=True)
    boot_acc = accuracy_score(y_test[idx], y_pred[idx])
    boot_accs.append(boot_acc)

boot_accs = np.array(boot_accs)
ci = (np.percentile(boot_accs, 2.5), np.percentile(boot_accs, 97.5))
print(f"정확도: {base_accuracy:.4f}")
print(f"95% 부트스트랩 CI: ({ci[0]:.4f}, {ci[1]:.4f})")
```

> **핵심 직관**: ML은 "예측"에 집중하지만, 예측의 신뢰도, 모델의 유의성, 실험의 타당성은 모두 통계적 추론에 의존합니다. si-01에서 si-11까지의 이론이 ML 실무의 모든 단계에 스며들어 있습니다.

---

## 핵심 정리

- **ML 예측의 불확실성은 인식론적(데이터 부족)과 우연적(본질적 노이즈)으로 분해되며, 베이즈/앙상블 방법으로 정량화합니다**
- **확률 교정(Temperature Scaling, Platt Scaling)은 모델 출력 확률을 실제 빈도에 맞추어 의사결정의 신뢰성을 높입니다**
- **등각 예측은 분포 가정 없이 유한 표본에서 $P(Y \in C(X)) \geq 1-\alpha$를 보장하는 모델 무관 방법입니다**
- **A/B 테스트는 가설 검정의 실무 적용이며, 표본 크기 설계(검정력), 다중 변형(FDR), 조기 종료(순차 분석)의 통합 설계가 필요합니다**
- **모델 비교는 교차 검증 점수의 통계적 검정으로 수행하며, 부트스트랩 CI와 함께 "차이가 유의한가?"에 엄밀히 답해야 합니다**
