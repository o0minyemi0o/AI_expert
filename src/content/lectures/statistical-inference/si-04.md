# 충분 통계량과 완비성

## 왜 충분 통계량이 중요한가

$n$개의 데이터를 모두 기억하지 않고도, 모수 추정에 필요한 모든 정보를 담은 요약 통계량이 존재할까요? 충분 통계량(sufficient statistic)은 바로 이 질문에 답합니다. si-01에서 다룬 "좋은 추정량"을 체계적으로 구성하는 핵심 도구이며, Rao-Blackwell 정리와 Lehmann-Scheffe 정리를 통해 최적 추정량을 유도하는 토대가 됩니다.

---

## 1. 충분 통계량의 정의

통계량 $T(X)$가 모수 $\theta$에 대해 **충분(sufficient)**하다는 것은, $T(X)$가 주어졌을 때 데이터 $X$의 조건부 분포가 $\theta$에 의존하지 않는다는 뜻입니다.

$$P(X = x | T(X) = t, \theta) = P(X = x | T(X) = t) \quad \forall \theta$$

| 분포 | 충분 통계량 | 차원 |
|------|-----------|------|
| $N(\mu, \sigma^2_0)$ ($\sigma^2_0$ 알려짐) | $\bar{X}$ | 1 |
| $N(\mu, \sigma^2)$ (둘 다 미지) | $(\bar{X}, \sum X_i^2)$ | 2 |
| $\text{Bernoulli}(p)$ | $\sum X_i$ | 1 |
| $U(0, \theta)$ | $X_{(n)}$ (최대 순서 통계량) | 1 |

> **핵심 직관**: 충분 통계량은 "데이터에서 모수에 대한 정보만 추출한 압축"입니다. $T(X)$를 알면 원래 데이터 $X$는 모수 추정에 추가 정보를 제공하지 않습니다.

```python
import numpy as np
from scipy.stats import binom

# 충분 통계량의 직관: 베르누이 데이터
# T = sum(X_i)만 알면 p 추정에 충분
n = 20
p_true = 0.3

# 서로 다른 데이터셋이지만 같은 충분 통계량
data1 = np.array([1,1,1,0,0,0,0,0,0,0,1,0,0,1,0,0,1,0,0,0])
data2 = np.array([0,0,1,1,1,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0])
print(f"T(data1) = {data1.sum()}, T(data2) = {data2.sum()}")
print(f"두 데이터셋의 MLE는 동일: {data1.mean():.2f} = {data2.mean():.2f}")
```

---

## 2. 인수분해 정리 (Fisher-Neyman Factorization)

충분 통계량을 실제로 찾을 때 가장 유용한 도구가 **인수분해 정리**입니다.

$T(X)$가 $\theta$에 대해 충분할 필요충분조건:

$$f(x; \theta) = g(T(x), \theta) \cdot h(x)$$

여기서 $g$는 $\theta$와 $T(x)$에만 의존하고, $h$는 $\theta$에 무관합니다.

**예시**: $X_1, \dots, X_n \sim N(\mu, 1)$

$$f(\mathbf{x}; \mu) = \prod \frac{1}{\sqrt{2\pi}} e^{-(x_i - \mu)^2/2} = \underbrace{e^{n\mu\bar{x} - n\mu^2/2}}_{g(\bar{x}, \mu)} \cdot \underbrace{(2\pi)^{-n/2} e^{-\sum x_i^2 / 2}}_{h(\mathbf{x})}$$

따라서 $T(X) = \bar{X}$가 $\mu$에 대한 충분 통계량입니다.

> **핵심 직관**: 인수분해 정리는 "밀도 함수를 $\theta$-의존 부분과 $\theta$-무관 부분으로 분리"하는 것입니다. $\theta$-의존 부분에 나타나는 데이터의 함수가 곧 충분 통계량입니다.

```python
import sympy as sp

# 인수분해 정리를 기호적으로 확인 (포아송 분포)
# f(x; lambda) = prod(lambda^xi * e^(-lambda) / xi!)
# = lambda^(sum xi) * e^(-n*lambda) * prod(1/xi!)
# g(T, theta) = lambda^T * e^(-n*lambda), h(x) = prod(1/xi!)
# T = sum(xi)가 충분 통계량

x = np.random.poisson(3, 50)
T = np.sum(x)
mle_lambda = T / len(x)
print(f"충분 통계량 T = sum(x) = {T}")
print(f"MLE = T/n = {mle_lambda:.2f}")
```

---

## 3. 지수족과 자연 충분 통계량

**지수족(exponential family)** 분포는 다음 형태를 가집니다.

$$f(x; \boldsymbol{\eta}) = h(x) \exp\left(\boldsymbol{\eta}^T \mathbf{T}(x) - A(\boldsymbol{\eta})\right)$$

| 구성 요소 | 의미 |
|-----------|------|
| $\boldsymbol{\eta}$ | 자연 모수 (natural parameter) |
| $\mathbf{T}(x)$ | 자연 충분 통계량 |
| $A(\boldsymbol{\eta})$ | 로그 정규화 상수 (로그 분배 함수) |
| $h(x)$ | 기저 측도 |

지수족의 핵심 성질:

$$E[\mathbf{T}(X)] = \nabla A(\boldsymbol{\eta}), \quad \text{Var}[\mathbf{T}(X)] = \nabla^2 A(\boldsymbol{\eta})$$

| 분포 | $\eta$ | $T(x)$ | $A(\eta)$ |
|------|--------|--------|-----------|
| $\text{Bernoulli}(p)$ | $\log\frac{p}{1-p}$ | $x$ | $\log(1+e^\eta)$ |
| $N(\mu, \sigma^2)$ | $(\mu/\sigma^2, -1/2\sigma^2)$ | $(x, x^2)$ | $\mu^2/2\sigma^2 + \log\sigma$ |
| $\text{Poisson}(\lambda)$ | $\log\lambda$ | $x$ | $e^\eta$ |

> **핵심 직관**: 지수족에서는 자연 충분 통계량의 차원이 모수의 차원과 같습니다. 이것이 "데이터 압축"이 최대로 이루어진 상태이며, si-02의 MLE가 닫힌 해를 가지는 이유이기도 합니다.

---

## 4. 완비 충분 통계량

충분 통계량 $T(X)$가 **완비(complete)**하다는 것은, $T$의 임의의 함수에 대해 다음이 성립함을 뜻합니다.

$$E_\theta[g(T)] = 0 \quad \forall \theta \in \Theta \implies g(T) = 0 \text{ a.s.}$$

즉, $T$의 함수 중에서 "항상 기대값이 0인 비자명 함수"가 존재하지 않습니다.

| 분포 | 완비 충분 통계량 |
|------|----------------|
| $N(\mu, \sigma^2_0)$ | $\bar{X}$ |
| $\text{Bernoulli}(p)$, $0 < p < 1$ | $\sum X_i$ |
| $U(0, \theta)$ | $X_{(n)}$ |
| $N(\mu, \sigma^2)$ | $(\bar{X}, \sum(X_i - \bar{X})^2)$ |

> **핵심 직관**: 완비성은 "충분 통계량에 불필요한 잉여 정보가 없다"는 것을 보장합니다. 완비 충분 통계량을 사용하면 유일한 최적 추정량을 얻을 수 있습니다.

---

## 5. Rao-Blackwell 정리

**Rao-Blackwell 정리**: $\tilde{\theta}$가 $\theta$의 불편 추정량이고 $T$가 충분 통계량이면,

$$\hat{\theta}_{RB} = E[\tilde{\theta} | T]$$

는 다음을 만족합니다:

1. $\hat{\theta}_{RB}$도 불편 추정량입니다
2. $\text{Var}(\hat{\theta}_{RB}) \leq \text{Var}(\tilde{\theta})$ (모든 $\theta$에서)

$$\text{MSE}(\hat{\theta}_{RB}) \leq \text{MSE}(\tilde{\theta})$$

> **핵심 직관**: 충분 통계량에 "조건부 기대"를 취하면, 불필요한 변동이 제거되어 더 나은 추정량을 얻습니다. "노이즈를 평균으로 상쇄"하는 것과 같습니다.

```python
# Rao-Blackwell 정리 시뮬레이션
# Bernoulli(p)에서 p^2의 불편 추정량
np.random.seed(42)
n = 20
p_true = 0.3
n_sim = 10000

naive_estimates = []
rb_estimates = []

for _ in range(n_sim):
    data = np.random.binomial(1, p_true, n)
    # 순진한 불편 추정량: X1 * X2
    naive = data[0] * data[1]
    naive_estimates.append(naive)
    # Rao-Blackwell 개선: E[X1*X2 | T=sum(X)]
    T = data.sum()
    rb = T * (T - 1) / (n * (n - 1)) if T >= 2 else 0
    rb_estimates.append(rb)

print(f"참값: p^2 = {p_true**2:.4f}")
print(f"순진한 추정량 - 평균: {np.mean(naive_estimates):.4f}, 분산: {np.var(naive_estimates):.6f}")
print(f"RB 추정량   - 평균: {np.mean(rb_estimates):.4f}, 분산: {np.var(rb_estimates):.6f}")
```

---

## 6. Lehmann-Scheffe 정리와 UMVUE

**Lehmann-Scheffe 정리**: $T$가 **완비 충분 통계량**이고 $\hat{\theta} = g(T)$가 $\theta$의 불편 추정량이면, $\hat{\theta}$는 **UMVUE(Uniformly Minimum Variance Unbiased Estimator)**입니다.

UMVUE를 찾는 두 가지 방법:

| 방법 | 절차 |
|------|------|
| 방법 1 | 완비 충분 통계량 $T$의 불편 함수 $g(T)$를 직접 구함 |
| 방법 2 | 임의의 불편 추정량에 Rao-Blackwell을 적용: $E[\tilde{\theta} \| T]$ |

**예시**: $X_1, \dots, X_n \sim N(\mu, \sigma^2)$에서 $\mu$의 UMVUE는 $\bar{X}$이고, $\sigma^2$의 UMVUE는 $S^2 = \frac{1}{n-1}\sum(X_i - \bar{X})^2$입니다.

> **핵심 직관**: Lehmann-Scheffe는 "완비 충분 통계량의 불편 함수는 자동으로 최선"이라고 말합니다. 완비성이 유일성을, 충분성이 최소 분산을, 불편성이 과녁 중심을 보장합니다.

```python
# UMVUE 확인: 정규 분포에서 sigma^2의 UMVUE
np.random.seed(42)
true_sigma2 = 4.0
n = 15
n_sim = 10000

umvue_estimates = []
mle_estimates = []

for _ in range(n_sim):
    data = np.random.normal(0, np.sqrt(true_sigma2), n)
    umvue = np.var(data, ddof=1)          # S^2 = 1/(n-1) * sum(xi - xbar)^2
    mle_est = np.var(data, ddof=0)         # 1/n * sum(xi - xbar)^2
    umvue_estimates.append(umvue)
    mle_estimates.append(mle_est)

print(f"참값: {true_sigma2}")
print(f"UMVUE 평균: {np.mean(umvue_estimates):.4f}, 분산: {np.var(umvue_estimates):.4f}")
print(f"MLE 평균:   {np.mean(mle_estimates):.4f}, 분산: {np.var(mle_estimates):.4f}")
```

---

## 7. 최소 충분 통계량

**최소 충분 통계량(minimal sufficient statistic)**은 가장 많이 압축된 충분 통계량입니다. 다른 모든 충분 통계량의 함수로 표현됩니다.

최소 충분 통계량을 찾는 방법 (**우도비 기준**):

$$\frac{f(\mathbf{x}; \theta)}{f(\mathbf{y}; \theta)} \text{가 } \theta\text{에 무관} \iff T(\mathbf{x}) = T(\mathbf{y})$$

| 개념 | 관계 |
|------|------|
| 충분 통계량 | 데이터 → 충분 통계량 (정보 손실 없는 축약) |
| 최소 충분 통계량 | 가장 작은 충분 통계량 |
| 완비 충분 통계량 | 최소 충분 + 잉여 없음 |

> **핵심 직관**: 최소 충분 통계량은 "정보 손실 없는 최대 압축"입니다. 지수족에서는 자연 충분 통계량이 바로 최소 충분 통계량이 됩니다. si-02에서 다룬 MLE는 최소 충분 통계량의 함수입니다.

---

## 핵심 정리

- **충분 통계량 $T(X)$는 모수에 대한 데이터의 모든 정보를 보존하는 요약이며, Fisher-Neyman 인수분해 정리로 찾습니다**
- **지수족 분포에서는 자연 충분 통계량이 모수 차원과 같은 차원의 최소 충분 통계량을 제공합니다**
- **완비성은 충분 통계량에 잉여 정보가 없음을 보장하며, UMVUE의 유일성 조건입니다**
- **Rao-Blackwell 정리는 불편 추정량을 충분 통계량에 조건부 기대하여 분산을 줄이는 체계적 방법입니다**
- **Lehmann-Scheffe 정리는 완비 충분 통계량의 불편 함수가 UMVUE임을 보장하며, 최적 추정의 완결 이론입니다**
