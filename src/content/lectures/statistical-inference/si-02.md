# 최대 우도 추정 (Maximum Likelihood Estimation)

## 왜 최대 우도 추정을 사용하는가

관측된 데이터를 가장 잘 설명하는 모수를 찾는 것은 통계적 추론의 핵심 과제입니다. 최대 우도 추정(MLE)은 "관측된 데이터가 나올 확률을 최대화하는 모수"를 선택하는 직관적이고 강력한 방법입니다. si-01에서 다룬 일치성, 효율성 등 바람직한 성질을 점근적으로 모두 만족하기 때문에 가장 널리 사용됩니다.

---

## 1. 우도 함수의 정의

관측 데이터 $x_1, x_2, \dots, x_n$이 i.i.d.이고 밀도(또는 질량) 함수 $f(x; \theta)$를 따를 때, **우도 함수(likelihood function)**는 다음과 같습니다.

$$L(\theta) = \prod_{i=1}^{n} f(x_i; \theta)$$

**로그 우도(log-likelihood)**는 계산의 편의를 위해 사용합니다.

$$\ell(\theta) = \sum_{i=1}^{n} \log f(x_i; \theta)$$

| 구분 | 확률 | 우도 |
|------|------|------|
| 고정 | $\theta$ | $x$ (데이터) |
| 변화 | $x$ | $\theta$ |
| 해석 | "이 모수에서 이 데이터가 나올 확률" | "이 데이터가 주어졌을 때 모수의 그럴듯함" |

> **핵심 직관**: 우도는 확률이 아닙니다. 모수 공간 위에서 정의된 함수이며, "이 모수값이 데이터를 얼마나 잘 설명하는가"를 측정합니다.

```python
import numpy as np
from scipy.stats import norm

# 정규 분포의 로그 우도 함수
data = np.array([2.1, 3.5, 2.8, 3.2, 2.9])

def log_likelihood(mu, sigma, data):
    return np.sum(norm.logpdf(data, loc=mu, scale=sigma))

# mu 후보에 대한 로그 우도 계산
mus = np.linspace(1, 5, 100)
ll_values = [log_likelihood(mu, 1.0, data) for mu in mus]
best_mu = mus[np.argmax(ll_values)]
print(f"최적 mu (그리드 탐색): {best_mu:.2f}, 표본 평균: {np.mean(data):.2f}")
```

---

## 2. MLE의 유도

MLE $\hat{\theta}_{MLE}$는 우도 함수를 최대화하는 모수값입니다.

$$\hat{\theta}_{MLE} = \arg\max_{\theta \in \Theta} L(\theta) = \arg\max_{\theta \in \Theta} \ell(\theta)$$

정규 조건 하에서 **스코어 방정식(score equation)**을 풀어 구합니다.

$$\frac{\partial \ell(\theta)}{\partial \theta} = 0$$

**예시**: 정규 분포 $N(\mu, \sigma^2)$에서

$$\hat{\mu}_{MLE} = \bar{X} = \frac{1}{n}\sum_{i=1}^n X_i, \quad \hat{\sigma}^2_{MLE} = \frac{1}{n}\sum_{i=1}^n (X_i - \bar{X})^2$$

동전 100번 던져 앞면 60번이면, 베르누이 분포의 MLE는 $\hat{p} = 60/100 = 0.6$입니다.

> **핵심 직관**: MLE는 "관측된 데이터를 가장 그럴듯하게 만드는 모수"를 선택합니다. 닫힌 해가 존재하지 않으면 수치적 최적화(Newton-Raphson 등)를 사용합니다.

```python
from scipy.optimize import minimize_scalar
from scipy.stats import bernoulli

# 베르누이 MLE
coin_data = np.array([1]*60 + [0]*40)  # 앞면 60, 뒷면 40

def neg_log_lik(p):
    if p <= 0 or p >= 1:
        return np.inf
    return -np.sum(bernoulli.logpmf(coin_data, p))

result = minimize_scalar(neg_log_lik, bounds=(0.01, 0.99), method='bounded')
print(f"MLE of p: {result.x:.4f}")  # 0.6
```

---

## 3. 정규성 조건 (Regularity Conditions)

MLE의 점근적 성질이 성립하려면 다음 **정규성 조건**이 필요합니다.

| 조건 | 설명 |
|------|------|
| 식별 가능성 | $\theta_1 \neq \theta_2 \Rightarrow P_{\theta_1} \neq P_{\theta_2}$ |
| 공통 지지 | $f(x; \theta)$의 지지가 $\theta$에 의존하지 않음 |
| 내점 조건 | 참 모수 $\theta_0$가 $\Theta$의 내점 |
| 미분 가능성 | $\log f(x; \theta)$가 $\theta$에 대해 3번 미분 가능 |
| 피셔 정보 유한 | $0 < I(\theta_0) < \infty$ |

> **핵심 직관**: 균등 분포 $U(0, \theta)$처럼 지지가 모수에 의존하는 경우, MLE는 존재하지만 점근 정규성이 성립하지 않습니다. 정규 조건은 "예쁜 결과"를 보장하기 위한 기술적 요구입니다.

---

## 4. MLE의 점근적 성질

정규 조건 하에서 MLE는 다음 세 가지 핵심 성질을 가집니다.

**(1) 일치성**: $\hat{\theta}_{MLE} \xrightarrow{P} \theta_0$

**(2) 점근 정규성**:

$$\sqrt{n}(\hat{\theta}_{MLE} - \theta_0) \xrightarrow{d} N(0, I(\theta_0)^{-1})$$

**(3) 점근 효율성**: 점근 분산이 Cramer-Rao 하한에 도달합니다.

$$\text{Var}(\hat{\theta}_{MLE}) \approx \frac{1}{n \cdot I(\theta_0)}$$

| 성질 | 의미 | si-01 연결 |
|------|------|-----------|
| 일치성 | $n \to \infty$이면 참값 수렴 | 일치성의 정의 |
| 점근 정규성 | 충분히 큰 $n$에서 정규 분포 | CI, 검정에 활용 |
| 점근 효율성 | CRLB 달성 | 효율성의 정의 |

> **핵심 직관**: MLE는 유한 표본에서는 편향될 수 있지만, 표본이 커지면 가장 효율적인 추정량으로 수렴합니다. si-08에서 점근 이론을 더 깊이 다룹니다.

```python
# MLE의 점근 정규성 시뮬레이션
from scipy.stats import expon

true_lambda = 2.0
n_sizes = [10, 50, 200, 1000]
n_sim = 5000

for n in n_sizes:
    mle_estimates = []
    for _ in range(n_sim):
        sample = np.random.exponential(1/true_lambda, n)
        mle_lambda = 1 / np.mean(sample)
        mle_estimates.append(mle_lambda)
    mle_arr = np.array(mle_estimates)
    print(f"n={n:4d}: 평균={mle_arr.mean():.3f}, "
          f"표준편차={mle_arr.std():.3f}, "
          f"이론적 SE={true_lambda/np.sqrt(n):.3f}")
```

---

## 5. 다변수 MLE와 관측 정보

모수가 벡터 $\boldsymbol{\theta} = (\theta_1, \dots, \theta_d)^T$일 때, 스코어 벡터와 피셔 정보 행렬은 다음과 같습니다.

$$\mathbf{S}(\boldsymbol{\theta}) = \nabla_{\boldsymbol{\theta}} \ell(\boldsymbol{\theta}), \quad \mathbf{I}(\boldsymbol{\theta}) = -E\left[\nabla^2_{\boldsymbol{\theta}} \ell(\boldsymbol{\theta})\right]$$

**관측 피셔 정보(observed Fisher information)**는 기대값 대신 관측값에서 직접 계산합니다.

$$\hat{\mathbf{I}}(\hat{\boldsymbol{\theta}}) = -\nabla^2_{\boldsymbol{\theta}} \ell(\boldsymbol{\theta})\Big|_{\boldsymbol{\theta}=\hat{\boldsymbol{\theta}}}$$

> **핵심 직관**: 관측 정보는 기대 정보의 플러그인 추정량이며, 실제 분석에서는 계산이 더 쉬운 관측 정보를 주로 사용합니다. 이는 si-07의 신뢰 구간 구축에 직접 활용됩니다.

```python
import statsmodels.api as sm

# 로지스틱 회귀에서의 MLE
np.random.seed(42)
n = 200
X = np.column_stack([np.ones(n), np.random.randn(n)])
beta_true = np.array([0.5, 1.2])
p = 1 / (1 + np.exp(-X @ beta_true))
y = np.random.binomial(1, p)

model = sm.Logit(y, X).fit(disp=0)
print(f"MLE: {model.params}")
print(f"표준 오차: {model.bse}")
print(f"관측 정보 행렬의 역행렬 (공분산):\n{model.cov_params()}")
```

---

## 6. MLE의 불변성과 한계

MLE의 **불변성(invariance)**: $\hat{\theta}$가 $\theta$의 MLE이면, 임의의 함수 $g$에 대해 $g(\hat{\theta})$는 $g(\theta)$의 MLE입니다.

$$\widehat{g(\theta)} = g(\hat{\theta}_{MLE})$$

예: $\hat{\sigma}^2_{MLE}$가 분산의 MLE이면, $\hat{\sigma}_{MLE} = \sqrt{\hat{\sigma}^2_{MLE}}$가 표준편차의 MLE입니다.

| 장점 | 한계 |
|------|------|
| 점근 최적성 | 소표본에서 편향 가능 |
| 불변성 | 정규 조건 필요 |
| 계산 용이 (로그 우도) | 다봉(multimodal) 우도에서 지역 최적 |
| 분포 가정만 필요 | 모델 오특정에 민감 |

> **핵심 직관**: MLE는 만능이 아닙니다. 소표본에서는 si-03에서 다룰 베이즈 추정이 더 나을 수 있고, 모델 오특정 시에는 비모수적 방법이 안전합니다.

---

## 7. 수치적 MLE: EM 알고리즘

닫힌 해가 없을 때, **EM(Expectation-Maximization) 알고리즘**이 자주 사용됩니다.

**E-step**: 현재 모수 $\theta^{(t)}$에서 잠재 변수의 기대값을 계산합니다.

$$Q(\theta | \theta^{(t)}) = E_{Z|X, \theta^{(t)}}[\log f(X, Z; \theta)]$$

**M-step**: $Q$ 함수를 최대화하여 모수를 갱신합니다.

$$\theta^{(t+1)} = \arg\max_\theta Q(\theta | \theta^{(t)})$$

```python
from sklearn.mixture import GaussianMixture

# 가우시안 혼합 모델의 EM 알고리즘
data = np.concatenate([
    np.random.normal(-2, 0.5, 150),
    np.random.normal(3, 1.0, 250)
]).reshape(-1, 1)

gmm = GaussianMixture(n_components=2, random_state=42).fit(data)
print(f"추정된 평균: {gmm.means_.flatten()}")
print(f"추정된 분산: {gmm.covariances_.flatten()}")
print(f"혼합 비율: {gmm.weights_}")
```

> **핵심 직관**: EM은 직접 최적화가 어려운 불완전 데이터 문제에서 우도를 단조 증가시키며 MLE로 수렴합니다. 단, 지역 최적에 빠질 수 있으므로 여러 초기값으로 시작해야 합니다.

---

## 핵심 정리

- **우도 함수 $L(\theta) = \prod f(x_i; \theta)$는 "데이터가 주어졌을 때 모수의 그럴듯함"을 측정하며, 확률이 아닙니다**
- **MLE는 스코어 방정식 $\partial \ell / \partial \theta = 0$을 풀어 구하며, 불변성을 가집니다**
- **정규 조건 하에서 MLE는 일치적이고 점근 정규이며 점근 효율적입니다: $\sqrt{n}(\hat{\theta} - \theta_0) \to N(0, I^{-1})$**
- **피셔 정보 $I(\theta)$는 추정의 정밀도를 결정하며, 관측 정보는 실제 추론에서 이를 대체합니다**
- **EM 알고리즘은 잠재 변수가 있는 모델에서 MLE를 반복적으로 구하는 표준 도구입니다**
