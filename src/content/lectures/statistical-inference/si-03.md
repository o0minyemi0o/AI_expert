# 베이즈 추정 (Bayesian Estimation)

## 왜 베이즈 추정을 사용하는가

si-02에서 다룬 MLE는 데이터만으로 모수를 추정합니다. 하지만 사전 지식이 있을 때 이를 활용하지 않는 것은 정보의 낭비입니다. 베이즈 추정은 사전 분포(prior)를 통해 기존 지식을 반영하고, 데이터가 들어오면 사후 분포(posterior)로 갱신하는 체계적 프레임워크를 제공합니다. 소표본에서 특히 강력하며, 불확실성의 완전한 표현이 가능합니다.

---

## 1. 베이즈 정리의 모수 추정 적용

pt-02에서 다룬 베이즈 정리를 모수 추정에 적용하면 다음과 같습니다.

$$p(\theta | x) = \frac{f(x | \theta) \cdot \pi(\theta)}{\int f(x | \theta) \cdot \pi(\theta) \, d\theta} \propto f(x | \theta) \cdot \pi(\theta)$$

| 구성 요소 | 기호 | 의미 |
|-----------|------|------|
| 사전 분포 (Prior) | $\pi(\theta)$ | 데이터 관측 전 모수에 대한 믿음 |
| 우도 (Likelihood) | $f(x \| \theta)$ | 모수가 주어졌을 때 데이터의 확률 |
| 사후 분포 (Posterior) | $p(\theta \| x)$ | 데이터 관측 후 모수에 대한 갱신된 믿음 |
| 주변 우도 (Evidence) | $m(x)$ | 정규화 상수 |

> **핵심 직관**: 베이즈 추정의 핵심은 "사후 $\propto$ 우도 $\times$ 사전"입니다. 데이터가 많아지면 우도가 지배하여 사전의 영향은 사라집니다.

```python
import numpy as np
from scipy.stats import beta, binom

# 동전 던지기의 베이즈 추정
# 사전: Beta(2, 2), 데이터: 100번 중 60번 앞면
a_prior, b_prior = 2, 2
n_trials, n_heads = 100, 60

# 사후: Beta(a_prior + n_heads, b_prior + n_trials - n_heads)
a_post = a_prior + n_heads
b_post = b_prior + (n_trials - n_heads)
posterior = beta(a_post, b_post)

print(f"사후 평균: {posterior.mean():.4f}")
print(f"사후 중앙값: {posterior.median():.4f}")
print(f"95% 신용 구간: ({posterior.ppf(0.025):.4f}, {posterior.ppf(0.975):.4f})")
print(f"MLE: {n_heads/n_trials:.4f}")
```

---

## 2. 켤레 사전 분포 (Conjugate Priors)

**켤레 사전 분포**는 사전과 사후가 같은 분포족에 속하도록 하는 편리한 사전입니다.

| 우도 | 켤레 사전 | 사후 |
|------|----------|------|
| $\text{Bernoulli}(p)$ | $\text{Beta}(\alpha, \beta)$ | $\text{Beta}(\alpha + \sum x_i, \beta + n - \sum x_i)$ |
| $\text{Poisson}(\lambda)$ | $\text{Gamma}(\alpha, \beta)$ | $\text{Gamma}(\alpha + \sum x_i, \beta + n)$ |
| $N(\mu, \sigma^2_0)$ ($\sigma^2_0$ 알려짐) | $N(\mu_0, \tau^2_0)$ | $N(\mu_n, \tau^2_n)$ |
| $N(\mu_0, \sigma^2)$ ($\mu_0$ 알려짐) | $\text{IG}(\alpha, \beta)$ | $\text{IG}(\alpha + n/2, \beta + \text{SS}/2)$ |

정규-정규 모델에서 사후 평균은 사전과 MLE의 **정밀도 가중 평균**입니다.

$$\mu_n = \frac{\frac{1}{\tau_0^2}\mu_0 + \frac{n}{\sigma^2}\bar{x}}{\frac{1}{\tau_0^2} + \frac{n}{\sigma^2}}$$

> **핵심 직관**: 켤레 사전은 계산의 편의를 위한 선택이지, 반드시 최선의 사전은 아닙니다. 사전의 초모수(hyperparameter)가 "가상 데이터"의 역할을 한다고 이해하면 직관적입니다.

---

## 3. MAP 추정과 점 추정

**MAP(Maximum A Posteriori)** 추정량은 사후 분포의 최빈값입니다.

$$\hat{\theta}_{MAP} = \arg\max_\theta p(\theta | x) = \arg\max_\theta [\ell(\theta) + \log \pi(\theta)]$$

| 추정량 | 정의 | 손실 함수 |
|--------|------|----------|
| 사후 평균 | $E[\theta \| x]$ | 제곱 손실 |
| 사후 중앙값 | $\text{Median}[\theta \| x]$ | 절대 손실 |
| MAP | $\arg\max p(\theta \| x)$ | 0-1 손실 |

MAP와 MLE의 관계:

$$\hat{\theta}_{MAP} = \hat{\theta}_{MLE} + \frac{\text{사전의 영향}}{n}$$

> **핵심 직관**: MAP는 MLE에 정규화(regularization)를 더한 것과 동일합니다. 정규 사전은 L2 정규화(Ridge), 라플라스 사전은 L1 정규화(Lasso)에 대응합니다. cm-12에서 배운 정규화의 베이즈적 해석입니다.

```python
from scipy.optimize import minimize_scalar

# MAP vs MLE 비교
# 데이터: 10번 중 7번 앞면
n, k = 10, 7

# MLE
mle = k / n

# MAP with Beta(2, 2) prior
def neg_log_posterior(p):
    if p <= 0 or p >= 1:
        return np.inf
    log_lik = k * np.log(p) + (n - k) * np.log(1 - p)
    log_prior = np.log(beta.pdf(p, 2, 2))
    return -(log_lik + log_prior)

result = minimize_scalar(neg_log_posterior, bounds=(0.01, 0.99), method='bounded')
print(f"MLE: {mle:.4f}, MAP: {result.x:.4f}")
# MAP가 0.5 방향으로 축소됨
```

---

## 4. 베이즈 위험과 최적성

**베이즈 위험(Bayes risk)**은 사전 분포에 대한 평균 위험입니다.

$$r(\pi, \hat{\theta}) = \int R(\theta, \hat{\theta}) \pi(\theta) \, d\theta = \int \int L(\theta, \hat{\theta}(x)) f(x|\theta) \pi(\theta) \, dx \, d\theta$$

**베이즈 추정량**은 사후 기대 손실을 최소화합니다.

$$\hat{\theta}_B = \arg\min_a E[L(\theta, a) | x]$$

| 손실 | 베이즈 추정량 | 공식 |
|------|-------------|------|
| $(\theta - a)^2$ | 사후 평균 | $E[\theta \| x]$ |
| $\|\theta - a\|$ | 사후 중앙값 | $\text{Med}[\theta \| x]$ |
| $w(\theta)(\theta - a)^2$ | 가중 사후 평균 | $E[w(\theta)\theta \| x] / E[w(\theta) \| x]$ |

> **핵심 직관**: 베이즈 추정량은 주어진 손실 함수와 사전 분포 하에서 최적입니다. "최적"의 의미는 손실 함수와 사전에 따라 달라지므로, 이들의 선택이 매우 중요합니다.

---

## 5. 무정보 사전 분포

사전 지식이 없을 때 사용하는 **무정보 사전(non-informative prior)**:

**(1) 균등 사전**: $\pi(\theta) \propto 1$

**(2) 제프리스 사전**: $\pi_J(\theta) \propto \sqrt{I(\theta)}$

제프리스 사전은 모수화에 불변(invariant)하다는 장점이 있습니다.

$$\pi_J(\phi) \propto \sqrt{I(\phi)} \quad \text{(재모수화 후에도 동일한 추론)}$$

| 분포 | 제프리스 사전 |
|------|-------------|
| $\text{Bernoulli}(p)$ | $\pi(p) \propto p^{-1/2}(1-p)^{-1/2}$ = Beta(1/2, 1/2) |
| $N(\mu, \sigma^2)$ ($\sigma$ 알려짐) | $\pi(\mu) \propto 1$ |
| $N(\mu, \sigma^2)$ | $\pi(\mu, \sigma) \propto 1/\sigma^2$ |

> **핵심 직관**: 완전히 "무정보적"인 사전은 존재하지 않습니다. 제프리스 사전은 모수화에 불변이라는 원칙에 기반하지만, 고차원에서는 문제가 될 수 있습니다.

```python
# 제프리스 사전 vs 균등 사전 비교
n_obs, k_obs = 5, 4  # 소표본

# 균등 사전: Beta(1, 1) -> 사후 Beta(5, 2)
post_uniform = beta(1 + k_obs, 1 + n_obs - k_obs)
# 제프리스 사전: Beta(0.5, 0.5) -> 사후 Beta(4.5, 1.5)
post_jeffreys = beta(0.5 + k_obs, 0.5 + n_obs - k_obs)

print(f"균등 사전 -> 사후 평균: {post_uniform.mean():.4f}")
print(f"제프리스 사전 -> 사후 평균: {post_jeffreys.mean():.4f}")
print(f"MLE: {k_obs/n_obs:.4f}")
```

---

## 6. 미니맥스 추정

**미니맥스(minimax)** 추정량은 최악의 경우 위험을 최소화합니다.

$$\hat{\theta}_{MM} = \arg\min_{\hat{\theta}} \max_\theta R(\theta, \hat{\theta})$$

미니맥스 추정량과 베이즈 추정량의 관계:

| 정리 | 내용 |
|------|------|
| 미니맥스 정리 | 일정 위험(constant risk) 베이즈 추정량은 미니맥스 |
| 최소 유리 사전 | 미니맥스 추정량은 "최악의" 사전에 대한 베이즈 추정량 |

> **핵심 직관**: 미니맥스는 "자연이 적대적으로 모수를 선택한다"는 가정 하에서의 최적 전략입니다. 보수적이지만, 최악의 상황에 대비한다는 점에서 가치가 있습니다.

```python
# 미니맥스 추정 예시: 정규 평균 추정
# 축소 추정량 (James-Stein)
np.random.seed(42)
p = 10  # 차원
true_theta = np.random.normal(0, 3, p)
X = true_theta + np.random.normal(0, 1, p)

# MLE
mle = X.copy()
# James-Stein
shrinkage = 1 - (p - 2) / np.sum(X**2)
js_est = max(0, shrinkage) * X

mse_mle = np.sum((mle - true_theta)**2)
mse_js = np.sum((js_est - true_theta)**2)
print(f"MLE MSE: {mse_mle:.4f}, James-Stein MSE: {mse_js:.4f}")
```

---

## 7. 베이즈 추정의 장단점과 빈도주의와의 비교

| 관점 | 빈도주의 (MLE) | 베이즈 |
|------|---------------|--------|
| 모수 | 고정된 미지수 | 확률 변수 |
| 불확실성 표현 | 신뢰 구간 | 신용 구간 (credible interval) |
| 사전 정보 | 사용하지 않음 | 사전 분포로 반영 |
| 소표본 성능 | 불안정할 수 있음 | 사전이 안정화 |
| 계산 | 최적화 문제 | 적분 문제 |
| 해석 | "반복 실험에서 95%" | "$\theta$가 구간에 있을 확률 95%" |

> **핵심 직관**: 빈도주의와 베이즈는 대립하는 것이 아니라 상호 보완적입니다. 데이터가 많으면 두 접근법은 수렴합니다. si-09에서 다룰 MCMC는 사후 분포의 계산 문제를 해결하는 핵심 도구입니다.

---

## 핵심 정리

- **베이즈 추정은 사후 $\propto$ 우도 $\times$ 사전 원리에 따라 모수의 불확실성을 사후 분포로 완전히 표현합니다**
- **켤레 사전 분포는 사전-사후가 같은 분포족을 유지하여 닫힌 해를 가능하게 하며, 초모수는 가상 데이터로 해석됩니다**
- **MAP 추정은 MLE + 로그 사전 최대화이며, 정규화(Ridge/Lasso)의 베이즈적 해석을 제공합니다**
- **제프리스 사전은 모수화 불변인 무정보 사전이며, 사전 선택은 베이즈 추론의 핵심적 의사결정입니다**
- **미니맥스 추정량은 최악의 위험을 최소화하며, 일정 위험 베이즈 추정량과 연결됩니다**
