# 우도비 검정 (Likelihood Ratio Test)

## 왜 우도비 검정이 필요한가

si-05에서 다룬 Neyman-Pearson 보조정리는 단순 가설에만 적용됩니다. 실제 분석에서는 복합 가설($H_0: \theta \in \Theta_0$)을 검정해야 하고, UMP 검정이 존재하지 않는 경우가 대부분입니다. 우도비 검정(LRT)은 이러한 상황에서 범용적으로 적용 가능한 검정 방법이며, Wilks 정리를 통해 점근적 분포를 쉽게 구할 수 있습니다.

---

## 1. 일반화 우도비 통계량

**일반화 우도비(Generalized Likelihood Ratio)** 통계량은 다음과 같이 정의됩니다.

$$\Lambda(x) = \frac{\sup_{\theta \in \Theta_0} L(\theta; x)}{\sup_{\theta \in \Theta} L(\theta; x)} = \frac{L(\hat{\theta}_0; x)}{L(\hat{\theta}; x)}$$

여기서 $\hat{\theta}_0$은 $\Theta_0$ 위에서의 제한 MLE, $\hat{\theta}$는 전체 $\Theta$ 위에서의 MLE입니다.

| 구성 요소 | 의미 |
|-----------|------|
| 분자 | $H_0$ 하에서 데이터의 최대 우도 |
| 분모 | 제한 없이 데이터의 최대 우도 |
| $0 \leq \Lambda \leq 1$ | 1에 가까우면 $H_0$ 적합, 0에 가까우면 $H_0$ 부적합 |

기각역: $\Lambda(x) \leq c$ (또는 동등하게 $-2\log\Lambda \geq c'$)

> **핵심 직관**: 우도비는 "$H_0$이 데이터를 얼마나 잘 설명하는가?"를 "$H_0$ 없이 얼마나 잘 설명하는가?"와 비교합니다. 비가 작으면 $H_0$이 데이터를 잘 설명하지 못한다는 의미입니다.

```python
import numpy as np
from scipy.stats import norm, chi2

# 정규 분포 평균의 LRT
# H0: mu = 0 vs H1: mu != 0 (sigma 알려짐, sigma=1)
np.random.seed(42)
n = 30
data = np.random.normal(0.5, 1, n)

# 우도비 통계량 계산
mu_hat = np.mean(data)  # 비제한 MLE
mu_0 = 0                # H0 하의 값

log_lik_full = np.sum(norm.logpdf(data, mu_hat, 1))
log_lik_null = np.sum(norm.logpdf(data, mu_0, 1))
neg2_log_lambda = -2 * (log_lik_null - log_lik_full)

p_value = 1 - chi2.cdf(neg2_log_lambda, df=1)
print(f"-2 log Lambda = {neg2_log_lambda:.4f}")
print(f"p-value = {p_value:.4f}")
```

---

## 2. Wilks 정리

**Wilks 정리**: 정규 조건 하에서, $n \to \infty$일 때

$$-2\log\Lambda(X) \xrightarrow{d} \chi^2_r$$

여기서 $r = \dim(\Theta) - \dim(\Theta_0)$은 $H_0$에 의해 제한되는 모수의 수입니다.

| 조건 | 설명 |
|------|------|
| 정규 조건 | si-02의 MLE 정규성 조건과 동일 |
| $\theta_0$가 $\Theta$의 내점 | 경계에서는 성립하지 않음 |
| 충분히 큰 $n$ | 점근적 결과이므로 소표본 주의 |

**자유도 결정**:

| 가설 | $\dim(\Theta)$ | $\dim(\Theta_0)$ | $r$ |
|------|---------------|-----------------|-----|
| $H_0: \mu = \mu_0$ ($\sigma$ 미지) | 2 | 1 | 1 |
| $H_0: \mu_1 = \mu_2$ (등분산) | 3 | 2 | 1 |
| $H_0: \Sigma = \Sigma_0$ ($p \times p$) | $p(p+1)/2$ | 0 | $p(p+1)/2$ |

> **핵심 직관**: Wilks 정리 덕분에 정확한 귀무 분포를 모르더라도, 큰 표본에서 카이제곱 분포를 사용해 p-value를 근사할 수 있습니다. 이것이 LRT의 실용적 가치입니다.

```python
# Wilks 정리 검증 시뮬레이션
from scipy.stats import chi2

n = 100
n_sim = 10000
neg2_log_lambdas = []

for _ in range(n_sim):
    data = np.random.normal(0, 1, n)  # H0: mu=0 참
    mu_hat = np.mean(data)
    ll_full = np.sum(norm.logpdf(data, mu_hat, 1))
    ll_null = np.sum(norm.logpdf(data, 0, 1))
    neg2_log_lambdas.append(-2 * (ll_null - ll_full))

# chi2(1)과 비교
from scipy.stats import kstest
stat, p = kstest(neg2_log_lambdas, 'chi2', args=(1,))
print(f"KS 검정 p-value: {p:.4f} (높으면 chi2(1)에 적합)")
print(f"이론적 평균=1, 시뮬레이션 평균={np.mean(neg2_log_lambdas):.4f}")
```

---

## 3. 다양한 우도비 검정의 예시

### 3.1 정규 분포 분산 검정

$H_0: \sigma^2 = \sigma_0^2$ vs $H_1: \sigma^2 \neq \sigma_0^2$

$$-2\log\Lambda = n\log\frac{\hat{\sigma}^2}{\sigma_0^2} + n\left(\frac{\sigma_0^2}{\hat{\sigma}^2} - 1\right)$$

### 3.2 이표본 평균 비교 (등분산 가정)

$H_0: \mu_1 = \mu_2$ vs $H_1: \mu_1 \neq \mu_2$

LRT 통계량은 **두 표본 t-검정 통계량의 제곱**과 단조 관계입니다.

### 3.3 독립성 검정 (분할표)

$$-2\log\Lambda = 2\sum_{i,j} O_{ij} \log\frac{O_{ij}}{E_{ij}} \xrightarrow{d} \chi^2_{(r-1)(c-1)}$$

> **핵심 직관**: 많은 고전적 검정(t-검정, 카이제곱 검정 등)이 사실 우도비 검정의 특수한 경우이거나, 우도비 검정과 점근적으로 동등합니다.

```python
from scipy.stats import chi2_contingency

# 독립성 검정 (분할표의 LRT)
observed = np.array([[30, 10], [15, 45]])
g_stat, p_val, dof, expected = chi2_contingency(observed, lambda_="log-likelihood")
print(f"G-통계량 (-2 log Lambda): {g_stat:.4f}")
print(f"자유도: {dof}")
print(f"p-value: {p_val:.4f}")
```

---

## 4. 점근적으로 동등한 검정들

세 가지 검정이 점근적으로 동등합니다.

**(1) 우도비 검정(LRT)**:

$$W_{LR} = -2[\ell(\hat{\theta}_0) - \ell(\hat{\theta})]$$

**(2) 왈드 검정(Wald test)**:

$$W_W = (\hat{\theta} - \theta_0)^T [\hat{I}(\hat{\theta})](\hat{\theta} - \theta_0)$$

**(3) 스코어 검정(Score/Rao test)**:

$$W_S = S(\theta_0)^T [I(\theta_0)]^{-1} S(\theta_0)$$

| 검정 | 필요한 계산 | 장점 |
|------|-----------|------|
| LRT | 제한/비제한 MLE 모두 | 직관적 |
| Wald | 비제한 MLE만 | 모델 적합 한 번 |
| Score | 제한 MLE만 ($\theta_0$ 평가) | $H_0$ 하에서만 계산 |

세 검정 모두 $H_0$ 하에서 $\chi^2_r$ 분포를 따릅니다.

> **핵심 직관**: LRT는 "두 지점의 높이 차이", Wald는 "정상에서의 거리", Score는 "출발점의 기울기"로 비유할 수 있습니다. 점근적으로 같지만, 유한 표본에서는 차이가 있을 수 있습니다.

```python
import statsmodels.api as sm

# 세 가지 검정 비교: 로지스틱 회귀
np.random.seed(42)
n = 200
X = np.column_stack([np.ones(n), np.random.randn(n), np.random.randn(n)])
beta_true = np.array([0, 0.8, 0])  # beta_2 = 0 검정
p = 1 / (1 + np.exp(-X @ beta_true))
y = np.random.binomial(1, p)

# 전체 모델
full_model = sm.Logit(y, X).fit(disp=0)
# 제한 모델 (beta_2 = 0)
restricted_model = sm.Logit(y, X[:, :2]).fit(disp=0)

# LRT
lrt_stat = -2 * (restricted_model.llf - full_model.llf)
lrt_p = 1 - chi2.cdf(lrt_stat, df=1)

# Wald
wald_stat = (full_model.params[2] / full_model.bse[2])**2
wald_p = 1 - chi2.cdf(wald_stat, df=1)

print(f"LRT:   stat={lrt_stat:.4f}, p={lrt_p:.4f}")
print(f"Wald:  stat={wald_stat:.4f}, p={wald_p:.4f}")
```

---

## 5. 복합 가설과 모델 선택

우도비 검정은 **내포 모델(nested models)** 비교에 자연스럽게 사용됩니다.

$$M_0: Y \sim f(x; \theta_0) \subset M_1: Y \sim f(x; \theta)$$

$$-2\log\Lambda = -2[\ell_{M_0} - \ell_{M_1}] \sim \chi^2_{p_1 - p_0}$$

| 정보 기준 | 공식 | LRT와의 관계 |
|-----------|------|-------------|
| AIC | $-2\ell + 2p$ | LRT + 모수 수 벌칙 |
| BIC | $-2\ell + p\log n$ | 더 강한 벌칙 |

> **핵심 직관**: LRT는 "더 복잡한 모델이 유의하게 더 나은가?"를 묻습니다. AIC/BIC는 예측 성능까지 고려한 확장으로, cm-12에서 배운 모델 선택과 직접 연결됩니다.

```python
# 내포 모델 비교: 다항 회귀
from sklearn.preprocessing import PolynomialFeatures

np.random.seed(42)
n = 100
x = np.random.uniform(0, 5, n)
y = 2 + 0.5*x + 0.3*x**2 + np.random.normal(0, 1, n)

# 선형 모델 vs 이차 모델
X1 = sm.add_constant(x)
X2 = sm.add_constant(np.column_stack([x, x**2]))

m1 = sm.OLS(y, X1).fit()
m2 = sm.OLS(y, X2).fit()

lrt = -2 * (m1.llf - m2.llf)
p = 1 - chi2.cdf(lrt, df=1)
print(f"LRT 통계량: {lrt:.4f}, p-value: {p:.4f}")
print(f"AIC: 선형={m1.aic:.2f}, 이차={m2.aic:.2f}")
print(f"BIC: 선형={m1.bic:.2f}, 이차={m2.bic:.2f}")
```

---

## 6. LRT의 유한 표본 보정

Wilks 정리는 점근적 결과이므로, 소표본에서는 보정이 필요할 수 있습니다.

**Bartlett 보정**: $-2\log\Lambda$에 스케일링 인자를 곱하여 $\chi^2$ 근사를 개선합니다.

$$W^* = \frac{-2\log\Lambda}{1 + a/n} \quad \text{where } a = E[-2\log\Lambda] - r + O(1/n)$$

| 상황 | 권장 방법 |
|------|---------|
| $n > 30p$ | Wilks 점근 이론 충분 |
| $n < 30p$ | Bartlett 보정 또는 정확 검정 |
| 매우 작은 $n$ | 순열 검정, 부트스트랩 |

> **핵심 직관**: 점근 이론은 편리하지만, 표본이 작을 때는 실제 귀무 분포가 $\chi^2$에서 크게 벗어날 수 있습니다. si-07에서 다룰 부트스트랩이 이 문제의 대안입니다.

---

## 7. 비정규 경계에서의 LRT

$H_0$의 모수값이 모수 공간의 **경계(boundary)**에 있으면 Wilks 정리가 표준 형태로 성립하지 않습니다.

**예시**: $H_0: \sigma^2 = 0$ (분산 성분 검정)

이 경우 점근 분포는 $\chi^2$ 혼합이 됩니다.

$$-2\log\Lambda \xrightarrow{d} \frac{1}{2}\chi^2_0 + \frac{1}{2}\chi^2_1$$

여기서 $\chi^2_0$은 점질량(point mass at 0)입니다.

| 경계 유형 | 점근 분포 |
|-----------|---------|
| 내점 | $\chi^2_r$ (표준) |
| 단면 경계 | $\frac{1}{2}\chi^2_0 + \frac{1}{2}\chi^2_1$ |
| 원뿔 제약 | 카이제곱 혼합의 일반형 |

> **핵심 직관**: 경계 문제는 분산 성분 모델, 혼합 모델의 성분 수 검정 등에서 자주 나타납니다. 이때 표준 $\chi^2$ 임계값을 사용하면 보수적이거나 반보수적인 결과를 얻게 됩니다.

---

## 핵심 정리

- **일반화 우도비 $\Lambda = L(\hat{\theta}_0) / L(\hat{\theta})$는 $H_0$의 적합도를 비제한 모델과 비교하는 범용 검정 통계량입니다**
- **Wilks 정리에 의해 $-2\log\Lambda \xrightarrow{d} \chi^2_r$이며, $r$은 $H_0$에 의해 제한되는 모수의 수입니다**
- **LRT, Wald 검정, Score 검정은 점근적으로 동등하지만, 계산 방식과 유한 표본 성능이 다릅니다**
- **내포 모델 비교에 LRT를 적용하면 AIC/BIC와 같은 모델 선택 기준과 자연스럽게 연결됩니다**
- **모수가 경계에 있거나 소표본인 경우, 표준 Wilks 점근 이론이 성립하지 않으므로 보정이나 대안적 방법이 필요합니다**
