# 변분 추론 (Variational Inference)

## 왜 변분 추론이 필요한가

si-09에서 다룬 MCMC는 사후 분포의 정확한 표본을 제공하지만, 대규모 데이터와 복잡한 모델에서는 계산 비용이 감당하기 어렵습니다. 변분 추론(VI)은 사후 분포의 근사를 최적화 문제로 변환하여, 속도와 확장성을 크게 향상시킵니다. 딥러닝의 VAE(Variational Autoencoder)부터 대규모 베이즈 모델까지, 현대 기계학습에서 핵심적인 역할을 합니다.

---

## 1. 변분 추론의 기본 아이디어

사후 분포 $p(\theta|x)$를 다루기 쉬운 분포족 $\mathcal{Q}$에서 가장 가까운 분포 $q^*(\theta)$로 근사합니다.

$$q^*(\theta) = \arg\min_{q \in \mathcal{Q}} \text{KL}(q(\theta) \| p(\theta|x))$$

**KL 발산(Kullback-Leibler divergence)**:

$$\text{KL}(q \| p) = \int q(\theta) \log \frac{q(\theta)}{p(\theta|x)} \, d\theta \geq 0$$

| 구분 | MCMC | VI |
|------|------|-----|
| 목표 | 사후 분포의 정확한 표본 | 사후 분포의 최적 근사 |
| 접근 방식 | 표본 추출 (확률적) | 최적화 (결정적) |
| 정확도 | 점근적으로 정확 | 근사적 (분포족 $\mathcal{Q}$에 제한) |
| 속도 | 느림 | 빠름 |
| 수렴 판정 | 어려움 ($\hat{R}$, ESS) | ELBO 모니터링 |
| 확장성 | 제한적 | 대규모 데이터에 적합 |

> **핵심 직관**: VI는 "사후 분포에서 표본을 뽑는 대신, 사후 분포를 닮은 쉬운 분포를 찾는다"는 발상입니다. 근사의 질은 분포족 $\mathcal{Q}$의 표현력에 의존합니다.

---

## 2. ELBO (Evidence Lower Bound)

KL 발산을 직접 최소화할 수 없으므로 (분모에 $p(x)$ 필요), 동등한 문제인 **ELBO 최대화**를 수행합니다.

$$\log p(x) = \text{ELBO}(q) + \text{KL}(q \| p)$$

$$\text{ELBO}(q) = E_q[\log p(x, \theta)] - E_q[\log q(\theta)]$$

또는 동등하게:

$$\text{ELBO}(q) = \underbrace{E_q[\log p(x|\theta)]}_{\text{재구성 (적합도)}} - \underbrace{\text{KL}(q(\theta) \| \pi(\theta))}_{\text{정규화 (복잡도 벌칙)}}$$

| 항 | 역할 | 최적화 방향 |
|----|------|-----------|
| $E_q[\log p(x\|\theta)]$ | 데이터 적합도 | 최대화 |
| $\text{KL}(q \| \pi)$ | 사전 분포에 대한 복잡도 | 최소화 |
| ELBO | $\log p(x)$의 하한 | 최대화 |

> **핵심 직관**: ELBO = 적합도 - 복잡도입니다. 이는 si-03의 MAP 추정(우도 + 사전)의 분포 수준 일반화이며, cm-12에서 다룬 정규화와 같은 구조입니다.

```python
import numpy as np
from scipy.stats import norm

# ELBO 계산 예시: 정규 사후 분포 근사
# 참 사후: p(mu|x) proportional to N(mu; x_bar, sigma^2/n) * N(mu; 0, tau^2)
# 변분 분포: q(mu) = N(mu; m, s^2)

n = 20
sigma = 1.0
tau = 2.0
x_bar = 2.5

# 참 사후 모수 (해석적)
prec_post = n/sigma**2 + 1/tau**2
mu_post = (n*x_bar/sigma**2) / prec_post
sigma_post = 1/np.sqrt(prec_post)

def compute_elbo(m, log_s, x_bar, n, sigma, tau):
    s = np.exp(log_s)
    # E_q[log p(x|mu)] = -n/(2*sigma^2) * (s^2 + (m - x_bar)^2) + const
    recon = -n/(2*sigma**2) * (s**2 + (m - x_bar)**2)
    # KL(q || prior) = KL(N(m,s^2) || N(0, tau^2))
    kl = 0.5 * (s**2/tau**2 + m**2/tau**2 - 1 - 2*log_s + 2*np.log(tau))
    return recon - kl

# 최적 변분 모수 (경사 상승)
m, log_s = 0.0, 0.0
lr = 0.01
for i in range(2000):
    # 수치 미분
    eps = 1e-5
    grad_m = (compute_elbo(m+eps, log_s, x_bar, n, sigma, tau) -
              compute_elbo(m-eps, log_s, x_bar, n, sigma, tau)) / (2*eps)
    grad_log_s = (compute_elbo(m, log_s+eps, x_bar, n, sigma, tau) -
                  compute_elbo(m, log_s-eps, x_bar, n, sigma, tau)) / (2*eps)
    m += lr * grad_m
    log_s += lr * grad_log_s

print(f"변분 추론: m={m:.4f}, s={np.exp(log_s):.4f}")
print(f"참 사후:   mu={mu_post:.4f}, sigma={sigma_post:.4f}")
```

---

## 3. 평균장 근사 (Mean-Field Approximation)

**평균장 변분 추론**은 변분 분포를 완전히 인수분해하는 가정입니다.

$$q(\boldsymbol{\theta}) = \prod_{j=1}^{d} q_j(\theta_j)$$

각 인자의 최적 해는 **좌표 상승(coordinate ascent)**으로 구합니다.

$$\log q_j^*(\theta_j) = E_{q_{-j}}[\log p(\boldsymbol{\theta}, x)] + \text{const}$$

여기서 $q_{-j}$는 $j$ 번째를 제외한 나머지 변분 인자들입니다.

| 장점 | 단점 |
|------|------|
| 계산 단순 | 변수 간 상관 무시 |
| 수렴 보장 (좌표 상승) | 사후 분산 과소 추정 |
| 고차원에서도 적용 | 다봉 분포 포착 불가 |

> **핵심 직관**: 평균장은 "모든 모수가 독립"이라고 가정합니다. 실제 사후에서 모수 간 강한 상관이 있으면, 이 근사는 분산을 과소 추정하고 왜곡된 결과를 줄 수 있습니다.

```python
# 평균장 VI: 정규 분포의 mu, sigma^2 추론
# CAVI (Coordinate Ascent VI)

np.random.seed(42)
true_mu, true_sigma2 = 3.0, 4.0
data = np.random.normal(true_mu, np.sqrt(true_sigma2), 50)
n = len(data)
x_bar = np.mean(data)
x_sq_bar = np.mean(data**2)

# 사전: mu ~ N(0, 100), sigma^2 ~ IG(1, 1)
mu_0, kappa_0 = 0, 0.01  # 넓은 사전
a_0, b_0 = 1, 1

# 변분 모수 초기화
m_mu = 0.0
s2_mu = 1.0
a_q = a_0 + n/2
b_q = 1.0

for iteration in range(100):
    # q(mu) = N(m_mu, s2_mu) 업데이트
    E_inv_sigma2 = a_q / b_q
    s2_mu = 1 / (kappa_0 + n * E_inv_sigma2)
    m_mu = s2_mu * (kappa_0 * mu_0 + n * x_bar * E_inv_sigma2)

    # q(sigma^2) = IG(a_q, b_q) 업데이트
    E_mu = m_mu
    E_mu2 = s2_mu + m_mu**2
    b_q = b_0 + 0.5 * (np.sum(data**2) - 2*E_mu*np.sum(data) + n*E_mu2)

print(f"q(mu): 평균={m_mu:.4f} (참값={true_mu})")
print(f"q(sigma^2): 평균={b_q/(a_q-1):.4f} (참값={true_sigma2})")
```

---

## 4. 확률적 변분 추론 (SVI)

대규모 데이터에서 ELBO의 그래디언트를 미니배치로 추정하는 **확률적 변분 추론(Stochastic VI)**:

$$\nabla_\phi \text{ELBO} \approx \frac{N}{B} \sum_{i \in \mathcal{B}} \nabla_\phi \log p(x_i | \theta^{(s)}) - \nabla_\phi \text{KL}(q_\phi \| \pi)$$

**재모수화 트릭 (Reparameterization trick)**: 그래디언트의 분산을 줄이는 핵심 기법입니다.

$$\theta = \mu + \sigma \cdot \epsilon, \quad \epsilon \sim N(0, 1)$$

$$\nabla_\phi E_{q_\phi}[f(\theta)] = E_\epsilon[\nabla_\phi f(\mu + \sigma \epsilon)]$$

| 그래디언트 추정 | 분산 | 적용 범위 |
|------------|------|---------|
| Score function (REINFORCE) | 높음 | 이산/연속 모두 |
| 재모수화 트릭 | 낮음 | 연속 잠재 변수 |
| 제어 변량 | 중간 | 일반적 |

> **핵심 직관**: 재모수화 트릭은 "확률적 노드를 결정적 노드로 변환"하여 역전파가 가능하게 합니다. 이것이 VAE(Variational Autoencoder)의 핵심이며, 딥러닝과 베이즈의 접점입니다.

```python
# 재모수화 트릭을 이용한 변분 추론 (자동 미분 대용으로 수치 미분)
def elbo_reparameterized(params, data, n_mc=10):
    mu, log_sigma = params
    sigma = np.exp(log_sigma)
    elbo = 0
    for _ in range(n_mc):
        eps = np.random.normal()
        theta = mu + sigma * eps
        # log p(x|theta) + log p(theta) - log q(theta)
        log_lik = np.sum(norm.logpdf(data, theta, 1))
        log_prior = norm.logpdf(theta, 0, 10)
        log_q = norm.logpdf(theta, mu, sigma)
        elbo += log_lik + log_prior - log_q
    return elbo / n_mc

# 경사 상승
params = np.array([0.0, 0.0])  # [mu, log_sigma]
lr = 0.01
data = np.random.normal(3, 1, 100)

for i in range(500):
    grad = np.zeros(2)
    for j in range(2):
        eps = 1e-4
        params_plus = params.copy(); params_plus[j] += eps
        params_minus = params.copy(); params_minus[j] -= eps
        grad[j] = (elbo_reparameterized(params_plus, data) -
                   elbo_reparameterized(params_minus, data)) / (2*eps)
    params += lr * grad

print(f"변분 평균: {params[0]:.4f}, 변분 표준편차: {np.exp(params[1]):.4f}")
print(f"MLE: {np.mean(data):.4f}")
```

---

## 5. VI vs MCMC: 언제 무엇을 사용할 것인가

| 기준 | MCMC 선호 | VI 선호 |
|------|----------|---------|
| 정확도 | 필수적 | 근사 허용 |
| 데이터 크기 | 소~중 | 대규모 |
| 모수 수 | 소~중 | 많음 |
| 사후 분포 형태 | 다봉, 복잡 | 단봉, 대략 정규 |
| 불확실성 정량화 | 정확한 CI 필요 | 대략적 CI 충분 |
| 반복 속도 | 시간 여유 | 빠른 반복 필요 |
| 모델 비교 | — | ELBO로 가능 |

> **핵심 직관**: MCMC는 "정확하지만 느린 금", VI는 "빠르지만 근사적인 은"입니다. 실무에서는 먼저 VI로 빠르게 탐색하고, 최종 보고에는 MCMC를 사용하는 전략이 효과적입니다.

---

## 6. 정규화 흐름과 VI의 확장

평균장의 한계를 극복하기 위한 **유연한 변분 분포**:

**(1) 정규화 흐름 (Normalizing Flows)**: 단순 분포를 가역 변환으로 복잡한 분포로 변환합니다.

$$z_K = f_K \circ \cdots \circ f_1(z_0), \quad z_0 \sim q_0$$

$$\log q_K(z_K) = \log q_0(z_0) - \sum_{k=1}^{K} \log\left|\det\frac{\partial f_k}{\partial z_{k-1}}\right|$$

**(2) 기타 확장**:

| 방법 | 아이디어 | 유연성 |
|------|---------|--------|
| 평균장 | 완전 독립 | 낮음 |
| 구조화 VI | 일부 상관 허용 | 중간 |
| 정규화 흐름 | 가역 변환 체인 | 높음 |
| Implicit VI | 암묵적 분포 (GAN 스타일) | 매우 높음 |

> **핵심 직관**: 정규화 흐름은 "단순한 분포를 복잡하게 비틀어 사후 분포를 근사"합니다. 변환이 가역이므로 밀도를 정확히 계산할 수 있고, ELBO 최적화가 가능합니다.

---

## 7. 변분 추론의 실용적 고려 사항

| 문제 | 증상 | 해결 |
|------|------|------|
| 지역 최적 | ELBO 정체 | 여러 초기값, 어닐링 |
| 분산 과소 추정 | 신용 구간이 너무 좁음 | 더 유연한 $\mathcal{Q}$ |
| KL 방향 | 모드 회피 (mode-seeking) | $\text{KL}(p\|q)$ 사용 (EP) |
| 그래디언트 분산 | 학습 불안정 | 재모수화, 제어 변량 |

**KL 발산의 방향**:

$$\text{KL}(q \| p): \text{모드 탐색 (mode-seeking)} \quad \text{vs} \quad \text{KL}(p \| q): \text{질량 커버 (mass-covering)}$$

> **핵심 직관**: 표준 VI는 $\text{KL}(q\|p)$를 최소화하므로 사후 분포의 하나의 봉에 집중하는 경향이 있습니다. 다봉 사후에서 이는 심각한 문제가 되므로, si-09의 MCMC가 더 적합합니다.

---

## 핵심 정리

- **변분 추론은 사후 분포의 근사를 최적화 문제($\min \text{KL}(q \| p)$, 또는 동등하게 $\max$ ELBO)로 변환합니다**
- **ELBO = 데이터 적합도 - KL 정규화로 분해되며, 이는 정규화된 우도 최대화의 분포 수준 일반화입니다**
- **평균장 근사는 $q(\theta) = \prod q_j(\theta_j)$로 변수 간 상관을 무시하며, 사후 분산을 과소 추정하는 경향이 있습니다**
- **재모수화 트릭은 저분산 그래디언트 추정을 가능하게 하여, VAE 등 딥 생성 모델의 학습을 가능하게 합니다**
- **MCMC는 정확하지만 느리고, VI는 빠르지만 근사적이므로, 정확도 요구와 계산 예산에 따라 선택해야 합니다**
