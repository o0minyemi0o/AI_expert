# 점근 이론 (Asymptotic Theory)

## 왜 점근 이론이 필요한가

정확한 유한 표본 분포를 구하는 것은 대부분의 경우 불가능합니다. 점근 이론은 표본 크기 $n$이 충분히 클 때 통계량의 분포를 근사하는 강력한 도구입니다. si-02에서 언급한 MLE의 점근 정규성, si-06의 Wilks 정리, si-07의 점근적 CI 모두 점근 이론에 기반합니다. 이 강의에서는 그 이론적 토대를 체계적으로 다룹니다.

---

## 1. 확률 수렴과 분포 수렴

점근 이론의 두 가지 기본 수렴 개념:

**(1) 확률 수렴 (Convergence in Probability)**:

$$X_n \xrightarrow{P} X \iff \forall \epsilon > 0, \; P(|X_n - X| > \epsilon) \to 0$$

**(2) 분포 수렴 (Convergence in Distribution)**:

$$X_n \xrightarrow{d} X \iff F_{X_n}(x) \to F_X(x) \; \text{at all continuity points of } F_X$$

| 수렴 유형 | 강도 | 의미 |
|-----------|------|------|
| 거의 확실한 수렴 (a.s.) | 가장 강함 | 표본 경로별 수렴 |
| 확률 수렴 (P) | 중간 | 큰 편차의 확률이 0으로 |
| 분포 수렴 (d) | 가장 약함 | CDF 수렴 |

관계: a.s. $\Rightarrow$ P $\Rightarrow$ d (역은 일반적으로 성립하지 않음)

> **핵심 직관**: 확률 수렴은 "값이 가까워짐", 분포 수렴은 "분포의 모양이 가까워짐"입니다. si-01의 일치성은 확률 수렴, MLE의 점근 정규성은 분포 수렴입니다.

```python
import numpy as np
from scipy.stats import norm

# 분포 수렴 시연: CLT
np.random.seed(42)
ns = [5, 20, 100, 500]
n_sim = 10000

for n in ns:
    # 지수 분포 (비대칭)에서의 표본 평균 표준화
    samples = np.random.exponential(1, (n_sim, n))
    z_stats = (samples.mean(axis=1) - 1) / (1 / np.sqrt(n))
    # 정규성 검정
    from scipy.stats import shapiro
    _, p = shapiro(z_stats[:5000])
    print(f"n={n:3d}: Shapiro p={p:.4f}, 평균={z_stats.mean():.4f}, 분산={z_stats.var():.4f}")
```

---

## 2. 대수의 법칙과 중심극한정리

**대수의 법칙 (LLN)**: $X_1, \dots, X_n$이 i.i.d.이고 $E[X_1] = \mu$이면

$$\bar{X}_n \xrightarrow{P} \mu \quad \text{(약한 대수의 법칙)}$$

**중심극한정리 (CLT)**: $\text{Var}(X_1) = \sigma^2 < \infty$이면

$$\sqrt{n}(\bar{X}_n - \mu) \xrightarrow{d} N(0, \sigma^2)$$

또는 동등하게

$$\frac{\bar{X}_n - \mu}{\sigma / \sqrt{n}} \xrightarrow{d} N(0, 1)$$

| 정리 | 결과 | 활용 |
|------|------|------|
| LLN | $\bar{X}_n \to \mu$ | 일치성 |
| CLT | $\sqrt{n}(\bar{X}_n - \mu) \to N(0, \sigma^2)$ | CI, 가설 검정 |
| Berry-Esseen | $\sup_x |F_n(x) - \Phi(x)| \leq \frac{C\rho}{\sigma^3\sqrt{n}}$ | 근사 정확도 |

> **핵심 직관**: LLN은 "평균이 어디로 가는지", CLT는 "얼마나 빨리 가는지, 그리고 주변에서 어떻게 퍼지는지"를 알려줍니다.

---

## 3. 델타 방법 (Delta Method)

$\sqrt{n}(\hat{\theta}_n - \theta_0) \xrightarrow{d} N(0, \sigma^2)$일 때, 미분 가능한 함수 $g$에 대해

$$\sqrt{n}(g(\hat{\theta}_n) - g(\theta_0)) \xrightarrow{d} N(0, [g'(\theta_0)]^2 \sigma^2)$$

**다변수 델타 방법**: $\sqrt{n}(\hat{\boldsymbol{\theta}} - \boldsymbol{\theta}_0) \xrightarrow{d} N(\mathbf{0}, \boldsymbol{\Sigma})$이면

$$\sqrt{n}(g(\hat{\boldsymbol{\theta}}) - g(\boldsymbol{\theta}_0)) \xrightarrow{d} N(0, \nabla g(\boldsymbol{\theta}_0)^T \boldsymbol{\Sigma} \, \nabla g(\boldsymbol{\theta}_0))$$

| 변환 $g(\theta)$ | $g'(\theta)$ | 용도 |
|-------------------|-------------|------|
| $\log(\theta)$ | $1/\theta$ | 양수 모수의 CI |
| $\text{logit}(\theta)$ | $1/[\theta(1-\theta)]$ | 비율의 CI |
| $\theta^2$ | $2\theta$ | 분산 관련 |
| $1/\theta$ | $-1/\theta^2$ | 비율의 역수 |

> **핵심 직관**: 델타 방법은 "추정량의 함수도 점근 정규"임을 보장합니다. $g'(\theta_0)$이 0이면 2차 델타 방법이 필요합니다.

```python
# 델타 방법: 오즈비의 CI
# log(OR) ~ N(log(OR), se^2)이면 OR의 CI = exp(log(OR) +/- z*se)
np.random.seed(42)

# 2x2 분할표
a, b, c, d = 30, 20, 10, 40
log_or = np.log(a*d / (b*c))
se_log_or = np.sqrt(1/a + 1/b + 1/c + 1/d)
or_hat = np.exp(log_or)

ci_log = (log_or - 1.96*se_log_or, log_or + 1.96*se_log_or)
ci_or = (np.exp(ci_log[0]), np.exp(ci_log[1]))

print(f"오즈비: {or_hat:.4f}")
print(f"log(OR)의 95% CI: ({ci_log[0]:.4f}, {ci_log[1]:.4f})")
print(f"OR의 95% CI (델타 방법): ({ci_or[0]:.4f}, {ci_or[1]:.4f})")
```

---

## 4. MLE의 점근 이론 심화

si-02에서 소개한 MLE의 점근 성질을 엄밀히 증명하는 핵심 도구:

**(1) 일치성 증명**: 우도 함수의 일양 수렴과 식별 가능성을 이용합니다.

$$\frac{1}{n}\ell(\theta) \xrightarrow{P} E_{\theta_0}[\log f(X; \theta)] \leq E_{\theta_0}[\log f(X; \theta_0)]$$

마지막 부등식은 **Kullback-Leibler 부등식**에 의합니다.

**(2) 점근 정규성 증명**: Taylor 전개를 사용합니다.

$$0 = \ell'(\hat{\theta}) = \ell'(\theta_0) + \ell''(\theta_0)(\hat{\theta} - \theta_0) + \cdots$$

정리하면

$$\sqrt{n}(\hat{\theta} - \theta_0) = -\frac{\frac{1}{\sqrt{n}}\ell'(\theta_0)}{\frac{1}{n}\ell''(\theta_0)} \xrightarrow{d} \frac{N(0, I(\theta_0))}{I(\theta_0)} = N(0, I(\theta_0)^{-1})$$

| 단계 | 사용된 결과 |
|------|-----------|
| 분자의 CLT | $\frac{1}{\sqrt{n}}\ell'(\theta_0) \xrightarrow{d} N(0, I(\theta_0))$ |
| 분모의 LLN | $\frac{1}{n}\ell''(\theta_0) \xrightarrow{P} -I(\theta_0)$ |
| Slutsky 정리 | 확률 수렴 + 분포 수렴의 결합 |

> **핵심 직관**: MLE의 점근 정규성은 LLN, CLT, Taylor 전개가 어우러진 결과입니다. 분자에서 CLT가, 분모에서 LLN이 각각 작용합니다.

---

## 5. 슬러츠키 정리와 연속 사상 정리

**슬러츠키 정리 (Slutsky's Theorem)**: $X_n \xrightarrow{d} X$이고 $Y_n \xrightarrow{P} c$ (상수)이면

$$X_n + Y_n \xrightarrow{d} X + c, \quad X_n Y_n \xrightarrow{d} cX, \quad X_n / Y_n \xrightarrow{d} X / c$$

**연속 사상 정리 (Continuous Mapping Theorem)**: $X_n \xrightarrow{d} X$이고 $g$가 연속이면

$$g(X_n) \xrightarrow{d} g(X)$$

| 정리 | 활용 예시 |
|------|---------|
| Slutsky | $\hat{\sigma}$를 $\sigma$로 대체해도 점근 분포 유지 |
| CMT | $X_n^2 \xrightarrow{d} X^2$ (e.g., $Z^2 \sim \chi^2_1$) |

> **핵심 직관**: Slutsky 정리 덕분에 모르는 모수를 추정값으로 대체한 "플러그인" 방법이 점근적으로 정당화됩니다. si-07의 점근적 CI에서 $\sigma$를 $\hat{\sigma}$로 대체하는 것이 바로 이 원리입니다.

```python
# Slutsky 정리: sigma를 s로 대체해도 점근적으로 같음
n_sim = 10000
n = 100
mu, sigma = 5, 2

z_exact = []
z_plugin = []

for _ in range(n_sim):
    data = np.random.normal(mu, sigma, n)
    xbar = np.mean(data)
    s = np.std(data, ddof=1)
    z_exact.append((xbar - mu) / (sigma / np.sqrt(n)))
    z_plugin.append((xbar - mu) / (s / np.sqrt(n)))

print(f"정확 Z - 분산: {np.var(z_exact):.4f}")
print(f"플러그인 Z - 분산: {np.var(z_plugin):.4f}")
print(f"(둘 다 1에 가까움, Slutsky 정리에 의해)")
```

---

## 6. 점근 상대 효율 (ARE)

두 추정량 $\hat{\theta}_1$, $\hat{\theta}_2$의 **점근 상대 효율(Asymptotic Relative Efficiency)**:

$$\text{ARE}(\hat{\theta}_1, \hat{\theta}_2) = \frac{v_2}{v_1}$$

여기서 $v_i$는 $\sqrt{n}(\hat{\theta}_i - \theta_0)$의 점근 분산입니다.

**해석**: ARE = 1.5이면, $\hat{\theta}_2$가 $\hat{\theta}_1$과 같은 정밀도를 얻으려면 1.5배의 데이터가 필요합니다.

| 추정량 비교 | ARE | 의미 |
|------------|-----|------|
| 표본 평균 vs 표본 중앙값 (정규) | $\pi/2 \approx 1.57$ | 평균이 57% 더 효율적 |
| 표본 중앙값 vs 표본 평균 (코시) | $\infty$ | 평균은 일치적이지도 않음 |
| MLE vs MoM (지수 분포) | 1 | 동일 |

> **핵심 직관**: ARE는 "어떤 추정량이 데이터를 더 효율적으로 사용하는가?"를 측정합니다. 분포 가정이 맞으면 MLE가 ARE 면에서 최적(ARE=1 대 모든 대안)이지만, 가정이 틀리면 강건한 추정량이 나을 수 있습니다.

```python
# ARE 시뮬레이션: 표본 평균 vs 중앙값 (정규 분포)
n_sim = 10000
n = 200
mu = 0

mean_estimates = []
median_estimates = []

for _ in range(n_sim):
    data = np.random.normal(mu, 1, n)
    mean_estimates.append(np.mean(data))
    median_estimates.append(np.median(data))

var_mean = np.var(mean_estimates) * n
var_median = np.var(median_estimates) * n
are = var_median / var_mean

print(f"점근 분산 - 평균: {var_mean:.4f}, 중앙값: {var_median:.4f}")
print(f"ARE(평균, 중앙값) = {are:.4f}")
print(f"이론값: pi/2 = {np.pi/2:.4f}")
```

---

## 7. 고차 점근 이론과 에지워스 전개

CLT의 근사 정확도를 개선하는 **에지워스 전개(Edgeworth expansion)**:

$$P\left(\frac{\sqrt{n}(\bar{X} - \mu)}{\sigma} \leq x\right) = \Phi(x) - \frac{\gamma_1}{6\sqrt{n}}(x^2 - 1)\phi(x) + O(n^{-1})$$

여기서 $\gamma_1 = E[(X-\mu)^3]/\sigma^3$은 왜도(skewness)입니다.

| 근사 수준 | 오차 | 방법 |
|-----------|------|------|
| 1차 | $O(n^{-1/2})$ | CLT |
| 2차 | $O(n^{-1})$ | 에지워스 1항 |
| 3차 | $O(n^{-3/2})$ | 에지워스 2항 |

**부트스트랩의 점근적 정당화**: 부트스트랩 CI는 에지워스 전개의 1차 항을 자동으로 보정하여 **2차 정확성**을 달성합니다. 이것이 si-07에서 부트스트랩이 점근적 CI보다 나은 이유입니다.

> **핵심 직관**: CLT는 1차 근사이고, 에지워스 전개는 비대칭성과 첨도를 반영한 고차 보정입니다. 부트스트랩의 우수성은 이 보정을 자동으로 수행하는 데서 옵니다.

```python
# 에지워스 보정의 효과
from scipy.stats import skew

# 비대칭 분포: 지수 분포 (skewness = 2)
n_sim = 50000
n = 30

standardized = []
for _ in range(n_sim):
    data = np.random.exponential(1, n)
    z = (np.mean(data) - 1) / (1 / np.sqrt(n))
    standardized.append(z)

standardized = np.array(standardized)
# P(Z <= -1.645) 비교: 이론적으로 5%
normal_approx = norm.cdf(-1.645)
actual = np.mean(standardized <= -1.645)
# 에지워스 보정
gamma1 = 2  # 지수 분포의 왜도
edgeworth = norm.cdf(-1.645) - gamma1/(6*np.sqrt(n)) * ((-1.645)**2 - 1) * norm.pdf(-1.645)

print(f"정규 근사: {normal_approx:.4f}")
print(f"에지워스 보정: {edgeworth:.4f}")
print(f"시뮬레이션: {actual:.4f}")
```

---

## 핵심 정리

- **확률 수렴($\xrightarrow{P}$)은 일치성의 기반이고, 분포 수렴($\xrightarrow{d}$)은 점근 분포 유도의 기반이며, a.s. $\Rightarrow$ P $\Rightarrow$ d의 관계를 가집니다**
- **CLT는 표본 평균의 점근 정규성을 보장하며, Berry-Esseen 부등식은 수렴 속도 $O(n^{-1/2})$를 정량화합니다**
- **델타 방법은 점근 정규 추정량의 함수에 대해 $\text{Var}(g(\hat{\theta})) \approx [g'(\theta)]^2 \text{Var}(\hat{\theta})$를 제공합니다**
- **Slutsky 정리는 모르는 모수를 일치 추정량으로 대체하는 "플러그인" 방법의 점근적 정당성을 보장합니다**
- **ARE는 두 추정량의 점근 분산 비로, MLE는 정규 조건 하에서 ARE 면에서 최적이지만 모델 오특정 시 강건 추정량이 우월할 수 있습니다**
