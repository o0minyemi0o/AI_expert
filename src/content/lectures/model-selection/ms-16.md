# 검색과 랭킹 시스템

## 왜 검색과 랭킹 전략을 알아야 하는가

검색은 정보 접근의 가장 기본적인 수단이며, LLM 시대에 RAG(Retrieval-Augmented Generation)의 핵심 구성요소로 더욱 중요해졌습니다. "올바른 문서를 찾지 못하면 아무리 좋은 LLM도 정확한 답을 생성할 수 없습니다." 키워드 검색, 벡터 검색, 하이브리드 검색 각각의 강점과 한계를 이해하고, 문제에 맞는 검색 파이프라인을 설계하는 것이 실무의 핵심입니다.

---

## 1. 검색 패러다임 비교

| 접근법 | 원리 | 장점 | 단점 | 적합한 상황 |
|--------|------|------|------|------------|
| 키워드 (BM25) | 단어 빈도 + IDF | 빠름, 정확한 키워드 매칭 | 의미 이해 불가 | 정확한 용어 검색 |
| 벡터 (Dense) | 의미적 유사도 | 동의어/다국어 처리 | 키워드 매칭 약함 | 의미 검색, QA |
| 희소 벡터 (SPLADE) | 학습된 단어 가중치 | 키워드+의미 결합 | 인덱싱 필요 | 균형 잡힌 검색 |
| 하이브리드 | BM25 + Dense 결합 | 양쪽 장점 | 복잡도 증가 | 실전 최적 |

```
검색 방식 선택 플로우:

검색 목적은?
├── 정확한 키워드 매칭 (법률, 특허)
│   └── BM25 / Elasticsearch
├── 의미적 유사 문서 (QA, 추천)
│   └── Dense Retrieval (bi-encoder)
├── 둘 다 필요 (대부분의 실전)
│   └── 하이브리드 (BM25 + Dense)
│       └── Reranking 추가? → Cross-encoder
└── RAG 파이프라인
    └── 하이브리드 검색 + Reranking + LLM
```

---

## 2. 키워드 검색: BM25

### BM25 점수

$$\text{BM25}(q, d) = \sum_{t \in q} \text{IDF}(t) \cdot \frac{f(t, d) \cdot (k_1 + 1)}{f(t, d) + k_1 \cdot (1 - b + b \cdot \frac{|d|}{\text{avgdl}})}$$

### 언제 쓰는가
- 사용자가 정확한 키워드를 사용할 때
- 전문 용어/코드/ID 검색
- 베이스라인으로 항상 유용
- 인프라 비용을 최소화할 때

### 언제 쓰지 않는가
- "비가 올 때 입는 옷" → "우비" (동의어 문제)
- 다국어 검색 (한국어 질문 → 영어 문서)
- 의미적으로 유사하지만 단어가 다른 경우

```python
# BM25 구현 (rank_bm25 라이브러리)
from rank_bm25 import BM25Okapi

documents = [
    "머신러닝은 인공지능의 한 분야입니다",
    "딥러닝은 심층 신경망을 사용합니다",
    "자연어 처리는 텍스트를 분석합니다",
    "컴퓨터 비전은 이미지를 인식합니다",
    "강화 학습은 보상을 최대화합니다",
]

# 토큰화 (실전에서는 형태소 분석기 사용)
tokenized = [doc.split() for doc in documents]
bm25 = BM25Okapi(tokenized)

query = "인공지능 신경망"
scores = bm25.get_scores(query.split())
top_idx = scores.argsort()[::-1]

for i in top_idx[:3]:
    print(f"[{scores[i]:.2f}] {documents[i]}")
```

> **핵심 직관**: BM25는 "이미 정답에 가까운 키워드를 알고 있을 때" 가장 강력합니다. 사용자의 의도가 모호할 때는 벡터 검색이 필요하지만, 정확한 용어 매칭에서는 BM25가 벡터 검색을 이깁니다.

### 시나리오: 법률 판례 검색 시스템

법률 AI 스타트업에서 변호사를 위한 판례 검색 시스템을 구축합니다. 데이터는 대법원/하급심 판례 10만 건(판례당 평균 5,000자), 변호사가 "임대차 계약 해지 사유" 같은 자연어 쿼리를 입력합니다. 판례문에는 "임대차보호법 제6조", "계약 해제" 등 정확한 법률 용어가 포함되어 있습니다.

법률 도메인에서는 BM25와 벡터 검색 모두가 필요합니다. "임대차보호법 제6조"처럼 정확한 조문 번호를 검색할 때는 BM25가 벡터 검색보다 압도적으로 정확합니다. 반면 "세입자가 보증금을 돌려받지 못하는 경우"처럼 의미적 질의에는 벡터 검색이 필수적입니다. 따라서 BM25(Elasticsearch) + 벡터 검색(법률 특화 임베딩 모델)을 RRF로 결합하는 하이브리드 검색이 최적입니다. 법률 용어의 정확한 매칭은 BM25가, 유사 판례의 의미적 검색은 벡터가 담당하여, 단일 방식 대비 nDCG@10이 15~20% 향상됩니다.

---

## 3. 벡터 검색: Dense Retrieval

### 핵심 원리

문서와 쿼리를 동일한 벡터 공간에 임베딩한 후, 코사인 유사도 등으로 가장 가까운 문서를 검색합니다 (la-03 참조).

### Bi-encoder vs Cross-encoder

| 방식 | 원리 | 속도 | 정확도 | 사용 단계 |
|------|------|------|--------|----------|
| Bi-encoder | 쿼리/문서 독립 인코딩 | 매우 빠름 | 중간 | 1차 검색 (Retrieval) |
| Cross-encoder | 쿼리+문서 동시 인코딩 | 매우 느림 | 높음 | 2차 재랭킹 (Reranking) |

### 근사 최근접 이웃 (ANN) 검색

수백만 문서에서 실시간 벡터 검색을 위해 FAISS, Annoy, HNSW 등의 ANN 알고리즘을 사용합니다.

| 라이브러리 | 특징 | 적합한 상황 |
|-----------|------|------------|
| FAISS | GPU 지원, 다양한 인덱스 | 대규모, GPU 가용 |
| Annoy | 메모리 효율적, 읽기 전용 | 정적 인덱스 |
| HNSW (hnswlib) | 높은 정확도, 빠른 검색 | 범용 |
| ScaNN | Google, 양자화 최적화 | 초대규모 |

```python
# FAISS를 사용한 벡터 검색 예시
import numpy as np
# import faiss

# 임베딩 생성 (실전에서는 sentence-transformers 등 사용)
np.random.seed(42)
n_docs = 10000
dim = 768  # BERT 임베딩 차원

# 문서 임베딩 (가상)
doc_embeddings = np.random.randn(n_docs, dim).astype('float32')
doc_embeddings /= np.linalg.norm(doc_embeddings, axis=1, keepdims=True)

# FAISS 인덱스 구축
# index = faiss.IndexFlatIP(dim)           # 정확한 내적 검색
# index = faiss.IndexIVFFlat(quantizer, dim, nlist)  # 근사 검색
# index.add(doc_embeddings)

# 쿼리 검색
# query_emb = encode("검색 쿼리").reshape(1, -1)
# distances, indices = index.search(query_emb, k=10)
```

> **핵심 직관**: Bi-encoder로 상위 100개를 빠르게 검색한 후, Cross-encoder로 재랭킹하는 **2단계 파이프라인**이 정확도와 속도의 최적 균형점입니다.

### 시나리오: 이커머스 상품 검색

대형 이커머스에서 "가성비 좋은 가벼운 노트북"이라는 쿼리에 최적의 상품을 검색합니다. 상품 100만 개, 각 상품에 제목, 상세 설명, 스펙표, 사용자 리뷰가 있습니다. 일 검색 쿼리 500만 건, 응답 시간 200ms 이내가 요구됩니다.

100만 상품을 200ms 내에 검색하려면 2단계 파이프라인이 필수적입니다. 1단계에서 bi-encoder(상품 임베딩 사전 계산)로 상위 100개 후보를 50ms 내에 검색합니다. "가벼운"은 무게 스펙과, "가성비"는 리뷰의 긍정적 가격 언급과 의미적으로 매칭됩니다. 2단계에서 cross-encoder가 쿼리와 100개 후보를 동시에 인코딩하여 정밀 재랭킹합니다. cross-encoder는 "가성비"라는 주관적 표현과 "가격 대비 성능이 좋다"는 리뷰를 깊이 이해하여 순위를 재조정합니다. 100만 개 전체에 cross-encoder를 적용하면 수 시간이 걸리지만, 100개에만 적용하면 150ms로 가능합니다. 이 2단계 구조로 단일 bi-encoder 대비 클릭률이 12% 향상됩니다.

---

## 4. 하이브리드 검색

### 왜 하이브리드인가

| 검색 유형 | "Python 에러 해결" | "코드가 안 돌아요" |
|----------|-------------------|-------------------|
| BM25 | 잘 찾음 (키워드 일치) | 못 찾음 (키워드 불일치) |
| Dense | 관련 문서 찾음 | 잘 찾음 (의미 이해) |
| 하이브리드 | 잘 찾음 | 잘 찾음 |

### 결합 방식

| 방식 | 원리 | 장점 | 단점 |
|------|------|------|------|
| 점수 합산 | BM25 + Dense 점수 가중 합 | 단순 | 점수 스케일 차이 |
| RRF (Reciprocal Rank Fusion) | 순위 기반 결합 | 스케일 무관 | 파라미터 $k$ |
| 학습 기반 | 점수를 피처로 학습 | 최적화 가능 | 학습 데이터 필요 |

```python
# RRF (Reciprocal Rank Fusion) 구현
def reciprocal_rank_fusion(rankings, k=60):
    """
    여러 랭킹 리스트를 RRF로 결합
    rankings: list of lists, 각 리스트는 doc_id 순서
    """
    scores = {}
    for ranking in rankings:
        for rank, doc_id in enumerate(ranking):
            if doc_id not in scores:
                scores[doc_id] = 0
            scores[doc_id] += 1.0 / (k + rank + 1)
    return sorted(scores.items(), key=lambda x: x[1], reverse=True)

# 예시
bm25_ranking = ['doc_A', 'doc_B', 'doc_C', 'doc_D']
dense_ranking = ['doc_C', 'doc_A', 'doc_E', 'doc_B']

fused = reciprocal_rank_fusion([bm25_ranking, dense_ranking])
print("RRF 결과:")
for doc_id, score in fused[:5]:
    print(f"  {doc_id}: {score:.4f}")
```

---

## 5. RAG 파이프라인에서의 검색 설계

```
RAG 검색 파이프라인 설계:

사용자 쿼리
├── Step 1: 쿼리 전처리
│   ├── 쿼리 확장 (동의어, LLM 재작성)
│   └── 쿼리 분해 (복합 질문 → 하위 질문)
├── Step 2: 1차 검색 (Retrieval)
│   ├── BM25 (키워드 매칭)
│   └── Dense Retrieval (의미 검색)
│   └── 결합: RRF 또는 가중 합산
├── Step 3: 재랭킹 (Reranking)
│   └── Cross-encoder로 상위 K개 재정렬
├── Step 4: 후처리
│   ├── 중복 제거
│   ├── 청크 통합 (같은 문서의 인접 청크)
│   └── 관련성 필터링 (임계값 이하 제거)
└── Step 5: LLM 생성
    └── 검색된 문서 + 쿼리 → 답변 생성
```

| 설계 결정 | 선택지 | 추천 |
|----------|--------|------|
| 청크 크기 | 256 / 512 / 1024 토큰 | 512 (시작점) |
| 청크 중첩 | 0% / 20% / 50% | 20% |
| 임베딩 모델 | OpenAI / BGE / E5 | BGE-large (한국어) |
| 검색 Top-K | 3 / 5 / 10 / 20 | 5~10 (재랭킹 포함 시 20) |
| 재랭킹 모델 | Cross-encoder / ColBERT | Cross-encoder |

> **핵심 직관**: RAG의 성능은 **검색 품질에 의해 결정**됩니다. LLM을 교체하는 것보다 검색 파이프라인을 개선하는 것이 대부분 더 큰 효과를 가져옵니다. "Garbage in, garbage out"은 RAG에서도 동일합니다.

---

## 핵심 정리

1. **BM25는 여전히 강력한 베이스라인**: 정확한 키워드 매칭에서는 벡터 검색을 능가하며, 비용이 거의 들지 않으므로 항상 첫 번째 시도입니다.
2. **벡터 검색은 의미적 유사도에 필수**: 동의어, 다국어, 의미적으로 유사한 문서를 찾을 때 BM25의 한계를 극복하지만, 정확한 키워드 매칭에서는 약합니다.
3. **하이브리드 검색이 실전 최적**: BM25와 Dense 검색을 RRF로 결합하면 양쪽의 약점을 보완하며, 대부분의 실전 시스템이 이 방식을 채택합니다.
4. **2단계 파이프라인(Retrieval + Reranking)이 표준**: Bi-encoder로 빠르게 후보를 선별한 후, Cross-encoder로 정밀 재랭킹하는 것이 정확도와 속도의 최적 균형입니다.
5. **RAG 성능은 검색 품질이 결정**: LLM을 바꾸는 것보다 청크 전략, 임베딩 모델, 재랭킹 파이프라인을 개선하는 것이 RAG 품질 향상에 더 효과적입니다.
