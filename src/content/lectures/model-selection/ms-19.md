# 실전 제약 조건과 의사결정

## 왜 실전 제약 조건을 먼저 파악해야 하는가

"가장 정확한 모델"과 "가장 좋은 모델"은 다릅니다. 실전에서는 지연시간, 비용, 해석가능성, 규제 준수 등 다양한 제약 조건이 모델 선택을 근본적으로 바꿉니다. Kaggle에서 1등을 차지한 앙상블 모델이 실제 서비스에서는 추론 시간 초과로 사용할 수 없거나, 규제 산업에서 블랙박스 모델이 감사를 통과하지 못하는 경우가 빈번합니다.

이 강의는 이 과정의 마지막 강의로서, 실전 제약 조건을 체계적으로 정리하고, 모든 조건을 종합한 최종 의사결정 체크리스트를 제시합니다.

---

## 1. 지연시간 제약: 실시간 vs 배치

| 요구사항 | 지연시간 | 예시 | 적합한 모델 |
|---------|---------|------|-----------|
| 실시간 (hard) | < 10ms | 광고 입찰, 자율주행 | 로지스틱, 경량 트리 |
| 실시간 (soft) | < 100ms | 웹 추천, 검색 | 작은 RF, 경량 신경망 |
| 준실시간 | < 1s | 챗봇, QA | BERT, 소형 LLM |
| 배치 | 분~시간 | 일별 예측, 리포트 | XGBoost, 대형 앙상블 |
| 오프라인 | 시간~일 | 모델 학습, 대규모 분석 | 어떤 모델이든 |

```
지연시간 기반 모델 선택 플로우:

요구 추론 시간은?
├── < 1ms
│   └── 로지스틱 회귀, 선형 모델
│       └── 피처 수 최소화
├── < 10ms
│   ├── 정형 → 작은 결정 트리, LightGBM (적은 트리)
│   └── 임베딩 → 사전 계산 + ANN 검색
├── < 100ms
│   ├── 정형 → XGBoost, LightGBM (풀 모델)
│   └── 비정형 → DistilBERT, MobileNet
├── < 1s
│   ├── BERT, ResNet
│   └── 소형 LLM (7B, 양자화)
└── 제약 없음 (배치)
    └── 앙상블, 대형 모델, 어떤 것이든
```

> **핵심 직관**: 모델 선택 전에 반드시 "추론 1건에 허용되는 최대 시간"을 확인하십시오. 이 숫자 하나가 사용 가능한 모델의 범위를 근본적으로 결정합니다.

### 시나리오: 실시간 사기 탐지 vs 배치 리포트

동일한 결제 사기 탐지 문제이지만, 서빙 요구사항에 따라 모델이 완전히 달라집니다. 데이터는 일 500만 건 결제, 피처 40개(결제 금액, 시간, 위치, 가맹점, 디바이스 등)로 동일합니다.

실시간 탐지(100ms 이내 응답 필수)에서는 경량 로지스틱 회귀(피처 10개로 축소) + 규칙 엔진(해외 결제 + 심야 + 고액의 조합 등)으로 구성합니다. 로지스틱 회귀는 1ms 이내 추론이 가능하고, 규칙 엔진은 알려진 사기 패턴을 즉시 차단합니다. F1은 약 78%이지만 100ms 제약을 준수합니다. 배치 탐지(야간 일괄 처리)에서는 전체 40개 피처를 사용한 XGBoost 5-모델 앙상블 + 네트워크 피처(동일 카드의 최근 24시간 패턴)를 적용합니다. 시간 제약이 없으므로 F1 91%를 달성합니다. 실무에서는 실시간으로 1차 차단(높은 Recall)하고, 배치로 정밀 분석(높은 Precision)하여 오탐을 복원하는 2단계 구조가 최적입니다.

---

## 2. 비용 분석: 학습 vs 추론 vs 인건비

| 비용 항목 | 발생 시점 | 크기 | 최적화 방법 |
|----------|----------|------|-----------|
| 학습 비용 | 1회 (재학습마다) | 중간~높음 | 효율적 하이퍼파라미터 탐색 |
| 추론 비용 | 매 요청마다 | 누적 시 매우 높음 | 모델 경량화, 캐싱 |
| 인건비 | 지속적 | 매우 높음 | 자동화, 간단한 모델 |
| 인프라 | 지속적 | 높음 | 클라우드 스팟 인스턴스 |
| 모니터링 | 지속적 | 중간 | 자동 알림 |

```python
# 비용 비교 계산 예시
daily_requests = 100_000
monthly_requests = daily_requests * 30

costs = {
    'LR (CPU)': {
        'inference_per_req': 0.000001,  # $0.001 / 1K req
        'infra_monthly': 50,            # 작은 서버
        'engineer_hours': 2,            # 월 유지보수
    },
    'XGBoost (CPU)': {
        'inference_per_req': 0.000005,
        'infra_monthly': 100,
        'engineer_hours': 4,
    },
    'BERT (GPU)': {
        'inference_per_req': 0.00005,
        'infra_monthly': 500,
        'engineer_hours': 8,
    },
    'GPT-4 (API)': {
        'inference_per_req': 0.003,      # ~$3/1K req
        'infra_monthly': 0,
        'engineer_hours': 4,
    },
}

engineer_hourly = 50  # $/hour
print(f"{'모델':18s} {'추론/월':>12s} {'인프라/월':>10s} {'인건비/월':>10s} {'총비용/월':>10s}")
print("-" * 65)
for name, c in costs.items():
    inference = monthly_requests * c['inference_per_req']
    infra = c['infra_monthly']
    engineer = c['engineer_hours'] * engineer_hourly * 4  # 주당 → 월
    total = inference + infra + engineer
    print(f"{name:18s} ${inference:>10,.0f} ${infra:>8,.0f} ${engineer:>8,.0f} ${total:>8,.0f}")
```

> **핵심 직관**: 대부분의 ML 프로젝트에서 **인건비가 가장 큰 비용**입니다. 복잡한 모델의 유지보수에 매달 40시간이 드는 것보다, 단순한 모델로 10시간에 운영하는 것이 총비용으로는 더 저렴할 수 있습니다.

---

## 3. 해석가능성: 규제 산업의 요구사항

| 산업 | 규제/요구 | 허용 모델 | 금지/주의 모델 |
|------|----------|----------|-------------|
| 금융 (신용평가) | 설명 의무 | 로지스틱, 점수카드, GAM | 블랙박스 DL |
| 의료 | 임상 검증, FDA | 해석 가능 모델 + 전문가 검증 | 비검증 DL |
| 보험 | 차별 금지, 설명 의무 | GLM, 결정 트리 | 복잡한 앙상블 |
| HR/채용 | 공정성, 편향 감사 | 선형, 규칙 기반 | 비감사 DL |
| 자율주행 | 안전 인증 | 검증 가능한 모듈 | 미검증 E2E |

### 해석가능성 도구

| 도구 | 유형 | 모델 의존 | 글로벌/로컬 | 적합한 상황 |
|------|------|---------|-----------|------------|
| SHAP | 사후 설명 | 모델 무관 | 양쪽 | 피처 중요도 설명 |
| LIME | 사후 설명 | 모델 무관 | 로컬 | 개별 예측 설명 |
| 피처 중요도 | 내장 | 트리 모델 | 글로벌 | 빠른 탐색 |
| GradCAM | 사후 설명 | CNN | 로컬 | 이미지 영역 설명 |
| Attention | 내장 | Transformer | 로컬 | 텍스트/시퀀스 |

```
해석가능성 요구 수준에 따른 선택:

규제가 "모델 자체가 해석 가능"을 요구하는가?
├── Yes (금융, 보험 등)
│   ├── 로지스틱 회귀 (계수 해석)
│   ├── GAM (비선형이지만 해석 가능)
│   ├── 결정 트리 (규칙 추출 가능)
│   └── 점수카드 (Score Card)
└── No → 사후 설명(post-hoc)으로 충분한가?
          ├── Yes → 어떤 모델이든 + SHAP/LIME
          │         └── XGBoost + SHAP이 가장 흔한 조합
          └── No → 해석가능 모델만 사용
```

> **핵심 직관**: "SHAP으로 설명하면 되지 않나?"는 규제 산업에서 통하지 않는 경우가 많습니다. 규제 기관은 모델 **자체의** 해석가능성을 요구하는 경우가 있으며, 사후 설명 도구는 보조 수단일 뿐입니다.

### 시나리오: 금융 대출 심사

시중 은행에서 개인 대출 승인/거절을 자동화합니다. 데이터는 월 5만 건 신청, 피처는 소득, 부채비율, 신용점수, 재직기간, 주거형태 등 20개이며, 레이블은 부실(1)/정상(0)입니다. 금융위원회 규제로 "왜 거절했는지" 신청자에게 설명할 의무가 있습니다.

XGBoost는 AUROC 0.87로 가장 높은 정확도를 보이지만, 500개 트리의 복합적인 의사결정 과정을 신청자에게 "귀하의 대출이 거절된 이유는..."으로 설명하기 어렵습니다. SHAP 값을 사용할 수 있지만, 규제 감사에서 "사후 설명"은 모델 자체의 해석가능성으로 인정받지 못하는 경우가 있습니다. 대신 로지스틱 회귀(AUROC 0.85) + SHAP을 선택합니다. 정확도는 2% 낮지만, 계수를 직접 해석할 수 있어 "소득 대비 부채비율이 60%를 초과하여 거절되었습니다"라는 명확한 설명이 가능합니다. 2%의 정확도 손해는 규제 준수와 고객 신뢰 확보라는 가치로 충분히 정당화됩니다.

---

## 4. A/B 테스트로 모델 효과 검증

| 요소 | 설명 | 권장 |
|------|------|------|
| 표본 크기 | 유의미한 차이를 감지할 수 있는 양 | 사전 검정력 분석 |
| 실험 기간 | 요일/계절 효과 포함 | 최소 1~2주 |
| 지표 | 1차 지표 1개 + 가드레일 지표 | 사전 정의 |
| 할당 | 랜덤 배정, 네트워크 효과 주의 | 사용자 단위 |
| 분석 | 양측 검정, 다중 비교 보정 | ms-07 참조 |

```python
# A/B 테스트 표본 크기 계산
from scipy.stats import norm
import numpy as np

def sample_size_two_proportion(p1, p2, alpha=0.05, power=0.80):
    """두 비율 비교를 위한 최소 표본 크기 (그룹당)"""
    z_alpha = norm.ppf(1 - alpha / 2)
    z_beta = norm.ppf(power)
    p_bar = (p1 + p2) / 2
    n = ((z_alpha * np.sqrt(2 * p_bar * (1 - p_bar)) +
          z_beta * np.sqrt(p1*(1-p1) + p2*(1-p2)))**2) / (p1 - p2)**2
    return int(np.ceil(n))

# 현재 전환율 5%, 새 모델로 5.5%를 기대
n = sample_size_two_proportion(0.05, 0.055)
print(f"필요 표본 크기 (그룹당): {n:,}")
print(f"총 필요 표본: {2*n:,}")
```

> **핵심 직관**: A/B 테스트는 "새 모델이 더 좋은가?"를 답하는 유일한 신뢰할 수 있는 방법입니다. 오프라인 지표(F1, AUROC)가 좋아도 온라인 지표(매출, 전환율)가 나빠지는 경우는 놀라울 정도로 흔합니다.

---

## 5. 종합 의사결정 체크리스트

```
AI 모델 선택 최종 체크리스트:

[1단계: 문제 정의] (ms-01)
 □ ML이 정말 필요한가? 규칙 기반으로 충분하지 않은가?
 □ 문제 유형은? (분류/회귀/클러스터링/생성/랭킹)
 □ 성공 지표는? (비즈니스 KPI와 연결)
 □ 베이스라인 성능은?

[2단계: 데이터 평가]
 □ 데이터 양은 충분한가? (ms-18)
 □ 레이블이 있는가? 비용은?
 □ 데이터 품질은? (결측, 노이즈, 편향)
 □ 정형 vs 비정형?

[3단계: 제약 조건 확인]
 □ 추론 지연시간 제약은?
 □ 학습/추론 비용 예산은?
 □ 해석가능성 요구는? (규제 확인)
 □ 데이터 프라이버시 요구는?
 □ 배포 환경은? (서버/모바일/엣지)

[4단계: 모델 선택]
 □ 베이스라인 모델 구축 (단순한 것부터)
 □ 후보 모델 2~3개 빠른 비교
 □ 최적 모델 하이퍼파라미터 튜닝 (ms-09)
 □ 제약 조건 만족 여부 확인

[5단계: 검증 및 배포]
 □ 오프라인 평가 (교차 검증, 홀드아웃)
 □ A/B 테스트 (온라인 검증)
 □ 모니터링 계획 (성능 저하 감지)
 □ 재학습 전략 (빈도, 트리거)
```

### 최종 의사결정 플로우

```
종합 모델 선택 플로우:

데이터 유형은?
├── 정형 (테이블)
│   ├── 해석 필요 → 로지스틱/GAM/결정 트리 (ms-03)
│   ├── 성능 우선 → XGBoost/LightGBM (ms-04, ms-08)
│   └── 초대규모 → LightGBM 또는 신경망 (ms-08)
├── 이미지
│   ├── 소량 → 사전학습 + fine-tuning (ms-10)
│   └── 대량 → ViT / EfficientNet (ms-10)
├── 텍스트
│   ├── 분류 → TF-IDF+LR 또는 BERT (ms-11)
│   ├── 생성 → LLM (ms-17)
│   └── QA → RAG (ms-16)
├── 시계열
│   ├── 단변량 → ARIMA/Prophet (ms-12)
│   └── 다변량 → XGBoost+lag / TFT (ms-12)
├── 그래프
│   ├── 구조만 → 그래프 특성+ML (ms-15)
│   └── 노드 피처도 → GNN (ms-15)
└── 기타
    ├── 추천 → 협업필터링/행렬분해 (ms-13)
    ├── 이상탐지 → Isolation Forest (ms-14)
    └── 검색 → 하이브리드 검색 (ms-16)
```

| 우선순위 | 원칙 | 이유 |
|---------|------|------|
| 1 | 단순한 모델부터 | 디버깅, 해석, 배포가 쉬움 |
| 2 | 피처에 투자 | 모델보다 피처가 성능에 큰 영향 |
| 3 | 제약 조건 준수 | 서빙 불가능한 모델은 무가치 |
| 4 | 점진적 복잡화 | 필요한 만큼만 복잡하게 |
| 5 | 데이터 품질 우선 | 나쁜 데이터로는 좋은 모델 불가 |

> **핵심 직관**: 가장 좋은 모델은 "가장 정확한 모델"이 아니라, **제약 조건 안에서 비즈니스 문제를 가장 효과적으로 해결하는 모델**입니다. 이 과정에서 배운 모든 비교와 의사결정 프레임워크의 궁극적 목표는 바로 이 판단을 체계적으로 내리는 것입니다.

---

## 핵심 정리

1. **지연시간 제약이 모델 범위를 결정**: 실시간(< 10ms)이면 선형/경량 모델만 가능하고, 배치라면 어떤 복잡한 모델도 사용할 수 있으므로 제약을 먼저 확인합니다.
2. **총비용에서 인건비가 가장 큰 비중**: 복잡한 모델의 유지보수 비용이 추론 비용보다 클 수 있으며, 단순한 모델이 총비용 측면에서 유리한 경우가 많습니다.
3. **규제 산업에서는 모델 자체의 해석가능성이 필수**: SHAP/LIME 같은 사후 설명 도구는 보조 수단이며, 금융/의료 등에서는 본질적으로 해석 가능한 모델이 요구됩니다.
4. **A/B 테스트가 유일한 신뢰할 수 있는 검증**: 오프라인 지표와 온라인 성과의 괴리가 흔하므로, 실제 사용자 대상 A/B 테스트로 모델 효과를 반드시 검증해야 합니다.
5. **단순한 모델부터 시작하여 점진적으로 복잡화**: 이 과정의 핵심 메시지는 "항상 가장 단순한 모델부터 시작하고, 제약 조건 안에서 성능이 부족할 때만 복잡성을 추가하라"입니다.
