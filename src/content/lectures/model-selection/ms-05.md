# 클러스터링 전략

## 왜 클러스터링 전략을 알아야 하는가

클러스터링은 레이블 없는 데이터에서 숨겨진 구조를 발견하는 비지도 학습의 핵심입니다. 고객 세분화, 유전자 발현 그룹화, 이상 탐지의 전처리 등 다양한 비즈니스 문제에 활용됩니다. 하지만 클러스터링 알고리즘마다 가정하는 클러스터 형태가 다르기 때문에, 잘못된 알고리즘을 선택하면 의미 없는 그룹이 만들어집니다.

이 강의에서는 주요 클러스터링 알고리즘의 특성을 비교하고, 데이터 형태에 따른 최적 선택법을 제시합니다.

---

## 1. K-Means: 빠르지만 구형 클러스터만

### 핵심 원리

각 클러스터의 중심(centroid)을 반복적으로 갱신하며, 각 데이터를 가장 가까운 중심에 할당합니다. 목적 함수: $\min \sum_{k} \sum_{x \in C_k} \|x - \mu_k\|^2$ (la-07 참조)

### 언제 쓰는가
- 클러스터가 구형(spherical)이고 비슷한 크기일 때
- 클러스터 수 $k$를 대략 알고 있을 때
- 대규모 데이터에서 빠른 결과가 필요할 때
- 전처리 단계로 빠른 그룹화가 필요할 때

### 언제 쓰지 않는가
- 클러스터 모양이 비구형(초승달, 고리 등)일 때
- 클러스터 크기/밀도가 매우 다를 때
- 이상치가 많을 때 (중심이 이상치 쪽으로 끌림)

```python
from sklearn.cluster import KMeans
from sklearn.datasets import make_blobs
from sklearn.metrics import silhouette_score

X, y_true = make_blobs(n_samples=1000, centers=4,
                         cluster_std=1.0, random_state=42)

# K-Means 적용
kmeans = KMeans(n_clusters=4, random_state=42, n_init=10)
labels = kmeans.fit_predict(X)

sil_score = silhouette_score(X, labels)
print(f"Silhouette Score: {sil_score:.3f}")
print(f"Inertia: {kmeans.inertia_:.1f}")
```

---

## 2. DBSCAN: 밀도 기반, 임의 형태

### 핵심 원리

밀도가 높은 영역을 클러스터로 묶고, 밀도가 낮은 영역은 노이즈로 처리합니다. 핵심 파라미터는 `eps`(이웃 반경)와 `min_samples`(최소 이웃 수)입니다.

### 언제 쓰는가
- 클러스터 형태가 비구형일 때
- 클러스터 수를 모를 때 (자동 결정)
- 노이즈/이상치를 자동으로 식별하고 싶을 때
- 클러스터 밀도가 비슷할 때

### 언제 쓰지 않는가
- 클러스터 밀도가 크게 다를 때 (단일 eps로 처리 불가)
- 고차원 데이터 (거리 개념이 무의미해짐, la-07 참조)
- 매우 대규모 데이터 ($O(n^2)$ 또는 인덱스 필요)

> **핵심 직관**: DBSCAN은 $k$를 지정할 필요가 없지만, 대신 `eps`와 `min_samples`를 적절히 설정해야 합니다. k-distance 그래프의 "elbow"를 찾는 것이 실전에서 가장 유용한 eps 결정법입니다.

### 시나리오: 이커머스 고객 세분화(RFM 분석)

대형 온라인 쇼핑몰이 100만 고객을 대상으로 마케팅 전략을 세분화하려 합니다. 피처는 RFM 3가지 — Recency(최근 구매일로부터 경과 일수), Frequency(최근 1년 구매 횟수), Monetary(최근 1년 총 결제 금액)입니다. 목표는 VIP, 일반, 이탈 위험 등 고객 그룹을 자동으로 분류하는 것입니다.

3개 피처의 저차원 데이터이고, 비즈니스적으로 "3~5개 세그먼트"라는 명확한 요구가 있으므로 K-Means가 적합합니다. StandardScaler로 RFM을 정규화한 뒤, $k=3$~$5$에서 Silhouette Score를 비교하여 최적 $k$를 결정합니다. 예를 들어 $k=4$일 때 VIP(높은 F, 높은 M), 충성 고객(높은 F, 중간 M), 신규 고객(낮은 F, 높은 R), 이탈 위험(높은 R, 낮은 F) 네 그룹이 자연스럽게 형성됩니다.

### 시나리오: GPS 기반 배달 구역 설정

배달 플랫폼이 50만 건의 주문 위치 좌표(위도, 경도)를 기반으로 배달 구역을 자동 설정하려 합니다. 도시의 배달 수요는 상권을 따라 불규칙한 형태로 밀집되어 있으며, 강이나 산 등 지형 장벽으로 인해 구형 클러스터 가정이 성립하지 않습니다.

이 경우 K-Means는 구형 클러스터만 생성하므로, 한강을 사이에 두고 양쪽에 분포한 주문을 하나의 구역으로 묶는 문제가 발생합니다. DBSCAN은 밀도 기반으로 임의 형태의 클러스터를 찾으므로, 상권 형태를 자연스럽게 반영한 배달 구역을 생성합니다. 또한 밀도가 낮은 외곽 주문은 노이즈로 분류되어, 배달 효율이 낮은 지역을 자동으로 식별할 수 있습니다. `eps`는 배달 가능 반경(예: 1km)에 맞춰 설정합니다.

```python
from sklearn.cluster import DBSCAN
from sklearn.datasets import make_moons

# 비구형 데이터 — K-Means는 실패, DBSCAN은 성공
X, y_true = make_moons(n_samples=500, noise=0.1, random_state=42)

dbscan = DBSCAN(eps=0.2, min_samples=5)
labels = dbscan.fit_predict(X)

n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
n_noise = list(labels).count(-1)
print(f"클러스터 수: {n_clusters}, 노이즈 포인트: {n_noise}")
```

---

## 3. GMM: 확률적 소프트 할당

### 핵심 원리

각 클러스터를 가우시안 분포로 모델링하고, EM 알고리즘으로 파라미터를 추정합니다 (pt-05, pt-07 참조). 각 데이터에 소속 확률을 부여합니다.

### 언제 쓰는가
- 소프트 할당(확률적 소속)이 필요할 때
- 타원형 클러스터를 포착해야 할 때
- 클러스터 간 경계가 모호할 때
- 생성 모델로 새 데이터를 샘플링하고 싶을 때

### 언제 쓰지 않는가
- 클러스터 형태가 가우시안과 거리가 멀 때
- 고차원에서 공분산 추정이 불안정할 때
- 대규모 데이터에서 속도가 문제일 때

| 모델 | 클러스터 형태 | 할당 방식 | 이상치 처리 | 클러스터 수 지정 |
|------|-------------|----------|-----------|----------------|
| K-Means | 구형 | 하드 | 민감 | 필수 ($k$) |
| DBSCAN | 임의 | 하드 + 노이즈 | 자동 제거 | 자동 (eps) |
| GMM | 타원형 | 소프트 (확률) | 약간 민감 | 필수 ($k$) |

---

## 4. 계층적 / 스펙트럴 클러스터링

### 계층적 클러스터링 (Agglomerative)

### 언제 쓰는가
- 덴드로그램으로 계층 구조를 시각화하고 싶을 때
- 다양한 수준의 그룹화를 동시에 분석할 때
- 생물학적 계통 분류 등 계층이 자연스러운 도메인

### 스펙트럴 클러스터링

### 언제 쓰는가
- 그래프 구조의 데이터
- 매니폴드 상의 클러스터 (la-07 참조)
- 데이터가 크지 않을 때 (유사도 행렬 $O(n^2)$ 필요)

```
특수 클러스터링 선택 플로우:

데이터에 자연스러운 계층이 있는가?
├── Yes → 계층적 클러스터링
│         └── 덴드로그램으로 cut 위치 결정
└── No → 데이터가 그래프/매니폴드 구조인가?
          ├── Yes → 스펙트럴 클러스터링 (n < 10K)
          │         └── 데이터 크면? → UMAP + K-Means (ms-06)
          └── No → K-Means / DBSCAN / GMM 중 선택
```

---

## 5. 클러스터 수 결정 방법

클러스터 수 $k$를 결정하는 것은 클러스터링에서 가장 어려운 문제 중 하나입니다.

| 방법 | 원리 | 장점 | 단점 |
|------|------|------|------|
| Elbow | Inertia 감소율 변화점 | 직관적 | 주관적 판단 필요 |
| Silhouette | 클러스터 내/간 거리 비율 | 정량적 비교 가능 | 대규모 데이터에서 느림 |
| Gap Statistic | 랜덤 대비 개선도 | 이론적 근거 | 계산 비용 높음 |
| BIC/AIC (GMM) | 모델 복잡도 패널티 | 자동 선택 가능 | GMM에만 적용 |

> **핵심 직관**: 단일 방법으로 $k$를 결정하지 마십시오. Elbow와 Silhouette를 함께 확인하고, **비즈니스 맥락**(예: "고객을 몇 개 세그먼트로 관리할 수 있는가?")을 반드시 고려해야 합니다.

```python
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from sklearn.datasets import make_blobs
import numpy as np

X, _ = make_blobs(n_samples=1000, centers=4, random_state=42)

# Elbow + Silhouette 동시 평가
print(f"{'k':>3s}  {'Inertia':>10s}  {'Silhouette':>10s}")
print("-" * 30)
for k in range(2, 8):
    km = KMeans(n_clusters=k, random_state=42, n_init=10)
    labels = km.fit_predict(X)
    sil = silhouette_score(X, labels)
    print(f"{k:3d}  {km.inertia_:10.1f}  {sil:10.3f}")
```

---

## 6. 종합 선택 가이드

```
클러스터링 알고리즘 선택 플로우:

클러스터 형태를 아는가?
├── 구형/균일 → K-Means (빠름, 확장성)
├── 타원형 → GMM (확률적)
├── 임의 형태 → DBSCAN
│               └── 밀도가 다양? → HDBSCAN
├── 계층 구조 → Agglomerative
└── 모르겠다 → 여러 알고리즘 비교
                ├── 시각화: UMAP/t-SNE로 확인 (ms-06)
                └── 정량 비교: Silhouette Score
```

> **핵심 직관**: 클러스터링 결과의 "정답"은 없습니다. 같은 데이터에 K-Means와 DBSCAN을 적용하면 다른 결과가 나오며, **어느 것이 비즈니스에 더 유용한가**로 판단해야 합니다.

```python
# 여러 알고리즘 비교
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering
from sklearn.mixture import GaussianMixture
from sklearn.metrics import silhouette_score
from sklearn.preprocessing import StandardScaler

X_scaled = StandardScaler().fit_transform(X)

results = {}

# K-Means
km = KMeans(n_clusters=4, random_state=42, n_init=10)
results['K-Means'] = km.fit_predict(X_scaled)

# DBSCAN
db = DBSCAN(eps=0.5, min_samples=5)
results['DBSCAN'] = db.fit_predict(X_scaled)

# GMM
gmm = GaussianMixture(n_components=4, random_state=42)
results['GMM'] = gmm.fit_predict(X_scaled)

# Agglomerative
agg = AgglomerativeClustering(n_clusters=4)
results['Agglom.'] = agg.fit_predict(X_scaled)

for name, labels in results.items():
    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
    if n_clusters > 1:
        mask = labels != -1
        sil = silhouette_score(X_scaled[mask], labels[mask])
    else:
        sil = -1
    print(f"{name:12s}  클러스터: {n_clusters}  Silhouette: {sil:.3f}")
```

---

## 핵심 정리

1. **K-Means는 구형 클러스터 전용**: 빠르고 확장성이 좋지만, 비구형 클러스터나 밀도가 다른 그룹에서는 실패하므로 데이터 형태를 먼저 확인합니다.
2. **DBSCAN은 클러스터 수를 모를 때**: 밀도 기반으로 자동으로 클러스터 수를 결정하고 노이즈를 식별하지만, eps 설정이 결과를 크게 좌우합니다.
3. **GMM은 소프트 할당이 필요할 때**: 각 데이터의 소속 확률을 얻을 수 있어 경계가 모호한 클러스터에 적합합니다.
4. **클러스터 수는 정량적 지표와 비즈니스 맥락을 함께 고려**: Elbow와 Silhouette만으로는 부족하며, "실제로 관리 가능한 세그먼트 수"를 반드시 고려해야 합니다.
5. **여러 알고리즘을 비교 실행하는 것이 최선**: 단일 알고리즘에 의존하지 말고, 시각화와 정량 지표를 함께 활용하여 가장 유의미한 결과를 선택합니다.
