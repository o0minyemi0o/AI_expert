# 이미지 데이터 모델 선택

## 왜 이미지 모델 선택 전략이 필요한가

컴퓨터 비전은 딥러닝이 가장 큰 성공을 거둔 분야입니다. 하지만 "어떤 아키텍처를 써야 하는가?"는 데이터의 양, 도메인, 컴퓨팅 자원에 따라 크게 달라집니다. 100장의 의료 영상으로 학습하는 것과 100만 장의 일반 이미지로 학습하는 것은 완전히 다른 전략이 필요합니다.

이 강의에서는 CNN, Vision Transformer, 사전학습 모델의 선택 기준과 데이터 양/도메인에 따른 실전 전략을 비교합니다.

---

## 1. 주요 아키텍처 비교

### CNN 계열

| 모델 | 파라미터 수 | 특징 | 적합한 상황 |
|------|-----------|------|------------|
| ResNet-50 | ~25M | 잔차 연결, 범용 | 전이학습 베이스라인 |
| EfficientNet-B0~B7 | 5M~66M | 스케일링 효율적 | 성능/효율 밸런스 |
| MobileNetV3 | ~5M | 경량, 모바일 최적화 | 엣지/모바일 배포 |
| ConvNeXt | ~89M | CNN의 현대화 | ViT와 경쟁하는 CNN |

### Vision Transformer (ViT) 계열

| 모델 | 파라미터 수 | 특징 | 적합한 상황 |
|------|-----------|------|------------|
| ViT-B/16 | ~86M | 패치 기반 Transformer | 대규모 데이터, 전이학습 |
| DeiT | ~86M | 데이터 효율적 ViT | 중간 규모 데이터 |
| Swin Transformer | ~88M | 계층적, 윈도우 어텐션 | 다양한 해상도 |

> **핵심 직관**: ViT는 대규모 데이터에서 CNN을 능가하지만, 소규모 데이터에서는 CNN이 더 강합니다 (dl-08 참조). 이것은 ViT에 **귀납적 편향(inductive bias)**이 부족하기 때문입니다. CNN의 지역성(locality)과 이동 불변성(translation invariance)은 소량 데이터에서 효율적인 학습을 가능하게 합니다.

---

## 2. 데이터 양에 따른 전략

```
이미지 데이터 양에 따른 전략:

데이터 양은?
├── 매우 적음 (< 100장)
│   ├── 사전학습 모델 + 선형 분류기 (feature extraction)
│   ├── 데이터 증강 최대화
│   └── few-shot learning 고려 (ms-18)
├── 적음 (100 ~ 1,000장)
│   ├── 사전학습 모델 + fine-tuning (마지막 층만)
│   ├── 강한 데이터 증강
│   └── EfficientNet-B0 또는 ResNet-18
├── 중간 (1,000 ~ 10,000장)
│   ├── 사전학습 모델 + full fine-tuning
│   ├── ResNet-50 / EfficientNet-B3
│   └── 데이터 증강 + 정규화
├── 많음 (10,000 ~ 100,000장)
│   ├── fine-tuning 또는 from scratch
│   ├── EfficientNet-B5+ / Swin Transformer
│   └── ViT + DeiT 전략
└── 매우 많음 (> 100,000장)
    ├── from scratch 가능
    ├── ViT 계열이 CNN보다 우세할 수 있음
    └── 자기 지도 학습 (MAE, DINO) 고려
```

| 데이터 양 | 추천 전략 | 추천 모델 | 학습 방식 |
|----------|----------|----------|----------|
| < 100 | Feature extraction | 사전학습 + Linear | 고정 백본 |
| 100~1K | 부분 fine-tuning | EfficientNet-B0 | 마지막 N층만 |
| 1K~10K | Full fine-tuning | ResNet-50 / EfficientNet | 전체 모델 |
| 10K~100K | Fine-tuning / From scratch | EfficientNet / Swin | 선택적 |
| > 100K | From scratch 가능 | ViT / ConvNeXt | 전체 학습 |

---

## 3. Transfer Learning vs Fine-tuning vs From Scratch

| 전략 | 학습 대상 | 데이터 요구 | 학습 시간 | 성능 |
|------|----------|-----------|----------|------|
| Feature extraction | 분류기만 | 매우 적음 | 빠름 | 중간 |
| Fine-tuning (부분) | 상위 N층 + 분류기 | 적음 | 중간 | 높음 |
| Fine-tuning (전체) | 전체 모델 | 중간 | 느림 | 매우 높음 |
| From scratch | 전체 모델 (랜덤 초기화) | 많음 | 매우 느림 | 데이터에 따라 |

```python
import torch
import torch.nn as nn
from torchvision import models

# 전략 1: Feature Extraction (백본 고정)
model = models.resnet50(weights='IMAGENET1K_V2')
for param in model.parameters():
    param.requires_grad = False  # 백본 고정
model.fc = nn.Linear(model.fc.in_features, 10)  # 분류기만 학습

# 전략 2: 부분 Fine-tuning (마지막 블록 + 분류기)
model = models.resnet50(weights='IMAGENET1K_V2')
for param in model.parameters():
    param.requires_grad = False
for param in model.layer4.parameters():
    param.requires_grad = True  # layer4만 학습 가능
model.fc = nn.Linear(model.fc.in_features, 10)

# 전략 3: 전체 Fine-tuning (낮은 학습률)
model = models.resnet50(weights='IMAGENET1K_V2')
model.fc = nn.Linear(model.fc.in_features, 10)
# 전체 파라미터 학습 가능, 낮은 lr 사용
```

> **핵심 직관**: 소스 도메인(ImageNet)과 타겟 도메인이 유사하면 feature extraction만으로도 충분하고, 다르면 fine-tuning이 필요합니다. 의료/위성 영상처럼 도메인이 매우 다르면 더 많은 층을 fine-tune해야 합니다.

---

### 시나리오: 피부암 진단 보조 시스템

대학병원 피부과가 피부 병변 영상으로 질환을 자동 분류하는 보조 진단 시스템을 구축합니다. 데이터는 피부경(dermoscope) 영상 2만 장이며, 7개 질환 클래스(멜라노마, 기저세포암, 양성 각화증 등)로 구성됩니다. 클래스 불균형이 심하여 멜라노마는 전체의 5%에 불과합니다.

2만 장은 from scratch 학습에는 부족하므로, ImageNet 사전학습 EfficientNet-B3를 fine-tuning하는 것이 최적 전략입니다. 의료 영상은 자연 이미지와 도메인이 다르므로, 백본 전체를 낮은 학습률(1e-4)로 fine-tuning해야 합니다. 클래스 불균형은 Focal Loss와 클래스별 가중 샘플링으로 처리하고, 멜라노마의 재현율(Recall)을 최우선 지표로 설정합니다. GradCAM으로 모델이 주목하는 영역을 시각화하여 의사의 신뢰를 확보하는 것도 필수입니다.

### 시나리오: 제조 라인 PCB 결함 검사

전자부품 제조사가 PCB(인쇄회로기판) 기판의 결함을 자동 검출하려 합니다. 데이터는 PCB 사진 5,000장이며, 이 중 결함 이미지는 300장(6%)에 불과합니다. 결함 유형은 납땜 불량, 회로 단선, 이물질 등 다양합니다.

극심한 불균형(정상 94% vs 결함 6%)과 소량 데이터가 동시에 문제입니다. 일반 분류 접근보다는 이상 탐지(anomaly detection) 관점이 효과적입니다. 정상 이미지만으로 사전학습된 AutoEncoder를 학습한 뒤, 복원 오차가 높은 이미지를 결함으로 판단합니다. 분류 접근을 사용한다면, ImageNet 사전학습 모델에 강한 데이터 증강(회전, 밝기 변환, CutMix)을 적용하고, 결함 이미지를 5~10배 오버샘플링하여 학습합니다. 300장이라는 극소량이므로, few-shot learning 기법(Prototypical Networks 등)도 검토할 가치가 있습니다.

## 4. 도메인별 접근

| 도메인 | 특수성 | 추천 접근법 |
|--------|--------|------------|
| 의료 영상 | 데이터 적음, 규제, 해석 필요 | 사전학습 + fine-tuning, GradCAM |
| 위성/항공 | 고해상도, 다중 채널 | 사전학습 + 도메인 적응 |
| 제조 결함 | 극심한 불균형 | 이상 탐지 접근 (ms-14) |
| 자율주행 | 실시간, 안전 필수 | 경량 모델 + TensorRT |
| OCR/문서 | 텍스트 + 레이아웃 | LayoutLM, DONUT |

```
도메인별 전략 선택:

도메인이 자연 이미지와 유사한가?
├── Yes (일상 사진, 상품 등)
│   └── ImageNet 사전학습 모델 직접 fine-tuning
└── No (의료, 위성, 현미경 등)
    ├── 도메인 데이터 충분? (>10K)
    │   ├── Yes → 도메인 내 자기 지도 사전학습 + fine-tuning
    │   └── No → ImageNet 사전학습 + 강한 증강 + fine-tuning
    └── 해석 필요?
        ├── Yes → GradCAM, Attention 시각화
        └── No → 성능 최적화에 집중
```

---

## 5. 데이터 증강 전략

| 증강 기법 | 효과 | 주의점 |
|----------|------|--------|
| 회전, 뒤집기 | 이동 불변성 강화 | 의미 변경 주의 (숫자 6↔9) |
| 색상 변환 | 조명 변화 강건성 | 의료 영상에서 주의 |
| RandomCrop | 위치 불변성 | 작은 객체 소실 가능 |
| Cutout/CutMix | 정규화 효과 | 분류에 효과적 |
| MixUp | 결정 경계 평활화 | 라벨도 혼합 필요 |
| AutoAugment | 자동 정책 탐색 | 계산 비용 높음 |

```python
from torchvision import transforms

# 실전 데이터 증강 파이프라인
train_transform = transforms.Compose([
    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),
    transforms.RandomHorizontalFlip(),
    transforms.ColorJitter(brightness=0.2, contrast=0.2,
                            saturation=0.2, hue=0.1),
    transforms.RandomRotation(15),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406],
                          std=[0.229, 0.224, 0.225]),
    transforms.RandomErasing(p=0.2),
])

# 검증/테스트에는 증강 없이
eval_transform = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406],
                          std=[0.229, 0.224, 0.225]),
])
```

> **핵심 직관**: 데이터 증강은 "공짜 데이터"가 아닙니다. 도메인에 맞지 않는 증강(예: 의료 X-ray에서 색상 변환)은 노이즈를 추가할 뿐입니다. 항상 **도메인의 실제 변동성**을 반영하는 증강만 적용하십시오.

---

## 핵심 정리

1. **데이터가 적으면 사전학습 + fine-tuning이 필수**: 100~1,000장 규모에서는 ImageNet 사전학습 모델의 fine-tuning이 from scratch 대비 압도적 성능을 보입니다.
2. **CNN vs ViT는 데이터 양으로 결정**: 소규모에서는 CNN의 귀납적 편향이 유리하고, 대규모에서는 ViT의 높은 용량이 유리합니다.
3. **도메인 유사성이 전이학습 효과를 결정**: ImageNet과 유사한 도메인은 feature extraction만으로 충분하지만, 이질적 도메인은 더 깊은 fine-tuning이 필요합니다.
4. **데이터 증강은 도메인에 맞게 선택**: 범용 증강(뒤집기, 크롭)은 대부분 유효하지만, 도메인 특수 증강은 전문가와 함께 설계해야 합니다.
5. **배포 환경을 먼저 확인**: 모바일이면 MobileNet, 서버이면 EfficientNet/ViT를 기본으로 하며, 실시간 제약이 있으면 경량 모델과 최적화를 우선합니다 (ms-19 참조).
