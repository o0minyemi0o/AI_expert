# 앙상블 방법 심화

## 왜 앙상블을 깊이 이해해야 하는가

Kaggle 대회 상위권의 거의 모든 솔루션은 앙상블을 사용합니다. 단일 모델의 한계를 넘어서는 가장 검증된 방법이기 때문입니다. 하지만 실무에서는 "어떤 앙상블 전략이 적합한가?"와 "앙상블이 정말 필요한가?"를 동시에 판단해야 합니다. 과도한 앙상블은 복잡성과 추론 비용만 높이고, 올바른 앙상블은 단일 모델 대비 유의미한 개선을 가져옵니다.

---

## 1. 앙상블 전략 비교: Bagging vs Boosting vs Stacking

| 전략 | 핵심 아이디어 | 대표 모델 | 병렬화 | 과적합 위험 |
|------|-------------|----------|--------|-----------|
| Bagging | 데이터 부트스트랩 + 평균 | Random Forest | 가능 | 낮음 |
| Boosting | 이전 오류에 집중 + 순차 학습 | XGBoost, LightGBM, CatBoost | 제한적 | 중간 |
| Stacking | 모델 예측을 피처로 사용 | 메타 모델 | 가능 | 높음 |

```
앙상블 전략 선택 플로우:

단일 모델 성능이 충분한가?
├── Yes → 앙상블 불필요 (단순성 유지)
└── No → 과적합이 문제인가?
          ├── Yes → Bagging (Random Forest)
          │         └── 분산 감소가 목표
          └── No → 편향이 문제인가?
                    ├── Yes → Boosting (XGBoost/LightGBM)
                    │         └── 잔여 오차 학습
                    └── 다양한 모델 결합 필요
                              └── Stacking
```

> **핵심 직관**: Bagging은 **분산(variance)을 줄이고**, Boosting은 **편향(bias)을 줄입니다**. 모델이 과적합하면 Bagging을, 과소적합하면 Boosting을 선택하십시오.

---

## 2. Bagging과 Random Forest

Random Forest는 결정 트리의 Bagging에 **무작위 피처 선택**을 추가한 것입니다.

### 언제 쓰는가
- 빠르게 강력한 베이스라인을 구축할 때
- 하이퍼파라미터 튜닝 없이도 안정적 성능이 필요할 때
- 피처 중요도를 빠르게 파악하고 싶을 때

### 언제 쓰지 않는가
- 최고 성능이 필요할 때 (Boosting이 보통 우세)
- 메모리가 제한될 때 (수백 개 트리 저장)
- 추론 속도가 매우 중요할 때

```python
from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import make_classification
from sklearn.model_selection import cross_val_score

X, y = make_classification(n_samples=5000, n_features=30,
                            n_informative=15, random_state=42)

# Random Forest는 기본 설정만으로도 강력
rf = RandomForestClassifier(n_estimators=100, random_state=42)
scores = cross_val_score(rf, X, y, cv=5, scoring='f1')
print(f"RF 기본 설정 F1: {scores.mean():.3f} ± {scores.std():.3f}")
```

---

## 3. Boosting 3대장: XGBoost vs LightGBM vs CatBoost

이 세 라이브러리는 정형 데이터에서 가장 강력한 모델들입니다. 각각의 차이를 이해하고 상황에 맞게 선택해야 합니다.

### 상세 비교표

| 특성 | XGBoost | LightGBM | CatBoost |
|------|---------|----------|----------|
| 트리 성장 방식 | Level-wise | Leaf-wise | Symmetric |
| 학습 속도 | 빠름 | 매우 빠름 | 느림 |
| 메모리 사용 | 중간 | 적음 | 많음 |
| 범주형 처리 | 수동 인코딩 필요 | 내장 지원 | 내장 지원 (최강) |
| GPU 지원 | 있음 | 있음 | 있음 (기본) |
| 과적합 방지 | 정규화 다양 | 주의 필요 | 자동 처리 |
| 결측치 처리 | 내장 | 내장 | 내장 |
| 대규모 데이터 | 우수 | 최우수 | 우수 |

```
Boosting 라이브러리 선택 플로우:

범주형 피처가 많은가?
├── Yes → CatBoost (범주형 처리 최강)
│         └── 학습 시간 여유 있는가?
│               ├── Yes → CatBoost 확정
│               └── No → LightGBM (categorical_feature 옵션)
└── No → 데이터 크기는?
          ├── 매우 큼 (>1M) → LightGBM (속도 최강)
          ├── 중간 → XGBoost 또는 LightGBM
          └── 작음 → XGBoost (안정적)
```

> **핵심 직관**: 범주형 피처가 많으면 CatBoost, 데이터가 크면 LightGBM, 안정성과 생태계가 중요하면 XGBoost — 이것이 실무의 경험적 규칙입니다.

### 시나리오: 보험 청구 금액 예측 (Kaggle AllState Claims)

보험사가 사고 발생 시 청구 금액을 예측하려 합니다. 데이터는 30만 건의 보험 청구 기록이며, 130개 피처(사고 유형, 차종, 지역, 운전자 정보 등 범주형과 수치형 혼합)를 포함합니다. 타겟은 청구 금액(연속 값, 회귀)입니다.

이 데이터셋은 XGBoost, LightGBM, CatBoost를 실전 비교하기에 이상적입니다. LightGBM은 30만 건을 가장 빠르게 학습하며(XGBoost 대비 2~3배 빠름), Leaf-wise 성장으로 더 낮은 MAE를 달성하는 경향이 있습니다. 그러나 130개 피처 중 범주형이 상당수이므로, CatBoost의 Ordered Target Encoding이 원-핫 인코딩보다 나은 성능을 보일 수 있습니다. 실전에서는 세 모델을 모두 학습한 뒤 검증 세트에서 MAE를 비교하여 최종 모델을 선택합니다.

### 시나리오: 은행 마케팅 캠페인 응답 예측

은행이 정기예금 가입 전화 마케팅의 응답률을 예측하려 합니다. 데이터는 4만 건의 고객 기록이며, 직업, 학력, 결혼 여부, 대출 여부, 연락 방법, 이전 캠페인 결과 등 범주형 피처가 전체의 60% 이상을 차지합니다. 타겟은 가입 여부(이진 분류)입니다.

범주형 피처가 다수인 이 상황에서 CatBoost의 장점이 두드러집니다. XGBoost를 사용하려면 직업(12개 범주), 학력(8개 범주) 등을 원-핫 인코딩하거나 라벨 인코딩해야 하는데, 원-핫 인코딩은 차원을 불필요하게 늘리고 라벨 인코딩은 순서 없는 범주에 인위적 순서를 부여합니다. CatBoost는 범주형 피처를 내부적으로 타겟 통계 기반 인코딩(Target Statistics)으로 처리하므로, 별도 전처리 없이도 더 높은 AUC를 달성합니다.

```python
from sklearn.datasets import make_classification
from sklearn.model_selection import cross_val_score
import xgboost as xgb
import lightgbm as lgb
# import catboost as cb  # pip install catboost

X, y = make_classification(n_samples=10000, n_features=30,
                            n_informative=15, random_state=42)

models = {
    'XGBoost': xgb.XGBClassifier(
        n_estimators=200, max_depth=6, learning_rate=0.1,
        eval_metric='logloss', random_state=42
    ),
    'LightGBM': lgb.LGBMClassifier(
        n_estimators=200, max_depth=-1, learning_rate=0.1,
        num_leaves=31, random_state=42, verbose=-1
    ),
    # 'CatBoost': cb.CatBoostClassifier(
    #     iterations=200, depth=6, learning_rate=0.1,
    #     random_state=42, verbose=0
    # ),
}

import time
for name, model in models.items():
    start = time.time()
    scores = cross_val_score(model, X, y, cv=5, scoring='f1')
    elapsed = time.time() - start
    print(f"{name:12s} F1: {scores.mean():.3f} ± {scores.std():.3f}  "
          f"({elapsed:.1f}s)")
```

---

## 4. Stacking

Stacking은 여러 모델의 예측을 **메타 모델의 입력 피처**로 사용합니다.

### 언제 쓰는가
- 다양한 모델이 서로 다른 패턴을 포착할 때
- 대회에서 마지막 0.1% 성능이 중요할 때
- 충분한 데이터가 있어 과적합 위험이 낮을 때

### 언제 쓰지 않는가
- 실시간 추론이 필요할 때 (여러 모델 순차 실행)
- 모델 해석이 필요할 때
- 유지보수 비용을 최소화해야 할 때

```python
from sklearn.ensemble import StackingClassifier, RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import LinearSVC
from sklearn.model_selection import cross_val_score
from sklearn.datasets import make_classification

X, y = make_classification(n_samples=5000, n_features=20,
                            n_informative=10, random_state=42)

stacking = StackingClassifier(
    estimators=[
        ('rf', RandomForestClassifier(n_estimators=50, random_state=42)),
        ('svc', LinearSVC(max_iter=5000)),
    ],
    final_estimator=LogisticRegression(),
    cv=5
)

scores = cross_val_score(stacking, X, y, cv=5, scoring='f1')
print(f"Stacking F1: {scores.mean():.3f} ± {scores.std():.3f}")
```

> **핵심 직관**: Stacking은 대회에서는 표준이지만, 실무에서는 유지보수 비용 대비 성능 향상이 미미한 경우가 많습니다. "Stacking의 0.5% 개선이 추론 비용 2배 증가를 정당화하는가?"를 항상 질문하십시오.

---

## 5. 앙상블이 과도한 경우

앙상블이 항상 답은 아닙니다. 다음 상황에서는 단일 모델이 더 적합합니다.

| 상황 | 이유 | 대안 |
|------|------|------|
| 실시간 추론 (< 10ms) | 다중 모델 추론 지연 | 단일 경량 모델 |
| 모바일/엣지 배포 | 메모리 제약 | 모델 경량화, 지식 증류 |
| 규제 산업 (금융, 의료) | 해석 요구 | 로지스틱, 결정 트리 |
| 빠른 프로토타입 | 개발 속도 우선 | 단일 모델 + AutoML |
| 데이터가 매우 적음 | 과적합 위험 | 단순 모델 + 정규화 |

```
앙상블 필요성 판단 플로우:

단일 모델로 비즈니스 요구를 만족하는가?
├── Yes → 앙상블 불필요 (KISS 원칙)
└── No → 성능 부족의 원인은?
          ├── 데이터 부족 → 데이터 확보 우선 (ms-18)
          ├── 피처 부족 → 피처 엔지니어링 (ms-06)
          └── 모델 한계 → 앙상블 시도
                ├── 추론 시간 여유 있음 → Stacking
                ├── 추론 시간 제한 → 단일 Boosting 강화
                └── 다양성 필요 → Voting/Blending
```

> **핵심 직관**: 앙상블보다 **피처 엔지니어링과 데이터 품질 개선**이 더 큰 성능 향상을 가져오는 경우가 훨씬 많습니다. 앙상블은 이미 좋은 피처가 있을 때 마지막 수단으로 사용하십시오.

---

## 핵심 정리

1. **Bagging은 분산을 줄이고 Boosting은 편향을 줄인다**: 과적합 문제에는 Random Forest(Bagging)를, 과소적합 문제에는 XGBoost/LightGBM(Boosting)을 적용합니다.
2. **XGBoost vs LightGBM vs CatBoost는 데이터 특성으로 선택**: 범주형이 많으면 CatBoost, 대규모이면 LightGBM, 범용적 안정성이면 XGBoost가 적합합니다.
3. **Stacking은 강력하지만 실무 비용이 높다**: 여러 모델을 조합하므로 추론 시간과 유지보수 복잡성이 크게 증가하며, 대회 외에서는 ROI를 신중히 평가해야 합니다.
4. **앙상블 전에 피처 엔지니어링을 먼저 시도**: 좋은 피처 하나가 모델 5개를 앙상블하는 것보다 더 큰 성능 향상을 가져올 수 있습니다.
5. **추론 제약 조건을 반드시 확인**: 실시간 서빙, 모바일 배포, 규제 요구 등이 있으면 앙상블 대신 단일 모델 최적화나 지식 증류를 검토해야 합니다.
