# 차원 축소와 피처 엔지니어링

## 왜 차원 축소와 피처 엔지니어링이 중요한가

"좋은 피처가 좋은 모델을 만든다"는 ML의 가장 오래된 격언입니다. 아무리 복잡한 모델도 쓸모없는 피처로는 좋은 결과를 낼 수 없습니다. 차원 축소는 고차원 데이터를 효율적으로 표현하여 과적합을 방지하고, 피처 엔지니어링은 원시 데이터에서 모델이 학습하기 좋은 신호를 추출합니다.

이 강의에서는 목적에 따른 차원 축소 기법 선택과 체계적 피처 엔지니어링 전략을 비교합니다.

---

## 1. PCA vs t-SNE vs UMAP: 목적별 선택

| 기법 | 목적 | 선형/비선형 | 전처리 사용 | 시각화 사용 | 확장성 |
|------|------|-----------|-----------|-----------|--------|
| PCA | 차원 축소, 전처리 | 선형 | 적합 | 제한적 | 매우 높음 |
| t-SNE | 시각화 전용 | 비선형 | 부적합 | 최적 | 낮음 |
| UMAP | 시각화 + 전처리 | 비선형 | 가능 | 최적 | 높음 |

### PCA (주성분 분석)

분산을 최대화하는 직교 축을 찾습니다 (la-06, la-07 상세 참조).

### 언제 쓰는가
- ML 파이프라인의 전처리로 차원을 줄일 때
- 피처 간 다중공선성을 제거할 때
- 노이즈 제거 (하위 주성분 제거)
- 설명된 분산 비율로 차원 수를 결정할 때

### 언제 쓰지 않는가
- 비선형 구조가 강한 데이터
- 시각화가 주 목적일 때 (2D/3D 축소 시 구조 왜곡)

```python
from sklearn.decomposition import PCA
from sklearn.datasets import load_digits
import numpy as np

X, y = load_digits(return_X_y=True)
print(f"원본 차원: {X.shape}")  # (1797, 64)

# 설명 분산 95%를 유지하는 차원 수 결정
pca = PCA(n_components=0.95)
X_pca = pca.fit_transform(X)
print(f"축소 차원: {X_pca.shape}")  # 약 (1797, 28)
print(f"사용 주성분 수: {pca.n_components_}")
```

### t-SNE vs UMAP

```
차원 축소 목적은?

전처리 (ML 파이프라인 입력)
├── 선형 관계 → PCA
├── 비선형 관계 → UMAP 또는 AutoEncoder
└── 피처 선택 필요 → 아래 섹션 3 참조

시각화 (2D/3D 플롯)
├── 데이터 < 10K → t-SNE (지역 구조 보존)
├── 데이터 > 10K → UMAP (빠르고 전역 구조도 보존)
└── 빠른 탐색 → PCA 2D (대략적 확인)
```

> **핵심 직관**: t-SNE는 "시각화 전용"입니다. t-SNE의 축은 해석 불가능하고, 새 데이터에 적용할 수 없으며, 하이퍼파라미터(perplexity)에 매우 민감합니다. 전처리로 사용하지 마십시오.

### 시나리오: 유전체 데이터 분석

대학병원 연구팀이 500명 환자의 유전체 데이터를 분석합니다. 각 환자에 대해 20,000개 유전자의 발현값이 측정되어 있으며, 타겟은 질환 유형(3개 클래스)입니다. 전형적인 $p \gg n$ (피처 20,000 > 샘플 500) 상황입니다.

20,000차원 데이터를 그대로 분류 모델에 입력하면 심각한 과적합이 발생합니다. PCA로 설명 분산 95%를 유지하는 주성분을 추출하면 약 50~100개 차원으로 축소되며, 이 축소된 데이터로 SVM이나 Random Forest를 학습하면 과적합 없이 안정적인 분류 성능을 얻을 수 있습니다. PCA는 선형 변환이므로, 각 주성분이 어떤 유전자에 높은 가중치를 갖는지 해석할 수 있어 생물학적 의미 부여도 가능합니다.

### 시나리오: 고객 행동 로그 시각화

이커머스 플랫폼이 고객 행동 패턴을 시각적으로 탐색하려 합니다. 데이터는 10만 고객의 100차원 행동 벡터(페이지 체류시간, 클릭 패턴, 검색 빈도, 카테고리별 구매 비율 등)입니다. 목표는 2D 시각화를 통해 고객 세그먼트를 직관적으로 파악하는 것입니다.

10만 건이라는 데이터 크기에서 t-SNE는 계산 시간이 수십 분 이상 소요되고 메모리 부담이 큽니다. UMAP은 t-SNE보다 10배 이상 빠르면서 지역 구조와 전역 구조를 모두 잘 보존합니다. UMAP 결과에서 자연스럽게 형성되는 군집을 확인한 뒤, K-Means로 클러스터를 할당하면 시각적 탐색과 정량적 세그먼트를 동시에 얻을 수 있습니다.

---

## 2. AutoEncoder 기반 차원 축소

### 언제 쓰는가
- 비선형 차원 축소가 필요하면서 새 데이터에도 적용해야 할 때
- 이미지/텍스트 등 비정형 데이터의 임베딩 추출
- 이상 탐지의 전처리 (ms-14 참조)

### 언제 쓰지 않는가
- 데이터가 소량일 때 (PCA가 더 안정적)
- 해석가능성이 필요할 때
- 빠른 프로토타이핑 단계

| 기법 | 학습 필요 | 비선형 | 새 데이터 적용 | 해석성 | 적합한 데이터 |
|------|----------|--------|-------------|--------|-------------|
| PCA | 불필요 | 불가 | 가능 | 높음 | 정형, 중소 규모 |
| t-SNE | 필요 | 가능 | 불가 | 없음 | 시각화 전용 |
| UMAP | 필요 | 가능 | 가능 | 낮음 | 범용 |
| AutoEncoder | 필요 (DL) | 가능 | 가능 | 없음 | 비정형, 대규모 |

```python
import torch
import torch.nn as nn

class AutoEncoder(nn.Module):
    def __init__(self, input_dim, latent_dim):
        super().__init__()
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, 128),
            nn.ReLU(),
            nn.Linear(128, latent_dim)
        )
        self.decoder = nn.Sequential(
            nn.Linear(latent_dim, 128),
            nn.ReLU(),
            nn.Linear(128, input_dim)
        )

    def forward(self, x):
        z = self.encoder(x)
        return self.decoder(z)

    def encode(self, x):
        return self.encoder(x)

# 학습 후 encoder.encode(X)로 저차원 임베딩 추출
```

---

## 3. 피처 선택 vs 피처 추출

| 방법 | 원리 | 해석가능성 | 예시 |
|------|------|-----------|------|
| 피처 선택 | 원본 피처 부분집합 선택 | 유지됨 | 중요 피처만 남기기 |
| 피처 추출 | 새로운 피처 생성 | 감소 | PCA, 임베딩 |

### 피처 선택의 3가지 접근법

```
피처 선택 방법 선택 플로우:

피처 수가 매우 많은가 (>1000)?
├── Yes → 필터 방법 (빠른 사전 선별)
│         └── 이후 래퍼/임베디드로 정제
└── No → 모델의 피처 중요도 확인
          ├── 트리 모델 사용 → 임베디드 방법 (내장 중요도)
          ├── 선형 모델 사용 → L1 정규화 (Lasso)
          └── 최적 부분집합 필요 → 래퍼 방법 (RFE)
```

| 방법 | 원리 | 속도 | 모델 의존 | 대표 기법 |
|------|------|------|---------|----------|
| 필터 | 통계적 상관/검정 | 매우 빠름 | 독립 | 분산, 상관, 카이제곱 |
| 래퍼 | 모델 성능으로 평가 | 느림 | 의존 | RFE, Forward/Backward |
| 임베디드 | 학습 중 자동 선택 | 빠름 | 의존 | Lasso, 트리 중요도 |

```python
from sklearn.feature_selection import (SelectKBest, f_classif,
                                         RFE, mutual_info_classif)
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.datasets import make_classification

X, y = make_classification(n_samples=1000, n_features=50,
                            n_informative=10, random_state=42)

# 1. 필터 방법: ANOVA F-값 기반
selector_filter = SelectKBest(f_classif, k=15)
X_filtered = selector_filter.fit_transform(X, y)
print(f"필터 방법 선택 피처 수: {X_filtered.shape[1]}")

# 2. 래퍼 방법: RFE (Recursive Feature Elimination)
rfe = RFE(LogisticRegression(max_iter=1000), n_features_to_select=15)
X_rfe = rfe.fit_transform(X, y)
print(f"RFE 선택 피처 수: {X_rfe.shape[1]}")

# 3. 임베디드 방법: Random Forest 중요도
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X, y)
importances = rf.feature_importances_
top_k = importances.argsort()[-15:]
print(f"RF 중요 피처 (top 15): {sorted(top_k)}")
```

> **핵심 직관**: 피처 선택은 해석가능성을 유지하면서 차원을 줄이고, 피처 추출(PCA 등)은 해석을 포기하는 대신 더 강력한 압축이 가능합니다. 규제 산업에서는 피처 선택이 필수입니다.

---

## 4. 실전 피처 엔지니어링 전략

| 데이터 유형 | 주요 기법 | 예시 |
|-----------|----------|------|
| 수치형 | 로그 변환, 빈닝, 상호작용 | $\log(\text{income})$, $\text{age} \times \text{salary}$ |
| 범주형 | 원핫, 타겟 인코딩, 임베딩 | ms-09 상세 참조 |
| 시간 | 요일, 월, 경과 시간, 주기 특성 | $\sin(2\pi \cdot \text{hour}/24)$ |
| 텍스트 | TF-IDF, Word2Vec, BERT 임베딩 | ms-11 참조 |
| 이미지 | 사전학습 모델 피처, HOG | ms-10 참조 |

> **핵심 직관**: 피처 엔지니어링의 핵심은 **도메인 지식**입니다. 통계적 방법은 자동화할 수 있지만, "어떤 변환이 비즈니스적으로 의미 있는가"는 도메인 전문가만 판단할 수 있습니다.

```python
import numpy as np
from sklearn.preprocessing import PolynomialFeatures

# 상호작용 피처 생성
X_sample = np.random.randn(100, 3)
poly = PolynomialFeatures(degree=2, interaction_only=True,
                           include_bias=False)
X_poly = poly.fit_transform(X_sample)
print(f"원본: {X_sample.shape[1]} 피처 → "
      f"상호작용 포함: {X_poly.shape[1]} 피처")
# 3개 피처 → 3(원본) + 3(상호작용) = 6 피처
```

---

## 5. 종합 선택 가이드

```
차원 축소 / 피처 전략 종합 플로우:

목적은?
├── 시각화 → UMAP (대규모) / t-SNE (소규모)
├── 전처리 (ML 입력) → 선형 충분?
│   ├── Yes → PCA
│   └── No → UMAP 또는 AutoEncoder
├── 피처 해석 유지 → 피처 선택
│   ├── 빠르게 → 필터 (분산, 상관)
│   ├── 정확하게 → RFE
│   └── 자동 → L1 정규화 / 트리 중요도
└── 성능 극대화 → 피처 엔지니어링
    ├── 도메인 기반 변환
    ├── 상호작용/다항 피처
    └── 타겟 인코딩 (ms-09)
```

> **핵심 직관**: "차원을 줄여야 하는가, 피처를 만들어야 하는가?"는 데이터에 따라 다릅니다. 피처가 이미 많고 노이즈가 섞여 있으면 축소를, 피처가 적고 비선형 관계가 의심되면 생성을 우선합니다.

---

## 핵심 정리

1. **PCA는 전처리의 기본, t-SNE는 시각화 전용**: PCA는 선형 차원 축소와 노이즈 제거에 적합하고, t-SNE는 2D 시각화에만 사용해야 합니다.
2. **UMAP은 범용 비선형 도구**: 시각화와 전처리 모두에 사용 가능하며, t-SNE보다 빠르고 전역 구조를 더 잘 보존합니다.
3. **피처 선택은 해석이 필요할 때, 피처 추출은 성능이 필요할 때**: 규제 산업에서는 원본 피처의 의미를 유지하는 선택이 필수적입니다.
4. **필터 → 래퍼 → 임베디드 순서로 시도**: 피처가 매우 많으면 필터로 빠르게 후보를 줄인 뒤, 래퍼나 임베디드 방법으로 정제합니다.
5. **도메인 지식이 최고의 피처 엔지니어링**: 자동화된 방법은 보조 도구이며, 비즈니스 맥락에서 의미 있는 피처를 만드는 것이 가장 큰 성능 향상을 가져옵니다.
