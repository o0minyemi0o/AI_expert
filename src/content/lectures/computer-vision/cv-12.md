# 비디오 이해

## 왜 비디오 이해가 어려운가

이미지가 정적인 순간이라면, 비디오는 **시간 축이 추가된 4차원 데이터**(H×W×C×T)입니다. "사람이 걷고 있다"를 이해하려면 단일 프레임이 아닌 **프레임 간의 변화**를 파악해야 합니다. 비디오는 이미지보다 데이터가 수십~수백 배 많고, 시간적 모델링이라는 새로운 차원의 도전이 있습니다. 자율주행, 보안, 스포츠 분석, 콘텐츠 이해 등 실세계 응용의 대부분이 비디오입니다.

> **핵심 직관**: 비디오 이해의 핵심 질문은 **"시간을 어떻게 모델링하는가"**입니다. 독립된 프레임을 따로 분석하면 "움직임"을 놓치고, 모든 프레임을 한 번에 처리하면 계산이 폭발합니다. 공간(이미지)과 시간(변화)을 **효율적으로 결합**하는 것이 비디오 모델의 핵심 설계 결정입니다.

## 1. 행동 인식의 기초

```
행동 인식 (Action Recognition):
  비디오 클립 → "달리기", "요리하기" 등 행동 분류

  초기 접근: 수작업 특징
  ├─ Optical Flow: 프레임 간 픽셀 이동 벡터
  ├─ HOG/HOF: 기울기/흐름 히스토그램
  └─ Dense Trajectories: 궤적을 따라 특징 추출

Two-Stream Networks (Simonyan & Zisserman, 2014):
  "공간 + 시간을 별도 스트림으로"

  공간 스트림: 단일 RGB 프레임 → CNN → 외형 특징
  시간 스트림: Optical Flow (10 프레임) → CNN → 운동 특징
  → 두 스트림의 예측을 결합 (late fusion)

  시간 스트림이 왜 중요한가:
  "달리기"와 "서있기"는 외형이 같을 수 있지만
  Optical Flow는 완전히 다름
  → 운동 정보가 행동 인식의 핵심

3D CNN:
  C3D (Tran et al., 2015):
  2D Conv(H×W) → 3D Conv(H×W×T)
  커널: 3×3 → 3×3×3 (공간×공간×시간)
  → 시공간 특징을 동시에 학습

  I3D (Carreira & Zisserman, 2017):
  2D Inception을 3D로 확장 (Inflate)
  ├─ ImageNet 사전학습 가중치를 3D로 팽창
  │   2D 커널 k×k → 3D 커널 k×k×t (각 시간에 복제)
  ├─ Two-Stream I3D: RGB + Flow I3D 결합
  └─ Kinetics 데이터셋에서 획기적 성능

  | 모델 | 입력 | 시간 모델링 | Kinetics-400 |
  |------|------|-----------|-------------|
  | Two-Stream | RGB+Flow | Late Fusion | 73.0% |
  | C3D | RGB | 3D Conv | 65.6% |
  | I3D | RGB | Inflated 3D | 72.1% |
  | I3D Two-Stream | RGB+Flow | Inflated 3D | 75.7% |
```

## 2. 효율적 비디오 모델

```
3D CNN의 문제: 계산 비용이 매우 큼
  3D Conv: O(k² × t × C_in × C_out × H × W × T)
  → 2D Conv의 t배!

분해 접근 (Factorized Convolutions):

  R(2+1)D (Tran et al., 2018):
  3D Conv를 2D 공간 + 1D 시간으로 분리

  3×3×3 Conv → 1×3×3 Conv(공간) + 3×1×1 Conv(시간)

  장점:
  ├─ 파라미터 감소 (3×3×3=27 → 9+3=12)
  ├─ 비선형성 추가 (ReLU가 중간에)
  └─ 학습이 더 안정적

  SlowFast Networks (Feichtenhofer et al., 2019):
  "느린 경로(세밀) + 빠른 경로(운동)"

  Slow pathway:
  ├─ 적은 프레임 (4 FPS)
  ├─ 높은 채널 수
  └─ 공간 구조와 세밀한 외형

  Fast pathway:
  ├─ 많은 프레임 (32 FPS, 8배)
  ├─ 낮은 채널 수 (1/8)
  └─ 빠른 운동과 시간적 변화

  Lateral connections로 두 경로 연결
  → Slow: 80% 계산, Fast: 20% 계산
  → 효율적이면서 정확

  Kinetics-400: 79.8% (당시 SOTA)

  | 모델 | 프레임 | FLOPs | Top-1 |
  |------|--------|-------|-------|
  | I3D | 64 | 108G | 72.1 |
  | R(2+1)D | 32 | 152G | 74.3 |
  | SlowFast | 4+32 | 36G | 75.6 |
  | SlowFast 16×8 | 16+128 | 234G | 79.8 |
```

## 3. Video Transformer

```
ViViT (Arnab et al., 2021):
  Video Vision Transformer

  시공간 토큰화:
  비디오(T×H×W) → 패치 튜브(t×h×w)로 분할
  → 시공간 패치가 Transformer의 토큰

  4가지 변형:

  Model 1: Spatio-temporal Attention
  모든 토큰에 Full Self-Attention
  → O(T²·N²) — 매우 비쌈!

  Model 2: Factorised Encoder
  공간 Transformer → 시간 Transformer
  → 공간과 시간을 순차적으로 처리

  Model 3: Factorised Self-Attention
  한 블록 내에서:
  공간 Attention (프레임 내) → 시간 Attention (프레임 간)
  → R(2+1)D의 Transformer 버전

  Model 4: Factorised Dot-Product Attention
  Q, K를 공간/시간으로 분해
  → 가장 효율적이지만 성능은 약간 낮음

TimeSformer (Bertasius et al., 2021):
  Divided Space-Time Attention

  각 블록에서:
  1. 시간 어텐션: 같은 공간 위치의 다른 시간 토큰끼리
  2. 공간 어텐션: 같은 시간의 공간 토큰끼리
  → ViViT Model 3과 유사

VideoMAE (Tong et al., 2022):
  MAE를 비디오로 확장 (cv-09 참조)

  ├─ 시공간 패치의 90% 마스킹! (이미지 MAE: 75%)
  ├─ 비디오의 시간적 중복이 더 크므로 높은 비율
  ├─ 큰 데이터셋 없이도 효과적 사전학습
  └─ ViT-L: Kinetics-400 86.1%

  | 모델 | 사전학습 | Kinetics-400 | 특징 |
  |------|---------|-------------|------|
  | ViViT-L | JFT | 84.9% | 대규모 데이터 |
  | TimeSformer-L | ImageNet-21K | 80.7% | 이미지→비디오 |
  | VideoMAE-L | 자기지도 | 86.1% | 데이터 효율적 |
```

> **핵심 직관**: 비디오에서 90% 마스킹이 가능한 이유는 **"시간적 중복"**입니다. 연속 프레임은 대부분 동일하고, 변하는 부분은 매우 적습니다. 이 중복성 때문에 10%만 보여줘도 나머지를 복원할 수 있으며, 모델은 **"무엇이 변했는가"**에 집중하게 됩니다.

## 4. 객체 추적

```
객체 추적 (Object Tracking):
  비디오에서 특정 객체를 프레임마다 따라감

  SOT (Single Object Tracking):
  첫 프레임에서 대상 지정 → 이후 추적

  MOT (Multiple Object Tracking):
  여러 객체를 동시에 추적하며 ID 유지

SOT 접근법:

  SiamFC (Bertinetto et al., 2016):
  ├─ 템플릿(첫 프레임 패치)과 검색 영역의 유사도
  ├─ 샴 네트워크: 같은 CNN으로 두 입력 처리
  └─ Cross-Correlation으로 위치 찾기

  현대 SOT: Transformer 기반
  ├─ MixFormer: 템플릿과 검색을 결합하여 처리
  ├─ OSTrack: One-Stream (템플릿+검색 concat)
  └─ SAM 2 (cv-05): 프롬프트 기반 비디오 추적

MOT 접근법:

  Tracking-by-Detection:
  1. 각 프레임에서 객체 탐지
  2. 프레임 간 탐지 결과를 매칭 (ID 할당)

  SORT (Simple Online Realtime Tracking):
  ├─ 칼만 필터: 위치 예측
  ├─ Hungarian Matching: IoU 기반 매칭
  └─ 매우 빠르고 실용적

  DeepSORT:
  ├─ SORT + 외형 특징 (Re-ID)
  ├─ 외형 유사도로 매칭 보완
  └─ 가려진 후 재등장 시 ID 유지

  ByteTrack (Zhang et al., 2022):
  ├─ 저신뢰 탐지 결과도 활용
  ├─ 1차: 고신뢰 탐지 매칭
  ├─ 2차: 저신뢰 탐지를 남은 트랙에 매칭
  └─ MOT17: 80.3 MOTA (간단하면서 SOTA)

  | 방법 | 외형 | 속도 | MOTA |
  |------|------|------|------|
  | SORT | 미사용 | 매우 빠름 | 59.8 |
  | DeepSORT | Re-ID | 빠름 | 75.4 |
  | ByteTrack | 미사용 | 매우 빠름 | 80.3 |
```

## 5. 시간적 행동 탐지와 비디오 캡셔닝

```
시간적 행동 탐지 (Temporal Action Detection):
  "언제 어떤 행동이 시작/끝나는가?"

  행동 인식: 전체 클립 → 하나의 라벨
  행동 탐지: 긴 비디오 → 여러 (시작, 끝, 라벨)

  ActionFormer (Zhang et al., 2022):
  ├─ 다스케일 시간 특징 피라미드
  ├─ 각 시간 위치에서 행동 분류 + 경계 회귀
  ├─ Transformer 기반 시간 모델링
  └─ FCOS(cv-03)의 시간 버전과 유사!

비디오 캡셔닝:
  비디오 → 자연어 설명

  Vid2Seq (Yang et al., 2023):
  ├─ 시간적 이벤트 토큰 + 텍스트 토큰을 하나의 시퀀스로
  ├─ "at [12.3s-15.7s] a man throws a ball"
  └─ 시간 정보를 언어로 통합

  Video-LLM:
  ├─ 비디오 프레임 → CLIP 특징 → LLM에 입력
  ├─ Video-ChatGPT: 비디오에 대한 대화
  ├─ VideoLLaMA: 시청각 통합
  └─ 비디오 질의응답, 요약, 설명 가능

  | 태스크 | 입력 | 출력 | 대표 모델 |
  |--------|------|------|----------|
  | 행동 인식 | 클립 | 라벨 | VideoMAE |
  | 행동 탐지 | 긴 비디오 | (시작,끝,라벨) | ActionFormer |
  | 객체 추적 | 비디오 | 궤적+ID | ByteTrack |
  | 비디오 캡셔닝 | 비디오 | 텍스트 | Video-LLM |
```

## 6. 비디오 생성과 실전

```
비디오 생성:
  cv-07의 Diffusion을 비디오로 확장

  Make-A-Video (Meta, 2022):
  ├─ 텍스트→이미지 Diffusion + 시간 확장
  ├─ 공간 레이어(동결) + 시간 레이어(추가)
  └─ 텍스트 → 짧은 비디오 클립

  Sora (OpenAI, 2024):
  ├─ DiT 기반 (cv-07)
  ├─ 패치: 시공간 패치 (Spacetime Patches)
  ├─ 가변 해상도, 가변 길이
  ├─ 최대 1분 고품질 비디오
  └─ 물리적 일관성: 3D 이해, 반사, 그림자

  Kling, Runway Gen-3 등 상용 모델:
  ├─ 텍스트/이미지 → 비디오
  ├─ 카메라 제어, 모션 브러시
  └─ 영상 제작 워크플로우에 통합

실전 비디오 처리:

  프레임 샘플링 전략:
  ├─ 균일 샘플링: T 프레임 중 N개 균등 추출
  ├─ 밀집 샘플링: 짧은 구간에서 많이 추출
  └─ 키프레임 샘플링: 장면 변화 기반 선택

  메모리 관리:
  비디오는 이미지의 T배 메모리 필요
  ├─ 프레임 수 제한 (8~32 프레임)
  ├─ 해상도 축소 (224×224)
  ├─ 혼합 정밀도 (FP16/BF16)
  └─ 그래디언트 체크포인팅

  추론 최적화:
  ├─ 프레임 캐싱: 중복 계산 방지
  ├─ 조기 종료: 신뢰도 높으면 나머지 프레임 건너뛰기
  └─ 경량 모델: MobileNet 기반 비디오 모델
```

## 핵심 정리

- **Two-Stream**은 외형(RGB)과 운동(Optical Flow)을 분리 처리하는 초기 접근이며, **I3D**는 2D CNN을 3D로 확장하여 시공간 특징을 동시에 학습합니다
- **SlowFast**는 느린(세밀)/빠른(운동) 두 경로로 효율적 시공간 모델링을 달성하며, R(2+1)D는 3D Conv을 2D+1D로 분해합니다
- **Video Transformer**(ViViT, TimeSformer)는 시공간 어텐션을 분해하여 효율성을 높이며, **VideoMAE**는 90% 마스킹으로 데이터 효율적 사전학습을 합니다
- **ByteTrack**은 저신뢰 탐지도 활용하여 간단하면서 SOTA 추적 성능을 달성하고, **SAM 2**는 프롬프트 기반 비디오 추적을 대중화합니다
- **Sora**는 DiT 기반 시공간 패치로 최대 1분의 물리적으로 일관된 비디오를 생성하며, 비디오 이해/생성 모두에서 Transformer가 표준이 되고 있습니다
