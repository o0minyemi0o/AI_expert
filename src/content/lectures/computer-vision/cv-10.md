# Vision Transformer

## 왜 Transformer가 CV에 왔는가

nlp-03에서 Transformer가 NLP를 지배했듯, CV에서도 **Transformer가 CNN을 대체**하기 시작했습니다. cv-01에서 다룬 CNN은 지역적 수용 영역과 귀납적 편향으로 효율적이지만, **전역적 관계 파악**에는 한계가 있습니다. Vision Transformer(ViT)는 이미지를 패치로 나누고 NLP Transformer를 그대로 적용하여, 충분한 데이터가 있을 때 CNN을 능가합니다.

> **핵심 직관**: CNN은 **"가까운 것부터 보라"**는 귀납적 편향이 있습니다. 커널이 지역적이므로 인접 픽셀의 관계를 먼저 학습합니다. ViT는 이런 편향 없이 **"모든 것을 동시에 보라"**부터 시작합니다. 데이터가 적으면 CNN의 편향이 유리하지만, 데이터가 충분하면 ViT의 유연성이 더 강력합니다.

## 1. ViT: 이미지를 패치 시퀀스로

```
ViT (Dosovitskiy et al., 2020):
  An Image is Worth 16x16 Words

  핵심 아이디어:
  이미지를 패치로 나누면 "단어 시퀀스"처럼 처리 가능

  파이프라인:
  1. 이미지 (224×224×3)를 16×16 패치로 분할
     → 14×14 = 196개 패치 (= 시퀀스 길이)
  2. 각 패치를 선형 투영으로 임베딩
     16×16×3 = 768 → D차원 벡터
  3. [CLS] 토큰 + 위치 임베딩 추가
  4. Transformer Encoder에 입력
  5. [CLS] 토큰의 출력으로 분류

  구조:
  [CLS] [P₁] [P₂] ... [P₁₉₆] + Position Embedding
    ↓
  Transformer Encoder (L 레이어)
  ├─ Multi-Head Self-Attention
  ├─ Layer Norm (Pre-Norm)
  ├─ MLP (GELU 활성화)
  └─ Residual Connection
    ↓
  [CLS] 출력 → Classification Head

  모델 변형:
  | 모델 | 레이어 | 헤드 | 차원 | 파라미터 |
  |------|--------|------|------|---------|
  | ViT-S | 12 | 6 | 384 | 22M |
  | ViT-B | 12 | 12 | 768 | 86M |
  | ViT-L | 24 | 16 | 1024 | 307M |
  | ViT-H | 32 | 16 | 1280 | 632M |

  핵심 발견:

  1. 데이터 규모와 성능:
     ImageNet (1.3M): ViT < CNN (데이터 부족)
     ImageNet-21K (14M): ViT ≈ CNN
     JFT-300M: ViT > CNN (데이터 충분)
     → 귀납적 편향 부족을 데이터로 보상

  2. 위치 임베딩:
     학습 가능한 1D 위치 임베딩 사용
     2D 구조 정보 없이도 학습 중 공간적 패턴 획득
     → 위치 임베딩의 유사도를 시각화하면
        인접 패치끼리 유사한 패턴이 나타남!

  3. 어텐션의 시각화:
     초기 레이어: 국지적 패턴 (CNN의 커널과 유사)
     후기 레이어: 전역적 패턴 (먼 패치 간 관계)
     → ViT가 자연스럽게 지역→전역 학습
```

> **핵심 직관**: ViT의 패치 임베딩은 **"이미지를 NLP 문제로 변환"**하는 것입니다. 16×16 패치가 "단어", 패치 시퀀스가 "문장"이 되어 NLP Transformer를 그대로 적용합니다. 이 단순함이 NLP의 스케일링 법칙(nlp-06)을 CV에도 그대로 가져올 수 있게 합니다.

## 2. DeiT: 데이터 효율적 ViT

```
DeiT (Touvron et al., 2021):
  Data-efficient Image Transformers

  ViT의 문제: JFT-300M 같은 대규모 데이터 필요
  DeiT의 해결: ImageNet만으로 ViT를 잘 학습

  핵심 기법:

  1. 강력한 데이터 증강:
     ├─ RandAugment: 랜덤 증강 조합
     ├─ Mixup: 두 이미지를 혼합
     │   x̃ = λx₁ + (1-λ)x₂
     ├─ CutMix: 영역을 잘라서 붙이기
     │   → Mixup보다 지역적 특징 보존
     ├─ Random Erasing: 랜덤 영역 지우기
     └─ 반복 증강 (Repeated Augmentation)

  2. 정규화:
     ├─ Stochastic Depth: 랜덤으로 레이어 건너뛰기
     ├─ Label Smoothing: 0.1
     └─ Weight Decay: 0.05

  3. Knowledge Distillation:
     Teacher: RegNetY-16GF (CNN)
     Student: DeiT

     Distillation Token: [CLS]와 별도의 [DIST] 토큰 추가
     [CLS]: 정답 라벨 학습
     [DIST]: Teacher의 출력 학습
     → CNN 교사의 귀납적 편향을 학습!

  | 모델 | 파라미터 | 데이터 | Top-1 |
  |------|---------|--------|-------|
  | ViT-B | 86M | JFT-300M | 77.9 |
  | DeiT-B | 86M | ImageNet | 81.8 |
  | DeiT-B↑384 | 86M | ImageNet | 83.1 |
  → ImageNet만으로 ViT보다 높은 성능!

DeiT III (Touvron et al., 2022):
  ├─ 개선된 학습 레시피
  ├─ 3-Augment: 간소화된 증강 (Gray+Solar+GBlur)
  ├─ Simple Random Cropping
  └─ ViT-H: 85.2% (ImageNet)
```

## 3. Swin Transformer: 계층적 ViT

```
Swin Transformer (Liu et al., 2021):
  "계층적 구조 + 윈도우 어텐션"

  ViT의 한계:
  ├─ 단일 해상도: 14×14 패치 맵만 출력
  │   → FPN, 세그멘테이션 등에 부적합
  ├─ O(N²) 복잡도: N=196이면 괜찮지만,
  │   고해상도에서 N이 커지면 비효율적
  └─ 위치 일반화: 다양한 해상도 처리 어려움

  Swin의 해결:

  1. 계층적 구조 (CNN처럼):
     Stage 1: 56×56 (4×4 패치) → C=96
     Stage 2: 28×28 (패치 병합) → C=192
     Stage 3: 14×14 (패치 병합) → C=384
     Stage 4: 7×7 (패치 병합) → C=768

     Patch Merging: 인접 2×2 패치를 concat + Linear
     → 해상도 절반, 채널 2배 (CNN의 풀링과 유사)

     → FPN에 바로 연결 가능!
     → 탐지, 세그멘테이션에 적합

  2. Window Attention:
     전역 어텐션 대신 7×7 윈도우 내에서만 어텐션
     → O(N²) → O(N × M²) (M=윈도우 크기 49)
     → 해상도에 대해 선형적!

  3. Shifted Window:
     윈도우 간 정보 교환을 위해 교대 이동

     레이어 l:   [A|B|C|D] 윈도우 분할
     레이어 l+1: 반 윈도우만큼 이동한 분할
     → 윈도우 경계의 정보가 교환됨!

     효율적 구현:
     Cyclic Shift + Masking으로
     윈도우 수를 늘리지 않으면서 Shifted 구현

  | 모델 | 파라미터 | FLOPs | Top-1 |
  |------|---------|-------|-------|
  | Swin-T | 29M | 4.5G | 81.3 |
  | Swin-S | 50M | 8.7G | 83.0 |
  | Swin-B | 88M | 15.4G | 83.5 |
  | Swin-L | 197M | 34.5G | 86.4 |

  다운스트림 영향:
  ├─ COCO 탐지: Swin-L + Cascade = 58.7 AP
  ├─ ADE20K 세그멘테이션: 53.5 mIoU
  └─ 다양한 CV 태스크의 새로운 표준 백본
```

> **핵심 직관**: Swin의 Shifted Window는 **"지역적으로 보되, 정보는 교환하라"**는 절충입니다. Window Attention은 CNN의 커널처럼 지역적이지만, Shift를 통해 인접 윈도우 간 정보가 흐릅니다. CNN의 효율성과 Transformer의 유연성을 결합한 설계입니다.

## 4. 효율적 ViT 변형들

```
PVT (Pyramid Vision Transformer):
  ViT에 계층적 구조를 도입 (Swin과 유사)
  SRA(Spatial Reduction Attention)로 K, V 다운샘플

CvT (Convolutional Vision Transformer):
  ├─ Convolutional Token Embedding
  │   → 패치 경계의 정보 손실 방지
  ├─ Convolutional Projection (Q, K, V)
  │   → 지역적 구조를 어텐션에 주입
  └─ 위치 임베딩 불필요!

EfficientViT (Liu et al., 2023):
  모바일/엣지 디바이스를 위한 경량 ViT
  ├─ Sandwich Layout: Conv → Attention → Conv
  ├─ Cascaded Group Attention
  └─ 1ms 미만의 지연 시간 (모바일)

FlashAttention과 ViT:
  nlp-11에서 다룬 FlashAttention이 ViT에도 핵심
  ├─ IO-aware 알고리즘으로 메모리 접근 최소화
  ├─ 긴 시퀀스(고해상도)에서 2-4배 속도 향상
  └─ ViT의 실전 배포에 필수적

해상도 적응:
  ViT의 위치 임베딩은 고정 크기로 학습
  다른 해상도에서 사용하려면?

  방법 1: 2D 보간
  224에서 학습한 14×14 위치 임베딩을
  384에서 24×24로 이중선형 보간
  → 간단하지만 성능 약간 하락

  방법 2: RoPE/ALiBi 적용
  nlp-11의 RoPE를 2D로 확장
  → 해상도에 무관한 위치 인코딩

  방법 3: FlexiViT
  다양한 패치 크기를 하나의 모델로
  → 추론 시 해상도/속도 트레이드오프 조절
```

## 5. CNN vs Transformer: 수렴과 공존

```
ConvNeXt의 반격 (Liu et al., 2022):
  "CNN도 ViT처럼 설계하면 경쟁력 있다"
  (cv-01에서 간략히 다룸, 여기서 상세히)

  ResNet-50에서 시작하여 Swin의 설계를 차용:

  1. 매크로 설계:
     스테이지 비율 (3,4,6,3) → (3,3,9,3) (Swin처럼)
     패치 분할 스템: 4×4 Conv, stride 4

  2. ResNeXt 스타일:
     Depthwise Separable Conv (MobileNet처럼)
     → 그룹 수 = 채널 수

  3. Inverted Bottleneck:
     좁은→넓은→좁은 대신 넓은→좁은→넓은
     (MobileNetV2와 동일)

  4. 큰 커널: 3×3 → 7×7
     → ViT의 큰 수용 영역을 모방

  5. 미세 설계:
     ReLU → GELU, BN → LN
     Fewer activation, fewer norm

  결과:
  | 모델 | 파라미터 | Top-1 | 특징 |
  |------|---------|-------|------|
  | Swin-T | 29M | 81.3 | Transformer |
  | ConvNeXt-T | 29M | 82.1 | CNN |
  | Swin-B | 88M | 83.5 | Transformer |
  | ConvNeXt-B | 89M | 83.8 | CNN |
  → CNN이 Transformer와 동등 또는 우위!

  교훈:
  ViT의 성능이 Transformer 아키텍처 자체가 아닌
  학습 기법(증강, 정규화, 스케줄)에서 온 부분이 큼
  → "아키텍처보다 레시피"

현재의 합의 (2024-2025):
  ├─ 분류: ConvNeXt ≈ Swin ≈ ViT
  ├─ 탐지/세그멘테이션: Swin, ConvNeXt 계층적 구조 선호
  ├─ 생성: DiT (ViT 기반) 우세
  ├─ SSL: ViT (MAE, DINO) 우세
  ├─ 멀티모달: ViT (CLIP) 우세
  └─ 엣지/모바일: CNN 또는 하이브리드

  → 단일 승자가 아닌, 태스크별 최적 선택
```

## 6. 실전 백본 선택

```
태스크별 백본 가이드:

  이미지 분류:
  ├─ 정확도 우선: ViT-L/H (큰 데이터셋 사전학습)
  ├─ 효율 우선: EfficientNet-B4, ConvNeXt-S
  └─ 모바일: EfficientViT, MobileNetV3

  객체 탐지:
  ├─ 정확도: Swin-L + Co-DETR
  ├─ 균형: ConvNeXt-B + Cascade R-CNN
  └─ 속도: YOLOv8 (CSP 백본)

  세그멘테이션:
  ├─ 시맨틱: Swin + Mask2Former
  ├─ 범용: DINOv2 + Linear
  └─ 의료: U-Net (ViT encoder)

  사전학습 선택:
  ├─ 범용: DINOv2 → 파인튜닝
  ├─ 분류: ImageNet-21K → 파인튜닝
  ├─ 멀티모달: CLIP → 파인튜닝
  └─ 생성: MAE → 파인튜닝

  학습 레시피 (DeiT 스타일):
  ├─ Optimizer: AdamW (lr=1e-3, wd=0.05)
  ├─ 스케줄: Cosine Decay + Warmup (5 에폭)
  ├─ 증강: RandAug + Mixup + CutMix
  ├─ 정규화: Stochastic Depth + Label Smoothing
  ├─ 에폭: 300 (ImageNet)
  └─ 배치: 1024 (분산 학습)
```

## 핵심 정리

- **ViT**는 이미지를 16×16 패치 시퀀스로 변환하여 NLP Transformer를 그대로 적용하며, 대규모 데이터(JFT-300M)에서 CNN을 능가합니다
- **DeiT**는 강력한 증강/정규화와 CNN 교사의 Knowledge Distillation으로 ImageNet만으로도 ViT를 효과적으로 학습합니다
- **Swin Transformer**는 계층적 구조와 Shifted Window Attention으로 O(N²)를 O(N)으로 줄이며, 탐지/세그멘테이션의 표준 백본입니다
- **ConvNeXt**는 CNN에 Transformer의 설계 원칙을 적용하여 동등한 성능을 달성했으며, "아키텍처보다 학습 레시피"가 중요함을 보여줍니다
- 현재는 CNN과 Transformer의 **공존** 시대이며, 분류/SSL/멀티모달은 ViT가, 탐지/세그멘테이션은 계층적 구조(Swin/ConvNeXt)가, 엣지는 CNN이 우세합니다
