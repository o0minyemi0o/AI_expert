# 제어 가능한 생성

## 왜 제어가 필요한가

cv-07의 Diffusion Model은 텍스트 프롬프트로 이미지를 생성합니다. 하지만 "빨간 드레스를 입은 여성이 해변에서 왼쪽을 바라보며 서 있다"를 텍스트만으로 정확히 제어하기는 어렵습니다. **포즈, 구도, 깊이, 엣지** 같은 공간적 구조를 직접 지정하고 싶다면? **ControlNet**이 이 문제를 해결하며, **IP-Adapter**는 참조 이미지의 스타일을, **Inpainting**은 이미지의 특정 영역만 편집합니다.

> **핵심 직관**: 제어 가능한 생성의 핵심은 **"무엇을(what) 그릴지와 어떻게(how) 그릴지를 분리"**하는 것입니다. 텍스트 프롬프트가 "무엇을"을 결정한다면, ControlNet의 조건 이미지가 "어떤 구조로"를 결정합니다. 이 분리가 정밀한 제어를 가능하게 합니다.

## 1. ControlNet

```
ControlNet (Zhang & Agrawala, 2023):
  "사전학습 Diffusion에 공간적 조건을 추가"

  핵심 아이디어:
  기존 SD의 U-Net 가중치를 동결하고,
  조건 처리를 위한 복제본을 추가

  구조:
  ┌─────────────────────────────────┐
  │  Locked U-Net (원본, 동결)       │ ← 텍스트 + 잡음
  │  출력에 ControlNet 출력을 더함    │
  └──────────┬──────────────────────┘
             │ + (잔차 연결)
  ┌──────────┴──────────────────────┐
  │  ControlNet (복제본, 학습)       │ ← 조건 이미지
  │  Zero Convolution으로 연결       │
  └─────────────────────────────────┘

  Zero Convolution:
  초기 가중치와 편향이 모두 0인 1×1 Conv
  → 학습 시작 시 ControlNet 출력 = 0
  → 원본 SD의 품질을 보존하며 점진적으로 조건 학습
  → 기존 모델을 해치지 않는 "안전한" 추가

  지원하는 조건 유형:
  ├─ Canny Edge: 엣지 맵으로 구조 지정
  ├─ Depth Map: 깊이로 공간 구조 지정
  ├─ OpenPose: 관절 키포인트로 포즈 지정
  ├─ Segmentation Map: 영역별 시맨틱 지정
  ├─ Normal Map: 표면 방향으로 3D 구조 지정
  ├─ Scribble/Sketch: 낙서/스케치로 대략적 형태
  ├─ HED Boundary: 소프트 엣지
  └─ M-LSD Lines: 직선 구조

  학습:
  ├─ 원본 SD 가중치 완전 동결
  ├─ ControlNet 복제본 + Zero Conv만 학습
  ├─ 데이터: (조건 이미지, 텍스트, 원본 이미지) 삼중쌍
  └─ GPU 1장으로도 학습 가능 (효율적)

  | 조건 | 제어 대상 | 정밀도 | 사용 예시 |
  |------|----------|--------|---------|
  | Canny | 윤곽선 | 높음 | 캐릭터 라인아트 |
  | Depth | 3D 구조 | 중간 | 건축/실내 |
  | Pose | 인체 포즈 | 중간 | 인물 생성 |
  | Seg | 영역 배치 | 낮음 | 장면 구성 |
  | Scribble | 대략적 형태 | 낮음 | 빠른 스케치 |
```

> **핵심 직관**: ControlNet의 Zero Convolution은 **"먼저 해를 끼치지 말라"**의 원칙입니다. nlp-07의 LoRA가 사전학습 가중치를 동결하고 저랭크 어댑터만 학습하듯, ControlNet은 원본 U-Net을 동결하고 조건 처리만 별도로 학습합니다. 학습 초기에 출력이 0이므로 원본 품질이 보존됩니다.

## 2. T2I-Adapter와 IP-Adapter

```
T2I-Adapter (Mou et al., 2023):
  ControlNet보다 가벼운 조건 제어

  ControlNet: U-Net 인코더 전체 복제 (~1.4B 파라미터)
  T2I-Adapter: 경량 어댑터 모듈만 추가 (~77M 파라미터)

  구조:
  조건 이미지 → 4개 다운샘플 블록 → 4개 특징 맵
  각 특징 맵을 U-Net 인코더의 해당 레벨에 더함

  장점: ControlNet 대비 20배 가벼움
  단점: 정밀도가 약간 낮을 수 있음

IP-Adapter (Ye et al., 2023):
  "참조 이미지의 스타일/내용을 전달"

  텍스트가 아닌 이미지로 조건 지정:
  "이 스타일로 그려줘", "이 캐릭터를 다른 포즈로"

  구조:
  참조 이미지 → CLIP Image Encoder → 이미지 특징
  → Decoupled Cross-Attention으로 U-Net에 주입

  Decoupled Cross-Attention:
  기존: Attention(Q, K_text, V_text)
  추가: Attention(Q, K_image, V_image)
  최종: 텍스트 어텐션 + λ × 이미지 어텐션

  → 텍스트와 이미지 조건을 독립적으로 처리!
  → λ로 이미지 조건의 강도 조절

  변형:
  ├─ IP-Adapter-Plus: CLIP 마지막 레이어 대신 중간 특징 사용
  │   → 더 세밀한 구조 보존
  ├─ IP-Adapter-FaceID: 얼굴 인식 모델 특징 사용
  │   → 인물 일관성 유지
  └─ InstantID: IP-Adapter + ControlNet(Face) 결합
     → 한 장의 사진으로 캐릭터 일관성

  제어 조합:
  텍스트("해변에서") + IP-Adapter("이 스타일로")
    + ControlNet-Pose("이 포즈로")
  → 다중 조건의 동시 적용 가능!
```

## 3. Inpainting과 Outpainting

```
Inpainting: 이미지의 특정 영역만 수정

  기본 원리:
  원본 이미지 x₀ + 마스크 m (편집할 영역)
  → 마스크 영역만 생성, 나머지는 원본 유지

  방법 1: 학습 기반 Inpainting
  SD Inpainting 모델:
  입력: [잡음 잠재, 마스크 잠재, 원본 잠재] concat
  → 9채널 입력 (기존 4 + 마스크 1 + 원본 4)
  → Inpainting 전용 파인튜닝

  방법 2: 추론 시 마스킹 (학습 불필요)
  각 디노이징 스텝에서:
  x_t = m ⊙ x_t^{generated} + (1-m) ⊙ x_t^{original}
  → 마스크 영역만 생성된 값, 나머지는 원본의 잡음 버전

  문제: 경계가 부자연스러울 수 있음
  해결: 마스크 블러링, 마스크 확장

  응용:
  ├─ 객체 제거: 불필요한 물체를 배경으로 채움
  ├─ 객체 교체: "자동차"를 "트럭"으로
  ├─ 부분 수정: 표정만 바꾸기, 옷만 바꾸기
  └─ 결함 복원: 손상된 사진 복원

Outpainting: 이미지 경계를 넘어 확장
  ├─ 좌/우/상/하로 이미지 확장
  ├─ 파노라마 생성에 활용
  └─ DALL-E 2가 대중화

  | 기법 | 입력 | 출력 | 학습 |
  |------|------|------|------|
  | Inpainting (전용) | 이미지+마스크 | 수정된 이미지 | 별도 학습 |
  | Inpainting (추론) | 이미지+마스크 | 수정된 이미지 | 불필요 |
  | Outpainting | 부분 이미지 | 확장된 이미지 | 별도 학습 |
```

## 4. 이미지 편집 기법

```
SDEdit (Meng et al., 2022):
  "이미지에 잡음을 추가한 뒤 역방향으로 편집"

  1. 원본 이미지 x₀에 t 스텝만큼 잡음 추가 → x_t
  2. x_t에서 새로운 프롬프트로 역방향 디노이징
  → 전체 구조는 유지하면서 스타일/내용 변경

  t가 작으면: 원본에 가까움 (미세한 변화)
  t가 크면: 새로운 이미지에 가까움 (큰 변화)
  → t로 편집 강도를 제어

InstructPix2Pix (Brooks et al., 2023):
  "텍스트 지시로 이미지 편집"

  입력: 이미지 + "턱수염을 추가해줘"
  출력: 턱수염이 추가된 이미지

  학습 데이터 생성:
  1. GPT-3로 편집 지시문 생성
     "사진" → "턱수염을 추가해줘"
  2. Prompt-to-Prompt로 편집된 이미지 쌍 생성
  → (원본, 지시문, 편집 결과) 삼중쌍 45만 개

  2개의 CFG 스케일:
  s_I: 원본 이미지 충실도 (높으면 원본 유지)
  s_T: 텍스트 지시 충실도 (높으면 편집 강하게)

Prompt-to-Prompt (Hertz et al., 2022):
  Cross-Attention 맵 조작으로 편집

  "고양이가 잔디에 앉아있다"
  → "개가 잔디에 앉아있다"

  Cross-Attention에서 "고양이" 토큰의 Attention 맵
  = "고양이"가 위치한 공간 영역
  → 이 맵을 "개"에 그대로 적용
  → 나머지 구조를 유지하면서 고양이만 개로!

  편집 유형:
  ├─ 단어 교체: 고양이 → 개
  ├─ 프롬프트 정제: 더 구체적인 묘사 추가
  └─ 가중치 조절: 특정 단어의 영향 강화/약화

LEDITS++ / Null-Text Inversion:
  실제 이미지의 정확한 재구성 + 편집
  ├─ DDIM Inversion: 실제 이미지 → 잠재 벡터
  ├─ Null-text 최적화: 비조건부 임베딩을 최적화
  └─ 편집: 변경된 프롬프트로 디코딩
```

## 5. 텍스트-이미지 생성의 진화

```
주요 텍스트-이미지 모델:

  DALL-E (OpenAI, 2021):
  ├─ dVAE + Autoregressive Transformer
  ├─ 텍스트 토큰 + 이미지 토큰을 순차 생성
  └─ 12B 파라미터

  DALL-E 2 (OpenAI, 2022):
  ├─ CLIP 텍스트 → CLIP 이미지 (Prior)
  ├─ CLIP 이미지 → 이미지 (Diffusion Decoder)
  └─ Unpainting, Variations 기능

  DALL-E 3 (OpenAI, 2023):
  ├─ GPT-4로 캡션을 자동 개선 (Recaptioning)
  │   → "사진" → "밝은 오후 햇살 아래 공원 벤치에..."
  ├─ 개선된 캡션으로 학습 → 텍스트 이해 대폭 향상
  └─ DiT 기반 아키텍처

  Midjourney:
  ├─ 독자적 모델 (비공개)
  ├─ 미학적 품질에 특화
  └─ Discord 기반 인터페이스

  Imagen (Google, 2022):
  ├─ T5-XXL 텍스트 인코더 (큰 언어 모델)
  ├─ 64×64 → 256×256 → 1024×1024 캐스케이드
  └─ 텍스트 인코더 크기가 품질에 핵심적

  핵심 교훈:
  ├─ 텍스트 인코더가 클수록 텍스트 이해가 좋다 (Imagen)
  ├─ 캡션 품질이 생성 품질을 결정한다 (DALL-E 3)
  ├─ 잠재 공간 Diffusion이 효율적이다 (SD)
  └─ DiT가 U-Net을 대체하는 추세 (SD3, DALL-E 3)

  | 모델 | 텍스트 인코더 | Diffusion | 특징 |
  |------|-------------|-----------|------|
  | SD 1.5 | CLIP-L | U-Net | 오픈소스 |
  | SDXL | CLIP-L + G | U-Net | 고해상도 |
  | SD 3 | CLIP + T5 | DiT(MMDiT) | Flow |
  | DALL-E 3 | T5 | DiT | Recaptioning |
  | Imagen | T5-XXL | U-Net 캐스케이드 | 텍스트 강조 |
```

## 6. 제어 기술의 조합과 실전

```
다중 조건 조합:

  ControlNet 다중 적용:
  Pose + Depth → 포즈와 공간 구조 동시 제어
  각 ControlNet의 가중치를 별도 설정

  ControlNet + IP-Adapter:
  구조(ControlNet) + 스타일(IP-Adapter)
  → "이 스타일로, 이 포즈의 이미지"

  Regional Prompting:
  이미지 영역별로 다른 프롬프트 적용
  → "왼쪽: 산, 오른쪽: 바다"

실전 워크플로우:

  캐릭터 일관성:
  1. 참조 이미지 → IP-Adapter-FaceID
  2. 포즈 지정 → ControlNet-Pose
  3. 텍스트로 배경/의상 지정
  → 같은 인물이 다른 상황에서

  건축 시각화:
  1. 3D 모델 → Depth/Normal Map
  2. ControlNet-Depth + 텍스트("모던 건축")
  → 3D 모델의 구조로 사실적 렌더링

  제품 사진:
  1. 제품 사진 → 배경 제거
  2. Inpainting으로 새 배경 생성
  3. ControlNet-Canny로 제품 윤곽 유지

ComfyUI / Automatic1111:
  ├─ 노드 기반 워크플로우 (ComfyUI)
  │   → 복잡한 파이프라인을 시각적으로 구성
  ├─ WebUI (A1111)
  │   → 간편한 인터페이스
  └─ 확장 생태계: ControlNet, IP-Adapter 등 통합

  | 도구 | 장점 | 적합한 사용자 |
  |------|------|-------------|
  | ComfyUI | 유연한 파이프라인 | 고급 사용자 |
  | A1111 | 간편한 UI | 일반 사용자 |
  | Diffusers | Python API | 개발자 |
```

## 핵심 정리

- **ControlNet**은 사전학습 U-Net을 동결하고 Zero Convolution으로 연결된 복제본을 학습하여, Canny/Depth/Pose 등 공간적 조건으로 생성을 제어합니다
- **IP-Adapter**는 Decoupled Cross-Attention으로 참조 이미지의 스타일/내용을 전달하며, 텍스트와 이미지 조건을 독립적으로 제어합니다
- **Inpainting**은 마스크 영역만 재생성하여 객체 제거/교체/수정을 수행하며, 학습 기반과 추론 시 마스킹 두 가지 접근이 있습니다
- **InstructPix2Pix**와 **Prompt-to-Prompt**는 텍스트 지시나 Cross-Attention 맵 조작으로 이미지를 편집하며, DALL-E 3의 Recaptioning은 텍스트 이해를 혁신적으로 개선했습니다
- 실전에서는 **ControlNet + IP-Adapter + 텍스트**의 다중 조건 조합으로 구조/스타일/내용을 독립적으로 제어하며, ComfyUI 같은 도구가 복잡한 파이프라인을 관리합니다
