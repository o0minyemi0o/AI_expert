# 3D 비전

## 왜 3D 이해가 필요한가

지금까지의 CV는 주로 **2D 이미지**를 다루었습니다. 하지만 현실 세계는 3차원입니다. 자율주행 차량은 보행자까지의 **거리**를 알아야 하고, AR/VR은 3D 공간을 **재구성**해야 합니다. **NeRF**는 여러 장의 사진에서 3D 장면을 복원하고, **3D Gaussian Splatting**은 이를 실시간으로 렌더링하며, **깊이 추정**은 단일 이미지에서 깊이를 예측합니다.

> **핵심 직관**: 2D 이미지에서 3D를 복원하는 것은 본질적으로 **"정보가 부족한 문제(ill-posed)"**입니다. 사진 한 장에서 깊이를 정확히 알 수 없습니다. 이를 해결하는 두 가지 접근: **여러 시점**의 이미지로 기하학적으로 복원하거나(NeRF), 대규모 데이터로 **깊이의 패턴을 학습**합니다(단안 깊이 추정).

## 1. NeRF: 신경 방사장

```
NeRF (Mildenhall et al., 2020):
  Neural Radiance Fields

  핵심: 장면을 연속 함수로 표현

  입력: 3D 위치 (x, y, z) + 시선 방향 (θ, φ)
  출력: 색상 (r, g, b) + 밀도 (σ)

  F_θ: (x, y, z, θ, φ) → (r, g, b, σ)
  → MLP (8 레이어, 256 차원)

  볼륨 렌더링:
  광선(ray)을 따라 색상과 밀도를 적분

  C(r) = ∫ T(t) · σ(r(t)) · c(r(t), d) dt
  T(t) = exp(-∫ σ(r(s)) ds)  (투과율)

  이산화:
  C = Σᵢ Tᵢ (1 - exp(-σᵢ δᵢ)) cᵢ
  Tᵢ = exp(-Σⱼ<ᵢ σⱼ δⱼ)

  학습:
  1. 여러 시점의 사진(50~100장) 수집
  2. 각 픽셀에 대해 카메라에서 광선 발사
  3. 광선 위의 점들에서 MLP로 색상/밀도 예측
  4. 볼륨 렌더링으로 픽셀 색상 계산
  5. 실제 픽셀과의 MSE 최소화

  핵심 기법:

  Positional Encoding:
  저차원 입력 (x,y,z)를 고차원으로 변환
  γ(p) = (sin(2⁰πp), cos(2⁰πp), ..., sin(2^(L-1)πp), cos(2^(L-1)πp))
  → MLP가 고주파 디테일을 학습할 수 있게 함
  → nlp-03의 위치 인코딩과 유사한 원리

  Hierarchical Sampling:
  1단계 (Coarse): 균일 샘플링으로 밀도 분포 파악
  2단계 (Fine): 밀도 높은 영역에 집중 샘플링
  → 빈 공간의 불필요한 계산 감소

  결과: 놀라운 Novel View Synthesis
  학습에 사용하지 않은 시점에서의 이미지 생성!

  한계:
  ├─ 학습 시간: 장면당 수 시간~수일
  ├─ 렌더링 속도: 30초/이미지 (실시간 불가)
  ├─ 정적 장면만: 움직이는 물체 처리 어려움
  └─ 장면별 학습: 새 장면마다 처음부터 학습
```

> **핵심 직관**: NeRF의 핵심은 **"장면을 함수로 기억"**하는 것입니다. 전통적 3D 표현(메시, 복셀)은 명시적으로 기하학을 저장하지만, NeRF는 MLP의 가중치에 장면 전체를 **암묵적으로** 인코딩합니다. 어떤 위치, 어떤 방향에서든 색상을 질의할 수 있는 "연속적 장면 함수"입니다.

## 2. NeRF의 진화

```
Instant-NGP (Müller et al., 2022):
  "NeRF를 수 초 만에 학습"

  핵심: Positional Encoding 대신 Hash Encoding

  Multi-resolution Hash Grid:
  여러 해상도의 격자에 학습 가능한 특징 저장
  (x,y,z) → 해시 함수 → 격자 정점의 특징 보간
  → MLP 입력으로 사용

  장점:
  ├─ 학습: 수 시간 → 수 초 (1000배 빠름!)
  ├─ 렌더링: 수십 ms (실시간에 근접)
  ├─ 메모리: 해시 충돌로 자동 압축
  └─ 품질: 원본 NeRF와 동등

Mip-NeRF (Barron et al., 2021):
  ├─ 점(point) 대신 원뿔(cone)로 광선 모델링
  ├─ 다중 스케일 렌더링 (앨리어싱 방지)
  └─ 다양한 거리에서 일관된 품질

Mip-NeRF 360 (Barron et al., 2022):
  ├─ 실외/무한 장면 처리
  ├─ 수축(contraction)으로 무한 공간 매핑
  └─ 360도 장면의 고품질 재구성

Dynamic NeRF:
  시간에 따라 변하는 장면

  D-NeRF: F(x, y, z, t) → (r, g, b, σ)
  ├─ Canonical space + Deformation field
  ├─ 시간 t에 따른 변형을 학습
  └─ 움직이는 물체, 사람 재구성

  | 모델 | 학습 시간 | 렌더링 | 품질 (PSNR) |
  |------|----------|--------|------------|
  | NeRF | ~12시간 | ~30초/장 | 31.0 |
  | Instant-NGP | ~5초 | ~15ms | 33.2 |
  | Mip-NeRF 360 | ~12시간 | ~5초/장 | 33.5 |
```

## 3. 3D Gaussian Splatting

```
3DGS (Kerbl et al., 2023):
  3D Gaussian Splatting — NeRF의 대안

  NeRF: 암묵적 (MLP가 장면 함수)
  3DGS: 명시적 (3D 가우시안 집합이 장면 표현)

  각 가우시안의 속성:
  ├─ 위치: μ = (x, y, z)
  ├─ 공분산: Σ (크기와 방향, 3D 타원체)
  ├─ 색상: c (구면 조화 계수, 방향 의존)
  ├─ 불투명도: α
  └─ 수만~수백만 개의 가우시안으로 장면 구성

  렌더링 (Splatting):
  1. 3D 가우시안을 카메라 평면에 투영 → 2D 가우시안
  2. 깊이순 정렬
  3. 알파 블렌딩으로 픽셀 색상 계산
  C = Σᵢ cᵢ αᵢ ∏ⱼ<ᵢ (1 - αⱼ)
  → 미분 가능한 래스터라이저!

  학습:
  1. SfM(Structure from Motion)으로 초기 점 구름 추출
  2. 각 점을 가우시안으로 초기화
  3. 렌더링 → 실제 이미지와 비교 → 역전파
  4. Adaptive Density Control:
     ├─ 클론: 작은 가우시안이 부족한 영역에 복제
     ├─ 분할: 큰 가우시안을 둘로 분할
     └─ 가지치기: 투명한 가우시안 제거

  장점:
  ├─ 실시간 렌더링: 100+ FPS!
  ├─ 학습 속도: Instant-NGP와 유사 (수 분)
  ├─ 높은 품질: NeRF와 동등 또는 우위
  └─ 편집 가능: 가우시안 직접 조작

  vs NeRF:
  | 속성 | NeRF | 3DGS |
  |------|------|------|
  | 표현 | 암묵적 (MLP) | 명시적 (가우시안) |
  | 렌더링 | 볼륨 렌더링 | 래스터라이제이션 |
  | 속도 | 느림 (광선 추적) | 매우 빠름 (GPU) |
  | 편집 | 어려움 | 직접 조작 가능 |
  | 메모리 | 작음 (MLP 가중치) | 큼 (가우시안 저장) |
  | 품질 | 높음 | 높음 |

후속 발전:
  ├─ 4DGS: 시간 축 추가 → 동적 장면
  ├─ DreamGaussian: 텍스트 → 3D 가우시안
  ├─ GaussianEditor: 텍스트로 3D 장면 편집
  └─ 압축: 수백 MB → 수 MB로 압축
```

> **핵심 직관**: 3DGS가 NeRF보다 빠른 이유는 **"렌더링 방식의 차이"**입니다. NeRF는 각 픽셀마다 광선을 따라 수백 개의 MLP 쿼리가 필요하지만, 3DGS는 가우시안을 한 번에 투영하고 블렌딩합니다. GPU의 병렬 처리에 래스터라이제이션이 최적화되어 있기에 100+ FPS가 가능합니다.

## 4. 깊이 추정

```
단안 깊이 추정 (Monocular Depth Estimation):
  단일 이미지에서 픽셀별 깊이 예측

  왜 가능한가?
  하나의 이미지에서 깊이를 수학적으로 결정할 수 없지만,
  인간은 단서를 사용합니다:
  ├─ 상대적 크기: 멀면 작게 보임
  ├─ 텍스처 기울기: 멀면 텍스처가 조밀
  ├─ 가림(Occlusion): 가리는 것이 앞에
  ├─ 원근법: 평행선이 소실점으로 수렴
  └─ 대기 투시: 먼 물체가 흐릿

  DPT (Ranftl et al., 2021):
  Dense Prediction Transformer
  ├─ ViT 인코더 + DPT 디코더
  ├─ 다중 데이터셋 혼합 학습 (MiDaS)
  ├─ 상대적 깊이(relative depth) 예측
  └─ 제로샷 일반화 능력

  Depth Anything (Yang et al., 2024):
  ├─ DINOv2 인코더 + DPT 디코더
  ├─ 6200만 라벨 없는 이미지로 자기지도 학습
  ├─ 다양한 도메인에서 강건한 깊이 추정
  └─ 현재 단안 깊이의 표준

  Depth Anything V2:
  ├─ 합성 데이터로 학습 (정확한 GT 깊이)
  ├─ 라벨 없는 실제 이미지로 자기지도 정제
  └─ 절대적 깊이(metric depth) 추정 가능

  | 모델 | 인코더 | 특징 | 상대/절대 |
  |------|--------|------|---------|
  | MiDaS | ResNet/ViT | 다중 데이터셋 | 상대 |
  | DPT | ViT | Transformer 기반 | 상대 |
  | Depth Anything | DINOv2 | SSL 사전학습 | 상대 |
  | Depth Anything V2 | DINOv2 | 합성+실제 | 절대 가능 |

스테레오 깊이 추정:
  두 카메라(좌/우)로 삼각측량

  시차(Disparity) = 좌우 이미지에서 같은 점의 수평 차이
  깊이 = (초점거리 × 기선거리) / 시차

  RAFT-Stereo: 반복적 시차 정제
  → 자율주행의 표준 방법
```

## 5. 3D 재구성과 생성

```
Structure from Motion (SfM):
  여러 사진에서 카메라 위치 + 3D 점 구름 추정

  COLMAP (Schönberger et al., 2016):
  ├─ 특징 매칭 (SIFT)
  ├─ 카메라 파라미터 추정
  ├─ 번들 조정 (Bundle Adjustment)
  └─ NeRF/3DGS의 전처리 단계

  DUSt3R (Wang et al., 2024):
  ├─ 이미지 쌍 → 직접 3D 점 구름 예측
  ├─ COLMAP의 복잡한 파이프라인을 NN으로 대체
  └─ 더 빠르고 강건

텍스트/이미지에서 3D 생성:

  DreamFusion (Poole et al., 2022):
  ├─ 텍스트 → 2D Diffusion의 Score로 3D NeRF 최적화
  ├─ SDS (Score Distillation Sampling):
  │   2D Diffusion이 "이 각도에서 이 물체는 이렇게 보여야"라고 가이드
  │   → 모든 각도에서 일관된 3D 형태 학습
  └─ 3D 학습 데이터 없이 텍스트 → 3D!

  Zero-1-to-3 / One-2-3-45:
  단일 이미지 → 여러 시점 생성 → 3D 재구성

  LRM (Large Reconstruction Model):
  ├─ 단일 이미지 → 5초 만에 3D 모델
  ├─ Transformer 기반 대규모 모델
  └─ 100만 개의 3D 데이터로 학습

  | 방법 | 입력 | 시간 | 3D 표현 |
  |------|------|------|--------|
  | DreamFusion | 텍스트 | ~1시간 | NeRF |
  | DreamGaussian | 텍스트 | ~2분 | 3DGS |
  | LRM | 이미지 1장 | ~5초 | TriPlane |
  | Zero-1-to-3 | 이미지 1장 | ~수분 | 메시 |
```

## 6. 3D 비전의 실전 응용

```
자율주행:
  ├─ LiDAR 포인트 클라우드: 직접적 3D 측정
  ├─ 카메라 기반 3D: BEVFormer, BEVDet
  │   → 다중 카메라 → Bird's Eye View 변환
  │   → LiDAR 없이 3D 탐지/세그멘테이션
  ├─ 깊이 추정 + 2D 탐지 → 3D 위치 추정
  └─ 점유 예측(Occupancy): 3D 공간의 점유 여부

  BEV (Bird's Eye View) 변환:
  다중 카메라 이미지 → 상공에서 본 3D 표현
  ├─ 시점 변환이 핵심 기술
  ├─ Transformer Cross-Attention으로 구현
  └─ Tesla, Waymo 등이 채택

AR/VR:
  ├─ SLAM (Simultaneous Localization and Mapping)
  │   → 실시간 카메라 위치 추정 + 3D 맵 구축
  ├─ 3DGS로 실시간 장면 렌더링
  ├─ 핸드 트래킹: 3D 손 포즈 추정
  └─ 물체 인식: 6DoF 포즈 추정

로봇:
  ├─ 파지(Grasping): 3D 포즈에서 잡기 위치 결정
  ├─ 네비게이션: 3D 장애물 회피
  ├─ 조작(Manipulation): 3D 이해 기반 물체 조작
  └─ Sim-to-Real: 시뮬레이션 3D → 실세계 전이

  | 응용 | 핵심 기술 | 실시간 요구 |
  |------|----------|-----------|
  | 자율주행 | BEV, 깊이 | 필수 (30Hz) |
  | AR/VR | SLAM, 3DGS | 필수 (90Hz) |
  | 로봇 | 깊이, 포즈 | 높음 (10Hz) |
  | 영화/게임 | NeRF, 3DGS | 오프라인 가능 |
  | 건축 | SfM, 3DGS | 오프라인 가능 |
```

## 핵심 정리

- **NeRF**는 MLP가 3D 위치/방향 → 색상/밀도를 학습하는 암묵적 표현이며, Positional Encoding과 Hierarchical Sampling으로 고주파 디테일을 복원합니다
- **Instant-NGP**는 Hash Encoding으로 NeRF 학습을 수 시간에서 수 초로 단축하며, **3D Gaussian Splatting**은 명시적 가우시안 + 래스터라이제이션으로 100+ FPS 실시간 렌더링을 달성합니다
- **Depth Anything**은 DINOv2 + 대규모 자기지도 학습으로 단안 깊이 추정의 표준이며, 합성 데이터 학습(V2)으로 절대적 깊이도 추정합니다
- **DreamFusion**은 2D Diffusion의 SDS로 텍스트 → 3D를 가능하게 하고, **LRM**은 단일 이미지에서 5초 만에 3D를 재구성합니다
- **자율주행의 BEV 변환**, **AR/VR의 SLAM**, **로봇의 파지** 등 실세계 응용에서 3D 비전이 핵심이며, 3DGS의 실시간 렌더링이 AR/VR을 혁신하고 있습니다
