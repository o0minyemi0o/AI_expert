# 이미지 생성: Diffusion Models

## 왜 Diffusion이 이미지 생성의 주류가 되었나

cv-06의 GAN은 생성기와 판별기의 경쟁으로 이미지를 만들었지만, 학습 불안정과 모드 붕괴라는 근본적 한계가 있었습니다. **Diffusion Model**은 완전히 다른 접근을 취합니다: 이미지에 점진적으로 잡음을 추가한 뒤, 그 **잡음을 되돌리는 과정을 학습**합니다. 이 단순한 아이디어가 Stable Diffusion, DALL-E, Midjourney의 기반이 되며, 현재 이미지 생성의 표준입니다.

> **핵심 직관**: Diffusion의 핵심은 **"파괴는 쉽고, 복원은 어렵다"**입니다. 깨끗한 이미지에 잡음을 더하는 것은 수학적으로 간단합니다. 모델이 학습하는 것은 이 과정의 **역방향**—잡음에서 구조를 복원하는 것입니다. 한 번에 복원하지 않고, **수백 스텝에 걸쳐 조금씩** 잡음을 제거하기에 안정적입니다.

## 1. DDPM: 잡음 제거로 이미지 생성

```
DDPM (Ho et al., 2020):
  Denoising Diffusion Probabilistic Models

  순방향 과정 (Forward Process):
  x₀ → x₁ → x₂ → ... → x_T
  깨끗한 이미지에 점진적으로 가우시안 잡음 추가

  q(x_t | x_{t-1}) = N(x_t; √(1-β_t) x_{t-1}, β_t I)
  β_t: 잡음 스케줄 (0.0001 → 0.02, T=1000)

  핵심 수학적 트릭:
  x_t = √(ᾱ_t) x₀ + √(1-ᾱ_t) ε,  ε ~ N(0, I)
  ᾱ_t = ∏_{s=1}^{t} (1 - β_s)

  → 임의의 t에서 x_t를 x₀로부터 직접 계산 가능!
  → 중간 단계를 거칠 필요 없음 (학습 효율)

  역방향 과정 (Reverse Process):
  x_T → x_{T-1} → ... → x₀
  잡음에서 점진적으로 이미지를 복원

  p_θ(x_{t-1} | x_t) = N(x_{t-1}; μ_θ(x_t, t), Σ_θ(x_t, t))
  → 모델이 각 스텝의 평균과 분산을 예측

  실제로는 잡음 예측으로 단순화:
  모델 ε_θ(x_t, t)가 x_t에 추가된 잡음 ε을 예측
  → MSE 손실: L = E[||ε - ε_θ(x_t, t)||²]

  이것이 전부! 놀라울 정도로 단순한 손실 함수

  학습 알고리즘:
  1. 깨끗한 이미지 x₀ 샘플
  2. 랜덤 t ~ Uniform(1, T) 선택
  3. 잡음 ε ~ N(0, I) 샘플
  4. x_t = √(ᾱ_t) x₀ + √(1-ᾱ_t) ε 계산
  5. 손실 ||ε - ε_θ(x_t, t)||² 최소화

  생성 알고리즘:
  1. x_T ~ N(0, I) (순수 잡음)
  2. for t = T, T-1, ..., 1:
     예측: ε̂ = ε_θ(x_t, t)
     x_{t-1} = (x_t - β_t/√(1-ᾱ_t) ε̂) / √(1-β_t) + σ_t z
  3. x₀ 반환

  → 1000 스텝 역방향 → 느림! (GAN은 1 스텝)
```

> **핵심 직관**: DDPM의 학습은 **"잡음 맞히기 게임"**입니다. "이 흐릿한 이미지에서 얼마만큼이 잡음인가?"를 맞히는 것이 전부입니다. GAN처럼 적대적 학습이 아니라 단순한 MSE이므로 안정적이고, 잡음 스케줄이 전체 분포를 커버하므로 모드 붕괴가 없습니다.

## 2. Score Matching과 SDE 관점

```
Score Matching (Song & Ermon, 2019):
  잡음 제거의 수학적 기초

  Score Function:
  ∇_x log p(x) = "데이터 분포의 기울기"
  → 데이터 밀도가 높은 방향을 가리킴

  Langevin Dynamics:
  x_{t+1} = x_t + η ∇_x log p(x_t) + √(2η) z
  → Score를 따라가면 데이터 분포에서 샘플링!

  문제: 낮은 밀도 영역에서 Score 추정이 부정확
  해결: 여러 잡음 수준으로 조건화

  NCSN (Noise Conditional Score Network):
  s_θ(x, σ) ≈ ∇_x log p_σ(x)
  → 잡음 수준 σ에 따른 Score 학습

SDE 통합 (Song et al., 2021):
  DDPM과 Score Matching이 같은 프레임워크!

  순방향 SDE: dx = f(x,t)dt + g(t)dw
  역방향 SDE: dx = [f(x,t) - g²(t)∇_x log p_t(x)]dt + g(t)dw̄

  DDPM: Variance Preserving SDE (VP-SDE)
  NCSN: Variance Exploding SDE (VE-SDE)
  → 두 모델이 연속 시간 SDE의 이산화!

  | 관점 | 순방향 | 역방향 | 학습 |
  |------|--------|--------|------|
  | DDPM | 잡음 추가 | 잡음 제거 | ε 예측 |
  | Score | 데이터 오염 | Langevin | Score 추정 |
  | SDE | 순방향 SDE | 역방향 SDE | Score 학습 |
  → 모두 같은 것의 다른 표현!
```

## 3. 빠른 샘플링: DDIM과 가속

```
DDIM (Song et al., 2020):
  Denoising Diffusion Implicit Models

  DDPM의 문제: 1000 스텝 = 매우 느린 생성
  DDIM의 해결: 비마르코프 과정으로 스텝 줄이기

  DDPM: x_T → x_{999} → x_{998} → ... → x₀ (1000 스텝)
  DDIM: x_T → x_{800} → x_{600} → ... → x₀ (스텝 건너뛰기!)

  DDIM 업데이트:
  x_{t-1} = √(ᾱ_{t-1}) · 예측_x₀
           + √(1-ᾱ_{t-1}-σ²) · ε_θ(x_t, t)
           + σ · z

  σ = 0: 결정론적 (같은 잡음 → 같은 이미지)
  σ = DDPM값: 확률적 (DDPM과 동일)

  50 스텝으로도 DDPM 1000 스텝에 근접!
  → 20배 속도 향상

  결정론적 특성의 활용:
  ├─ 잠재 공간 보간: z₁ → z₂ 사이의 연속적 변화
  ├─ 이미지 인코딩: x₀ → x_T (역 DDIM)
  └─ 이미지 편집: 인코딩 → 잠재 조작 → 디코딩

DPM-Solver (Lu et al., 2022):
  ODE 풀이기법을 Diffusion에 적용
  → 10-20 스텝으로 고품질 생성
  → DDIM보다 더 빠르고 정확

Consistency Models (Song et al., 2023):
  임의의 x_t에서 직접 x₀를 예측하도록 학습
  → 1-2 스텝 생성 가능!
  → GAN 수준의 속도 + Diffusion의 안정성

  | 방법 | 스텝 수 | 품질 | 속도 |
  |------|---------|------|------|
  | DDPM | 1000 | 최고 | 매우 느림 |
  | DDIM | 50 | 좋음 | 보통 |
  | DPM-Solver | 10-20 | 좋음 | 빠름 |
  | Consistency | 1-2 | 좋음 | 매우 빠름 |
```

## 4. U-Net과 Diffusion 아키텍처

```
Diffusion의 백본: U-Net

  cv-04에서 다룬 U-Net 구조를 확장:

  인코더:        [64] → [128] → [256] → [512]
  디코더:        [512] → [256] → [128] → [64]
  Skip:          각 레벨에서 연결
  + 추가 요소:   시간 임베딩, Self-Attention

  시간 임베딩 (Time Embedding):
  t → Sinusoidal Embedding → MLP → 각 블록에 주입
  → 모델이 "현재 어느 단계인지" 알 수 있음
  → Transformer의 위치 인코딩과 동일 원리 (nlp-03)

  ResBlock with Time:
  x → GroupNorm → SiLU → Conv
    → + time_embed(t)
    → GroupNorm → SiLU → Dropout → Conv
    → + skip_connection

  Self-Attention 블록:
  중간 해상도(16×16, 32×32)에서 적용
  → 전역 관계 포착 (먼 픽셀 간 일관성)
  → 계산이 비싸므로 저해상도에서만

  Cross-Attention (조건부 생성):
  텍스트 조건 c를 Cross-Attention으로 주입
  Q: 이미지 특징, K/V: 텍스트 임베딩
  → "고양이" 토큰이 고양이 영역에 주목

DiT (Diffusion Transformer, Pei et al., 2023):
  U-Net을 Transformer로 대체

  이미지 → Patchify → Transformer blocks → 예측
  + AdaLN (Adaptive Layer Norm): t와 c를 LayerNorm에 주입

  장점:
  ├─ U-Net의 수작업 구조 불필요
  ├─ 스케일링에 유리 (Transformer의 강점)
  └─ DALL-E 3, Sora의 기반 아키텍처

  | 백본 | 장점 | 단점 | 대표 모델 |
  |------|------|------|----------|
  | U-Net | 검증된 구조, Skip | 스케일링 한계 | SD 1.5/2.1 |
  | DiT | 스케일링 용이 | 계산 비용 | SD 3, DALL-E 3 |
```

## 5. Latent Diffusion과 Stable Diffusion

```
Latent Diffusion Model (LDM, Rombach et al., 2022):
  "픽셀 공간이 아닌 잠재 공간에서 Diffusion"

  문제: 512×512 이미지에서 직접 Diffusion → 매우 비쌈
  해결: 압축된 잠재 공간에서 Diffusion 수행

  구조:
  이미지(512×512×3)
    → Encoder(E) → 잠재 벡터(64×64×4)
    → Diffusion (잡음 추가/제거)
    → Decoder(D) → 이미지(512×512×3)

  VAE (Variational AutoEncoder):
  ├─ Encoder: 이미지 → 잠재 공간 (8× 압축)
  ├─ Decoder: 잠재 공간 → 이미지
  └─ KL 정규화: 잠재 공간을 가우시안에 가깝게

  계산량: 512² × 3 = 786K → 64² × 4 = 16K
  → 약 48배 차원 축소!
  → 동일 품질에서 훨씬 빠른 학습/추론

  조건부 메커니즘:
  ├─ 텍스트: CLIP Text Encoder → Cross-Attention
  ├─ 이미지: CLIP Image Encoder → Cross-Attention
  ├─ 클래스: Class Embedding → AdaLN
  └─ 레이아웃: 세그맵 → Concatenation

Stable Diffusion (Stability AI, 2022):
  LDM의 오픈소스 구현

  SD 1.5:
  ├─ VAE: KL-f8 (8× 다운샘플)
  ├─ U-Net: 860M 파라미터
  ├─ Text Encoder: CLIP ViT-L/14
  ├─ 해상도: 512×512
  └─ 학습: LAION-5B 서브셋

  SD 2.1:
  ├─ Text Encoder: OpenCLIP ViT-H/14
  ├─ 해상도: 768×768
  └─ 개선된 VAE

  SDXL (SD XL, 2023):
  ├─ U-Net: 2.6B 파라미터 (3배)
  ├─ 듀얼 텍스트 인코더: CLIP-L + OpenCLIP-G
  ├─ 해상도: 1024×1024
  ├─ Refiner 모델: 디테일 후처리
  └─ 크기/크롭 조건: 학습 아티팩트 해결

  SD 3 / Flux:
  ├─ DiT 기반 (U-Net → Transformer)
  ├─ MMDiT: 텍스트와 이미지 토큰을 동시 처리
  ├─ Flow Matching: DDPM 대신 연속 흐름
  ├─ T5 텍스트 인코더 추가 (텍스트 이해 강화)
  └─ 텍스트 렌더링 크게 개선
```

> **핵심 직관**: Latent Diffusion의 혁신은 **"중요한 정보만 남긴 공간에서 생성"**하는 것입니다. 픽셀 공간에는 인접 픽셀 간 중복이 많습니다. VAE가 이 중복을 제거한 잠재 공간에서 Diffusion을 수행하면, 의미적으로 중요한 구조에만 집중하므로 효율과 품질이 모두 향상됩니다.

## 6. Classifier-Free Guidance

```
Guidance: 조건에 더 충실한 생성

  Classifier Guidance (Dhariwal & Nichol, 2021):
  별도의 분류기 ∇_x log p(y|x_t)를 사용
  Score = ε_θ(x_t) - s · ∇_x log p(y|x_t)
  → 분류기 그래디언트로 생성 방향 유도
  → 별도 분류기 학습 필요 (번거로움)

  Classifier-Free Guidance (CFG, Ho & Salimans, 2022):
  분류기 없이, 조건부/비조건부 예측의 차이로 유도

  학습: 랜덤으로 조건 c를 ∅(null)로 드롭 (10-20%)
  → 하나의 모델이 조건부/비조건부 모두 학습

  추론:
  ε̂ = ε_θ(x_t, ∅) + w · (ε_θ(x_t, c) - ε_θ(x_t, ∅))

  w = 1: 순수 조건부 (약한 가이던스)
  w = 7.5: 일반적 설정 (품질과 다양성 균형)
  w = 15+: 강한 가이던스 (과포화, 아티팩트)

  직관:
  ε_θ(x_t, c) - ε_θ(x_t, ∅) = "조건의 영향"
  이것을 w배 증폭 → 조건에 더 충실!
  → "고양이"라는 조건의 영향을 증폭하면
     더 고양이답게 생성

  왜 CFG가 중요한가:
  ├─ 별도 모델 불필요 (학습 시 드롭만 추가)
  ├─ w 하나로 품질/다양성 트레이드오프 제어
  ├─ 거의 모든 조건부 Diffusion의 표준
  └─ Stable Diffusion의 핵심 구성요소

  | w 값 | 효과 | 적합한 상황 |
  |------|------|-----------|
  | 1 | 다양하지만 불명확 | 탐색적 생성 |
  | 5-8 | 균형 | 일반적 사용 |
  | 10-15 | 명확하지만 단조 | 특정 프롬프트 |
  | 20+ | 과포화/아티팩트 | 비추천 |

Negative Prompt:
  CFG의 확장 — "이것은 피하라"

  ε̂ = ε_θ(x_t, c_neg) + w · (ε_θ(x_t, c_pos) - ε_θ(x_t, c_neg))
  → ∅ 대신 부정 프롬프트 c_neg 사용
  → "blurry, low quality"를 부정 조건으로
     → 선명하고 고품질인 방향으로 유도
```

## 핵심 정리

- **DDPM**은 가우시안 잡음의 점진적 추가/제거로 이미지를 생성하며, 단순한 MSE 손실(잡음 예측)로 GAN보다 안정적이고 모드 붕괴가 없습니다
- **Score Matching**과 **SDE 관점**은 DDPM의 수학적 기초를 제공하며, 잡음 제거가 데이터 분포의 Score를 따라가는 것과 동일함을 보여줍니다
- **DDIM**은 비마르코프 과정으로 샘플링 스텝을 줄이고(1000→50), **Consistency Model**은 1-2 스텝 생성까지 가능하게 합니다
- **Latent Diffusion**은 VAE로 압축된 잠재 공간에서 Diffusion을 수행하여 48배 차원 축소를 달성하며, **Stable Diffusion**이 이를 대중화했습니다
- **Classifier-Free Guidance**는 조건부/비조건부 예측의 차이를 증폭하여 텍스트 프롬프트에 충실한 생성을 유도하며, 가이던스 스케일 w로 품질/다양성을 제어합니다
