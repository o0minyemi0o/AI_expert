# 비전-언어 모델

## 왜 비전과 언어를 결합하는가

지금까지 CV는 이미지만 다루었습니다. 하지만 인간은 이미지를 볼 때 자연스럽게 **언어로 설명**합니다. "해변에서 뛰어노는 강아지"—이 문장은 이미지의 의미를 풍부하게 표현합니다. **CLIP**은 이미지와 텍스트를 같은 공간에 매핑하여, 텍스트로 이미지를 검색하고, 학습하지 않은 클래스도 인식합니다. 이것이 cv-07~08의 텍스트 조건부 생성과 mm-01~12의 멀티모달 AI의 기반입니다.

> **핵심 직관**: CLIP의 핵심은 **"이미지와 텍스트의 공유 표현 공간"**입니다. "강아지"라는 텍스트와 강아지 사진이 같은 벡터 공간에서 가까이 있다면, 텍스트로 이미지를 검색하고, 이미지로 텍스트를 검색하고, 새로운 개념도 텍스트만으로 인식할 수 있습니다.

## 1. CLIP: 대조적 언어-이미지 사전학습

```
CLIP (Radford et al., 2021):
  Contrastive Language-Image Pre-training

  핵심: 4억 개의 이미지-텍스트 쌍으로 대조 학습

  구조:
  이미지 → Image Encoder → 이미지 임베딩 (d차원)
  텍스트 → Text Encoder → 텍스트 임베딩 (d차원)

  대조 학습:
  배치 N개에서 N개의 양성 쌍 (이미지ᵢ, 텍스트ᵢ)
  N²-N개의 음성 쌍 (이미지ᵢ, 텍스트ⱼ)

  손실: InfoNCE (양방향)
  L_i2t = -log(exp(sim(I_i, T_i)/τ) / Σ_j exp(sim(I_i, T_j)/τ))
  L_t2i = -log(exp(sim(T_i, I_i)/τ) / Σ_j exp(sim(T_i, I_j)/τ))
  L = (L_i2t + L_t2i) / 2

  → cv-09 SimCLR와 같은 구조이되, 두 모달리티 사이!

  Image Encoder: ViT-L/14 또는 ResNet-50×64
  Text Encoder: Transformer (12 레이어, 512차원)

  학습 데이터: WIT (WebImageText)
  ├─ 4억 이미지-텍스트 쌍
  ├─ 인터넷에서 수집 (자연스러운 텍스트)
  └─ ImageNet 스타일의 "단어 라벨"이 아닌 자연어 설명

  배치 크기: 32,768 (매우 큼!)
  → 대조 학습에서 큰 배치 = 더 많은 음성 쌍

제로샷 분류:
  CLIP의 가장 놀라운 능력

  기존: 1000 클래스 학습 → 1000 클래스만 분류
  CLIP: 학습 없이 임의의 클래스 분류!

  방법:
  1. 각 클래스에 대해 텍스트 프롬프트 생성
     "a photo of a {class_name}"
  2. Text Encoder로 각 프롬프트의 임베딩 계산
  3. 이미지의 임베딩과 모든 텍스트의 코사인 유사도
  4. 가장 유사한 텍스트의 클래스 = 예측

  프롬프트 엔지니어링:
  "a photo of a dog" → 기본
  "a photo of a dog, a type of pet" → 문맥 추가
  80개의 프롬프트 앙상블 → 성능 향상

  | 데이터셋 | CLIP 제로샷 | ResNet-50 지도학습 |
  |---------|-----------|-----------------|
  | ImageNet | 76.2% | 76.1% |
  | Food101 | 92.9% | 86.3% |
  | Cars | 77.3% | 49.7% |
  | 평균(27개) | 다수에서 우세 | 일부에서 우세 |
  → 제로샷이 지도학습과 동등!
```

> **핵심 직관**: CLIP의 제로샷이 가능한 이유는 **"라벨이 아닌 설명으로 학습"**했기 때문입니다. ImageNet은 "이것은 클래스 #243"이지만, CLIP은 "a golden retriever playing in the park"으로 학습합니다. 자연어 설명에는 시각적 속성, 관계, 문맥이 포함되어 훨씬 풍부한 표현을 학습합니다.

## 2. CLIP의 활용과 확장

```
CLIP의 다양한 활용:

  1. 이미지-텍스트 검색:
     텍스트 쿼리로 이미지 검색 (또는 반대)
     → 검색 엔진, 추천 시스템

  2. 텍스트 기반 이미지 생성 (cv-07~08):
     CLIP Text Encoder가 Stable Diffusion의 텍스트 인코더
     CLIP Image Encoder가 IP-Adapter의 이미지 인코더

  3. 이미지 분류 (제로샷/퓨샷):
     파인튜닝 없이 임의 클래스 분류
     소수 샘플로 파인튜닝 시 더욱 강력

  4. 비디오 이해:
     프레임별 CLIP 특징 → 시간적 집계
     → 비디오 분류, 검색

  5. 의미적 세그멘테이션:
     CLIP의 패치별 유사도 → 오픈 어휘 세그멘테이션

OpenCLIP (Ilharco et al., 2021):
  CLIP의 오픈소스 재현
  ├─ LAION-2B/5B 데이터셋으로 학습
  ├─ ViT-G/14: 1.8B 파라미터
  ├─ 원본 CLIP을 능가하는 성능
  └─ Stable Diffusion 2.x의 텍스트 인코더

SigLIP (Zhai et al., 2023):
  CLIP의 손실 함수 개선
  ├─ Softmax 대신 Sigmoid: 각 쌍을 독립적으로 판단
  ├─ 큰 배치 불필요 (메모리 효율)
  └─ 소규모 배치에서도 효과적

EVA-CLIP (Fang et al., 2023):
  ├─ EVA-02 (MAE 사전학습) + CLIP 학습
  ├─ 4.4B 파라미터
  ├─ ImageNet 제로샷 82.0%
  └─ 현재 최고 성능의 공개 CLIP 모델
```

## 3. Open-Vocabulary 인식

```
Open-Vocabulary Detection:
  "학습하지 않은 클래스도 탐지"

  기존 탐지: COCO 80 클래스만 탐지 가능
  Open-Vocab: "giraffe", "fire hydrant" 등 임의 클래스

  OWL-ViT (Minderer et al., 2022):
  CLIP + 탐지 헤드
  ├─ CLIP Image Encoder로 영역 특징 추출
  ├─ CLIP Text Encoder로 클래스 텍스트 임베딩
  ├─ 영역-텍스트 유사도로 분류
  └─ 학습 시 본 적 없는 클래스도 탐지!

  Grounding DINO (Liu et al., 2024):
  텍스트 프롬프트로 탐지 대상 지정
  ├─ "a red car near a tree" → 해당 객체 탐지
  ├─ DINO(cv-03) + 언어 기반
  ├─ 텍스트의 각 구문이 객체에 매핑
  └─ SAM과 결합 → Grounded SAM (cv-05)

Open-Vocabulary Segmentation:
  "학습하지 않은 클래스도 세그멘테이션"

  LSeg (Language-driven Semantic Seg, 2022):
  ├─ 픽셀 임베딩을 CLIP 텍스트 임베딩과 정렬
  └─ 임의 텍스트 레이블로 세그멘테이션

  FC-CLIP (Yu et al., 2023):
  ├─ Frozen CLIP + 경량 디코더
  ├─ CLIP의 마지막 레이어: 시맨틱 정보
  ├─ CLIP의 중간 레이어: 공간 정보
  └─ 파노픽/시맨틱/인스턴스 통합

  Grounded SAM:
  텍스트 → Grounding DINO(박스) → SAM(마스크)
  → 텍스트로 지정한 어떤 것이든 분할!

  | 방법 | 입력 | 출력 | 특징 |
  |------|------|------|------|
  | OWL-ViT | 텍스트 | 박스 | CLIP 기반 탐지 |
  | Grounding DINO | 텍스트 | 박스 | 구문-객체 매핑 |
  | Grounded SAM | 텍스트 | 마스크 | 탐지+세그멘테이션 |
  | FC-CLIP | 텍스트 | 세그맵 | 픽셀 수준 |
```

## 4. 이미지 캡셔닝과 VQA

```
이미지 캡셔닝:
  이미지 → 자연어 설명

  초기 방법 (Encoder-Decoder):
  CNN(이미지) → 특징 벡터 → LSTM → "a dog running..."
  → nlp-02의 Seq2Seq와 동일 구조

  Transformer 기반 캡셔닝:
  ViT(이미지) → Cross-Attention → Transformer Decoder
  → "a golden retriever running on the beach"

VQA (Visual Question Answering):
  이미지 + 질문 → 답변

  Q: "What color is the car?"
  A: "Red"

  Q: "How many people are in the image?"
  A: "Three"

  → 이미지 이해 + 언어 이해 + 추론 필요

  BLIP-2 (Li et al., 2023):
  Bootstrapping Language-Image Pre-training

  구조:
  Image Encoder(동결) → Q-Former → LLM(동결)

  Q-Former (Querying Transformer):
  ├─ 학습 가능한 쿼리 토큰 (32개)
  ├─ Image Encoder 출력에 Cross-Attention
  ├─ 시각 정보를 LLM이 이해할 수 있는 토큰으로 변환
  └─ 이미지 인코더와 LLM 사이의 "다리"

  2단계 학습:
  1단계: Image-Text 대조 + 생성 + 매칭 학습
  2단계: LLM에 연결하여 생성 학습

  → 동결된 Image Encoder + 동결된 LLM
     Q-Former만 학습 → 매우 효율적

LLaVA (Liu et al., 2023):
  Visual Instruction Tuning

  구조가 더 단순:
  이미지 → CLIP ViT → Linear Projection → LLM

  CLIP의 이미지 특징을 선형 투영으로
  LLM의 단어 임베딩 공간에 맞춤
  → 이미지 토큰 + 텍스트 토큰을 LLM에 입력

  학습:
  1단계: Linear Projection만 학습 (정렬)
  2단계: LLM + Projection 함께 학습 (지시 튜닝)

  | 모델 | Image Enc | LLM | 학습 방식 |
  |------|-----------|-----|---------|
  | BLIP-2 | EVA-CLIP | Flan-T5/Vicuna | Q-Former |
  | LLaVA | CLIP ViT-L | Vicuna/LLaMA | Linear Proj |
  | InternVL | InternViT | InternLM | QLLaMA |
```

> **핵심 직관**: BLIP-2와 LLaVA의 핵심 차이는 **"브릿지의 복잡도"**입니다. BLIP-2는 Q-Former라는 복잡한 다리로 시각과 언어를 연결하지만, LLaVA는 단순 선형 투영만으로 연결합니다. LLaVA의 성공은 **"CLIP의 표현이 이미 충분히 언어와 정렬되어 있다"**는 것을 보여줍니다.

## 5. 멀티모달 대규모 모델

```
GPT-4V / GPT-4o (OpenAI, 2023-2024):
  ├─ 이미지 이해 + 텍스트 생성의 통합
  ├─ 복잡한 시각적 추론 능력
  ├─ OCR, 차트 이해, 공간 추론
  └─ 아키텍처 비공개

Gemini (Google, 2023):
  ├─ 네이티브 멀티모달 (이미지/텍스트/오디오/비디오)
  ├─ 인터리빙된 멀티모달 입력
  └─ 다양한 크기: Nano, Pro, Ultra

Claude의 Vision (Anthropic):
  ├─ 이미지 이해와 분석
  ├─ 문서, 차트, 다이어그램 해석
  └─ 안전성에 중점

  이들 모델의 공통 패턴:
  시각 인코더(CLIP 계열) + LLM
  → CLIP이 모든 멀티모달 모델의 시각적 기반

  평가 벤치마크:
  | 벤치마크 | 측정 대상 | 예시 |
  |---------|----------|------|
  | VQAv2 | 시각 질문 답변 | "몇 명?" |
  | GQA | 구성적 추론 | "빨간 물체 왼쪽에?" |
  | TextVQA | 이미지 내 텍스트 | "간판에 뭐라고?" |
  | MMMU | 전문 지식 | 의학/과학 이미지 |
  | MathVista | 수학 추론 | 그래프/도표 해석 |
```

## 6. CLIP 기반 시스템 실전

```
CLIP 임베딩 활용:

  이미지 검색 시스템:
  1. 오프라인: 모든 이미지 → CLIP 임베딩 → 벡터 DB
  2. 온라인: 텍스트 쿼리 → CLIP 임베딩 → 유사도 검색
  → nlp-10의 RAG와 동일 구조!

  콘텐츠 모더레이션:
  CLIP 유사도로 부적절한 콘텐츠 필터링
  "nudity", "violence" 텍스트와 유사도 계산
  → 임계값 기반 자동 필터링

  제품 추천:
  사용자가 본 이미지와 유사한 제품 검색
  → 크로스 모달: "빨간 운동화"(텍스트) → 유사 제품(이미지)

CLIP의 한계:

  1. 조합적 이해 부족:
     "빨간 차와 파란 집" vs "파란 차와 빨간 집"
     → 속성-객체 바인딩이 약함

  2. 공간적 관계:
     "위에", "아래에", "옆에" 등의 관계 이해 약함

  3. 카운팅:
     "3마리의 고양이"와 "5마리의 고양이" 구분 어려움

  4. 부정:
     "고양이가 없는 사진"을 올바르게 이해 못함

  5. 세밀한 차이:
     유사한 품종, 미세한 결함 등의 구분 약함

  대응:
  ├─ NegCLIP: 부정 예제 학습 추가
  ├─ SVLC: 조합적 이해 개선
  └─ 전문 도메인: 도메인 특화 파인튜닝 필요
```

## 핵심 정리

- **CLIP**은 4억 이미지-텍스트 쌍의 대조 학습으로 이미지와 텍스트를 공유 공간에 매핑하며, 제로샷 분류에서 ImageNet 지도학습과 동등한 성능을 달성합니다
- **Open-Vocabulary 인식**(OWL-ViT, Grounding DINO)은 CLIP 기반으로 학습하지 않은 클래스도 탐지/세그멘테이션하며, Grounded SAM은 텍스트로 지정한 어떤 것이든 분할합니다
- **BLIP-2**의 Q-Former와 **LLaVA**의 Linear Projection은 동결된 Image Encoder와 LLM을 연결하는 두 가지 접근이며, CLIP의 표현이 이미 언어와 잘 정렬되어 있음을 보여줍니다
- GPT-4V, Gemini 등 **멀티모달 대규모 모델**은 모두 CLIP 계열의 시각 인코더 + LLM 구조로, 복잡한 시각적 추론과 대화가 가능합니다
- CLIP은 **조합적 이해, 공간 관계, 카운팅**에서 한계가 있으며, 이를 해결하기 위한 연구가 활발히 진행 중입니다
