# 시맨틱 세그멘테이션

## 왜 시맨틱 세그멘테이션이 중요한가

탐지가 "어디에 무엇이 있는가"를 바운딩 박스로 답했다면, 세그멘테이션은 **"각 픽셀이 무엇인가"**를 답합니다. 자율주행에서 도로/인도/차량의 영역을 구분하고, 의료 영상에서 종양의 정확한 경계를 찾고, 위성 이미지에서 건물/도로/녹지를 분류합니다. 픽셀 수준의 이해가 필요한 모든 응용에 세그멘테이션이 핵심입니다.

> **핵심 직관**: 세그멘테이션의 핵심 도전은 **"의미(semantics)와 위치(localization)의 동시 추구"**입니다. CNN의 풀링은 의미 정보를 높이지만 공간 해상도를 잃습니다. "전체가 고양이임은 알지만, 정확히 어디까지가 고양이인지 모른다"—이 딜레마를 해결하는 것이 세그멘테이션 아키텍처의 역사입니다.

## 1. FCN (Fully Convolutional Network)

```
FCN (Long et al., 2015):
  세그멘테이션의 시작 — "FC를 Conv로 대체"

  분류 네트워크: Conv...Conv → FC → 1000 클래스
  세그멘테이션: Conv...Conv → 1×1 Conv → H×W×C 클래스 맵

  핵심 아이디어:
  1. FC → 1×1 Conv 변환: 임의 크기 입력 처리 가능
  2. 전치 합성곱(Transposed Conv)으로 업샘플링
  3. Skip Connection으로 다스케일 정보 결합

  FCN-32s: 1/32 특징 맵에서 32× 업샘플링 → 거친 결과
  FCN-16s: pool4 + 2× 업샘플(conv7) → 16× 업샘플
  FCN-8s: pool3 + 2× 업샘플(pool4+conv7) → 8× 업샘플
  → Skip으로 세밀한 경계 복원

  한계:
  ├─ 해상도 손실이 여전히 큼
  ├─ 전역 문맥 부족
  └─ 경계가 거침
```

## 2. U-Net

```
U-Net (Ronneberger et al., 2015):
  의료 영상 세그멘테이션의 표준 — "인코더-디코더 + Skip"

  구조 (U자형):
  인코더 (수축 경로):
  [64] → [128] → [256] → [512] → [1024]
   ↓       ↓       ↓       ↓
  pool    pool    pool    pool

  디코더 (확장 경로):
  [1024] → [512] → [256] → [128] → [64] → output
    ↑        ↑       ↑       ↑
  up+cat  up+cat  up+cat  up+cat
                              ↑
                          Skip Connection!

  Skip Connection의 역할:
  인코더의 고해상도 특징을 디코더에 직접 전달
  → 위치 정보(인코더 초기) + 의미 정보(디코더)를 결합
  → FCN의 Skip보다 훨씬 풍부한 정보 전달

  왜 U-Net이 의료에서 성공했나:
  ├─ 적은 데이터에서도 작동 (512×512, 수십 장)
  ├─ 데이터 증강: 탄성 변형(elastic deformation)
  ├─ 경계가 정확 (Skip 덕분)
  └─ 구조가 직관적이고 확장 용이

  U-Net++, Attention U-Net:
  ├─ U-Net++: 밀집 Skip 연결 (중간 노드 추가)
  ├─ Attention U-Net: Skip에 어텐션 게이트 추가
  │   → 관련 없는 특징은 억제
  └─ nnU-Net: 자동으로 최적 U-Net 구성 선택
     → 의료 영상 세그멘테이션 벤치마크의 표준
```

> **핵심 직관**: U-Net의 성공 비결은 **"인코더에서 잃은 정보를 Skip으로 복원"**하는 것입니다. CNN은 풀링으로 "무엇인지"를 학습하면서 "어디인지"를 잃습니다. Skip Connection은 "어디인지" 정보를 디코더에 전달하여, 의미와 위치를 동시에 얻습니다.

## 3. DeepLab 계열

```
DeepLab v1/v2 (Chen et al., 2015, 2017):
  Dilated/Atrous Convolution 도입

  일반 Conv: 인접 픽셀만 참조
  Dilated Conv: 간격을 두고 참조 → 수용 영역 확대
  rate=1: [x x x]     → 3×3 수용 영역
  rate=2: [x . x . x] → 5×5 수용 영역
  rate=4: [x . . . x . . . x] → 9×9 수용 영역
  → 해상도를 유지하면서 넓은 문맥!

  ASPP (Atrous Spatial Pyramid Pooling):
  여러 rate의 dilated conv를 병렬 적용
  rate={6, 12, 18} + 1×1 Conv + Global Avg Pool
  → 다양한 스케일의 문맥 동시 포착
  → Inception의 다스케일 아이디어와 유사

DeepLab v3/v3+ (Chen et al., 2017, 2018):
  v3: 개선된 ASPP + 배치 정규화
  v3+: 인코더-디코더 + ASPP

  인코더: ResNet/Xception + ASPP
  디코더: 저수준 특징 결합 + 업샘플링

  | 모델 | 백본 | mIoU (VOC) | 특징 |
  |------|------|-----------|------|
  | DeepLab v1 | VGG | 71.6% | CRF 후처리 |
  | DeepLab v2 | ResNet | 79.7% | ASPP |
  | DeepLab v3 | ResNet | 85.7% | 개선된 ASPP |
  | DeepLab v3+ | Xception | 89.0% | Encoder-Decoder |
```

## 4. SegFormer와 Transformer 기반

```
SegFormer (Xie et al., 2021):
  Transformer 기반 세그멘테이션의 표준

  구조: 계층적 Transformer 인코더 + 경량 디코더

  인코더 (Mix Transformer, MiT):
  ├─ 계층적: 4 스테이지, 점진적 해상도 감소
  │   1/4 → 1/8 → 1/16 → 1/32
  ├─ Overlapping Patch Embedding
  │   → 패치 경계의 정보 손실 방지
  ├─ 효율적 Self-Attention
  │   → K, V를 R배 다운샘플링
  │   → O(N²/R²) 복잡도
  └─ Mix-FFN: 3×3 Depthwise Conv 포함
     → 위치 인코딩 불필요!

  디코더 (MLP Decoder):
  각 스테이지의 특징 맵을 같은 크기로 업샘플
  → 채널 통합(concat) → MLP → 예측
  → 매우 경량 (DeepLab 대비 1/10 파라미터)

  | 모델 | 파라미터 | FLOPs | mIoU (ADE20K) |
  |------|---------|-------|--------------|
  | SegFormer-B0 | 3.8M | 8.4G | 37.4 |
  | SegFormer-B2 | 27.4M | 62.4G | 46.5 |
  | SegFormer-B5 | 84.7M | 183G | 51.0 |
  | DeepLab v3+ | 62.7M | 177G | 45.5 |

  → 비슷한 비용에서 SegFormer가 우위

Mask2Former (Cheng et al., 2022):
  "모든 세그멘테이션을 통합하는 아키텍처"
  ├─ 시맨틱, 인스턴스, 파노픽 세그멘테이션을 하나로
  ├─ Masked Attention: 각 쿼리가 해당 마스크 영역에만 주목
  ├─ Multi-scale Deformable Attention
  └─ ADE20K mIoU 57.8% (현재 최고 수준)
```

## 5. 세그멘테이션 손실과 평가

```
손실 함수:

  1. Cross-Entropy Loss:
     L = -Σ_c y_c log(p_c) (각 픽셀에서)
     가장 기본적, 클래스 불균형 시 문제

  2. Dice Loss:
     Dice = 2|P ∩ G| / (|P| + |G|)
     L_dice = 1 - Dice
     → 클래스 불균형에 강건 (의료에서 선호)
     → IoU와 유사하지만 미분 가능

  3. Focal Loss:
     cv-03에서 다룬 것과 동일 적용
     → 쉬운 픽셀의 기여 감소

  4. 조합:
     L = L_CE + λ × L_Dice
     → 두 손실의 장점 결합이 일반적

평가 지표:

  mIoU (mean Intersection over Union):
  IoU_c = TP_c / (TP_c + FP_c + FN_c)
  mIoU = (1/C) Σ_c IoU_c

  Pixel Accuracy:
  전체 픽셀 중 정확히 분류된 비율
  → 클래스 불균형 시 오해의 소지 (배경이 90%면 90% 자동 달성)

  주요 벤치마크:
  | 데이터셋 | 클래스 | 이미지 수 | 용도 |
  |---------|--------|----------|------|
  | PASCAL VOC | 21 | 11K | 기본 벤치마크 |
  | Cityscapes | 19 | 5K | 자율주행 |
  | ADE20K | 150 | 25K | 복잡한 장면 |
  | COCO-Stuff | 171 | 164K | 대규모 |
```

## 6. 세그멘테이션 실전

```
도메인별 선택:

  의료 영상:
  → nnU-Net (자동 구성), Attention U-Net
  → 적은 데이터, 정밀한 경계가 핵심
  → 3D 볼류메트릭 세그멘테이션 (CT, MRI)

  자율주행:
  → SegFormer, DeepLab v3+
  → 실시간 필요: BiSeNet, STDC
  → 도로, 차량, 보행자 등 정확한 분리

  위성/항공:
  → U-Net, SegFormer
  → 큰 이미지 → 패치 기반 처리
  → 건물, 도로, 녹지, 수역 분류

  일반 장면:
  → Mask2Former, SegFormer-B5
  → ADE20K 기준 mIoU 50%+ 달성

데이터 증강의 중요성:
  ├─ 랜덤 크롭, 리사이즈, 회전, 반전
  ├─ 색상 변환 (밝기, 대비, 채도)
  ├─ CutOut, CutMix (영역 잘라내기)
  └─ 의료: 탄성 변형, 강도 변환

세그멘테이션 진화:
  2015: FCN — "FC를 Conv로"
  2015: U-Net — "인코더-디코더 + Skip"
  2017: DeepLab v3 — "Dilated Conv + ASPP"
  2021: SegFormer — "Transformer + 경량 디코더"
  2022: Mask2Former — "통합 세그멘테이션"
  → cv-05에서 인스턴스/파노픽 세그멘테이션으로 확장
```

## 핵심 정리

- **FCN**은 FC를 1×1 Conv로 대체하여 임의 크기 입력의 픽셀별 분류를 가능하게 했으며, Skip Connection으로 다스케일 정보를 결합합니다
- **U-Net**의 인코더-디코더 + Skip 구조는 적은 데이터에서도 정밀한 경계를 복원하며, 의료 영상 세그멘테이션의 표준입니다
- **DeepLab**의 Dilated Conv은 해상도를 유지하면서 수용 영역을 확대하고, ASPP는 다중 스케일 문맥을 동시에 포착합니다
- **SegFormer**는 계층적 Transformer + 경량 MLP 디코더로 효율과 성능을 동시에 달성하며, CNN 기반을 대체하고 있습니다
- 세그멘테이션 평가는 **mIoU**가 표준이며, 손실은 Cross-Entropy + Dice의 조합이 일반적입니다
