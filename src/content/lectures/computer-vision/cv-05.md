# 인스턴스 및 파노픽 세그멘테이션

## 왜 인스턴스 세그멘테이션이 필요한가

cv-04의 시맨틱 세그멘테이션은 "이 픽셀은 자동차"라고 말하지만, **"어느 자동차인지"**는 구분하지 못합니다. 3대의 자동차가 모두 같은 색으로 표시됩니다. **인스턴스 세그멘테이션**은 각 객체를 개별적으로 분리하며, **파노픽 세그멘테이션**은 시맨틱과 인스턴스를 통합합니다. 그리고 **SAM(Segment Anything)**은 프롬프트 기반으로 어떤 것이든 분할하는 파운데이션 모델입니다.

> **핵심 직관**: 시맨틱은 **"클래스"**, 인스턴스는 **"개체"**, 파노픽은 **"모든 것"**을 분할합니다. 시맨틱이 "이 영역은 사람"이라면, 인스턴스는 "사람1, 사람2, 사람3"을 구분하고, 파노픽은 여기에 "하늘, 도로, 건물"까지 포함합니다.

## 1. Mask R-CNN

```
Mask R-CNN (He et al., 2017):
  Faster R-CNN + 마스크 예측 헤드

  Faster R-CNN:
  이미지 → Backbone+FPN → RPN → RoI Align → 분류 + 박스

  Mask R-CNN: 여기에 마스크 분기 추가!
  이미지 → Backbone+FPN → RPN → RoI Align →
    ├─ 분류 + 박스 (기존)
    └─ 마스크 예측 (새로 추가)

  마스크 헤드:
  RoI Align(14×14) → 4개 Conv(256) → Deconv(2×)
  → 28×28 × C 바이너리 마스크 (클래스별)

  핵심 설계 결정:
  ├─ 클래스별 독립 마스크: C개의 28×28 마스크 예측
  │   → 클래스 경쟁 없이 각 마스크가 독립적
  │   → 분류 헤드가 클래스 결정, 마스크는 형태만 결정
  ├─ RoI Align (RoI Pooling이 아님!)
  │   → 양자화 오차 제거로 마스크 품질 크게 향상
  └─ 멀티태스크 손실:
     L = L_cls + L_box + L_mask

  마스크 손실:
  L_mask = BCE(예측 마스크, GT 마스크)
  → GT 클래스의 마스크에만 적용 (다른 클래스 무시)

  | 모델 | Backbone | AP^mask | AP^box |
  |------|---------|--------|--------|
  | Mask R-CNN | ResNet-50 | 34.2 | 37.8 |
  | Mask R-CNN | ResNet-101 | 35.7 | 39.8 |

  확장:
  ├─ 키포인트 탐지: 관절 위치 예측 (포즈 추정)
  ├─ 3D 형태: Mesh R-CNN
  └─ 비디오: TrackR-CNN
```

> **핵심 직관**: Mask R-CNN의 우아함은 **"마스크 예측을 분류와 분리"**한 것입니다. 마스크 헤드는 "이 영역의 형태가 어떤가?"만 답하고, "이것이 무엇인가?"는 분류 헤드에 맡깁니다. 이 역할 분리가 학습을 안정적으로 만들고, 각 클래스의 마스크 품질을 높입니다.

## 2. SAM (Segment Anything Model)

```
SAM (Kirillov et al., 2023, Meta):
  "어떤 것이든 분할하는" 파운데이션 모델

  핵심 혁신:
  ├─ 프롬프터블(Promptable): 점, 박스, 텍스트로 분할 대상 지정
  ├─ 제로샷: 학습하지 않은 도메인에서도 작동
  ├─ 데이터: SA-1B (11M 이미지, 1.1B 마스크)
  └─ Interactive: 사용자 피드백으로 반복 정제

  구조:
  이미지 → Image Encoder(ViT-H) → 이미지 임베딩
                                        ↓
  프롬프트 → Prompt Encoder ─────→ Mask Decoder
                                        ↓
                                  마스크 + 신뢰도

  Image Encoder:
  ├─ ViT-H (632M 파라미터)
  ├─ 이미지당 한 번만 계산
  └─ MAE 사전학습 (cv-09)

  Prompt Encoder:
  ├─ 점(Point): 위치 임베딩 + 양성/음성 토큰
  ├─ 박스(Box): 두 코너 점의 임베딩
  ├─ 마스크: Conv로 다운샘플링
  └─ 텍스트: CLIP 텍스트 인코더

  Mask Decoder:
  ├─ 2개 Transformer 레이어
  ├─ 프롬프트 토큰과 이미지 임베딩의 Cross-Attention
  ├─ 3개의 마스크 출력 (모호성 해결)
  │   → "이 점이 부분? 전체? 더 큰 구조?"
  └─ 매우 경량 (~4ms per mask)

학습 전략:
  1. 수동 주석 단계: 전문가가 마스크 작성
  2. 반자동 단계: SAM이 제안, 전문가가 보완
  3. 자동 단계: SAM이 완전 자동으로 마스크 생성
  → 점진적으로 데이터 확장 (Data Engine)
```

```
SAM 2 (Ravi et al., 2024):
  SAM을 비디오로 확장

  ├─ 시간적 일관성: 프레임 간 마스크 추적
  ├─ Memory Bank: 이전 프레임 마스크를 기억
  ├─ 프롬프트 전파: 한 프레임에서 지정 → 전체 비디오
  └─ SA-V 데이터셋: 51K 비디오, 600K+ 마스크

  실용적 활용:
  ├─ 의료: CT/MRI 슬라이스 자동 분할
  ├─ 로봇: 물체 파지를 위한 실시간 분할
  ├─ 영상 편집: 객체 제거/교체
  ├─ AR/VR: 실시간 장면 이해
  └─ 자율주행: 도로 장면의 범용 분할

  SAM vs 전통 세그멘테이션:
  | 속성 | 전통 | SAM |
  |------|------|-----|
  | 클래스 | 사전 정의 | 임의 |
  | 도메인 | 특화 학습 필요 | 제로샷 |
  | 상호작용 | 불가 | 점/박스/텍스트 |
  | 출력 | 시맨틱 레이블 | 바이너리 마스크 |
  | 데이터 | 수천~수만 | 10억+ 마스크 |
```

## 3. 파노픽 세그멘테이션

```
파노픽 세그멘테이션 (Kirillov et al., 2019):
  시맨틱 + 인스턴스를 통합

  Things (셀 수 있는 것): 사람, 차, 동물
  → 인스턴스별로 분리 필요

  Stuff (셀 수 없는 것): 하늘, 도로, 잔디
  → 영역으로만 분류 (인스턴스 구분 불필요)

  파노픽 = Things의 인스턴스 분할 + Stuff의 시맨틱 분할
  → 이미지의 모든 픽셀에 의미를 부여

  PQ (Panoptic Quality):
  PQ = SQ × RQ
  SQ (Segmentation Quality) = 매칭된 마스크의 평균 IoU
  RQ (Recognition Quality) = TP / (TP + FP/2 + FN/2)
  → 분할 품질과 인식 품질을 곱함

초기 접근: 별도 모델 합치기
  시맨틱 모델 (DeepLab) + 인스턴스 모델 (Mask R-CNN)
  → 결과를 합치는 후처리
  → 복잡하고 비효율적

통합 접근:

  Panoptic FPN (Kirillov et al., 2019):
  FPN 위에 시맨틱 + 인스턴스 헤드를 동시에
  → 공유 백본으로 효율적

  Panoptic-DeepLab (Cheng et al., 2020):
  Bottom-up 방식: 각 픽셀의 클래스 + 인스턴스 중심 오프셋
  → 인스턴스를 중심점 기반으로 그룹화

  Mask2Former (Cheng et al., 2022):
  Transformer 쿼리 기반으로 모든 세그멘테이션 통합
  ├─ N개의 쿼리 → N개의 마스크 + 클래스 예측
  ├─ Things: 쿼리가 개별 인스턴스에 매칭
  ├─ Stuff: 쿼리가 영역에 매칭
  └─ Hungarian Matching (DETR과 동일)
  → 세 가지 세그멘테이션을 하나의 아키텍처로!
```

## 4. 세그멘테이션의 실전 응용

```
도메인별 응용:

  의료 영상:
  ├─ 장기 분할: 간, 폐, 심장 등 3D 볼류메트릭
  ├─ 병변 탐지: 종양 경계 정밀 분할
  ├─ 세포 분석: 현미경 이미지의 세포 카운팅
  └─ 도구: nnU-Net, MedSAM (SAM의 의료 버전)

  자율주행:
  ├─ 도로 장면 파노픽: 차량, 보행자, 도로, 건물
  ├─ 실시간 요구: 30+ FPS
  ├─ 3D 세그멘테이션: LiDAR 포인트 클라우드
  └─ 데이터셋: Cityscapes, nuScenes

  위성/원격탐사:
  ├─ 토지 이용 분류: 건물, 도로, 농경지
  ├─ 재난 피해 평가: 침수, 화재 영역
  └─ 초고해상도: 수만 × 수만 픽셀 → 패치 기반

  영상 편집:
  ├─ 배경 제거: 인물 분리 (portrait segmentation)
  ├─ 객체 제거/교체: 인페인팅과 결합
  └─ 특수 효과: 영역별 스타일 적용

세그멘테이션 기법 선택:
  | 요구사항 | 방법 | 모델 |
  |---------|------|------|
  | 클래스별 영역 | 시맨틱 | SegFormer, DeepLab |
  | 개체별 분리 | 인스턴스 | Mask R-CNN, SAM |
  | 모든 픽셀 | 파노픽 | Mask2Former |
  | 범용/제로샷 | 프롬프터블 | SAM, SAM2 |
  | 의료 | 3D 볼류메트릭 | nnU-Net |
```

## 5. 인터랙티브 세그멘테이션과 미래

```
인터랙티브 세그멘테이션:
  사용자 입력으로 점진적 정제

  1단계: 클릭 (양성/음성 점)
  2단계: 마스크 생성
  3단계: 사용자 피드백 (추가 클릭)
  4단계: 마스크 업데이트
  → 반복

  SAM이 이 패러다임을 대중화
  → 주석 도구, 영상 편집 도구에 통합

Open-Vocabulary 세그멘테이션:
  학습하지 않은 클래스도 분할
  ├─ CLIP 기반: 텍스트 "고양이"로 고양이 영역 분할
  ├─ Grounded SAM: 텍스트 → 박스(Grounding DINO) → 마스크(SAM)
  └─ X-Decoder: 텍스트/이미지 프롬프트로 범용 분할

  미래 방향:
  ├─ 3D 이해: 2D 세그멘테이션 → 3D 재구성
  ├─ 비디오: 시간적 일관성 있는 분할 (SAM 2)
  ├─ 멀티모달: 텍스트+이미지 프롬프트 결합
  └─ 자동 주석: 파운데이션 모델로 라벨링 자동화
```

## 핵심 정리

- **Mask R-CNN**은 Faster R-CNN에 마스크 분기를 추가하여 인스턴스 세그멘테이션의 표준을 확립했으며, 클래스별 독립 마스크와 RoI Align이 핵심입니다
- **SAM**은 11억 마스크로 학습된 파운데이션 모델로, 점/박스/텍스트 프롬프트로 어떤 것이든 제로샷 분할하며, SAM 2는 비디오로 확장합니다
- **파노픽 세그멘테이션**은 Things(인스턴스)와 Stuff(영역)를 통합하며, **Mask2Former**가 Transformer 쿼리로 세 가지 세그멘테이션을 하나의 아키텍처로 통합합니다
- 의료(nnU-Net), 자율주행(실시간), 위성(초고해상도) 등 **도메인별 요구사항**에 따라 아키텍처와 학습 전략이 달라집니다
- **Open-Vocabulary 세그멘테이션**(Grounded SAM)은 학습하지 않은 클래스도 텍스트 프롬프트로 분할하며, 범용 시각 이해의 미래 방향입니다
