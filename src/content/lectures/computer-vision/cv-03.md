# 객체 탐지: One-Stage와 DETR

## 왜 One-Stage 탐지가 필요한가

cv-02의 Two-Stage는 높은 정확도를 달성하지만, RPN과 Detection Head의 2단계 구조가 속도의 병목입니다. **One-Stage 탐지기**는 영역 제안 없이 특징 맵에서 직접 탐지하여, 실시간 응용에 적합합니다. 그리고 **DETR**은 앵커, NMS, 수작업 구성 요소를 모두 제거한 혁신적 패러다임을 제시합니다.

> **핵심 직관**: One-Stage 탐지의 핵심은 **"모든 위치를 동시에 평가"**하는 것입니다. Two-Stage가 "후보 선별 → 정밀 분류"라면, One-Stage는 "모든 위치에서 한 번에 분류와 위치를 예측"합니다. 더 빠르지만, 극단적 클래스 불균형 문제를 해결해야 합니다.

## 1. YOLO 계열

```
YOLOv1 (Redmon et al., 2016):
  "You Only Look Once" — 한 번의 포워드로 탐지

  이미지를 S×S 그리드로 나눔 (S=7)
  각 셀이 B개의 바운딩 박스와 C개의 클래스 확률 예측
  출력: S × S × (B × 5 + C)
  → 7 × 7 × (2 × 5 + 20) = 7 × 7 × 30

  각 박스: (x, y, w, h, confidence)
  confidence = P(object) × IoU(pred, truth)

  속도: 45 FPS (Fast YOLO: 155 FPS!)
  but: 정확도는 Faster R-CNN보다 낮음

  한계:
  ├─ 셀당 2개 박스 → 밀집 객체 탐지 약함
  ├─ 작은 객체 탐지 어려움 (7×7 그리드)
  └─ 위치 정확도 낮음

YOLOv3 (Redmon & Farhadi, 2018):
  ├─ FPN과 유사한 다스케일 탐지
  │   3개 스케일: 13×13, 26×26, 52×52
  ├─ 앵커 박스 도입 (k-means 클러스터링으로 결정)
  ├─ Darknet-53 백본
  └─ 멀티레이블 분류 (sigmoid, softmax 아님)

YOLOv5/v8 (Ultralytics):
  ├─ PyTorch 기반 구현 (원래 YOLO는 C/Darknet)
  ├─ 모자이크 데이터 증강
  ├─ 자동 앵커 학습
  ├─ CSP 백본 (Cross Stage Partial)
  └─ 다양한 크기: n/s/m/l/x

  | 모델 | 파라미터 | FPS | AP (COCO) |
  |------|---------|-----|-----------|
  | YOLOv8n | 3.2M | 170 | 37.3 |
  | YOLOv8s | 11.2M | 130 | 44.9 |
  | YOLOv8m | 25.9M | 80 | 50.2 |
  | YOLOv8l | 43.7M | 50 | 52.9 |
```

## 2. SSD와 Focal Loss

```
SSD (Single Shot MultiBox Detector, Liu et al., 2016):
  다중 스케일 특징 맵에서 직접 탐지

  VGG-16 + 추가 Conv 레이어:
  38×38 → 19×19 → 10×10 → 5×5 → 3×3 → 1×1
  각 스케일에서 앵커 기반 탐지

  장점: 다양한 크기의 객체를 자연스럽게 처리
  → FPN의 아이디어를 먼저 사용한 셈

Focal Loss (Lin et al., 2017):
  One-Stage의 핵심 문제: 극단적 클래스 불균형

  배경 앵커: ~100,000개
  물체 앵커: ~10개
  → 쉬운 배경이 손실을 지배

  Cross-Entropy: CE = -log(p_t)
  Focal Loss: FL = -α_t (1 - p_t)^γ log(p_t)

  γ = 2일 때:
  p_t = 0.9 (쉬운 예): (1-0.9)² = 0.01 → 가중치 1%
  p_t = 0.1 (어려운 예): (1-0.1)² = 0.81 → 가중치 81%
  → 쉬운 예의 기여를 100배 줄임!

  α_t: 클래스별 가중치 (양성 α=0.25, 음성 1-α=0.75)

RetinaNet:
  FPN + Focal Loss = One-Stage이면서 Two-Stage 정확도!
  → Focal Loss가 One-Stage의 정확도 문제를 해결

  이전: "Two-Stage가 정확, One-Stage가 빠름"
  이후: "One-Stage도 정확할 수 있다"
```

> **핵심 직관**: Focal Loss의 핵심은 **"쉬운 예제에서 배우지 마라"**입니다. 배경 분류는 대부분 쉽습니다(p=0.99). 이런 예제의 손실 기여를 $(1-p)^\gamma$로 줄이면, 모델은 어려운 경계 사례에 집중합니다. 이것은 ad-05의 가지치기와 유사한 "불필요한 계산 제거"의 원리입니다.

## 3. Anchor-Free 탐지

```
앵커의 문제점:
  ├─ 앵커 크기/비율을 사전 정의해야 함
  ├─ 데이터셋마다 최적 앵커가 다름
  ├─ 양성/음성 매칭 규칙이 성능에 큰 영향
  └─ 앵커 수가 많아 메모리/계산 낭비

FCOS (Fully Convolutional One-Stage, Tian et al., 2019):
  "앵커 없이, 각 위치에서 직접 거리 예측"

  특징 맵의 각 위치 (x, y)에서:
  ├─ 분류: C 클래스 확률
  ├─ 회귀: (l, t, r, b) = 왼쪽/위/오른쪽/아래까지 거리
  └─ Centerness: 중심에 가까울수록 높은 점수

  Centerness:
  centerness = sqrt(min(l,r)/max(l,r) × min(t,b)/max(t,b))
  → 객체 중심에서 예측한 박스가 더 높은 가중치
  → 객체 가장자리의 낮은 품질 예측 억제

  다중 스케일 할당:
  FPN P3: 작은 객체 (0~64 픽셀)
  FPN P4: 중간 객체 (64~128)
  FPN P5: 큰 객체 (128~256)
  → 크기 범위로 FPN 레벨에 할당

CenterNet (Zhou et al., 2019):
  "객체를 중심점으로 탐지"

  히트맵: 각 클래스별 중심점 확률 (가우시안)
  오프셋: 양자화 오차 보정
  크기: (w, h) 직접 예측

  → NMS도 불필요! (히트맵 피크 = 탐지)
  → 매우 간단하고 범용적 (2D/3D, 포즈 등)
```

## 4. DETR: 탐지의 패러다임 전환

```
DETR (Detection Transformer, Carion et al., 2020):
  "앵커, NMS, 수작업 구성 요소를 모두 제거"

  구조:
  이미지 → CNN(ResNet) → Transformer Encoder
  → Transformer Decoder(N개 쿼리) → 분류 + 박스

  핵심 요소:

  1. Object Queries (100개):
     학습 가능한 N개의 위치 임베딩
     각 쿼리가 하나의 객체를 담당
     → "슬롯": 각 쿼리가 하나의 객체를 할당받음

  2. Transformer Encoder:
     CNN 특징에 Self-Attention 적용
     → 전역 문맥 이해 (큰 객체의 관계)

  3. Transformer Decoder:
     Object Queries가 인코더 출력에 Cross-Attention
     → 각 쿼리가 관련 영역에 "주목"

  4. Hungarian Matching (이분 매칭):
     예측 N개와 GT M개의 최적 1:1 매칭
     → ga-05의 이분 매칭 알고리즘 적용!
     NMS 불필요 — 각 쿼리가 고유한 객체 담당

  Hungarian Loss:
  매칭 비용 = λ_cls × L_cls + λ_box × L_box + λ_giou × L_giou
  최적 매칭을 찾은 후 해당 쌍에 대해 손실 계산

  혁신:
  ├─ 앵커 없음 → 하이퍼파라미터 감소
  ├─ NMS 없음 → 후처리 단순화
  ├─ 집합 예측(Set Prediction) → 중복 없는 탐지
  └─ 전역 추론 → 큰 객체/문맥 이해에 강함

  한계:
  ├─ 수렴 느림 (500 에폭 vs 일반 36 에폭)
  ├─ 작은 객체 탐지 약함
  └─ 고정된 쿼리 수 (100개)
```

```
DETR의 후속 개선:

  Deformable DETR (Zhu et al., 2021):
  ├─ 모든 위치에 어텐션 → 소수의 학습된 위치만
  │   → O(N²) → O(NK) (K: 참조 포인트 수)
  ├─ 다스케일 특징 활용
  └─ 수렴 10배 빠름, 작은 객체 개선

  DAB-DETR, DN-DETR:
  ├─ 쿼리에 앵커 포인트 정보 직접 부여
  └─ Denoising 학습으로 수렴 가속

  DINO (Detection with Improved deNoising anchOr):
  ├─ Deformable + DN + 대조 학습
  ├─ COCO AP 63.2% (최고 수준)
  └─ 혼합 쿼리: 학습된 + 앵커 기반

  Co-DETR:
  ├─ 협력적 학습: 보조 헤드로 정보 공유
  └─ COCO AP 66%+ (Swin-L 백본)

  | 모델 | AP | 수렴(에폭) | 특징 |
  |------|-----|----------|------|
  | DETR | 42.0 | 500 | 첫 Transformer 탐지 |
  | Deformable | 46.2 | 50 | 빠른 수렴 |
  | DINO | 63.2 | 36 | SOTA |
  | Co-DETR | 66.0 | 36 | 현재 최고 |
```

> **핵심 직관**: DETR의 혁신은 **"탐지를 집합 예측(set prediction)으로 재정의"**한 것입니다. 기존 탐지기는 "수만 개의 후보에서 걸러내기"였지만, DETR은 "N개의 슬롯에 객체를 할당하기"입니다. 이분 매칭이 중복 없는 할당을 보장하므로 NMS가 불필요합니다.

## 5. 탐지 평가 지표

```
COCO 평가 지표:

  AP (Average Precision):
  Precision-Recall 곡선의 아래 면적

  AP@0.5: IoU 임계값 0.5에서의 AP
  AP@0.75: IoU 임계값 0.75에서의 AP
  AP (COCO): IoU 0.5:0.05:0.95의 평균
  → 10개 임계값의 평균으로 엄격한 평가

  크기별:
  AP_S: 작은 객체 (area < 32²)
  AP_M: 중간 객체 (32² < area < 96²)
  AP_L: 큰 객체 (area > 96²)

  | 지표 | 의미 | 중요한 상황 |
  |------|------|-----------|
  | AP@0.5 | 대략적 탐지 | 존재 여부만 중요 |
  | AP@0.75 | 정밀 탐지 | 정확한 위치 필요 |
  | AP_S | 작은 객체 | 원거리, 의료 |
  | AP_L | 큰 객체 | 근거리 |

  mAP: 모든 클래스의 AP 평균
  COCO의 "AP" = 보통 mAP@[.5:.95]를 의미
```

## 6. 탐지기 선택 가이드

```
실무 탐지기 선택:

  실시간 (30+ FPS):
  → YOLOv8 s/m, RT-DETR
  → 자율주행, 로봇, 영상 감시

  높은 정확도:
  → Co-DETR, DINO + Swin-L/ConvNeXt-L
  → 의료 영상, 위성 이미지

  경량 (엣지):
  → YOLOv8n, MobileNet-SSD
  → 스마트폰, IoT 디바이스

  범용 연구:
  → Deformable DETR, DINO
  → 다양한 태스크 확장 용이

탐지 패러다임 진화:
  2014: R-CNN — CNN + 수작업 제안
  2015: Faster R-CNN — RPN (학습된 제안)
  2016: YOLO/SSD — One-Stage (앵커 기반)
  2017: RetinaNet — Focal Loss (정확한 One-Stage)
  2019: FCOS — Anchor-Free
  2020: DETR — Transformer (Set Prediction)
  2023: Co-DETR — Transformer SOTA

  → cv-02~03에서 "수작업 → 학습 → 제거"의 흐름:
    수작업 제안 → 학습된 RPN → 앵커 → 앵커 제거 → NMS 제거
```

## 핵심 정리

- **YOLO**는 "한 번 보기"로 실시간 탐지를 달성했고, YOLOv8까지 진화하며 속도와 정확도를 모두 개선했습니다
- **Focal Loss**는 $(1-p_t)^\gamma$ 가중치로 쉬운 예제의 영향을 줄여, One-Stage 탐지의 정확도를 Two-Stage 수준으로 올렸습니다
- **Anchor-Free**(FCOS, CenterNet)는 앵커 설계의 번거로움을 제거하고 각 위치에서 직접 거리/크기를 예측합니다
- **DETR**은 Transformer + Hungarian Matching으로 탐지를 집합 예측으로 재정의하여, 앵커와 NMS를 모두 제거했습니다
- DETR 후속(Deformable, DINO, Co-DETR)이 수렴 속도와 작은 객체 문제를 해결하며 **COCO AP 66%+**를 달성했습니다
