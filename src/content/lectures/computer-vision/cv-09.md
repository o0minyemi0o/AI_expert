# 자기지도 시각 학습

## 왜 자기지도 학습이 중요한가

ImageNet의 140만 장 라벨링에는 수만 시간의 인간 노동이 필요합니다. 하지만 인터넷에는 **수십억 장의 라벨 없는 이미지**가 있습니다. **자기지도 학습(Self-Supervised Learning, SSL)**은 라벨 없이 이미지 자체에서 학습 신호를 만들어, 범용적인 시각 표현을 학습합니다. NLP에서 nlp-04의 BERT가 마스킹으로 사전학습하듯, CV에서도 SSL이 사전학습의 표준이 되어가고 있습니다.

> **핵심 직관**: 자기지도 학습의 핵심은 **"구실 과제(pretext task)를 통해 구조를 이해"**하는 것입니다. "이 이미지의 회전 각도는?" "이 두 패치는 같은 이미지인가?" 같은 과제를 풀면서, 모델은 의미 있는 특징 표현을 자연스럽게 학습합니다. 라벨이 없어도 **데이터 자체의 구조가 감독 신호**입니다.

## 1. 대조 학습: SimCLR

```
SimCLR (Chen et al., 2020):
  "Simple Framework for Contrastive Learning"

  핵심 아이디어:
  같은 이미지의 두 변환은 가깝게,
  다른 이미지의 변환은 멀게

  파이프라인:
  이미지 x
    → 증강 t₁ → x_i → Encoder → h_i → Projector → z_i
    → 증강 t₂ → x_j → Encoder → h_j → Projector → z_j
  z_i와 z_j를 가깝게 (양성 쌍)
  다른 이미지의 z_k를 멀게 (음성 쌍)

  NT-Xent Loss (Normalized Temperature-scaled CE):
  L = -log(exp(sim(z_i, z_j)/τ) / Σ_k exp(sim(z_i, z_k)/τ))

  sim(u, v) = u·v / (||u|| ||v||)  (코사인 유사도)
  τ: 온도 (0.1~0.5, 작을수록 날카로운 분포)

  배치 내 2N개의 샘플에서:
  양성 쌍: 1개 (같은 이미지의 다른 증강)
  음성 쌍: 2(N-1)개 (다른 모든 이미지)

  핵심 발견:

  1. 데이터 증강이 가장 중요:
     ├─ Random Crop + Resize (가장 핵심)
     ├─ Color Distortion (색상 왜곡)
     ├─ Gaussian Blur
     └─ 조합이 개별보다 훨씬 강력

     왜 Random Crop이 중요한가:
     작은 패치와 큰 패치가 같은 이미지임을 배우면
     → 지역적 특징과 전역적 특징의 관계를 학습
     → 자연스럽게 의미적 표현 획득

  2. Projection Head가 필수:
     Encoder 출력 h (2048d)
     → MLP(2048→2048→128) → z
     학습은 z에서, 전이학습은 h에서!
     → Projection이 증강에 특화된 정보를 제거

  3. 큰 배치가 필수:
     배치 4096~8192가 최적
     → 음성 쌍이 많아야 좋은 표현 학습
     → 큰 배치 = 더 다양한 음성 예제

  | 방법 | 배치 | Top-1 (ImageNet) | 비고 |
  |------|------|-------------------|------|
  | 지도학습 | 256 | 76.5% | ResNet-50 |
  | SimCLR | 4096 | 69.3% | 100 에폭 |
  | SimCLR v2 | 4096 | 71.7% | + 큰 모델 |
  | + 10% 라벨 | 4096 | 76.6% | 지도학습과 동등! |
```

## 2. 음성 쌍 없이: BYOL과 SimSiam

```
대조 학습의 문제:
  큰 배치(4096+)가 필요 → 큰 GPU 자원
  음성 쌍의 품질이 성능에 영향
  → 음성 쌍 없이 학습할 수 없을까?

BYOL (Grill et al., 2020):
  Bootstrap Your Own Latent — 음성 쌍 불필요!

  구조:
  Online Network:  x_i → Encoder → Projector → Predictor → q
  Target Network:  x_j → Encoder → Projector → z

  손실: L = ||q̄ - z̄||² (정규화된 벡터의 MSE)
  → q가 z를 예측하도록 학습

  Target Network: Online의 EMA (지수이동평균)
  ξ ← τ·ξ + (1-τ)·θ  (τ=0.996)
  → 천천히 업데이트되는 "교사"

  왜 붕괴(collapse)가 일어나지 않는가?
  양성 쌍만 가깝게 → 모든 출력이 같아지면 안 되나?
  ├─ Predictor의 비대칭성이 붕괴 방지
  ├─ EMA Target이 안정적 타겟 제공
  └─ 배치 정규화의 암묵적 음성 쌍 효과

SimSiam (Chen & He, 2021):
  BYOL을 더 단순화 — EMA도 불필요!

  구조:
  x_i → Encoder → Projector → Predictor → p₁
  x_j → Encoder → Projector → z₂ (stop-gradient!)

  손실: L = -cos(p₁, stopgrad(z₂))

  Stop-Gradient가 핵심:
  z₂ 방향의 그래디언트를 차단
  → 한쪽만 업데이트 → 비대칭 → 붕괴 방지

  | 방법 | 음성 쌍 | EMA | 배치 | 성능 |
  |------|---------|-----|------|------|
  | SimCLR | 필요 | 없음 | 4096 | 69.3 |
  | MoCo v2 | 필요(큐) | 있음 | 256 | 71.1 |
  | BYOL | 불필요 | 있음 | 4096 | 74.3 |
  | SimSiam | 불필요 | 없음 | 256 | 71.3 |
```

> **핵심 직관**: BYOL이 붕괴하지 않는 이유는 **"교사-학생의 비대칭"**입니다. Online이 빠르게 변하고 Target이 느리게 따라가는 구조에서, 붕괴(상수 출력)는 Predictor가 학습할 것이 없는 상태입니다. Predictor가 의미 있는 예측을 계속하려면 표현이 구조적이어야 합니다.

## 3. DINO: 자기 증류

```
DINO (Caron et al., 2021):
  Self-Distillation with No Labels

  핵심: ViT + Self-Distillation → 놀라운 시맨틱 특징

  구조:
  Student: x의 작은 크롭 → ViT → softmax(z_s/τ_s)
  Teacher: x의 큰 크롭 → ViT → softmax(z_t/τ_t)

  Multi-Crop 전략:
  ├─ Global Crop: 2개 (이미지의 50%+ 영역)
  ├─ Local Crop: 여러 개 (이미지의 20%- 영역)
  ├─ Teacher: Global Crop만 사용
  └─ Student: 모든 Crop 사용

  → Student가 작은 패치에서 전체 정보를 추론하도록!

  손실: Cross-Entropy(Student, Teacher)
  Teacher: Student의 EMA (BYOL과 유사)

  Centering + Sharpening:
  ├─ Centering: Teacher 출력에서 평균을 빼기
  │   → 하나의 차원이 지배하는 것을 방지
  ├─ Sharpening: Teacher의 낮은 τ_t
  │   → 날카로운 분포 → 의미적 구분
  └─ 이 두 기법이 붕괴 방지의 핵심

  놀라운 발견:
  DINO-ViT의 Self-Attention 맵이 자동으로
  **객체의 세그멘테이션을 학습!**
  → 어떤 세그멘테이션 라벨도 없이
  → CLS 토큰의 어텐션 = 전경 객체 마스크
  → "ViT가 장면 구조를 자연스럽게 이해한다"

DINOv2 (Oquab et al., 2023):
  DINO를 대규모로 확장

  ├─ 1.42억 이미지로 학습 (자동 큐레이션)
  ├─ ViT-g (1.1B 파라미터)
  ├─ 분류, 세그멘테이션, 깊이 등 범용 특징
  ├─ 파인튜닝 없이도 강력한 성능
  └─ 시각 파운데이션 모델의 시작

  | 태스크 | 방법 | 성능 |
  |--------|------|------|
  | ImageNet | Linear Probe | 83.0% |
  | ADE20K | Linear | 49.0 mIoU |
  | NYUd | Linear | 0.28 RMSE |
  → 하나의 모델이 모든 시각 태스크의 기초!
```

## 4. MAE: 마스크 오토인코더

```
MAE (He et al., 2022):
  Masked Autoencoders — "BERT의 CV 버전"

  핵심 아이디어:
  이미지 패치의 75%를 마스킹하고 복원

  파이프라인:
  1. 이미지를 패치로 나눔 (예: 16×16)
  2. 랜덤으로 75% 패치 마스킹
  3. 보이는 25% 패치만 Encoder(ViT)에 입력
  4. Decoder가 마스킹된 패치를 복원
  5. 손실: 마스킹된 패치의 MSE

  구조:
  [■ □ □ ■ □ ■ □ □ ■ □ □ ■ □ □ □ □]
   ↓ (보이는 패치만)
  [■ ■ ■ ■] → ViT Encoder (경량)
   ↓ (마스크 토큰 추가)
  [■ M M ■ M ■ M M ■ M M ■ M M M M]
   → Decoder (경량) → 원본 복원

  설계 결정:

  1. 높은 마스킹 비율 (75%):
     NLP의 BERT는 15% → CV는 75%!
     이미지는 중복이 많아서 25%만으로도 복원 가능
     → 높은 비율이 더 의미 있는 학습 강제

  2. 비대칭 구조:
     Encoder: 보이는 25% 패치만 처리 → 빠름!
     Decoder: 전체 패치 복원 (경량 Transformer)
     → 학습 시간 3배 절약

  3. 픽셀 복원:
     토큰이 아닌 원시 픽셀을 복원
     per-patch normalized MSE 사용

  | 방법 | 구실 과제 | 마스킹 | 사전학습 | ImageNet |
  |------|----------|--------|---------|----------|
  | MAE | 패치 복원 | 75% | 1600 에폭 | 84.0% |
  | BEiT | 토큰 예측 | 40% | 800 에폭 | 83.2% |
  | SimMIM | 픽셀 복원 | 60% | 800 에폭 | 83.8% |

MAE vs 대조 학습:

  대조 학습 (SimCLR, DINO):
  ├─ 불변성(invariance) 학습: 증강에 불변한 표현
  ├─ 높은 수준의 시맨틱 특징에 강함
  └─ 분류, 검색에 우수

  마스크 모델링 (MAE):
  ├─ 예측(prediction) 학습: 빈 부분을 채우는 능력
  ├─ 저수준 + 고수준 특징 모두 학습
  └─ 세그멘테이션, 탐지에 우수

  → 두 접근의 결합이 최선 (예: iBOT)
```

> **핵심 직관**: MAE의 75% 마스킹은 **"너무 쉬우면 배우지 않는다"**의 원리입니다. 이미지는 인접 픽셀 간 상관이 높아, 50% 마스킹은 단순 보간으로 풀 수 있습니다. 75%를 가려야 모델이 객체의 형태, 텍스처, 의미를 **이해**해야 복원할 수 있습니다.

## 5. 시각 기반 모델과 통합

```
시각 파운데이션 모델의 흐름:

  2020: SimCLR, MoCo v2 — 대조 학습의 시작
  2020: BYOL — 음성 쌍 제거
  2021: DINO — ViT + Self-Distillation
  2022: MAE — 마스크 오토인코더
  2023: DINOv2 — 대규모 시각 파운데이션 모델
  2023: SAM — 세그멘테이션 파운데이션 (MAE 사전학습)

  SAM과 SSL의 연결:
  SAM의 Image Encoder = ViT-H
  사전학습 방법 = MAE!
  → SSL이 파운데이션 모델의 기초

  CLIP (cv-11에서 상세히):
  이미지-텍스트 대조 학습
  → SSL을 멀티모달로 확장
  → 시각+언어 표현의 통합

사전학습 전략 비교:

  | 방법 | 데이터 | 특징 | 적합한 태스크 |
  |------|-------|------|-------------|
  | ImageNet 지도 | 라벨 필요 | 분류 특화 | 분류 |
  | SimCLR/DINO | 이미지만 | 시맨틱 불변성 | 분류, 검색 |
  | MAE | 이미지만 | 구조 이해 | 탐지, 세그멘테이션 |
  | CLIP | 이미지+텍스트 | 멀티모달 | 제로샷, 검색 |
  | DINOv2 | 대규모 이미지 | 범용 | 모든 태스크 |
```

## 6. SSL의 실전 적용

```
전이학습 프로토콜:

  1. Linear Probing:
     Encoder 동결 + 마지막에 선형 분류기만 학습
     → 표현의 품질을 순수하게 평가
     → "특징이 이미 충분히 분리되어 있는가?"

  2. Fine-tuning:
     전체 모델을 다운스트림 데이터로 미세조정
     → 실전 성능, 보통 Linear보다 높음
     → 적은 데이터에서도 효과적

  3. k-NN Evaluation:
     학습 없이 특징 공간에서 k-최근접 이웃 분류
     → 가장 순수한 표현 품질 평가

  도메인별 SSL 적용:

  의료 영상:
  ├─ 라벨이 매우 비쌈 (전문가 주석)
  ├─ SSL 사전학습 → 소량 라벨로 파인튜닝
  ├─ 조직 병리: DINO로 조직 패턴 학습
  └─ 방사선: MAE로 해부학적 구조 학습

  위성/원격탐사:
  ├─ 대량의 라벨 없는 위성 이미지
  ├─ 시간적 대조 학습: 같은 지역, 다른 시간
  └─ SatMAE: 위성 이미지용 MAE

  자율주행:
  ├─ 3D 포인트 클라우드 SSL
  ├─ 시간적 대조: 연속 프레임
  └─ 크로스 모달: 카메라-LiDAR 대조 학습

  SSL 선택 가이드:
  ├─ 대규모 모델 → DINOv2 (범용 최강)
  ├─ 의료/과학 → MAE 사전학습 + 파인튜닝
  ├─ 빠른 실험 → SimCLR (구현이 간단)
  └─ 멀티모달 → CLIP 기반 (cv-11)
```

## 핵심 정리

- **SimCLR**은 같은 이미지의 두 증강을 가깝게, 다른 이미지를 멀게 하는 대조 학습으로, Random Crop과 Color Distortion이 핵심 증강이며 큰 배치가 필수입니다
- **BYOL/SimSiam**은 음성 쌍 없이 학습하며, EMA Target(BYOL)이나 Stop-Gradient(SimSiam)의 비대칭 구조가 표현 붕괴를 방지합니다
- **DINO**는 ViT + Self-Distillation으로 세그멘테이션 라벨 없이도 객체 경계를 학습하며, **DINOv2**는 이를 대규모로 확장한 시각 파운데이션 모델입니다
- **MAE**는 75% 마스킹된 이미지 패치를 복원하는 과제로 구조적 이해를 학습하며, 비대칭 인코더-디코더로 학습 효율을 3배 높입니다
- 대조 학습(불변성)과 마스크 모델링(예측)은 **상호 보완적**이며, 실전에서는 DINOv2나 CLIP 기반 사전학습이 범용적으로 가장 강력합니다
