# CI/CD 파이프라인 구축

## 왜 CI/CD가 중요한가

ML 프로젝트에서 수동으로 테스트를 실행하고, 린트를 확인하고, 모델을 빌드하는 과정은 시간이 지남에 따라 점점 건너뛰게 됩니다. CI/CD(Continuous Integration/Continuous Deployment)는 이 모든 검증을 자동화하여, 코드가 저장소에 들어오는 순간부터 배포까지의 품질을 보장합니다.

> **핵심 직관**: CI/CD는 "코드가 항상 배포 가능한 상태"를 유지하게 해줍니다. 수동 검증에 의존하면 반드시 빈틈이 생깁니다.

## 1. CI/CD 파이프라인 구성 요소

| 단계 | 목적 | 실행 시점 | 실행 시간 |
|------|------|----------|----------|
| 린트/포맷팅 | 코드 스타일 검증 | 모든 PR | < 1분 |
| 타입 검사 | 정적 타입 오류 검출 | 모든 PR | 1-3분 |
| 유닛 테스트 | 개별 함수 검증 | 모든 PR | 1-5분 |
| 통합 테스트 | 컴포넌트 연결 검증 | 모든 PR | 5-15분 |
| 모델 테스트 | 모델 동작 검증 | merge 시 | 15-60분 |
| 빌드/패키징 | Docker 이미지 생성 | merge 시 | 5-15분 |
| 배포 | 스테이징/프로덕션 | 수동 승인 후 | 5-30분 |

```
CI/CD 파이프라인 흐름:
┌────────┐   ┌──────────┐   ┌──────────┐   ┌──────────┐
│ PR 생성 │──→│  린트    │──→│ 타입검사  │──→│ 유닛     │
│        │   │  (ruff)  │   │ (mypy)   │   │ 테스트   │
└────────┘   └────┬─────┘   └────┬─────┘   └────┬─────┘
                  │ 실패 시      │ 실패 시      │ 실패 시
                  │ PR 차단      │ PR 차단      │ PR 차단
                  ↓              ↓              ↓
             ┌──────────────────────────────────────┐
             │        merge to main                  │
             └──────────┬───────────────────────────┘
                        ↓
             ┌──────────┐   ┌──────────┐
             │ 모델     │──→│ Docker   │──→ 배포
             │ 테스트   │   │ 빌드     │
             └──────────┘   └──────────┘
```

## 2. GitHub Actions — 기본 워크플로

```yaml
# .github/workflows/ci.yml
name: CI Pipeline

on:
  pull_request:
    branches: [main]
  push:
    branches: [main]

env:
  PYTHON_VERSION: "3.11"

jobs:
  lint:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: astral-sh/setup-uv@v3
      - run: uv sync --frozen
      - run: uv run ruff check src/
      - run: uv run ruff format --check src/

  type-check:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: astral-sh/setup-uv@v3
      - run: uv sync --frozen
      - run: uv run mypy src/

  test:
    runs-on: ubuntu-latest
    needs: [lint, type-check]
    steps:
      - uses: actions/checkout@v4
      - uses: astral-sh/setup-uv@v3
      - run: uv sync --frozen
      - run: uv run pytest tests/ -m "not slow" --tb=short -q
      - name: Upload coverage
        uses: actions/upload-artifact@v4
        with:
          name: coverage-report
          path: htmlcov/
```

## 3. 모델 테스트 자동화

모델 테스트는 일반 소프트웨어 테스트와 다른 고유한 특성을 가집니다. CI에서 어떻게 자동화하는지 살펴봅시다.

```yaml
# .github/workflows/model-test.yml
name: Model Tests

on:
  push:
    branches: [main]
  schedule:
    - cron: '0 2 * * *'  # 매일 새벽 2시 실행

jobs:
  model-test:
    runs-on: ubuntu-latest
    timeout-minutes: 60
    steps:
      - uses: actions/checkout@v4
      - uses: astral-sh/setup-uv@v3

      - name: Cache model artifacts
        uses: actions/cache@v4
        with:
          path: |
            ~/.cache/huggingface
            ./data/processed
          key: model-cache-${{ hashFiles('uv.lock') }}

      - run: uv sync --frozen

      - name: Run model tests
        run: uv run pytest tests/ -m "slow" --tb=long -v

      - name: Model performance check
        run: |
          uv run python scripts/evaluate_model.py \
            --threshold-accuracy 0.85 \
            --threshold-latency-ms 100
```

### 시나리오: 모델 성능 회귀 탐지

새 코드가 merge되면서 모델 정확도가 85%에서 80%로 떨어졌습니다. CI에서 성능 임계치를 체크하면 이를 자동으로 탐지할 수 있습니다.

```python
# scripts/evaluate_model.py
import argparse
import sys

def evaluate_model(threshold_accuracy: float, threshold_latency_ms: float):
    model = load_latest_model()
    test_data = load_test_data()

    accuracy = model.evaluate(test_data)
    latency = measure_inference_latency(model, test_data)

    print(f"Accuracy: {accuracy:.4f} (threshold: {threshold_accuracy})")
    print(f"Latency: {latency:.1f}ms (threshold: {threshold_latency_ms}ms)")

    if accuracy < threshold_accuracy:
        print(f"FAIL: Accuracy below threshold")
        sys.exit(1)
    if latency > threshold_latency_ms:
        print(f"FAIL: Latency above threshold")
        sys.exit(1)

    print("PASS: All performance checks passed")
```

> **핵심 직관**: ML CI에서는 "코드가 동작하는가"뿐만 아니라 "모델 성능이 유지되는가"도 검증해야 합니다. 성능 임계치를 CI에 통합하면 회귀를 자동으로 차단할 수 있습니다.

## 4. 고급 워크플로 패턴

### 캐싱 전략

```yaml
# 의존성 캐싱으로 CI 속도 향상
- name: Cache uv packages
  uses: actions/cache@v4
  with:
    path: ~/.cache/uv
    key: uv-${{ runner.os }}-${{ hashFiles('uv.lock') }}
    restore-keys: |
      uv-${{ runner.os }}-
```

### 매트릭스 전략

```yaml
# 여러 Python 버전과 OS에서 테스트
jobs:
  test-matrix:
    strategy:
      matrix:
        python-version: ["3.10", "3.11", "3.12"]
        os: [ubuntu-latest, macos-latest]
      fail-fast: false
    runs-on: ${{ matrix.os }}
    steps:
      - uses: actions/checkout@v4
      - uses: astral-sh/setup-uv@v3
        with:
          python-version: ${{ matrix.python-version }}
      - run: uv sync --frozen
      - run: uv run pytest tests/ -m "not slow"
```

## 5. Docker 빌드와 배포

```yaml
# .github/workflows/deploy.yml
name: Build and Deploy

on:
  push:
    tags: ['v*']

jobs:
  build-and-push:
    runs-on: ubuntu-latest
    permissions:
      contents: read
      packages: write
    steps:
      - uses: actions/checkout@v4

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Login to Container Registry
        uses: docker/login-action@v3
        with:
          registry: ghcr.io
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Build and push
        uses: docker/build-push-action@v5
        with:
          push: true
          tags: ghcr.io/${{ github.repository }}:${{ github.ref_name }}
          cache-from: type=gha
          cache-to: type=gha,mode=max
          target: serving
```

### 시나리오: 전체 CI/CD 파이프라인 설계

```
완전한 ML CI/CD 파이프라인:
┌─────────┐
│ PR 생성  │
└────┬────┘
     ↓
┌─────────────────────────────────┐
│ CI (자동, 모든 PR)              │
│ ┌─────┐ ┌──────┐ ┌───────────┐ │
│ │ruff │→│mypy  │→│pytest     │ │
│ │check│ │check │ │(unit+intg)│ │
│ └─────┘ └──────┘ └───────────┘ │
└────────────────┬────────────────┘
                 ↓ merge
┌─────────────────────────────────┐
│ CD (자동, main branch)          │
│ ┌──────────┐ ┌────────────────┐ │
│ │model test│→│Docker build    │ │
│ │+ 성능체크 │ │+ push          │ │
│ └──────────┘ └────────────────┘ │
└────────────────┬────────────────┘
                 ↓ tag
┌─────────────────────────────────┐
│ Deploy (수동 승인)              │
│ ┌──────────┐ ┌────────────────┐ │
│ │staging   │→│production      │ │
│ │deploy    │ │deploy          │ │
│ └──────────┘ └────────────────┘ │
└─────────────────────────────────┘
```

> **핵심 직관**: CI는 빠른 피드백이 핵심입니다. PR 단계의 CI는 10분 이내로 유지하고, 시간이 오래 걸리는 모델 테스트는 merge 후 또는 야간 스케줄로 분리하십시오.

린트 및 테스트 도구의 상세 설정은 se-05와 se-03을, Docker 빌드 최적화는 se-06을, MLOps 배포 전략은 mo-05를 참조하십시오.

## 핵심 정리

- **단계적 파이프라인**: 린트 → 타입검사 → 유닛테스트 → 통합테스트 → 모델테스트 → 빌드 → 배포 순서로 구성합니다
- **속도 최적화**: 캐싱, 병렬 실행, 테스트 분류를 활용하여 PR 단계 CI를 10분 이내로 유지합니다
- **모델 성능 게이트**: 정확도, 지연시간 등의 임계치를 CI에 통합하여 성능 회귀를 자동으로 차단합니다
- **매트릭스 테스트**: 여러 Python 버전과 OS 조합에서 테스트하여 호환성을 보장합니다
- **배포 자동화**: Docker 빌드와 레지스트리 push를 태그 기반으로 자동화하되, 프로덕션 배포는 수동 승인을 거칩니다
