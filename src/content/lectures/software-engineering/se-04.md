# 설정 관리와 재현성

## 왜 설정 관리가 중요한가

"어제까지 잘 되던 모델이 오늘은 결과가 다릅니다." ML 프로젝트에서 가장 자주 듣는 말입니다. 하이퍼파라미터, 데이터 경로, 랜덤 시드, 환경 버전 등 수많은 변수가 결과에 영향을 미칩니다. 체계적인 설정 관리 없이는 실험 재현이 불가능합니다.

> **핵심 직관**: 재현성은 과학적 ML의 기본입니다. 설정 파일 하나만 있으면 동일한 실험을 언제 어디서든 재현할 수 있어야 합니다.

## 1. 설정 관리 방식 비교

| 방식 | 장점 | 단점 | 적합한 상황 |
|------|------|------|------------|
| argparse | 표준 라이브러리 | 복잡한 중첩 설정 어려움 | 간단한 스크립트 |
| YAML 직접 파싱 | 가독성 좋음 | 타입 검증 없음 | 소규모 프로젝트 |
| Hydra | 조합/오버라이드 강력 | 학습 곡선 있음 | 실험 관리 |
| Pydantic Settings | 타입 검증 강력 | 조합 기능 제한 | 프로덕션 설정 |
| dataclasses | 간결, 타입 힌트 | 검증 로직 별도 | 중간 규모 프로젝트 |

## 2. YAML 기반 설정 구조

```yaml
# configs/experiment/baseline.yaml
defaults:
  - _self_
  - model: xgboost
  - data: tabular_v1
  - preprocessing: standard

experiment:
  name: "baseline_experiment"
  seed: 42
  output_dir: "./outputs/${experiment.name}"

training:
  epochs: 100
  batch_size: 32
  learning_rate: 0.001
  early_stopping:
    patience: 10
    metric: "val_loss"
```

```yaml
# configs/model/xgboost.yaml
model:
  name: "xgboost"
  params:
    n_estimators: 100
    max_depth: 6
    learning_rate: 0.1
    subsample: 0.8
```

## 3. Hydra를 활용한 실험 관리

Hydra는 Facebook Research에서 개발한 설정 프레임워크로, ML 실험 관리에 가장 널리 사용됩니다.

```python
# train.py
import hydra
from omegaconf import DictConfig, OmegaConf

@hydra.main(config_path="configs", config_name="experiment/baseline", version_base=None)
def train(cfg: DictConfig) -> float:
    print(OmegaConf.to_yaml(cfg))  # 전체 설정 출력

    set_seed(cfg.experiment.seed)
    data = load_data(cfg.data)
    model = build_model(cfg.model)

    metrics = model.fit(data, epochs=cfg.training.epochs)
    return metrics["val_loss"]

if __name__ == "__main__":
    train()
```

```bash
# 커맨드라인 오버라이드
python train.py model=lightgbm training.learning_rate=0.01

# 멀티런: 하이퍼파라미터 스윕
python train.py --multirun model=xgboost,lightgbm training.learning_rate=0.001,0.01,0.1
```

> **핵심 직관**: Hydra의 핵심 가치는 "설정의 조합"입니다. 모델, 데이터, 전처리 설정을 독립적으로 정의하고 자유롭게 조합하면, 실험 경우의 수를 체계적으로 관리할 수 있습니다.

## 4. 시드 관리와 결정적 실행

랜덤 시드 관리는 재현성의 핵심이지만, 단순히 시드만 고정한다고 재현이 보장되지는 않습니다.

```python
import random
import numpy as np
import torch

def set_seed(seed: int) -> None:
    """모든 랜덤 소스의 시드를 고정"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    # 결정적 알고리즘 사용 (성능 저하 가능)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

# 설정에서 시드를 주입
set_seed(cfg.experiment.seed)
```

### 재현성을 방해하는 요소들

```
재현성 체크리스트:
┌─────────────────────────────────┐
│ 1. 랜덤 시드 고정               │──→ random, numpy, torch
│ 2. 데이터 버전 고정             │──→ DVC, 스냅샷 (mo-03 참조)
│ 3. 라이브러리 버전 고정         │──→ lock 파일 (se-06 참조)
│ 4. 하드웨어 결정성              │──→ cudnn.deterministic
│ 5. 데이터 순서 고정             │──→ shuffle seed, worker seed
│ 6. 설정 전체 저장               │──→ Hydra 자동 저장
└─────────────────────────────────┘
```

## 5. 환경 격리

### 시나리오: "내 컴퓨터에서는 되는데요"

팀원 A의 로컬 환경에서는 정상 동작하지만, 팀원 B의 환경에서는 결과가 다릅니다. 원인은 numpy 버전 차이였습니다.

```python
# Pydantic을 활용한 설정 검증 (py-07 Pydantic 참조)
from pydantic import BaseModel, Field, field_validator

class TrainingConfig(BaseModel):
    seed: int = Field(ge=0, description="랜덤 시드")
    epochs: int = Field(ge=1, le=10000)
    learning_rate: float = Field(gt=0, lt=1)
    batch_size: int = Field(ge=1)

    @field_validator("batch_size")
    @classmethod
    def must_be_power_of_two(cls, v: int) -> int:
        if v & (v - 1) != 0:
            raise ValueError("batch_size는 2의 거듭제곱이어야 합니다")
        return v

# 잘못된 설정을 조기에 감지
config = TrainingConfig(seed=42, epochs=100, learning_rate=0.001, batch_size=32)
```

## 6. 실험 추적과 설정 저장

```python
# 실험 메타데이터를 자동으로 기록
import json
from datetime import datetime
from pathlib import Path

def save_experiment_metadata(cfg: DictConfig, metrics: dict, output_dir: str):
    metadata = {
        "timestamp": datetime.now().isoformat(),
        "config": OmegaConf.to_container(cfg, resolve=True),
        "metrics": metrics,
        "git_hash": get_git_hash(),
        "python_version": sys.version,
        "torch_version": torch.__version__,
    }
    Path(output_dir).mkdir(parents=True, exist_ok=True)
    with open(Path(output_dir) / "metadata.json", "w") as f:
        json.dump(metadata, f, indent=2, ensure_ascii=False)
```

### 시나리오: 3개월 전 실험 재현

3개월 전 최고 성능을 기록한 실험을 재현해야 합니다. Hydra가 자동 저장한 `.hydra/config.yaml`과 `metadata.json`의 git hash를 사용하면 됩니다.

```bash
# Hydra 출력 구조
outputs/
  2024-01-15/
    14-30-22/
      .hydra/
        config.yaml      # 전체 설정 (오버라이드 포함)
        hydra.yaml        # Hydra 자체 설정
        overrides.yaml    # CLI 오버라이드 기록
      metadata.json       # 실험 메타데이터
      model.pt           # 학습된 모델
```

> **핵심 직관**: 설정 관리의 궁극적 목표는 "시간 여행"입니다. 과거 어떤 실험이든 설정 파일 하나로 정확히 같은 상태를 복원할 수 있어야 합니다.

MLOps에서의 실험 추적 도구(MLflow, W&B)와의 연동은 mo-02를 참조하십시오. Docker를 활용한 환경 격리는 se-06에서 자세히 다룹니다.

## 핵심 정리

- **설정 분리**: 코드에 하드코딩된 값을 YAML 설정으로 분리하면 실험 관리와 재현성이 비약적으로 향상됩니다
- **Hydra 활용**: 설정 조합, 커맨드라인 오버라이드, 멀티런 등을 통해 체계적인 실험 관리가 가능합니다
- **시드 관리**: random, numpy, torch 등 모든 랜덤 소스의 시드를 고정하되, 하드웨어 결정성도 함께 고려합니다
- **설정 검증**: Pydantic 등을 통해 설정값의 타입과 범위를 자동 검증하여 잘못된 실험을 조기에 차단합니다
- **메타데이터 기록**: 설정, git hash, 라이브러리 버전, 메트릭을 함께 저장하여 언제든 과거 실험을 재현합니다
