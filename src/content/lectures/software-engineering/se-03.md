# 테스트 전략

## 왜 테스트가 중요한가

ML 코드에서 버그는 에러 메시지 없이 조용히 성능을 떨어뜨립니다. 잘못된 전처리, 뒤바뀐 레이블, 차원 불일치 등은 코드가 실행되지만 결과가 틀리는 "silent bug"를 만듭니다. 테스트는 이런 문제를 조기에 발견하는 유일한 안전망입니다.

> **핵심 직관**: ML 코드의 가장 위험한 버그는 예외를 발생시키지 않습니다. 코드가 돌아가지만 결과가 틀린 상황을 테스트로 방어해야 합니다.

## 1. 테스트 피라미드와 ML

전통적인 테스트 피라미드를 ML 맥락에 맞게 재해석합니다.

| 테스트 유형 | 범위 | ML 적용 예시 | 실행 시간 |
|------------|------|-------------|----------|
| 유닛 테스트 | 함수/클래스 단위 | 전처리 함수, 메트릭 계산 | 밀리초 |
| 통합 테스트 | 컴포넌트 간 연결 | 데이터 로딩 → 전처리 → 모델 입력 | 초 |
| 모델 테스트 | 모델 동작 검증 | 학습 가능성, 과적합 확인 | 분 |
| E2E 테스트 | 전체 파이프라인 | 학습 → 저장 → 로드 → 추론 | 분~시간 |

```
테스트 피라미드 (ML 버전):
         /\
        /E2E\         <- 적지만 반드시 필요
       /------\
      / 모델   \      <- 학습 가능성, 불변성
     /----------\
    /   통합     \    <- 컴포넌트 간 데이터 흐름
   /--------------\
  /    유닛 테스트  \  <- 빠르고 많이
 /------------------\
```

## 2. 유닛 테스트 — pytest 기초와 고급 활용

```python
# tests/test_preprocessing.py
import pytest
import numpy as np

from src.data.transforms import normalize, encode_labels

# 기본 유닛 테스트
def test_normalize_output_range():
    X = np.array([1.0, 2.0, 3.0, 4.0, 5.0])
    result = normalize(X)
    assert result.min() >= 0.0
    assert result.max() <= 1.0

# parametrize로 다수 케이스를 간결하게
@pytest.mark.parametrize("input_arr,expected_shape", [
    (np.zeros((10, 5)), (10, 5)),
    (np.ones((1, 3)), (1, 3)),
    (np.random.randn(100, 20), (100, 20)),
])
def test_normalize_preserves_shape(input_arr, expected_shape):
    assert normalize(input_arr).shape == expected_shape

# fixture를 활용한 테스트 데이터 관리
@pytest.fixture
def sample_dataset():
    np.random.seed(42)
    X = np.random.randn(100, 10)
    y = np.random.randint(0, 3, size=100)
    return X, y
```

## 3. 데이터 의존 코드 테스트

ML 테스트에서 가장 까다로운 부분은 데이터에 의존하는 코드의 테스트입니다.

### 시나리오: 외부 데이터소스에 의존하는 전처리 코드

프로덕션에서는 S3에서 데이터를 읽지만, 테스트에서는 로컬 fixture를 사용해야 합니다.

```python
# conftest.py — 프로젝트 전체에서 공유하는 fixture
import pytest
from pathlib import Path

@pytest.fixture(scope="session")
def test_data_dir():
    return Path(__file__).parent / "fixtures"

@pytest.fixture
def mock_s3_data(test_data_dir, monkeypatch):
    """S3 호출을 로컬 파일로 대체 (se-01 DIP 참조)"""
    def mock_download(bucket, key):
        return test_data_dir / key
    monkeypatch.setattr("src.data.loader.download_from_s3", mock_download)

# 데이터 스키마 검증 테스트
def test_data_schema(sample_dataset):
    X, y = sample_dataset
    assert X.dtype == np.float64
    assert set(np.unique(y)).issubset({0, 1, 2})
    assert len(X) == len(y)
```

> **핵심 직관**: 테스트에서 실제 데이터를 사용하지 마십시오. 작고 결정적인 fixture 데이터를 만들어 사용하면 테스트 속도와 재현성을 모두 확보할 수 있습니다.

## 4. 모델 테스트 — ML 특화 검증

모델 자체의 동작을 검증하는 테스트는 전통 소프트웨어에는 없는 ML 고유의 영역입니다.

```python
# tests/test_model.py
import torch

def test_model_can_overfit_single_batch(sample_dataset):
    """단일 배치에 대해 과적합 가능 여부 확인 — 학습 루프 검증"""
    X, y = sample_dataset
    model = SimpleClassifier(input_dim=10, num_classes=3)
    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

    X_t = torch.tensor(X[:8], dtype=torch.float32)
    y_t = torch.tensor(y[:8], dtype=torch.long)

    for _ in range(200):
        loss = torch.nn.functional.cross_entropy(model(X_t), y_t)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

    assert loss.item() < 0.01, "모델이 단일 배치에 과적합하지 못함"

def test_model_output_shape(sample_dataset):
    X, _ = sample_dataset
    model = SimpleClassifier(input_dim=10, num_classes=3)
    output = model(torch.tensor(X[:5], dtype=torch.float32))
    assert output.shape == (5, 3)

def test_model_save_load_consistency(tmp_path):
    """저장 후 로드한 모델이 동일한 출력을 생성하는지 확인"""
    model = SimpleClassifier(input_dim=10, num_classes=3)
    X = torch.randn(5, 10)
    original_output = model(X)

    path = tmp_path / "model.pt"
    torch.save(model.state_dict(), path)

    loaded = SimpleClassifier(input_dim=10, num_classes=3)
    loaded.load_state_dict(torch.load(path))
    loaded_output = loaded(X)

    assert torch.allclose(original_output, loaded_output)
```

## 5. pytest 고급 기능 활용

```python
# 마커를 활용한 테스트 분류
@pytest.mark.slow
def test_full_training_pipeline(): ...

@pytest.mark.gpu
@pytest.mark.skipif(not torch.cuda.is_available(), reason="GPU 필요")
def test_gpu_training(): ...

# pytest.ini 또는 pyproject.toml 설정
# [tool.pytest.ini_options]
# markers = [
#     "slow: 실행 시간이 긴 테스트",
#     "gpu: GPU가 필요한 테스트",
# ]
# addopts = "-m 'not slow' --tb=short"
```

### 시나리오: CI에서의 테스트 실행 전략

```
CI 테스트 실행 흐름:
┌─────────────┐     ┌──────────────┐     ┌──────────────┐
│ PR 생성 시   │────→│ 유닛 + 통합   │────→│ 모델 테스트   │
│ (매번 실행)  │     │ (< 5분)      │     │ (merge 시만)  │
└─────────────┘     └──────────────┘     └──────────────┘
                                                │
                                          ┌─────┴──────┐
                                          │ E2E 테스트  │
                                          │ (야간/주간)  │
                                          └────────────┘
```

CI/CD 파이프라인에서의 구체적인 구현은 se-08을 참조하십시오. 테스트에서 활용하는 mock과 fixture 패턴은 Python 고급 기능(py-05)과 밀접하게 연관됩니다.

> **핵심 직관**: 모든 테스트를 매번 실행할 필요는 없습니다. 마커와 CI 전략을 결합하여 빠른 피드백과 철저한 검증을 모두 달성하십시오.

## 핵심 정리

- **테스트 피라미드**: 유닛 테스트를 가장 많이, E2E 테스트를 가장 적게 작성하되, ML 고유의 모델 테스트 계층을 추가합니다
- **데이터 격리**: 실제 데이터 대신 결정적인 fixture를 사용하고, 외부 의존성은 monkeypatch로 대체합니다
- **모델 검증**: 단일 배치 과적합, 출력 형태, 저장/로드 일관성 등 ML 고유의 테스트를 반드시 포함합니다
- **pytest 활용**: parametrize, fixture, marker를 적극 활용하여 테스트 코드의 중복을 줄이고 관리성을 높입니다
- **CI 전략**: 테스트를 속도별로 분류하여 PR 시에는 빠른 테스트만, merge 시에는 전체 테스트를 실행합니다
