# 분산 감소 기법

## 왜 분산을 줄여야 하는가

ed-02에서 배운 표본 크기 공식 $n \propto \sigma^2 / \delta^2$이 보여주듯, 결과 변수의 분산 $\sigma^2$이 클수록 더 많은 표본이 필요합니다. 분산을 절반으로 줄이면 동일한 검정력을 절반의 표본으로 달성할 수 있습니다. 이는 곧 실험 기간 단축과 자원 절약을 의미하며, 빠르게 의사결정해야 하는 실무 환경에서 핵심 역량입니다.

---

## 1. 분산 감소의 직관

처치 효과 추정량의 분산은 다음과 같습니다.

$$\text{Var}(\hat{\tau}) = \text{Var}(\bar{Y}_T - \bar{Y}_C) = \frac{\sigma_T^2}{n_T} + \frac{\sigma_C^2}{n_C}$$

분산을 줄이는 전략은 크게 두 가지입니다.

| 전략 | 방법 | 효과 |
|------|------|------|
| $n$ 증가 | 더 많은 표본 수집 | 비용 증가 |
| $\sigma^2$ 감소 | 사전 정보 활용한 보정 | 비용 동일, 정밀도 향상 |

> **핵심 직관**: 분산 감소는 "더 많은 데이터를 모으는" 대신 "같은 데이터에서 더 많은 정보를 추출하는" 전략입니다.

---

## 2. CUPED (Controlled-experiment Using Pre-Experiment Data)

CUPED는 Microsoft에서 개발한 분산 감소 기법으로, 실험 전 데이터를 공변량으로 활용합니다.

### 핵심 아이디어

결과 변수 $Y$에서 실험 전 변수 $X$의 영향을 제거한 보정 변수를 사용합니다.

$$\tilde{Y}_i = Y_i - \theta (X_i - \bar{X})$$

최적의 $\theta$는 분산을 최소화하는 값입니다.

$$\theta^* = \frac{\text{Cov}(Y, X)}{\text{Var}(X)}$$

이때 보정된 변수의 분산은 다음과 같이 감소합니다.

$$\text{Var}(\tilde{Y}) = \text{Var}(Y)(1 - \rho^2_{XY})$$

여기서 $\rho_{XY}$는 $X$와 $Y$의 상관계수입니다.

| 상관계수 $\rho$ | 분산 감소율 | 등가 표본 크기 배율 |
|-----------------|-------------|---------------------|
| 0.3 | 9% | 1.1배 |
| 0.5 | 25% | 1.33배 |
| 0.7 | 51% | 2.04배 |
| 0.9 | 81% | 5.26배 |

```python
import numpy as np
from scipy import stats

np.random.seed(42)
n = 5000

# 실험 전 지표 (예: 지난 주 클릭 수)
x_pre = np.random.normal(10, 3, 2 * n)
# 실험 후 지표: 실험 전 지표와 상관 + 처치 효과
treatment = np.array([0]*n + [1]*n)
true_effect = 0.5
y_post = 0.7 * x_pre + np.random.normal(0, 2, 2*n) + true_effect * treatment

# 보정 전: 단순 평균 차이
y_treat = y_post[treatment == 1]
y_control = y_post[treatment == 0]
naive_est = y_treat.mean() - y_control.mean()
naive_se = np.sqrt(y_treat.var()/n + y_control.var()/n)

# CUPED 보정
theta = np.cov(y_post, x_pre)[0, 1] / np.var(x_pre)
y_adjusted = y_post - theta * (x_pre - x_pre.mean())
y_adj_t = y_adjusted[treatment == 1]
y_adj_c = y_adjusted[treatment == 0]
cuped_est = y_adj_t.mean() - y_adj_c.mean()
cuped_se = np.sqrt(y_adj_t.var()/n + y_adj_c.var()/n)

print(f"보정 전: ATE = {naive_est:.3f}, SE = {naive_se:.4f}")
print(f"CUPED:   ATE = {cuped_est:.3f}, SE = {cuped_se:.4f}")
print(f"분산 감소율: {1 - (cuped_se/naive_se)**2:.1%}")
```

> **핵심 직관**: CUPED의 핵심은 실험 전 데이터가 결과 변수의 변동을 '설명'하는 만큼 분산을 줄인다는 것입니다. 상관이 높을수록 효과가 큽니다.

넷플릭스 추천 실험에서 사용자의 지난 주 시청 시간을 공변량으로 사용하면, 시청 습관의 개인 차이를 보정하여 추정 정밀도를 크게 향상할 수 있습니다.

---

## 3. 층화 (Stratification)

층화는 ed-01에서 소개한 블록 설계의 분석 단계 적용입니다. 모집단을 동질적인 하위 집단(층, strata)으로 나누고, 각 층 내에서 효과를 추정한 후 가중 평균합니다.

$$\hat{\tau}_{\text{strat}} = \sum_{s=1}^{S} w_s \hat{\tau}_s, \quad w_s = \frac{N_s}{N}$$

| 층화 변수 예시 | 이유 |
|----------------|------|
| 국가/지역 | 문화적 차이로 인한 이질성 |
| 플랫폼(모바일/PC) | 사용 패턴 차이 |
| 가입 기간 | 신규/기존 사용자의 행동 차이 |
| 활동량 분위수 | 고/저 활동 사용자 분리 |

```python
import numpy as np

np.random.seed(42)

# 두 층: 고활동(높은 평균), 저활동(낮은 평균)
n_high, n_low = 3000, 7000

# 고활동 사용자
y_high_t = np.random.normal(20, 5, n_high // 2)
y_high_c = np.random.normal(19.5, 5, n_high // 2)
tau_high = y_high_t.mean() - y_high_c.mean()

# 저활동 사용자
y_low_t = np.random.normal(5, 2, n_low // 2)
y_low_c = np.random.normal(4.7, 2, n_low // 2)
tau_low = y_low_t.mean() - y_low_c.mean()

# 층화 추정
w_high = n_high / (n_high + n_low)
w_low = n_low / (n_high + n_low)
tau_strat = w_high * tau_high + w_low * tau_low

# 단순 추정 (비층화)
y_all_t = np.concatenate([y_high_t, y_low_t])
y_all_c = np.concatenate([y_high_c, y_low_c])
tau_naive = y_all_t.mean() - y_all_c.mean()

print(f"층화 추정: {tau_strat:.3f}")
print(f"단순 추정: {tau_naive:.3f}")
```

> **핵심 직관**: 층화는 층 간 변동을 제거하여 잔류 분산만 남기는 전략입니다. 층 간 이질성이 클수록 분산 감소 효과가 큽니다.

---

## 4. 회귀 보정 (Regression Adjustment)

회귀 보정은 CUPED의 일반화로, 여러 공변량을 동시에 활용합니다.

$$Y_i = \alpha + \tau Z_i + \mathbf{X}_i'\boldsymbol{\beta} + \epsilon_i$$

여기서 $Z_i$는 처치 지시 변수, $\mathbf{X}_i$는 공변량 벡터입니다.

### Lin(2013)의 완전 상호작용 모형

무작위 실험에서도 공변량의 중심화(demeaning)와 처치-공변량 상호작용을 포함해야 합니다.

$$Y_i = \alpha + \tau Z_i + \tilde{\mathbf{X}}_i'\boldsymbol{\beta} + Z_i \cdot \tilde{\mathbf{X}}_i'\boldsymbol{\gamma} + \epsilon_i$$

여기서 $\tilde{\mathbf{X}}_i = \mathbf{X}_i - \bar{\mathbf{X}}$는 중심화된 공변량입니다.

```python
import numpy as np
from scipy import stats
import statsmodels.api as sm

np.random.seed(42)
n = 5000

# 공변량 생성
x1 = np.random.normal(0, 1, 2*n)  # 연속형
x2 = np.random.binomial(1, 0.6, 2*n)  # 이진형
z = np.array([0]*n + [1]*n)  # 처치
true_tau = 0.3

# 결과 변수
y = 2 + true_tau * z + 1.5*x1 + 0.8*x2 + np.random.normal(0, 1, 2*n)

# Lin의 회귀 보정
x1_dm = x1 - x1.mean()
x2_dm = x2 - x2.mean()
X_lin = np.column_stack([z, x1_dm, x2_dm, z*x1_dm, z*x2_dm])
X_lin = sm.add_constant(X_lin)
model = sm.OLS(y, X_lin).fit(cov_type='HC2')

print(f"보정 전 추정: {y[z==1].mean() - y[z==0].mean():.3f}")
print(f"회귀 보정 추정: {model.params[1]:.3f}")
print(f"회귀 보정 SE: {model.bse[1]:.4f}")
```

| 방법 | 장점 | 단점 |
|------|------|------|
| CUPED | 단순, 단일 공변량에 최적 | 다중 공변량 처리 어려움 |
| 층화 | 비모수적, 가정 최소 | 연속형 변수 처리 어려움 |
| 회귀 보정 | 다중 공변량, 유연함 | 모형 설정 필요 |

> **핵심 직관**: 회귀 보정은 결과 변수의 변동 중 공변량으로 설명되는 부분을 제거합니다. ci-05의 잠재 결과 프레임워크에서 무작위 배정이 보장되면, 회귀 계수의 인과적 해석은 $\tau$에만 적용됩니다.

---

## 5. 분산 감소와 실험 기간의 관계

분산 감소는 실험 기간에 직접적으로 영향을 줍니다.

$$T_{\text{new}} = T_{\text{old}} \times (1 - \rho^2)$$

여기서 $T$는 필요한 실험 기간, $\rho$는 사전 변수와의 상관계수입니다.

| 분산 감소율 | 기존 14일 실험 → | 기존 30일 실험 → |
|-------------|------------------|------------------|
| 25% | 10.5일 | 22.5일 |
| 50% | 7일 | 15일 |
| 75% | 3.5일 | 7.5일 |

```python
import numpy as np

# 분산 감소에 따른 실험 기간 절약 계산
original_days = 14
variance_reductions = [0.1, 0.25, 0.5, 0.75]

for vr in variance_reductions:
    new_days = original_days * (1 - vr)
    saved = original_days - new_days
    print(f"분산 감소 {vr:.0%}: {new_days:.1f}일 (절약 {saved:.1f}일)")
```

> **핵심 직관**: 분산 감소 50%는 실험 기간을 절반으로 줄이는 것과 같습니다. 연간 수백 개의 실험을 운영하는 조직에서 이 효과는 엄청난 누적 가치를 만듭니다.

---

## 6. 델타 방법과 비율 지표

비율 지표(예: 사용자당 매출 = 총 매출 / 총 사용자)의 분산 추정에는 델타 방법(delta method)이 필요합니다.

비율 $R = \bar{Y} / \bar{N}$의 분산은 다음과 같습니다.

$$\text{Var}(R) \approx \frac{1}{\bar{N}^2} \left[ \text{Var}(\bar{Y}) + R^2 \text{Var}(\bar{N}) - 2R \cdot \text{Cov}(\bar{Y}, \bar{N}) \right]$$

```python
import numpy as np

np.random.seed(42)
n_users = 10000

# 사용자당 수익 (비율 지표)
revenue_per_user = np.random.exponential(5, n_users)
sessions_per_user = np.random.poisson(3, n_users) + 1
revenue_per_session = revenue_per_user / sessions_per_user

# 델타 방법 분산 추정
y_bar = revenue_per_user.mean()
n_bar = sessions_per_user.mean()
ratio = y_bar / n_bar

var_y = np.var(revenue_per_user) / n_users
var_n = np.var(sessions_per_user) / n_users
cov_yn = np.cov(revenue_per_user, sessions_per_user)[0, 1] / n_users

var_ratio = (var_y + ratio**2 * var_n - 2 * ratio * cov_yn) / n_bar**2
se_ratio = np.sqrt(var_ratio)

print(f"비율 추정값: {ratio:.3f}")
print(f"델타 방법 SE: {se_ratio:.4f}")
```

> **핵심 직관**: 비율 지표는 분자와 분모가 모두 확률 변수이므로, 단순 분산 공식 대신 델타 방법으로 정확한 분산을 추정해야 합니다.

---

## 7. 실무 가이드라인

| 상황 | 권장 기법 | 이유 |
|------|-----------|------|
| 실험 전 동일 지표 존재 | CUPED | 가장 높은 상관, 최대 분산 감소 |
| 범주형 공변량 존재 | 층화 | 비모수적, 간단 |
| 다수 공변량 활용 | Lin의 회귀 보정 | 다차원 보정 가능 |
| 비율 지표 | 델타 방법 + CUPED | 정확한 분산 추정 |
| 이질적 처치 효과 예상 | 층화 + ed-08 기법 | 서브그룹별 분석 |

분산 감소 기법을 적용할 때 가장 중요한 원칙은 **사전 공변량만 사용**하는 것입니다. 실험 후(post-treatment) 변수를 공변량으로 사용하면 처치 효과의 일부를 흡수하여 편향이 발생합니다.

> **핵심 직관**: 분산 감소에 사용하는 공변량은 반드시 처치 배정 이전에 결정된 값이어야 합니다. 실험 후 변수를 사용하면 "나쁜 통제(bad control)" 문제가 발생합니다.

---

## 핵심 정리

- **CUPED는 실험 전 데이터와의 상관 $\rho$를 활용하여 분산을 $(1-\rho^2)$만큼 감소시킵니다**
- **층화는 집단 간 이질성을 사전에 제거하여 잔류 분산만으로 추론하는 비모수적 기법입니다**
- **Lin의 회귀 보정은 여러 공변량을 동시에 활용하며, 처치-공변량 상호작용 항을 반드시 포함해야 합니다**
- **비율 지표는 델타 방법으로 분산을 추정해야 하며, 분자·분모의 공분산 구조를 고려해야 합니다**
- **분산 감소에 사용하는 공변량은 반드시 사전(pre-treatment) 변수여야 하며, 사후 변수를 사용하면 편향이 발생합니다**
