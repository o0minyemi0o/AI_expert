# 다중 밴딧 알고리즘

## 왜 탐색과 활용의 균형이 중요한가

ed-02의 전통적 A/B 테스트는 실험 기간 동안 동일한 비율로 트래픽을 배분하지만, 실험 중 열등한 처치에 계속 트래픽을 보내는 것은 비용입니다. 다중 밴딧(multi-armed bandit) 알고리즘은 데이터가 축적됨에 따라 더 유망한 처치에 더 많은 트래픽을 배분하여, 학습과 수익의 균형을 동시에 추구합니다. 이는 실험의 목적이 "어떤 것이 최선인가?"를 넘어 "실험 기간 동안에도 최대 성과를 내자"로 확장되는 패러다임 전환입니다.

---

## 1. 탐색-활용 딜레마

탐색-활용(exploration-exploitation) 딜레마는 불확실성 하에서의 근본적 문제입니다.

| 전략 | 행동 | 장점 | 단점 |
|------|------|------|------|
| **탐색(Exploration)** | 미지의 선택지 시도 | 정보 획득 | 단기 손실 |
| **활용(Exploitation)** | 현재 최선 선택 | 단기 이득 | 정보 부족 |

최적 전략은 둘 사이의 동적 균형입니다.

$$\text{후회(Regret)} = T \cdot \mu^* - \sum_{t=1}^T \mu_{A_t}$$

여기서 $\mu^* = \max_a \mu_a$는 최적 팔의 기대 보상, $A_t$는 시점 $t$에서 선택한 팔입니다.

> **핵심 직관**: 후회(regret)는 "최적 선택을 항상 했더라면 얻었을 보상"과 "실제 얻은 보상"의 차이입니다. 좋은 밴딧 알고리즘은 후회의 증가 속도를 최소화합니다.

---

## 2. Epsilon-Greedy 알고리즘

가장 단순한 밴딧 알고리즘으로, 확률 $\epsilon$으로 탐색하고 $1-\epsilon$으로 활용합니다.

$$A_t = \begin{cases} \text{균일 랜덤 선택} & \text{확률 } \epsilon \\ \arg\max_a \hat{\mu}_a(t) & \text{확률 } 1-\epsilon \end{cases}$$

```python
import numpy as np

class EpsilonGreedy:
    def __init__(self, n_arms, epsilon=0.1):
        self.n_arms = n_arms
        self.epsilon = epsilon
        self.counts = np.zeros(n_arms)
        self.values = np.zeros(n_arms)

    def select_arm(self):
        if np.random.random() < self.epsilon:
            return np.random.randint(self.n_arms)
        return np.argmax(self.values)

    def update(self, arm, reward):
        self.counts[arm] += 1
        n = self.counts[arm]
        self.values[arm] += (reward - self.values[arm]) / n

# 시뮬레이션: 3개 팔, 진짜 확률 [0.05, 0.07, 0.06]
np.random.seed(42)
true_probs = [0.05, 0.07, 0.06]
bandit = EpsilonGreedy(3, epsilon=0.1)
total_reward = 0

for t in range(10000):
    arm = bandit.select_arm()
    reward = np.random.binomial(1, true_probs[arm])
    bandit.update(arm, reward)
    total_reward += reward

print(f"추정 확률: {bandit.values.round(4)}")
print(f"선택 횟수: {bandit.counts.astype(int)}")
print(f"총 보상: {total_reward}")
```

| 장점 | 단점 |
|------|------|
| 구현이 매우 간단 | $\epsilon$ 선택이 임의적 |
| 직관적 이해 | 팔 간 차이가 명확해져도 탐색 비율 유지 |
| 수렴 보장 ($\epsilon > 0$) | 최적 후회 한계에 미달 |

> **핵심 직관**: Epsilon-Greedy는 정보 상태와 무관하게 항상 $\epsilon$ 비율로 탐색합니다. 불확실성이 줄어들어도 탐색을 줄이지 않는 것이 비효율성의 원인입니다.

---

## 3. UCB (Upper Confidence Bound)

UCB는 "낙관주의 원칙(optimism in the face of uncertainty)"에 기반합니다. 각 팔의 보상 상한을 계산하고, 상한이 가장 높은 팔을 선택합니다.

$$A_t = \arg\max_a \left[ \hat{\mu}_a(t) + c\sqrt{\frac{\ln t}{N_a(t)}} \right]$$

여기서 $N_a(t)$는 시점 $t$까지 팔 $a$가 선택된 횟수, $c$는 탐색 파라미터입니다.

| 항 | 역할 | 행동 |
|----|------|------|
| $\hat{\mu}_a(t)$ | 활용 | 평균 보상이 높은 팔 선호 |
| $c\sqrt{\ln t / N_a(t)}$ | 탐색 | 덜 시도된 팔 선호 |

```python
import numpy as np

class UCB1:
    def __init__(self, n_arms, c=2.0):
        self.n_arms = n_arms
        self.c = c
        self.counts = np.zeros(n_arms)
        self.values = np.zeros(n_arms)
        self.t = 0

    def select_arm(self):
        self.t += 1
        # 각 팔 최소 1회 시도
        for a in range(self.n_arms):
            if self.counts[a] == 0:
                return a
        ucb_values = self.values + self.c * np.sqrt(
            np.log(self.t) / self.counts
        )
        return np.argmax(ucb_values)

    def update(self, arm, reward):
        self.counts[arm] += 1
        n = self.counts[arm]
        self.values[arm] += (reward - self.values[arm]) / n

# 시뮬레이션
np.random.seed(42)
true_probs = [0.05, 0.07, 0.06]
ucb = UCB1(3, c=0.5)
total_reward = 0

for t in range(10000):
    arm = ucb.select_arm()
    reward = np.random.binomial(1, true_probs[arm])
    ucb.update(arm, reward)
    total_reward += reward

print(f"추정 확률: {ucb.values.round(4)}")
print(f"선택 횟수: {ucb.counts.astype(int)}")
print(f"총 보상: {total_reward}")
```

UCB1의 후회 한계는 로그적으로 증가합니다.

$$E[\text{Regret}(T)] \leq \sum_{a: \mu_a < \mu^*} \frac{8 \ln T}{\Delta_a} + (1 + \frac{\pi^2}{3}) \sum_{a} \Delta_a$$

여기서 $\Delta_a = \mu^* - \mu_a$는 최적 팔과의 격차입니다.

> **핵심 직관**: UCB는 불확실성 자체를 보상의 일부로 취급합니다. 불확실한 팔은 "아직 좋을 가능성이 있으므로" 시도할 가치가 있습니다.

---

## 4. Thompson Sampling

Thompson Sampling은 베이지안 접근으로, 각 팔의 보상 분포에서 샘플링하여 최대 샘플값을 가진 팔을 선택합니다.

### 베르누이 밴딧의 경우

사전 분포: $\mu_a \sim \text{Beta}(\alpha_a, \beta_a)$

매 시점 $t$에서:
1. 각 팔의 사후 분포에서 샘플: $\tilde{\mu}_a \sim \text{Beta}(\alpha_a, \beta_a)$
2. 최대 샘플 팔 선택: $A_t = \arg\max_a \tilde{\mu}_a$
3. 보상 관측 후 사후 분포 갱신

```python
import numpy as np

class ThompsonSampling:
    def __init__(self, n_arms):
        self.n_arms = n_arms
        self.alpha = np.ones(n_arms)  # 성공 횟수 + 1
        self.beta = np.ones(n_arms)   # 실패 횟수 + 1

    def select_arm(self):
        samples = np.random.beta(self.alpha, self.beta)
        return np.argmax(samples)

    def update(self, arm, reward):
        if reward == 1:
            self.alpha[arm] += 1
        else:
            self.beta[arm] += 1

# 시뮬레이션
np.random.seed(42)
true_probs = [0.05, 0.07, 0.06]
ts = ThompsonSampling(3)
total_reward = 0
arm_history = []

for t in range(10000):
    arm = ts.select_arm()
    reward = np.random.binomial(1, true_probs[arm])
    ts.update(arm, reward)
    total_reward += reward
    arm_history.append(arm)

print(f"사후 분포 파라미터:")
for a in range(3):
    count = arm_history.count(a)
    print(f"  팔 {a}: Beta({ts.alpha[a]:.0f}, {ts.beta[a]:.0f}), 선택 {count}회")
print(f"총 보상: {total_reward}")
```

| 특성 | Epsilon-Greedy | UCB | Thompson Sampling |
|------|---------------|-----|-------------------|
| 이론적 기반 | 임의적 | 빈도주의 | 베이지안 |
| 후회 한계 | $O(\sqrt{T})$ | $O(\log T)$ | $O(\log T)$ |
| 자연적 탐색 감소 | 아니요 | 예 | 예 |
| 구현 난이도 | 낮음 | 중간 | 중간 |
| 실무 성능 | 보통 | 좋음 | 매우 좋음 |

> **핵심 직관**: Thompson Sampling은 "각 팔이 최적일 확률에 비례하여 선택"합니다. 불확실성이 큰 팔은 높은 샘플값이 나올 가능성이 있어 자연스럽게 탐색됩니다.

---

## 5. A/B 테스트 vs 밴딧: 언제 무엇을 쓸 것인가

| 차원 | A/B 테스트 | 밴딧 |
|------|-----------|------|
| 목적 | 인과 효과 추정 | 보상 최대화 |
| 배정 | 고정 비율 | 적응적 |
| 통계적 보장 | 유의 수준, 검정력 | 후회 한계 |
| 실험 중 비용 | 높음 (열등 처치에도 배분) | 낮음 |
| 효과 크기 추정 | 비편향 | 편향 가능 |
| 적합한 상황 | 정밀한 인과 추론 필요 | 빠른 최적화 필요 |

```python
import numpy as np

# A/B 테스트 vs 밴딧의 총 보상 비교
np.random.seed(42)
T = 10000
true_probs = [0.05, 0.07]  # 최적: 팔 1

# A/B 테스트: 50-50 배분
ab_reward = sum(
    np.random.binomial(1, true_probs[t % 2]) for t in range(T)
)

# Thompson Sampling
ts = ThompsonSampling(2)
ts_reward = 0
for t in range(T):
    arm = ts.select_arm()
    reward = np.random.binomial(1, true_probs[arm])
    ts.update(arm, reward)
    ts_reward += reward

print(f"A/B 테스트 총 보상: {ab_reward}")
print(f"Thompson Sampling 총 보상: {ts_reward}")
print(f"밴딧 개선: {(ts_reward - ab_reward)/ab_reward:.1%}")
```

> **핵심 직관**: A/B 테스트는 "학습의 정확성"을, 밴딧은 "학습 중 수익"을 최적화합니다. 둘은 상충 관계이며, 비즈니스 맥락에 따라 선택해야 합니다.

---

## 6. 문맥적 밴딧 (Contextual Bandits)

문맥적 밴딧은 사용자의 특성(문맥, context)을 활용하여 개인화된 팔 선택을 수행합니다.

$$A_t = \arg\max_a E[r_a \mid \mathbf{x}_t]$$

여기서 $\mathbf{x}_t$는 시점 $t$의 문맥 벡터입니다.

### LinUCB 알고리즘

보상을 문맥의 선형 함수로 모델링합니다.

$$r_{a,t} = \mathbf{x}_t' \boldsymbol{\theta}_a + \epsilon_t$$

$$A_t = \arg\max_a \left[ \mathbf{x}_t' \hat{\boldsymbol{\theta}}_a + \alpha \sqrt{\mathbf{x}_t' \mathbf{A}_a^{-1} \mathbf{x}_t} \right]$$

```python
import numpy as np

class LinUCB:
    def __init__(self, n_arms, d, alpha=1.0):
        self.n_arms = n_arms
        self.d = d
        self.alpha = alpha
        self.A = [np.eye(d) for _ in range(n_arms)]
        self.b = [np.zeros(d) for _ in range(n_arms)]

    def select_arm(self, x):
        ucb_values = np.zeros(self.n_arms)
        for a in range(self.n_arms):
            A_inv = np.linalg.inv(self.A[a])
            theta = A_inv @ self.b[a]
            ucb_values[a] = x @ theta + self.alpha * np.sqrt(x @ A_inv @ x)
        return np.argmax(ucb_values)

    def update(self, arm, x, reward):
        self.A[arm] += np.outer(x, x)
        self.b[arm] += reward * x

# 간단한 예시: 2차원 문맥, 2개 팔
np.random.seed(42)
d = 2
linucb = LinUCB(n_arms=2, d=d, alpha=0.5)
theta_true = [np.array([1.0, 0.5]), np.array([0.3, 1.2])]

for t in range(5000):
    x = np.random.randn(d)
    arm = linucb.select_arm(x)
    reward = x @ theta_true[arm] + np.random.normal(0, 0.1)
    linucb.update(arm, x, reward)

print("학습된 파라미터:")
for a in range(2):
    theta_hat = np.linalg.inv(linucb.A[a]) @ linucb.b[a]
    print(f"  팔 {a}: {theta_hat.round(3)} (진짜: {theta_true[a]})")
```

> **핵심 직관**: 문맥적 밴딧은 "모든 사용자에게 같은 최적 팔"이 아니라 "각 사용자에게 최적인 팔"을 찾습니다. ed-08의 이질적 처치 효과와 연결되는 개인화 전략입니다.

---

## 7. 실무 고려사항과 함정

| 고려사항 | 설명 | 대응 |
|----------|------|------|
| 지연 보상(Delayed Reward) | 전환이 즉시 발생하지 않음 | 보상 귀속 윈도우 설정 |
| 비정상성(Non-stationarity) | 보상 분포가 시간에 따라 변화 | 감소 가중, 윈도우 기반 |
| 배치 업데이트 | 실시간 업데이트가 불가능한 경우 | 배치 밴딧 알고리즘 |
| 통계적 추론 | 편향된 효과 추정 | 역확률 가중(IPW) 보정 |

밴딧 알고리즘의 적응적 배정은 효과 크기 추정에 편향을 만들 수 있습니다.

$$\hat{\tau}_{\text{naive}} = \bar{Y}_T - \bar{Y}_C \neq E[\tau]$$

역확률 가중(IPW)으로 보정합니다.

$$\hat{\tau}_{\text{IPW}} = \frac{1}{T} \sum_{t=1}^T \frac{Z_t Y_t}{e_t} - \frac{(1-Z_t) Y_t}{1-e_t}$$

여기서 $e_t = P(Z_t = 1 \mid \mathcal{H}_{t-1})$은 시점 $t$의 배정 확률입니다.

> **핵심 직관**: 밴딧의 적응적 배정은 학습 효율을 높이지만 추론에 편향을 만듭니다. 최적화와 추론을 동시에 원한다면, 이 편향을 명시적으로 보정해야 합니다.

---

## 핵심 정리

- **다중 밴딧은 탐색(정보 획득)과 활용(보상 최대화)의 균형을 동적으로 조절하는 프레임워크입니다**
- **UCB는 낙관주의 원칙으로, Thompson Sampling은 베이지안 사후 분포 샘플링으로 탐색-활용을 해결합니다**
- **A/B 테스트는 정밀한 인과 추론에, 밴딧은 실험 기간 중 보상 최대화에 적합하며, 목적에 따라 선택해야 합니다**
- **문맥적 밴딧(LinUCB 등)은 사용자 특성을 활용한 개인화된 처치 배정을 가능하게 합니다**
- **밴딧의 적응적 배정은 효과 크기 추정에 편향을 만들며, 역확률 가중(IPW)으로 보정이 필요합니다**
