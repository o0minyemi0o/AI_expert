# 네트워크 간섭

## 왜 네트워크 간섭을 고려해야 하는가

ed-01에서 배운 무작위 배정의 핵심 가정은 한 단위의 처치가 다른 단위의 결과에 영향을 주지 않는다는 것입니다. 그러나 소셜 네트워크, 마켓플레이스, 공유 경제 플랫폼에서는 이 가정이 근본적으로 위반됩니다. 카카오톡에서 새로운 메시지 기능을 A/B 테스트할 때, 처치 집단 사용자가 보낸 메시지를 통제 집단 사용자도 경험하게 됩니다. 이러한 간섭(interference)을 무시하면 처치 효과를 심각하게 왜곡할 수 있습니다.

---

## 1. SUTVA와 그 위반

**SUTVA(Stable Unit Treatment Value Assumption)**는 인과 추론의 근본 가정입니다.

$$Y_i(\mathbf{z}) = Y_i(z_i), \quad \forall \mathbf{z}, \mathbf{z}' \text{ s.t. } z_i = z_i'$$

즉, 단위 $i$의 잠재 결과는 자신의 처치 배정 $z_i$에만 의존하고, 다른 단위의 배정 $\mathbf{z}_{-i}$에는 의존하지 않습니다.

| SUTVA 구성 | 의미 | 위반 예시 |
|-----------|------|-----------|
| 간섭 없음 | 타인의 처치가 내 결과에 무영향 | 소셜 기능 실험 |
| 처치 일관성 | 같은 처치 → 같은 잠재 결과 | 처치 버전 차이 |

SUTVA가 위반되면 잠재 결과의 수가 폭발합니다.

$$\text{SUTVA 하}: 2N \text{개 잠재 결과} \quad \rightarrow \quad \text{SUTVA 위반}: 2^N \times N \text{개 잠재 결과}$$

> **핵심 직관**: SUTVA 위반 하에서 "처치 효과"의 정의 자체가 모호해집니다. $Y_i(1)$이 단일 값이 아니라 다른 모든 사용자의 배정에 따라 달라지기 때문입니다.

---

## 2. 간섭의 유형과 메커니즘

| 간섭 유형 | 메커니즘 | 예시 |
|-----------|----------|------|
| **직접 간섭** | 처치 경험이 네트워크를 통해 전파 | 바이럴 기능 테스트 |
| **간접 간섭(스필오버)** | 자원/시장 공유로 인한 영향 | 택시 할인 → 비처치 지역 수요 감소 |
| **사회적 간섭** | 사회적 비교, 모방 행동 | 친구가 새 기능 사용 → 자신도 관심 |
| **균형 효과** | 시장 균형의 변화 | 가격 실험 → 전체 시장 가격 변동 |

넷플릭스 추천 실험에서 가족 구성원이 같은 계정을 공유한다면, 한 사용자에 대한 추천 변경이 다른 구성원의 시청 패턴에 영향을 줄 수 있습니다.

```python
import numpy as np

def simulate_interference(n=1000, network_effect=0.3, direct_effect=0.5):
    """네트워크 간섭 하에서 처치 효과 편향 시뮬레이션"""
    np.random.seed(42)

    # 처치 배정
    z = np.random.binomial(1, 0.5, n)

    # 이웃의 처치 비율 (단순 모형: 무작위 10명의 이웃)
    neighbor_treat_frac = np.zeros(n)
    for i in range(n):
        neighbors = np.random.choice(n, 10, replace=False)
        neighbor_treat_frac[i] = z[neighbors].mean()

    # 결과: 직접 효과 + 네트워크 효과
    y = 5 + direct_effect * z + network_effect * neighbor_treat_frac + \
        np.random.normal(0, 1, n)

    # 순진한 추정 (간섭 무시)
    naive_ate = y[z==1].mean() - y[z==0].mean()
    true_direct = direct_effect

    print(f"진짜 직접 효과: {true_direct:.3f}")
    print(f"순진한 ATE 추정: {naive_ate:.3f}")
    print(f"편향: {naive_ate - true_direct:.3f}")

simulate_interference()
```

> **핵심 직관**: 간섭이 존재하면 순진한 ATE 추정량은 직접 효과와 간접 효과(스필오버)를 혼합합니다. 보통 총 효과를 과소추정하게 됩니다.

---

## 3. 클러스터 랜덤화

클러스터 랜덤화(cluster randomization)는 개인 대신 그룹(클러스터)을 무작위 배정하여 클러스터 내 간섭을 허용합니다.

$$Z_c \sim \text{Bernoulli}(p), \quad z_i = Z_{c(i)}$$

여기서 $c(i)$는 단위 $i$가 속한 클러스터입니다.

| 클러스터 단위 | 적합한 실험 | 근거 |
|---------------|-------------|------|
| 지리적 지역 | 배달/택시 실험 | 공급-수요 균형이 지역적 |
| 시간대 | 마켓플레이스 가격 실험 | 시간대 내 사용자 상호작용 |
| 소셜 그래프 커뮤니티 | 소셜 기능 실험 | 커뮤니티 내 상호작용 |
| 학교/학급 | 교육 실험 | 학급 내 학생 상호작용 |

```python
import numpy as np
from scipy import stats

def cluster_randomization(n_clusters=200, cluster_size=50, effect=0.5):
    """클러스터 랜덤화 실험 시뮬레이션"""
    np.random.seed(42)

    # 클러스터 수준 배정
    z_cluster = np.random.binomial(1, 0.5, n_clusters)

    cluster_means = []
    for c in range(n_clusters):
        # 클러스터 내 상관 (ICC)
        cluster_effect = np.random.normal(0, 2)  # 클러스터 랜덤 효과
        y = cluster_effect + effect * z_cluster[c] + \
            np.random.normal(0, 1, cluster_size)
        cluster_means.append(y.mean())

    cluster_means = np.array(cluster_means)
    treat_means = cluster_means[z_cluster == 1]
    control_means = cluster_means[z_cluster == 0]

    ate = treat_means.mean() - control_means.mean()
    se = np.sqrt(treat_means.var()/len(treat_means) +
                 control_means.var()/len(control_means))

    print(f"클러스터 수준 ATE: {ate:.3f}")
    print(f"SE: {se:.3f}")
    print(f"95% CI: [{ate-1.96*se:.3f}, {ate+1.96*se:.3f}]")

cluster_randomization()
```

클러스터 랜덤화의 유효 표본 크기는 클러스터 내 상관(ICC)에 의해 감소합니다.

$$n_{\text{eff}} = \frac{n}{1 + (m-1)\rho}$$

여기서 $m$은 클러스터 크기, $\rho$는 ICC(Intraclass Correlation Coefficient)입니다.

| ICC ($\rho$) | 클러스터 크기 50 | 유효 표본 비율 |
|-------------|-----------------|---------------|
| 0.01 | 50 | 67% |
| 0.05 | 50 | 29% |
| 0.10 | 50 | 16% |
| 0.20 | 50 | 9% |

> **핵심 직관**: 클러스터 랜덤화는 간섭을 해결하지만, 클러스터 내 상관(ICC)으로 인해 유효 표본 크기가 크게 줄어듭니다. ICC가 높을수록 더 많은 클러스터가 필요합니다.

---

## 4. 노출 매핑과 부분 간섭

모든 간섭을 클러스터 내로 제한할 수 없을 때, **노출 매핑(exposure mapping)**을 사용합니다.

$$Y_i(\mathbf{z}) = Y_i(z_i, g_i(\mathbf{z}_{-i}))$$

여기서 $g_i(\mathbf{z}_{-i})$는 이웃의 처치 배정을 요약하는 함수입니다.

일반적인 노출 매핑의 예시입니다.

| 노출 유형 | $g_i(\mathbf{z}_{-i})$ | 의미 |
|-----------|------------------------|------|
| 이웃 처치 비율 | $\frac{1}{|\mathcal{N}_i|} \sum_{j \in \mathcal{N}_i} z_j$ | 이웃 중 처치 받은 비율 |
| 이웃 처치 수 | $\sum_{j \in \mathcal{N}_i} z_j$ | 처치 받은 이웃의 수 |
| 이웃 처치 여부 | $\mathbb{1}[\exists j \in \mathcal{N}_i: z_j = 1]$ | 하나라도 처치 받은 이웃 존재 |

이 프레임워크에서 효과를 분해합니다.

$$\tau_{\text{direct}}(g) = E[Y_i(1, g) - Y_i(0, g)]$$
$$\tau_{\text{spillover}}(z) = E[Y_i(z, g') - Y_i(z, g)]$$
$$\tau_{\text{total}} = \tau_{\text{direct}} + \tau_{\text{spillover}}$$

```python
import numpy as np

def estimate_spillover_effects(n=5000, n_neighbors=10):
    """노출 매핑 기반 스필오버 효과 추정"""
    np.random.seed(42)

    z = np.random.binomial(1, 0.5, n)

    # 이웃의 처치 비율 계산
    neighbor_frac = np.zeros(n)
    for i in range(n):
        neighbors = np.random.choice(n, n_neighbors, replace=False)
        neighbor_frac[i] = z[neighbors].mean()

    # 결과 생성
    direct = 0.5
    spillover = 0.3
    y = 5 + direct * z + spillover * neighbor_frac + np.random.normal(0, 1, n)

    # 이웃 처치 비율을 이진화 (중앙값 기준)
    high_exposure = neighbor_frac > np.median(neighbor_frac)

    # 2x2 추정
    groups = {
        '처치+고노출': y[(z==1) & high_exposure],
        '처치+저노출': y[(z==1) & ~high_exposure],
        '통제+고노출': y[(z==0) & high_exposure],
        '통제+저노출': y[(z==0) & ~high_exposure],
    }

    for name, values in groups.items():
        print(f"{name}: mean = {values.mean():.3f}, n = {len(values)}")

    direct_est = (groups['처치+저노출'].mean() - groups['통제+저노출'].mean())
    spillover_est = (groups['통제+고노출'].mean() - groups['통제+저노출'].mean())
    print(f"\n직접 효과 추정: {direct_est:.3f} (진짜: {direct})")
    print(f"스필오버 추정: {spillover_est:.3f} (진짜: ~{spillover*0.2:.3f})")

estimate_spillover_effects()
```

> **핵심 직관**: 노출 매핑은 SUTVA를 완화하여, "내 처치"와 "이웃의 처치"가 결과에 미치는 영향을 분리합니다.

---

## 5. 이중 랜덤화 설계

이중 랜덤화(two-stage randomization)는 클러스터 수준과 개인 수준의 배정을 분리합니다.

**1단계**: 클러스터를 고처치율/저처치율 그룹으로 배정

$$p_c \in \{p_H, p_L\}, \quad p_H > p_L$$

**2단계**: 각 클러스터 내에서 개인을 $p_c$ 확률로 처치 배정

이 설계로 직접 효과와 스필오버 효과를 동시에 식별합니다.

| 비교 | 추정하는 효과 |
|------|---------------|
| 같은 클러스터 내: 처치 vs 통제 | 직접 효과 |
| 같은 개인 상태: 고처치율 vs 저처치율 클러스터 | 스필오버 효과 |
| 고처치율 처치 vs 저처치율 통제 | 총 효과 |

```python
import numpy as np

def two_stage_randomization(n_clusters=100, cluster_size=100,
                             p_high=0.7, p_low=0.3):
    """이중 랜덤화 시뮬레이션"""
    np.random.seed(42)
    direct = 0.5
    spillover = 0.3

    results = []
    for c in range(n_clusters):
        # 1단계: 클러스터 배정
        is_high = np.random.binomial(1, 0.5)
        p_c = p_high if is_high else p_low

        # 2단계: 개인 배정
        z = np.random.binomial(1, p_c, cluster_size)
        treat_frac = z.mean()

        # 결과
        y = 5 + direct * z + spillover * treat_frac + \
            np.random.normal(0, 1, cluster_size)

        for i in range(cluster_size):
            results.append({
                'cluster': c, 'high_saturation': is_high,
                'treated': z[i], 'y': y[i], 'treat_frac': treat_frac
            })

    import pandas as pd
    df = pd.DataFrame(results)

    # 직접 효과: 같은 포화도 내 처치 vs 통제
    for sat in [0, 1]:
        sub = df[df['high_saturation'] == sat]
        de = sub[sub['treated']==1]['y'].mean() - sub[sub['treated']==0]['y'].mean()
        label = '고' if sat else '저'
        print(f"직접 효과 ({label}포화): {de:.3f}")

    # 스필오버: 같은 처치 상태, 다른 포화도
    for t in [0, 1]:
        sub = df[df['treated'] == t]
        se = sub[sub['high_saturation']==1]['y'].mean() - \
             sub[sub['high_saturation']==0]['y'].mean()
        label = '처치' if t else '통제'
        print(f"스필오버 ({label} 집단): {se:.3f}")

two_stage_randomization()
```

> **핵심 직관**: 이중 랜덤화는 클러스터의 "처치 포화도"를 실험적으로 변화시켜, 스필오버를 인과적으로 식별하는 유일한 실험 설계입니다.

---

## 6. 그래프 기반 클러스터링

효과적인 클러스터 랜덤화의 핵심은 클러스터 간 간섭을 최소화하는 그래프 분할입니다.

$$\min_{\mathcal{C}} \sum_{c \neq c'} |\{(i,j) : i \in c, j \in c', (i,j) \in \mathcal{E}\}|$$

| 방법 | 장점 | 단점 |
|------|------|------|
| 지리적 분할 | 직관적, 구현 쉬움 | 지리적 경계 간 상호작용 무시 |
| 그래프 커뮤니티 탐지 | 실제 상호작용 패턴 반영 | 계산 비용, 커뮤니티 불안정 |
| 스펙트럴 클러스터링 | 수학적 최적화 보장 | 대규모 그래프에서 비효율 |
| Ego-network 기반 | 개인 중심 클러스터링 | 클러스터 중첩 가능 |

```python
import numpy as np

def simple_graph_clustering(n=1000, n_edges=5000, n_clusters=20):
    """간단한 그래프 분할 예시"""
    np.random.seed(42)

    # 그래프 생성 (인접 리스트)
    edges = set()
    while len(edges) < n_edges:
        i, j = np.random.randint(0, n, 2)
        if i != j:
            edges.add((min(i,j), max(i,j)))

    # 단순 분할: 노드 ID 기반 (실무에서는 그래프 알고리즘 사용)
    cluster_assignment = np.arange(n) % n_clusters

    # 클러스터 간 간섭 비율 계산
    cross_cluster = sum(1 for i, j in edges
                        if cluster_assignment[i] != cluster_assignment[j])
    total_edges = len(edges)

    print(f"총 에지 수: {total_edges}")
    print(f"클러스터 간 에지: {cross_cluster} ({cross_cluster/total_edges:.1%})")
    print(f"클러스터 내 에지: {total_edges-cross_cluster} ({1-cross_cluster/total_edges:.1%})")

simple_graph_clustering()
```

> **핵심 직관**: 클러스터 분할의 품질은 "클러스터 간 에지 비율"로 평가합니다. 이 비율이 낮을수록 간섭이 잘 차단됩니다.

---

## 7. 실무 전략과 바이어스-분산 트레이드오프

| 전략 | 바이어스 | 분산 | 적합한 상황 |
|------|----------|------|-------------|
| 개인 랜덤화 | 높음 (간섭 무시) | 낮음 | 간섭 미미한 경우 |
| 클러스터 랜덤화 | 낮음 | 높음 (ICC) | 간섭 강한 경우 |
| 이중 랜덤화 | 낮음 | 중간 | 직접/간접 효과 분리 필요 |
| 시간 분할 (switchback) | 중간 | 중간 | 마켓플레이스 실험 |

시간 분할(switchback) 설계는 같은 시장에서 처치/통제를 시간대별로 교대합니다. 배달 플랫폼에서 오전은 처치, 오후는 통제로 운영하여 시장 균형 효과를 분리합니다.

ed-01의 내적 타당도 관점에서 네트워크 간섭은 가장 심각한 위협 요인 중 하나입니다. 간섭의 존재를 무시하고 표준 A/B 테스트를 수행하면, 추정된 효과는 실제 효과의 방향조차 잘못될 수 있습니다.

> **핵심 직관**: 네트워크 간섭 하에서의 실험 설계는 "편향을 줄이기 위해 분산을 희생하는" 트레이드오프입니다. 비즈니스의 네트워크 구조를 이해하는 것이 설계의 출발점입니다.

---

## 핵심 정리

- **SUTVA(간섭 없음 가정)가 위반되면 순진한 ATE 추정량은 편향되며, 총 효과를 과소추정하는 경향이 있습니다**
- **클러스터 랜덤화는 클러스터 내 간섭을 허용하지만, ICC로 인한 유효 표본 크기 감소를 고려해야 합니다**
- **노출 매핑은 이웃의 처치 배정을 요약하여, 직접 효과와 스필오버 효과를 분리하는 프레임워크입니다**
- **이중 랜덤화는 클러스터의 처치 포화도를 실험적으로 변화시켜 스필오버를 인과적으로 식별합니다**
- **효과적인 클러스터 분할은 클러스터 간 에지를 최소화하는 그래프 알고리즘에 기반하며, 분할 품질이 추정의 편향을 결정합니다**
