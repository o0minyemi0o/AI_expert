# 라그랑주 승수법

## 왜 라그랑주 승수법을 배워야 하는가

지금까지는 제약 없이 $\min f(\mathbf{x})$를 풀었습니다. 하지만 현실의 많은 문제에는 **제약 조건(constraint)**이 있습니다. "확률의 합은 1이어야 한다", "에너지 소비를 초과할 수 없다", "정규화 조건을 만족해야 한다" 등이 그렇습니다.

**등식 제약(equality constraint)** 이 있는 최적화 문제를 풀기 위한 가장 우아한 도구가 라그랑주 승수법(Lagrange multiplier method)입니다. la-01에서 배운 부분 공간과 co-02의 그래디언트 개념이 여기서 아름답게 합류합니다.

---

## 1. 등식 제약 최적화 문제

$$
\min_{\mathbf{x}} f(\mathbf{x}) \quad \text{s.t.} \quad g_i(\mathbf{x}) = 0, \quad i = 1, \dots, m
$$

**예시**: 단위 구 위에서 함수 최소화

$$
\min_{\mathbf{x}} f(\mathbf{x}) \quad \text{s.t.} \quad \|\mathbf{x}\|^2 - 1 = 0
$$

직접 대입하면 변수를 줄일 수 있지만, 고차원에서는 비실용적입니다. 라그랑주 승수법은 제약을 **목적 함수에 통합**합니다.

---

## 2. 기하학적 해석: 등고선의 접선

2변수, 1제약 문제로 핵심 아이디어를 봅니다.

$$
\min_{x, y} f(x, y) \quad \text{s.t.} \quad g(x, y) = 0
$$

최적점에서 $f$의 등고선과 $g = 0$ 곡선이 **접합니다**. 즉, 두 그래디언트가 **평행**합니다.

```
     y
     │     ╭── f = 5 (등고선)
     │   ╭─┤
     │  │  ╰── f = 3
     │  │  ╭── f = 1
     │  │ │
     │  ╰─·───── g = 0 (제약 곡선)
     │     ↑
     │  ∇f ∥ ∇g  (접하는 점 = 최적!)
     │
     +────────────→ x
```

만약 $\nabla f$와 $\nabla g$가 평행하지 않으면, $g = 0$ 곡선을 따라 이동하여 $f$를 더 줄일 수 있습니다.

> **핵심 직관**: 최적점에서 목적 함수의 그래디언트와 제약의 그래디언트가 평행합니다. 즉, $\nabla f = \lambda \nabla g$인 $\lambda$가 존재합니다. 이 $\lambda$가 **라그랑주 승수**입니다.

---

## 3. 라그랑주 함수

**라그랑주 함수(Lagrangian)**를 정의합니다:

$$
\mathcal{L}(\mathbf{x}, \boldsymbol{\lambda}) = f(\mathbf{x}) + \sum_{i=1}^{m} \lambda_i g_i(\mathbf{x})
$$

여기서 $\lambda_i \in \mathbb{R}$는 **라그랑주 승수(Lagrange multiplier)**입니다.

최적성 조건은 라그랑주 함수의 정류점입니다:

$$
\nabla_\mathbf{x} \mathcal{L} = \nabla f + \sum_{i=1}^{m} \lambda_i \nabla g_i = \mathbf{0}
$$

$$
\nabla_{\boldsymbol{\lambda}} \mathcal{L} = \mathbf{g}(\mathbf{x}) = \mathbf{0}
$$

이것은 $n + m$개의 미지수($\mathbf{x} \in \mathbb{R}^n$, $\boldsymbol{\lambda} \in \mathbb{R}^m$)에 대한 $n + m$개의 방정식입니다.

---

## 4. 구체적 예제

**문제**: 둘레가 $2p$인 직사각형 중 넓이가 최대인 것

$$
\max_{x, y} \; xy \quad \text{s.t.} \quad 2x + 2y = 2p
$$

최소화 형태로 변환: $\min -xy$ s.t. $x + y - p = 0$

$$
\mathcal{L}(x, y, \lambda) = -xy + \lambda(x + y - p)
$$

$$
\frac{\partial \mathcal{L}}{\partial x} = -y + \lambda = 0 \implies \lambda = y
$$

$$
\frac{\partial \mathcal{L}}{\partial y} = -x + \lambda = 0 \implies \lambda = x
$$

$$
\frac{\partial \mathcal{L}}{\partial \lambda} = x + y - p = 0
$$

따라서 $x = y = p/2$, 즉 **정사각형**이 최대 넓이를 갖습니다.

---

## 5. ML 예제: 라그랑주 승수와 소프트맥스

소프트맥스 함수는 "확률 합 = 1" 제약에서 엔트로피를 최대화하는 문제의 해입니다.

$$
\max_{\mathbf{p}} \sum_{i} p_i z_i - \sum_{i} p_i \ln p_i \quad \text{s.t.} \quad \sum_{i} p_i = 1
$$

라그랑주 함수:

$$
\mathcal{L} = \sum_i p_i z_i - \sum_i p_i \ln p_i + \lambda\left(\sum_i p_i - 1\right)
$$

$$
\frac{\partial \mathcal{L}}{\partial p_i} = z_i - \ln p_i - 1 + \lambda = 0 \implies p_i = e^{z_i + \lambda - 1}
$$

제약 $\sum p_i = 1$을 적용하면:

$$
p_i = \frac{e^{z_i}}{\sum_j e^{z_j}} = \text{softmax}(z_i)
$$

> **핵심 직관**: 소프트맥스 함수는 라그랑주 승수법의 결과입니다. "확률 합 = 1"이라는 등식 제약 하에서 최적화하면 자연스럽게 유도됩니다.

---

## 6. Python 구현

```python
import numpy as np
from scipy.optimize import minimize

# 문제: 단위 원 위에서 f(x,y) = x + 2y 최소화
# min f(x,y) = x + 2y  s.t.  x^2 + y^2 - 1 = 0

# 방법 1: 라그랑주 연립방정식 직접 풀기
from scipy.optimize import fsolve

def lagrange_system(vars):
    x, y, lam = vars
    # ∂L/∂x = 1 + 2λx = 0
    # ∂L/∂y = 2 + 2λy = 0
    # g = x^2 + y^2 - 1 = 0
    eq1 = 1 + 2*lam*x
    eq2 = 2 + 2*lam*y
    eq3 = x**2 + y**2 - 1
    return [eq1, eq2, eq3]

sol = fsolve(lagrange_system, [-0.5, -1.0, 1.0])
x_opt, y_opt, lam_opt = sol
print(f"최적점: ({x_opt:.4f}, {y_opt:.4f})")
print(f"라그랑주 승수: λ = {lam_opt:.4f}")
print(f"최소 f값: {x_opt + 2*y_opt:.4f}")

# 방법 2: scipy의 제약 최적화
result = minimize(
    lambda x: x[0] + 2*x[1],
    x0=[0.5, 0.5],
    constraints={'type': 'eq', 'fun': lambda x: x[0]**2 + x[1]**2 - 1}
)
print(f"\nscipy 결과: ({result.x[0]:.4f}, {result.x[1]:.4f})")
print(f"최소 f값: {result.fun:.4f}")
```

출력:
```
최적점: (-0.4472, -0.8944)
라그랑주 승수: λ = 1.1180
최소 f값: -2.2361

scipy 결과: (-0.4472, -0.8944)
최소 f값: -2.2361
```

---

## 7. 라그랑주 승수의 경제학적 해석

$\lambda_i$는 제약 $g_i = 0$이 **약간 완화**되었을 때 최적값이 얼마나 변하는지를 나타냅니다.

$$
\frac{df^*}{dc_i} = -\lambda_i \quad (\text{제약이 } g_i = c_i \text{로 완화될 때})
$$

이것은 경제학의 **그림자 가격(shadow price)** 개념과 동일하며, co-12(쌍대 이론)에서 더 깊이 다룹니다.

> **핵심 직관**: 라그랑주 승수의 크기는 해당 제약이 "얼마나 비싼지"를 나타냅니다. $|\lambda|$가 크면 그 제약을 완화하는 것이 목적 함수에 큰 이득을 줍니다.

---

## 핵심 정리

1. **등식 제약 최적화**에서 최적점은 $\nabla f$와 $\nabla g_i$들이 선형 종속인 점, 즉 $\nabla f = -\sum \lambda_i \nabla g_i$이다.
2. **라그랑주 함수** $\mathcal{L} = f + \sum \lambda_i g_i$의 정류점이 최적 후보이며, $n+m$ 연립방정식으로 풀 수 있다.
3. 기하학적으로, 최적점에서 **목적 함수의 등고선과 제약 곡면이 접한다** (그래디언트가 평행).
4. **라그랑주 승수** $\lambda_i$는 제약 완화에 대한 목적 함수의 민감도(**그림자 가격**)를 나타낸다.
5. 소프트맥스 함수는 $\sum p_i = 1$ 제약 하의 라그랑주 문제의 해이며, ML의 많은 모델이 제약 최적화로 유도된다.
