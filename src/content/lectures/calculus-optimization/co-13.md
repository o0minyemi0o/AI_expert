# 볼록 최적화 알고리즘

## 왜 볼록 최적화 알고리즘을 배워야 하는가

co-07에서 경사 하강법의 수렴 이론을 배웠고, co-08~co-09에서 볼록 함수의 좋은 성질을 확인했습니다. 이제 실전에서 **제약이 있는 볼록 문제**를 효율적으로 푸는 알고리즘들을 다룹니다.

투영 경사 하강법(projected gradient descent), 근위 연산자(proximal operator), 그리고 내부점 방법(interior point method)은 현대 ML 프레임워크의 내부에서 끊임없이 사용됩니다. L1 정규화의 해를 구하는 ISTA/FISTA, SVM을 푸는 내부점 방법 등이 대표적입니다.

---

## 1. 투영 경사 하강법

볼록 제약 집합 $C$ 위에서의 최적화:

$$
\min_{\mathbf{x} \in C} f(\mathbf{x})
$$

**투영 경사 하강법(Projected Gradient Descent)**:

$$
\mathbf{x}_{k+1} = \Pi_C\left(\mathbf{x}_k - \eta \nabla f(\mathbf{x}_k)\right)
$$

여기서 $\Pi_C(\mathbf{z}) = \arg\min_{\mathbf{x} \in C} \|\mathbf{x} - \mathbf{z}\|^2$는 $C$ 위로의 **투영(projection)** 입니다.

```
투영 경사 하강법의 직관:

    ╭────────╮
   │  C       │
   │          │
   │  x_k ·   │
   │    ↓ -η∇f │
   │    ·z     │ ← 그래디언트 스텝 (C 밖으로 나갈 수 있음)
   │  · x_{k+1}│ ← 투영하여 C 안으로 복귀
    ╰────────╯
```

| 제약 집합 $C$ | 투영 $\Pi_C$ | 계산 비용 |
|-------------|-------------|----------|
| 박스: $a_i \leq x_i \leq b_i$ | $\text{clip}(z_i, a_i, b_i)$ | $O(n)$ |
| 구: $\|\mathbf{x}\| \leq r$ | $r\mathbf{z}/\max(\|\mathbf{z}\|, r)$ | $O(n)$ |
| 확률 단체: $\sum x_i = 1, x_i \geq 0$ | 정렬 기반 알고리즘 | $O(n \log n)$ |
| 양반정치 행렬: $X \succeq 0$ | 음의 고유값을 0으로 | $O(n^3)$ |

> **핵심 직관**: 투영 경사 하강법은 "자유롭게 이동한 뒤 타당 영역으로 되돌아가기"입니다. 투영이 저비용이면 매우 실용적입니다.

---

## 2. 근위 연산자

미분 불가능한 함수(예: $\|\mathbf{x}\|_1$)를 포함하는 문제에 사용됩니다.

$$
\min_\mathbf{x} f(\mathbf{x}) + g(\mathbf{x})
$$

여기서 $f$는 매끄럽고, $g$는 볼록이지만 미분 불가능할 수 있습니다.

**근위 연산자(proximal operator)**:

$$
\text{prox}_{\eta g}(\mathbf{z}) = \arg\min_\mathbf{x} \left\{ g(\mathbf{x}) + \frac{1}{2\eta}\|\mathbf{x} - \mathbf{z}\|^2 \right\}
$$

**근위 경사 하강법(Proximal Gradient Descent)**:

$$
\mathbf{x}_{k+1} = \text{prox}_{\eta g}\left(\mathbf{x}_k - \eta \nabla f(\mathbf{x}_k)\right)
$$

---

## 3. ISTA와 FISTA: L1 정규화

L1 정규화(LASSO)는 근위 경사법의 대표적 응용입니다.

$$
\min_\mathbf{x} \frac{1}{2}\|A\mathbf{x} - \mathbf{b}\|^2 + \lambda\|\mathbf{x}\|_1
$$

여기서 $f(\mathbf{x}) = \frac{1}{2}\|A\mathbf{x}-\mathbf{b}\|^2$ (매끄러움), $g(\mathbf{x}) = \lambda\|\mathbf{x}\|_1$ (미분 불가).

$\|\cdot\|_1$의 근위 연산자는 **소프트 임계화(soft thresholding)**:

$$
[\text{prox}_{\eta\lambda\|\cdot\|_1}(\mathbf{z})]_i = \text{sign}(z_i) \max(|z_i| - \eta\lambda, 0)
$$

| 알고리즘 | 수렴 속도 | 가속 |
|---------|----------|------|
| ISTA | $O(1/k)$ | 없음 |
| FISTA | $O(1/k^2)$ | 모멘텀 사용 (Nesterov 가속) |

```python
import numpy as np

def soft_threshold(z, threshold):
    """소프트 임계화 (L1의 근위 연산자)"""
    return np.sign(z) * np.maximum(np.abs(z) - threshold, 0)

def ista(A, b, lam, n_iter=500, eta=None):
    """ISTA: Iterative Shrinkage-Thresholding Algorithm"""
    if eta is None:
        eta = 1.0 / np.linalg.norm(A.T @ A, 2)
    x = np.zeros(A.shape[1])
    for _ in range(n_iter):
        grad = A.T @ (A @ x - b)
        x = soft_threshold(x - eta * grad, eta * lam)
    return x

def fista(A, b, lam, n_iter=500, eta=None):
    """FISTA: Fast ISTA (Nesterov 가속)"""
    if eta is None:
        eta = 1.0 / np.linalg.norm(A.T @ A, 2)
    x = np.zeros(A.shape[1])
    y = x.copy()
    t = 1.0
    for _ in range(n_iter):
        grad = A.T @ (A @ y - b)
        x_new = soft_threshold(y - eta * grad, eta * lam)
        t_new = (1 + np.sqrt(1 + 4*t**2)) / 2
        y = x_new + (t - 1) / t_new * (x_new - x)
        x, t = x_new, t_new
    return x

# 테스트: 희소 신호 복원
np.random.seed(42)
n, d = 50, 100
A = np.random.randn(n, d)
x_true = np.zeros(d)
x_true[:5] = [3, -2, 1.5, -1, 0.5]  # 5개만 비영
b = A @ x_true + 0.1 * np.random.randn(n)

x_ista = ista(A, b, lam=0.1)
x_fista = fista(A, b, lam=0.1)

print(f"ISTA  - 비영 원소 수: {np.sum(np.abs(x_ista) > 0.01)}, "
      f"복원 오차: {np.linalg.norm(x_ista - x_true):.4f}")
print(f"FISTA - 비영 원소 수: {np.sum(np.abs(x_fista) > 0.01)}, "
      f"복원 오차: {np.linalg.norm(x_fista - x_true):.4f}")
```

출력:
```
ISTA  - 비영 원소 수: 5, 복원 오차: 0.1423
FISTA - 비영 원소 수: 5, 복원 오차: 0.1287
```

> **핵심 직관**: ISTA는 "그래디언트 스텝 → 소프트 임계화"를 반복합니다. FISTA는 Nesterov 모멘텀을 추가하여 $O(1/k) \to O(1/k^2)$로 수렴 속도를 개선합니다.

---

## 4. 내부점 방법

부등식 제약 $g_i(\mathbf{x}) \leq 0$을 **장벽 함수(barrier function)** 로 대체합니다.

$$
\min_\mathbf{x} f(\mathbf{x}) + \frac{1}{t}\sum_{i=1}^{m} \phi(g_i(\mathbf{x}))
$$

여기서 $\phi(u) = -\ln(-u)$는 **로그 장벽**이며, $u \to 0^-$일 때 $\phi \to \infty$입니다.

```
로그 장벽 함수: φ(u) = -ln(-u)

φ(u)
  │     │
  │     │  ← u=0에서 무한대
  │     │
  │    ╱
  │  ╱
  │╱
  +──────→ u (u < 0)
```

**장벽 방법의 절차**:

1. 적당한 $t > 0$으로 시작
2. 무제약 문제 $\min f + \frac{1}{t}\sum \phi(g_i)$를 뉴턴 방법으로 풀기
3. $t$를 증가시키고 (장벽 약화), 반복
4. $t \to \infty$이면 원래 문제의 해에 수렴

---

## 5. 알고리즘 비교

| 알고리즘 | 적용 문제 | 핵심 연산 | 수렴 속도 |
|---------|----------|----------|----------|
| 투영 경사 하강 | 볼록 집합 제약 | 투영 $\Pi_C$ | $O(1/k)$ |
| ISTA | L1 등 비매끄러운 정규화 | 근위 연산자 | $O(1/k)$ |
| FISTA | 위와 동일 + 가속 | 근위 + 모멘텀 | $O(1/k^2)$ |
| 내부점 | 일반 볼록 제약 | 뉴턴 스텝 | 다항 시간 |

> **핵심 직관**: 알고리즘 선택은 문제 구조에 달려 있습니다. 투영이 저비용이면 투영 경사법, 미분 불가 정규화가 있으면 근위 방법, 정밀한 해가 필요하면 내부점 방법이 적합합니다.

---

## 6. 가속의 원리: Nesterov 모멘텀

FISTA의 가속은 Nesterov의 아이디어에 기반합니다.

$$
\mathbf{y}_k = \mathbf{x}_k + \frac{k-1}{k+2}(\mathbf{x}_k - \mathbf{x}_{k-1})
$$

$$
\mathbf{x}_{k+1} = \text{prox}_{\eta g}(\mathbf{y}_k - \eta \nabla f(\mathbf{y}_k))
$$

모멘텀 항 $\frac{k-1}{k+2}(\mathbf{x}_k - \mathbf{x}_{k-1})$이 "관성"을 부여하여, 일반 경사법보다 빠르게 수렴합니다.

ISTA와 FISTA의 수렴 속도 차이는 반복이 진행될수록 뚜렷해집니다. FISTA의 $O(1/k^2)$ 수렴은 100번 반복 이후 ISTA 대비 10배 이상 정밀한 해를 제공합니다.

---

## 핵심 정리

1. **투영 경사 하강법**은 "그래디언트 스텝 → 타당 영역으로 투영"을 반복하며, 투영이 저비용인 제약에 효과적이다.
2. **근위 연산자**는 미분 불가능한 정규화($\ell_1$ 등)를 포함하는 문제를 풀기 위한 도구이며, 소프트 임계화가 대표적이다.
3. **ISTA**는 $O(1/k)$, **FISTA**는 Nesterov 가속으로 $O(1/k^2)$ 수렴을 달성한다.
4. **내부점 방법**은 로그 장벽으로 부등식 제약을 목적 함수에 통합하여, 뉴턴 방법으로 고정밀 해를 구한다.
5. 알고리즘 선택은 **문제 구조**(투영 비용, 정규화 종류, 정밀도 요구)에 따라 결정한다.
