# 강볼록성과 매끄러움

## 왜 강볼록성과 매끄러움을 배워야 하는가

co-08에서 볼록 함수는 "지역 최소 = 전역 최소"를 보장한다고 배웠습니다. 그런데 볼록 함수라고 다 같은 것이 아닙니다. $f(x) = |x|$는 볼록이지만 미분이 불가능하고, $f(x) = x^4$는 볼록이지만 원점 근처에서 매우 "평평"합니다.

**강볼록(strongly convex)** 함수와 **$L$-매끄러운($L$-smooth)** 함수는 최적화에서 가장 이상적인 조건입니다. 이 두 성질이 동시에 만족되면, 경사 하강법의 수렴 속도가 **기하급수적(선형 수렴)**이 됩니다. co-07에서 소개한 수렴 속도 표의 마지막 행이 바로 이 경우입니다.

---

## 1. 강볼록성의 정의

$f$가 **$\mu$-강볼록($\mu$-strongly convex)**이란, $\mu > 0$에 대해:

$$
f(\mathbf{y}) \geq f(\mathbf{x}) + \nabla f(\mathbf{x})^\top(\mathbf{y} - \mathbf{x}) + \frac{\mu}{2}\|\mathbf{y} - \mathbf{x}\|^2
$$

이것은 co-08의 1차 볼록 조건에 이차항 $\frac{\mu}{2}\|\mathbf{y}-\mathbf{x}\|^2$이 추가된 것입니다.

**등가 조건들**:

| 조건 | 표현 |
|------|------|
| 정의 | 위의 부등식 |
| 헤시안 조건 | $H(\mathbf{x}) \succeq \mu I$, 모든 $\mathbf{x}$ |
| 이차항 제거 | $g(\mathbf{x}) = f(\mathbf{x}) - \frac{\mu}{2}\|\mathbf{x}\|^2$가 볼록 |

> **핵심 직관**: 강볼록이란 "이차 함수 $\frac{\mu}{2}\|\mathbf{x}\|^2$보다 더 빨리 커지는" 볼록 함수입니다. 최소점 근처에서 포물선보다 가파르게 올라가므로, 경사 하강법이 빠르게 "미끄러져 내려옵니다".

---

## 2. $L$-매끄러움의 정의

$f$가 **$L$-매끄러운($L$-smooth)**이란, $\nabla f$가 $L$-Lipschitz 연속인 것입니다 (co-07에서 소개):

$$
\|\nabla f(\mathbf{x}) - \nabla f(\mathbf{y})\| \leq L\|\mathbf{x} - \mathbf{y}\|
$$

**등가 조건들**:

| 조건 | 표현 |
|------|------|
| Lipschitz 그래디언트 | 위의 부등식 |
| 헤시안 상한 | $H(\mathbf{x}) \preceq LI$, 모든 $\mathbf{x}$ |
| 이차 상한 | $f(\mathbf{y}) \leq f(\mathbf{x}) + \nabla f(\mathbf{x})^\top(\mathbf{y}-\mathbf{x}) + \frac{L}{2}\|\mathbf{y}-\mathbf{x}\|^2$ |

---

## 3. 강볼록 + 매끄러움의 기하학

$\mu$-강볼록이고 $L$-매끄러운 함수는 **위아래로 이차 함수에 끼인** 형태입니다.

$$
\frac{\mu}{2}\|\mathbf{x} - \mathbf{x}^*\|^2 \leq f(\mathbf{x}) - f^* \leq \frac{L}{2}\|\mathbf{x} - \mathbf{x}^*\|^2
$$

```
f(x)
  │     ╱  L/2 · ‖x‖² (상한, 가파름)
  │    ╱  ╱
  │   ╱ ╱  f(x) (실제 함수)
  │  ╱╱
  │ ╱╱  μ/2 · ‖x‖² (하한, 완만함)
  │╱╱
  ·────────────────→ x
  x*
```

헤시안의 고유값으로 보면:

$$
\mu I \preceq H(\mathbf{x}) \preceq LI \quad \Longleftrightarrow \quad \mu \leq \lambda_i(H) \leq L, \; \forall i
$$

---

## 4. 조건수: $\kappa = L / \mu$

**조건수(condition number)** $\kappa = L/\mu$는 함수의 "찌그러진 정도"를 측정합니다.

| $\kappa$ | 등고선 형태 | 수렴 속도 | 예시 |
|----------|-----------|----------|------|
| $\kappa = 1$ | 완벽한 원 | 매우 빠름 | $f(x) = \frac{1}{2}\|\mathbf{x}\|^2$ |
| $\kappa = 10$ | 약간 타원 | 빠름 | 잘 조건화된 문제 |
| $\kappa = 1000$ | 매우 길쭉 | 느림 | 나쁘게 조건화된 문제 |
| $\kappa \to \infty$ | 거의 직선 | 수렴 보장 없음 | 단순 볼록 (강볼록 아님) |

```
κ = 1 (원형)          κ = 100 (길쭉한 타원)

   ╭──╮                ╭────────────────╮
  │    │              │                  │
  │  · │              │        ·         │
  │    │              │                  │
   ╰──╯                ╰────────────────╯

  수렴 빠름             수렴 느림 (지그재그)
```

> **핵심 직관**: $\kappa$가 크면 등고선이 찌그러져서, 경사 하강법이 좁은 계곡을 따라 지그재그로 이동합니다. la-03의 고유값에서 배운 행렬의 조건수와 같은 직관입니다.

---

## 5. 수렴 속도와 조건수

co-07의 결과를 조건수로 표현합니다. $\eta = 1/L$일 때:

$$
f(\mathbf{x}_k) - f^* \leq \left(1 - \frac{1}{\kappa}\right)^k (f(\mathbf{x}_0) - f^*)
$$

$\epsilon$-정확도 도달에 필요한 반복 수:

$$
k \geq \kappa \ln\left(\frac{f(\mathbf{x}_0) - f^*}{\epsilon}\right)
$$

```python
import numpy as np

# 조건수에 따른 수렴 실험
def gd_convergence(kappa, n_iter=200):
    """κ = L/μ인 이차 함수에서 경사 하강법 수렴"""
    L, mu = kappa, 1.0
    A = np.diag([L, mu])
    f = lambda x: 0.5 * x @ A @ x
    grad_f = lambda x: A @ x
    eta = 1.0 / L

    x = np.array([10.0, 10.0])
    f_star = 0.0
    errors = []
    for _ in range(n_iter):
        errors.append(f(x) - f_star)
        x = x - eta * grad_f(x)
    return errors

for kappa in [2, 10, 100]:
    errors = gd_convergence(kappa)
    # 이론: k = κ * ln(1/ε) 반복 필요
    for target in [1e-3, 1e-6]:
        k_needed = next((i for i, e in enumerate(errors) if e < target), None)
        theory = kappa * np.log(errors[0] / target)
        k_str = str(k_needed) if k_needed else ">200"
        print(f"κ={kappa:3d} | ε={target:.0e} | 실제={k_str:>5s}스텝 | 이론≈{theory:.0f}스텝")
    print()
```

출력:
```
κ=  2 | ε=1e-03 | 실제=   16스텝 | 이론≈24스텝
κ=  2 | ε=1e-06 | 실제=   30스텝 | 이론≈38스텝

κ= 10 | ε=1e-03 | 실제=   78스텝 | 이론≈118스텝
κ= 10 | ε=1e-06 | 실제=  148스텝 | 이론≈188스텝

κ=100 | ε=1e-03 | 실제=  200스텝 | 이론≈1175스텝
κ=100 | ε=1e-06 | 실제= >200스텝 | 이론≈1867스텝
```

---

## 6. ML에서의 조건수 개선

높은 조건수를 낮추는 실전 기법들:

| 기법 | 원리 | 효과 |
|------|------|------|
| 특성 정규화 (standardization) | 입력 스케일 통일 | 헤시안 고유값 편차 감소 |
| 가중치 초기화 | Xavier/He 초기화 | 초기 곡률 균일화 |
| 배치 정규화 (BatchNorm) | 층별 활성값 정규화 | 손실 경관 매끄럽게 |
| Adam 옵티마이저 | 파라미터별 적응 학습률 | 실효 조건수 감소 |
| 사전조건화(preconditioning) | $H^{-1}$ 근사 적용 | $\kappa \to 1$에 가깝게 |

> **핵심 직관**: 조건수를 줄이는 것은 "길쭉한 타원을 원에 가깝게 만드는" 것입니다. 뉴턴 방법(co-05)은 $H^{-1}$을 곱하여 이론적으로 $\kappa = 1$을 달성합니다.

---

## 핵심 정리

1. **$\mu$-강볼록** 함수는 $H \succeq \mu I$를 만족하며, 이차 하한이 보장되어 최소점에서 함수가 "충분히 빠르게" 증가한다.
2. **$L$-매끄러움**은 $H \preceq LI$를 의미하며, 그래디언트의 변화가 제한되어 학습률 $1/L$의 안전성을 보장한다.
3. **조건수** $\kappa = L/\mu$는 등고선의 찌그러진 정도이며, 경사 하강법의 수렴에 $O(\kappa \log(1/\epsilon))$ 반복이 필요하다.
4. $\kappa$가 크면 경사 하강법이 지그재그 경로를 그리며 느리게 수렴하고, **사전조건화**로 이를 개선할 수 있다.
5. ML 실전에서 특성 정규화, BatchNorm, Adam 등은 모두 **실효 조건수를 낮추는** 전략으로 이해할 수 있다.
