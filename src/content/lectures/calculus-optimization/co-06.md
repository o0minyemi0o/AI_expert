# 무제약 최적화

## 왜 무제약 최적화를 배워야 하는가

머신러닝에서 가장 흔한 문제는 "손실 함수를 최소화하라"입니다. 파라미터에 특별한 제약이 없을 때, 이것은 **무제약 최적화(unconstrained optimization)** 문제입니다.

$$
\min_{\mathbf{x} \in \mathbb{R}^n} f(\mathbf{x})
$$

co-02에서 그래디언트가 0인 점(정류점)을 찾는 것이 첫 단계임을 배웠고, co-03에서 헤시안의 고유값으로 극값을 판별하는 법을 배웠습니다. 이번 강의에서는 이를 **최적성 조건(optimality conditions)** 이라는 통일된 프레임워크로 정리합니다.

---

## 1. 극값의 정의

함수 $f : \mathbb{R}^n \to \mathbb{R}$에 대해:

- **전역 최소점(global minimum)**: 모든 $\mathbf{x}$에 대해 $f(\mathbf{x}^*) \leq f(\mathbf{x})$
- **지역 최소점(local minimum)**: $\mathbf{x}^*$의 어떤 근방 $\mathcal{N}$ 내에서 $f(\mathbf{x}^*) \leq f(\mathbf{x})$
- **엄격한 지역 최소점(strict local minimum)**: $\mathbf{x} \neq \mathbf{x}^*$이면 $f(\mathbf{x}^*) < f(\mathbf{x})$

```
함수 경관의 예시:

f(x)
  │   ╭╮                ╭╮
  │  ╱  ╲  지역 최소   ╱  ╲
  │ ╱    ╲   ↓    ╱╲  ╱    ╲
  │╱      ╰─·──╯╱  ╲╱      ╲
  │              안장점      전역 최소
  │                          ↓
  │                         ·
  +──────────────────────────→ x
```

> **핵심 직관**: 볼록 함수가 아닌 이상, 지역 최소점은 전역 최소점과 다를 수 있습니다. 신경망의 손실 함수는 비볼록이므로, 이 구분이 중요합니다. co-08에서 볼록 함수의 "지역=전역" 성질을 다룹니다.

---

## 2. 1차 필요조건 (FONC)

**정리**: $\mathbf{x}^*$가 지역 최소점이고 $f$가 미분 가능하면:

$$
\nabla f(\mathbf{x}^*) = \mathbf{0}
$$

**증명 스케치**: 임의의 방향 $\mathbf{d}$에 대해, $g(t) = f(\mathbf{x}^* + t\mathbf{d})$는 $t=0$에서 지역 최소입니다. 따라서 $g'(0) = \nabla f(\mathbf{x}^*)^\top \mathbf{d} = 0$. 이것이 모든 $\mathbf{d}$에 성립하므로 $\nabla f(\mathbf{x}^*) = \mathbf{0}$.

이 조건을 만족하는 점을 **정류점(stationary point)** 이라 하며, 세 가지로 분류됩니다:

| 정류점 종류 | 특징 | ML에서의 의미 |
|-----------|------|-------------|
| 지역 최소점 | 주변보다 함수값이 작음 | 학습 목표 |
| 지역 최대점 | 주변보다 함수값이 큼 | 최소화에서는 관심 없음 |
| 안장점 | 어떤 방향은 증가, 어떤 방향은 감소 | 고차원에서 매우 빈번 |

---

## 3. 2차 필요조건 (SONC)

**정리**: $\mathbf{x}^*$가 지역 최소점이면:

$$
\nabla f(\mathbf{x}^*) = \mathbf{0} \quad \text{그리고} \quad H(\mathbf{x}^*) \succeq 0 \; (\text{양반정치})
$$

여기서 $H \succeq 0$는 모든 고유값이 $\geq 0$이라는 뜻입니다.

---

## 4. 2차 충분조건 (SOSC)

**정리**: 다음 조건을 만족하면 $\mathbf{x}^*$는 **엄격한 지역 최소점**입니다:

$$
\nabla f(\mathbf{x}^*) = \mathbf{0} \quad \text{그리고} \quad H(\mathbf{x}^*) \succ 0 \; (\text{양정치})
$$

필요조건과 충분조건의 관계를 정리합니다:

```
                ┌─────────────────────────┐
                │ FONC: ∇f = 0            │ ← 필요조건 (정류점)
                │   ┌───────────────────┐ │
                │   │ SONC: H ≥ 0       │ │ ← 필요조건 (반정치)
                │   │   ┌─────────────┐ │ │
                │   │   │ SOSC: H > 0 │ │ │ ← 충분조건 (양정치)
                │   │   └─────────────┘ │ │
                │   └───────────────────┘ │
                └─────────────────────────┘
```

> **핵심 직관**: 1차 조건($\nabla f = 0$)은 "평평한 곳"을 찾는 것이고, 2차 조건($H \succ 0$)은 "그 평평한 곳이 밑으로 오목한지" 확인하는 것입니다.

---

## 5. 안장점의 판별

고차원에서 안장점은 극소점보다 **훨씬 흔합니다**. $n$차원에서 헤시안의 $n$개 고유값이 모두 양수일 확률은 (대략) $2^{-n}$이므로, 차원이 높아질수록 안장점의 비율이 기하급수적으로 증가합니다.

$$
H \text{ 부정치} \iff \text{양수 고유값과 음수 고유값이 모두 존재} \iff \text{안장점}
$$

```python
import numpy as np

# 예제: 원숭이 안장 f(x,y) = x^2 - y^2
def f(x, y):
    return x**2 - y**2

def grad_f(x, y):
    return np.array([2*x, -2*y])

def hessian_f(x, y):
    return np.array([[2, 0], [0, -2]])

# (0, 0) 분석
pt = (0, 0)
g = grad_f(*pt)
H = hessian_f(*pt)
eigvals = np.linalg.eigvalsh(H)

print(f"정류점: {pt}")
print(f"그래디언트: {g}")
print(f"헤시안 고유값: {eigvals}")
print(f"판별: 양수({eigvals[1]:.0f})와 음수({eigvals[0]:.0f}) → 안장점")
```

출력:
```
정류점: (0, 0)
그래디언트: [0 0]
헤시안 고유값: [-2.  2.]
판별: 양수(2)와 음수(-2) → 안장점
```

---

## 6. 종합 판별 알고리즘

```python
import numpy as np

def classify_critical_point(grad, hessian, tol=1e-8):
    """정류점을 분류하는 알고리즘"""
    # 1차 확인: 그래디언트가 0인가?
    if np.linalg.norm(grad) > tol:
        return "정류점 아님"

    # 2차 확인: 헤시안의 고유값
    eigvals = np.linalg.eigvalsh(hessian)
    min_eig = eigvals[0]
    max_eig = eigvals[-1]

    if min_eig > tol:
        return "엄격한 지역 최소점 (H 양정치)"
    elif max_eig < -tol:
        return "엄격한 지역 최대점 (H 음정치)"
    elif min_eig < -tol and max_eig > tol:
        return "안장점 (H 부정치)"
    else:
        return "판별 불가 (고차 정보 필요)"

# 테스트: f(x,y) = x^4 + y^4 - 2x^2 - 2y^2
# 정류점: (0,0), (1,1), (1,-1), (-1,1), (-1,-1)
test_cases = {
    (0, 0):   (np.array([0, 0]),   np.array([[-4, 0], [0, -4]])),
    (1, 1):   (np.array([0, 0]),   np.array([[8, 0], [0, 8]])),
    (1, 0):   (np.array([0, 0]),   np.array([[8, 0], [0, -4]])),
}

for pt, (g, H) in test_cases.items():
    result = classify_critical_point(g, H)
    print(f"점 {pt}: {result}")
```

출력:
```
점 (0, 0): 엄격한 지역 최대점 (H 음정치)
점 (1, 1): 엄격한 지역 최소점 (H 양정치)
점 (1, 0): 안장점 (H 부정치)
```

> **핵심 직관**: 고차원 신경망에서 그래디언트가 0에 가깝다면, 그것이 진짜 최소점인지 안장점인지 구분하는 것이 중요합니다. co-14에서 비볼록 최적화의 안장점 탈출 전략을 다룹니다.

---

## 핵심 정리

1. **1차 필요조건(FONC)**: 지역 최소점이면 반드시 $\nabla f = \mathbf{0}$이다. 이것은 정류점을 찾는 조건이다.
2. **2차 충분조건(SOSC)**: $\nabla f = \mathbf{0}$이고 $H \succ 0$(양정치)이면, 해당 점은 엄격한 지역 최소점이다.
3. **안장점**은 헤시안이 부정치(양수·음수 고유값 혼재)인 정류점이며, 고차원에서 극소점보다 훨씬 빈번하다.
4. 필요조건과 충분조건 사이의 **갭**($H$가 양반정치인 경우)은 고차 미분 정보로만 해결할 수 있다.
5. 최적성 조건은 co-10(라그랑주 승수법)과 co-11(KKT 조건)에서 **제약 조건이 있는 경우**로 확장된다.
