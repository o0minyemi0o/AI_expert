# 다변수 체인 룰

## 왜 다변수 체인 룰을 배워야 하는가

신경망의 역전파(backpropagation)는 수학적으로 **체인 룰(chain rule)의 반복 적용**에 불과합니다. la-05에서 행렬 버전의 체인 룰을 소개했지만, 그 기초가 되는 다변수 체인 룰 자체를 엄밀하게 다루지는 않았습니다.

이번 강의에서는 합성 함수의 미분을 단변수 → 다변수 → 계산 그래프 순서로 확장합니다. 이를 통해 역전파가 **왜** 체인 룰의 자동화인지를 명확히 이해할 수 있습니다.

---

## 1. 단변수 체인 룰 복습

$y = f(g(x))$이면:

$$
\frac{dy}{dx} = \frac{dy}{dg} \cdot \frac{dg}{dx} = f'(g(x)) \cdot g'(x)
$$

이것은 "바깥 함수의 미분 $\times$ 안쪽 함수의 미분"이라는 직관입니다.

---

## 2. 다변수 체인 룰: 편미분 버전

$z = f(x, y)$이고 $x = x(t)$, $y = y(t)$일 때:

$$
\frac{dz}{dt} = \frac{\partial f}{\partial x}\frac{dx}{dt} + \frac{\partial f}{\partial y}\frac{dy}{dt}
$$

더 일반적으로, $f : \mathbb{R}^n \to \mathbb{R}$이고 $\mathbf{x} = \mathbf{x}(t) : \mathbb{R} \to \mathbb{R}^n$이면:

$$
\frac{df}{dt} = \sum_{i=1}^{n} \frac{\partial f}{\partial x_i} \frac{dx_i}{dt} = \nabla f^\top \frac{d\mathbf{x}}{dt}
$$

> **핵심 직관**: 다변수 체인 룰은 "각 경로를 통한 변화율을 모두 합산"하는 것입니다. 한 변수가 여러 경로로 출력에 영향을 미치면, 모든 경로의 기여를 더해야 합니다.

---

## 3. 벡터-벡터 합성: 야코비안의 곱

$\mathbf{f} : \mathbb{R}^n \to \mathbb{R}^m$, $\mathbf{g} : \mathbb{R}^p \to \mathbb{R}^n$의 합성 $\mathbf{h} = \mathbf{f} \circ \mathbf{g}$에 대해:

$$
J_\mathbf{h}(\mathbf{x}) = J_\mathbf{f}(\mathbf{g}(\mathbf{x})) \cdot J_\mathbf{g}(\mathbf{x})
$$

이것이 la-05에서 언급했던 **야코비안의 곱 법칙**입니다.

| 합성 단계 | 야코비안 크기 | 결과 |
|----------|-------------|------|
| $\mathbf{g} : \mathbb{R}^p \to \mathbb{R}^n$ | $n \times p$ | |
| $\mathbf{f} : \mathbb{R}^n \to \mathbb{R}^m$ | $m \times n$ | |
| $\mathbf{f} \circ \mathbf{g} : \mathbb{R}^p \to \mathbb{R}^m$ | $(m \times n)(n \times p) = m \times p$ | 크기가 맞음 |

---

## 4. 계산 그래프에서의 체인 룰

신경망은 계산 그래프(computational graph)로 표현됩니다. 각 노드는 연산이고, 간선은 데이터 흐름입니다.

```
순전파 (Forward Pass):

  x ──→ [×w] ──→ z ──→ [σ] ──→ a ──→ [L] ──→ loss
          ↑
          w

역전파 (Backward Pass):

  ∂L/∂x ←── [×] ←── ∂L/∂z ←── [×] ←── ∂L/∂a ←── [×] ←── ∂L/∂loss = 1
              ↑                  ↑                   ↑
           ∂z/∂x=w           ∂a/∂z=σ'           ∂loss/∂a
```

역전파의 핵심 규칙은 단순합니다:

$$
\frac{\partial L}{\partial x_i} = \sum_{j \in \text{children}(i)} \frac{\partial L}{\partial x_j} \cdot \frac{\partial x_j}{\partial x_i}
$$

> **핵심 직관**: 역전파는 출력에서 입력 방향으로 **국소 미분값을 곱하며 전달**하는 과정입니다. 각 노드는 자기 연산의 미분만 알면 되고, 전체 체인은 자동으로 완성됩니다.

---

## 5. 구체적 예제: 2층 신경망

$\mathbf{h} = \sigma(W_1 \mathbf{x} + \mathbf{b}_1)$, $\hat{y} = W_2 \mathbf{h} + \mathbf{b}_2$, $L = \frac{1}{2}\|\hat{y} - y\|^2$

역전파 체인:

$$
\frac{\partial L}{\partial W_1} = \frac{\partial L}{\partial \hat{y}} \cdot \frac{\partial \hat{y}}{\partial \mathbf{h}} \cdot \frac{\partial \mathbf{h}}{\partial (W_1\mathbf{x}+\mathbf{b}_1)} \cdot \frac{\partial (W_1\mathbf{x}+\mathbf{b}_1)}{\partial W_1}
$$

각 항을 풀면:
- $\frac{\partial L}{\partial \hat{y}} = \hat{y} - y$
- $\frac{\partial \hat{y}}{\partial \mathbf{h}} = W_2$
- $\frac{\partial \mathbf{h}}{\partial (W_1\mathbf{x}+\mathbf{b}_1)} = \text{diag}(\sigma'(\cdot))$
- $\frac{\partial (W_1\mathbf{x}+\mathbf{b}_1)}{\partial W_1}$: 외적 형태

---

## 6. Python 구현: 자동 미분의 핵심

```python
import numpy as np

class ComputeNode:
    """간단한 계산 그래프 노드"""
    def __init__(self, value, children=(), op=''):
        self.value = value
        self.grad = 0.0
        self._children = children
        self._backward = lambda: None

def multiply(a, b):
    """곱셈 노드: z = a * b"""
    out = ComputeNode(a.value * b.value, (a, b), '*')
    def _backward():
        a.grad += b.value * out.grad   # ∂L/∂a = b * ∂L/∂z
        b.grad += a.value * out.grad   # ∂L/∂b = a * ∂L/∂z
    out._backward = _backward
    return out

def add(a, b):
    """덧셈 노드: z = a + b"""
    out = ComputeNode(a.value + b.value, (a, b), '+')
    def _backward():
        a.grad += 1.0 * out.grad
        b.grad += 1.0 * out.grad
    out._backward = _backward
    return out

# 예제: f(x, w) = (x * w + b)^2
x = ComputeNode(2.0)
w = ComputeNode(3.0)
b = ComputeNode(1.0)

# 순전파
xw = multiply(x, w)       # xw = 6.0
z = add(xw, b)            # z = 7.0
loss = multiply(z, z)     # loss = 49.0

# 역전파
loss.grad = 1.0
loss._backward()           # z.grad = 2 * z = 14.0
z._backward()              # xw.grad = 14.0, b.grad = 14.0
xw._backward()             # x.grad = w * 14 = 42, w.grad = x * 14 = 28

print(f"∂L/∂x = {x.grad} (해석해: 2*w*(x*w+b) = {2*3*(2*3+1)})")
print(f"∂L/∂w = {w.grad} (해석해: 2*x*(x*w+b) = {2*2*(2*3+1)})")
print(f"∂L/∂b = {b.grad} (해석해: 2*(x*w+b) = {2*(2*3+1)})")
```

출력:
```
∂L/∂x = 42.0 (해석해: 2*w*(x*w+b) = 42)
∂L/∂w = 28.0 (해석해: 2*x*(x*w+b) = 28)
∂L/∂b = 14.0 (해석해: 2*(x*w+b) = 14)
```

---

## 7. 경로 합산 규칙

한 변수가 여러 경로를 통해 출력에 영향을 미칠 때, 모든 경로의 기여를 더합니다.

```
        ┌──→ f ──┐
  x ──→ ┤        ├──→ L
        └──→ g ──┘

  ∂L/∂x = (∂L/∂f)(∂f/∂x) + (∂L/∂g)(∂g/∂x)
```

이것이 코드에서 `a.grad += ...`로 **누적**하는 이유입니다. `=`가 아니라 `+=`인 것이 핵심입니다.

> **핵심 직관**: 변수가 여러 경로로 출력에 기여하면, 각 경로의 미분을 합산합니다. 이것이 역전파에서 그래디언트를 **누적(accumulate)** 하는 이유입니다.

---

## 핵심 정리

1. **다변수 체인 룰**은 합성 함수의 미분을 편미분의 합으로 표현하며, 핵심은 "모든 경로의 기여를 합산"하는 것이다.
2. 벡터-벡터 합성의 체인 룰은 **야코비안의 행렬 곱**으로 표현되며, 이것이 la-05의 행렬 체인 룰이다.
3. **계산 그래프**에서 역전파는 출력 → 입력 방향으로 국소 미분을 곱하며 전달하는 과정이다.
4. 한 변수가 여러 경로로 출력에 기여하면 그래디언트를 **누적(+=)** 해야 하며, 이것이 경로 합산 규칙이다.
5. 자동 미분(autograd)의 본질은 체인 룰의 기계적 적용이며, 각 노드는 자기 연산의 국소 미분만 알면 된다.
