# 테일러 근사와 이차 근사

## 왜 테일러 근사를 배워야 하는가

최적화 알고리즘은 본질적으로 복잡한 함수를 **단순한 함수로 근사**한 뒤 그 근사를 최적화하는 과정입니다. 경사 하강법은 함수를 **1차(선형)** 로 근사하고, 뉴턴 방법은 **2차(이차)** 로 근사합니다. 이 근사의 수학적 기반이 바로 테일러 전개(Taylor expansion)입니다.

co-02에서 배운 그래디언트와 co-03에서 배운 헤시안이 테일러 전개의 1차항과 2차항에 정확히 대응합니다. 이번 강의에서는 이 연결을 명확히 하고, 뉴턴 방법의 수학적 기초를 세웁니다.

---

## 1. 단변수 테일러 전개 복습

함수 $f : \mathbb{R} \to \mathbb{R}$가 $a$에서 충분히 미분 가능할 때:

$$
f(a + h) = f(a) + f'(a)h + \frac{1}{2}f''(a)h^2 + \frac{1}{6}f'''(a)h^3 + \cdots
$$

$k$차 근사는 처음 $k+1$개 항만 취한 것입니다.

| 근사 차수 | 사용하는 미분 정보 | 근사 형태 | 오차 |
|----------|-----------------|----------|------|
| 0차 | $f(a)$ | 상수 | $O(h)$ |
| 1차 | $f(a), f'(a)$ | 접선 | $O(h^2)$ |
| 2차 | $f(a), f'(a), f''(a)$ | 포물선 | $O(h^3)$ |

---

## 2. 다변수 테일러 전개

$f : \mathbb{R}^n \to \mathbb{R}$의 점 $\mathbf{a}$에서의 테일러 전개:

$$
f(\mathbf{a} + \mathbf{h}) = f(\mathbf{a}) + \nabla f(\mathbf{a})^\top \mathbf{h} + \frac{1}{2} \mathbf{h}^\top H(\mathbf{a}) \mathbf{h} + O(\|\mathbf{h}\|^3)
$$

각 항의 역할을 정리합니다.

| 항 | 수학적 표현 | 기하학적 의미 | 최적화에서의 역할 |
|----|-----------|-------------|----------------|
| 0차 | $f(\mathbf{a})$ | 현재 함수값 | 기준점 |
| 1차 | $\nabla f^\top \mathbf{h}$ | 접평면 | 경사 하강 방향 |
| 2차 | $\frac{1}{2}\mathbf{h}^\top H \mathbf{h}$ | 곡률 보정 | 뉴턴 방법의 기초 |

> **핵심 직관**: 1차 근사는 "현재 위치에서 함수가 어느 방향으로 기울어져 있는가"만 알려줍니다. 2차 근사는 추가로 "얼마나 빨리 휘어지는가"까지 알려주어, 더 정확한 발걸음을 내딛게 합니다.

---

## 3. 이차 근사와 최소점

정류점 근처에서 $\nabla f(\mathbf{a}) = \mathbf{0}$이면, 이차 근사는:

$$
f(\mathbf{a} + \mathbf{h}) \approx f(\mathbf{a}) + \frac{1}{2} \mathbf{h}^\top H \mathbf{h}
$$

이 이차 형식의 최소점은 co-03에서 배운 양정치 조건과 직결됩니다.

일반적인 점에서, 이차 근사 $q(\mathbf{h})$를 최소화하면:

$$
q(\mathbf{h}) = f(\mathbf{a}) + \nabla f(\mathbf{a})^\top \mathbf{h} + \frac{1}{2}\mathbf{h}^\top H(\mathbf{a}) \mathbf{h}
$$

$$
\nabla_\mathbf{h} q = \nabla f(\mathbf{a}) + H(\mathbf{a})\mathbf{h} = \mathbf{0}
$$

$$
\mathbf{h}^* = -H(\mathbf{a})^{-1} \nabla f(\mathbf{a})
$$

이것이 바로 **뉴턴 스텝(Newton step)**입니다.

---

## 4. 뉴턴 방법

뉴턴 방법의 업데이트 규칙:

$$
\mathbf{x}_{k+1} = \mathbf{x}_k - H(\mathbf{x}_k)^{-1} \nabla f(\mathbf{x}_k)
$$

경사 하강법과 비교:

| | 경사 하강법 | 뉴턴 방법 |
|--|-----------|----------|
| 업데이트 | $-\eta \nabla f$ | $-H^{-1} \nabla f$ |
| 사용 정보 | 1차 (그래디언트) | 1차 + 2차 (그래디언트 + 헤시안) |
| 학습률 | 수동 설정 ($\eta$) | 곡률이 자동 결정 |
| 수렴 속도 | 선형 ~ 초선형 | **이차 수렴** (최적점 근처) |
| 계산 비용 | $O(n)$ | $O(n^3)$ (헤시안 역행렬) |

> **핵심 직관**: 경사 하강법은 "어디로 갈지"만 알고 "얼마나 갈지"는 학습률에 맡깁니다. 뉴턴 방법은 곡률까지 활용하여 "어디로 얼마나 갈지"를 동시에 결정합니다.

---

## 5. 이차 함수에서의 뉴턴 방법

$f(\mathbf{x}) = \frac{1}{2}\mathbf{x}^\top A\mathbf{x} - \mathbf{b}^\top \mathbf{x}$처럼 정확히 이차인 함수에서 뉴턴 방법은 **한 번에** 최적점을 찾습니다.

$$
\nabla f = A\mathbf{x} - \mathbf{b}, \quad H = A
$$

$$
\mathbf{x}_1 = \mathbf{x}_0 - A^{-1}(A\mathbf{x}_0 - \mathbf{b}) = A^{-1}\mathbf{b}
$$

```python
import numpy as np

# 이차 함수: f(x) = 0.5 * x^T A x - b^T x
A = np.array([[4, 1], [1, 3]])
b = np.array([1, 2])

# 뉴턴 방법 (1스텝이면 충분)
x0 = np.array([0.0, 0.0])
grad = A @ x0 - b
H = A
x1 = x0 - np.linalg.solve(H, grad)
print(f"1스텝 후: x = {x1}")
print(f"정확해:   x = {np.linalg.solve(A, b)}")
```

출력:
```
1스텝 후: x = [0.09090909 0.63636364]
정확해:   x = [0.09090909 0.63636364]
```

---

## 6. 비이차 함수에서의 뉴턴 방법

```python
import numpy as np

# Rosenbrock 함수: f(x,y) = (1-x)^2 + 100(y-x^2)^2
def rosenbrock(x):
    return (1 - x[0])**2 + 100*(x[1] - x[0]**2)**2

def grad_rosenbrock(x):
    dx = -2*(1 - x[0]) - 400*x[0]*(x[1] - x[0]**2)
    dy = 200*(x[1] - x[0]**2)
    return np.array([dx, dy])

def hessian_rosenbrock(x):
    dxx = 2 - 400*(x[1] - x[0]**2) + 800*x[0]**2
    dxy = -400*x[0]
    dyy = 200
    return np.array([[dxx, dxy], [dxy, dyy]])

# 뉴턴 방법
x = np.array([-1.0, 1.0])
for i in range(20):
    g = grad_rosenbrock(x)
    H = hessian_rosenbrock(x)
    step = np.linalg.solve(H, g)
    x = x - step
    if np.linalg.norm(g) < 1e-10:
        print(f"수렴! 반복 {i+1}회, x = ({x[0]:.8f}, {x[1]:.8f})")
        break

print(f"f(x) = {rosenbrock(x):.2e}")
```

출력:
```
수렴! 반복 8회, x = (1.00000000, 1.00000000)
f(x) = 2.07e-25
```

경사 하강법으로는 수천 번 이상 반복해야 하는 Rosenbrock 함수를 뉴턴 방법은 8번만에 수렴시킵니다.

---

## 7. 테일러 근사의 한계

| 상황 | 문제점 | 해결책 |
|------|-------|-------|
| 최적점에서 멀리 떨어진 경우 | 이차 근사가 부정확 | 신뢰 영역(trust region) 방법 |
| 헤시안이 비양정치 | 뉴턴 스텝이 상승 방향 | 수정 뉴턴 방법 (양정치 보정) |
| $n$이 매우 클 때 | $O(n^3)$ 계산 비용 | 준뉴턴 방법 (BFGS, L-BFGS) |

> **핵심 직관**: 테일러 근사는 "국소적"으로만 정확합니다. 멀리 갈수록 근사 오차가 커지므로, 실전에서는 스텝 크기를 제한하거나 근사를 저비용으로 만드는 전략이 필요합니다. co-13에서 이를 자세히 다룹니다.

---

## 핵심 정리

1. **다변수 테일러 전개**는 $f(\mathbf{a}+\mathbf{h}) \approx f(\mathbf{a}) + \nabla f^\top \mathbf{h} + \frac{1}{2}\mathbf{h}^\top H\mathbf{h}$이며, 그래디언트와 헤시안의 역할을 명확히 보여준다.
2. **1차 근사**는 접평면(선형)이고 경사 하강법의 기초이며, **2차 근사**는 곡률 정보를 포함하여 뉴턴 방법의 기초이다.
3. **뉴턴 스텝** $\mathbf{h}^* = -H^{-1}\nabla f$는 이차 근사의 최소점이며, 최적점 근처에서 이차 수렴한다.
4. 이차 함수에서 뉴턴 방법은 **정확히 1스텝**에 수렴하며, 비이차 함수에서도 매우 빠른 수렴을 보인다.
5. 헤시안의 $O(n^3)$ 비용과 비양정치 문제 때문에, 실전에서는 **준뉴턴 방법**(L-BFGS 등)이 널리 사용된다.
