# ML에서의 최적화 응용

## 왜 이 강의가 필요한가

지금까지 co-01부터 co-14까지, 미적분학과 최적화의 핵심 이론을 쌓아 왔습니다. 편미분(co-01)에서 출발하여, 그래디언트(co-02), 헤시안(co-03), 체인 룰(co-04), 테일러 근사(co-05), 최적성 조건(co-06), 수렴 이론(co-07), 볼록성(co-08~09), 제약 최적화(co-10~12), 알고리즘(co-13), 비볼록 최적화(co-14)까지.

이번 마지막 강의에서는 이 모든 이론이 **실제 ML 문제**에 어떻게 적용되는지 종합합니다. SVM, 로지스틱 회귀, 신경망이라는 세 가지 대표 모델을 통해, 지금까지 배운 내용의 총정리를 수행합니다.

---

## 1. SVM: 쌍대 이론의 완벽한 응용

### 1.1 원시 문제

하드 마진 SVM의 원시 문제:

$$
\min_{\mathbf{w}, b} \frac{1}{2}\|\mathbf{w}\|^2 \quad \text{s.t.} \quad y_i(\mathbf{w}^\top \mathbf{x}_i + b) \geq 1, \; i = 1, \dots, N
$$

- 목적 함수: 이차(co-05) + 볼록(co-08)
- 제약: 선형 부등식
- 이것은 **이차 계획(QP)** 문제입니다

### 1.2 라그랑주 함수와 KKT

co-10의 라그랑주 함수:

$$
\mathcal{L}(\mathbf{w}, b, \boldsymbol{\alpha}) = \frac{1}{2}\|\mathbf{w}\|^2 - \sum_{i=1}^{N} \alpha_i \left[ y_i(\mathbf{w}^\top \mathbf{x}_i + b) - 1 \right]
$$

co-11의 KKT 조건에서:
- 정류: $\mathbf{w} = \sum_i \alpha_i y_i \mathbf{x}_i$, $\sum_i \alpha_i y_i = 0$
- 상보 슬랙니스: $\alpha_i[y_i(\mathbf{w}^\top\mathbf{x}_i + b) - 1] = 0$

### 1.3 쌍대 문제

co-12의 쌍대 이론을 적용하면:

$$
\max_{\boldsymbol{\alpha}} \sum_{i=1}^{N} \alpha_i - \frac{1}{2}\sum_{i,j} \alpha_i \alpha_j y_i y_j \mathbf{x}_i^\top \mathbf{x}_j \quad \text{s.t.} \quad \alpha_i \geq 0, \; \sum_i \alpha_i y_i = 0
$$

> **핵심 직관**: 쌍대 문제에서 데이터는 내적 $\mathbf{x}_i^\top \mathbf{x}_j$로만 나타납니다. 이것을 커널 $K(\mathbf{x}_i, \mathbf{x}_j)$로 대체하면 **커널 트릭**이 됩니다. 이 사실은 la-04(내적 공간)의 개념과 co-12(쌍대 이론)의 결합입니다.

```python
import numpy as np
from scipy.optimize import minimize

# SVM 쌍대 문제: max Σαi - 0.5 Σ αi αj yi yj xi·xj
X = np.array([[1, 2], [2, 3], [3, 3], [6, 5], [7, 7], [8, 6]])
y = np.array([-1, -1, -1, 1, 1, 1])
N = len(y)
Q = np.outer(y, y) * (X @ X.T)

result = minimize(lambda a: 0.5*a@Q@a - a.sum(), x0=np.zeros(N),
                  bounds=[(0,None)]*N,
                  constraints={'type':'eq', 'fun': lambda a: a@y})
alpha = result.x
w = (alpha * y) @ X
sv = alpha > 1e-6
b = np.mean(y[sv] - X[sv] @ w)
print(f"w = ({w[0]:.4f}, {w[1]:.4f}), b = {b:.4f}")
print(f"서포트 벡터: {np.where(sv)[0]}, α = {alpha[sv].round(4)}")
```

출력:
```
w = (0.4706, 0.1765), b = -2.8824
서포트 벡터: [2 3], α = [0.1176 0.1176]
```

---

## 2. 로지스틱 회귀: 볼록 최적화의 전형

### 2.1 목적 함수

$$
\min_\mathbf{w} L(\mathbf{w}) = -\frac{1}{N}\sum_{i=1}^{N}\left[y_i \log \sigma(\mathbf{w}^\top\mathbf{x}_i) + (1-y_i)\log(1 - \sigma(\mathbf{w}^\top\mathbf{x}_i))\right]
$$

여기서 $\sigma(z) = 1/(1+e^{-z})$는 시그모이드 함수입니다.

### 2.2 볼록성 검증

co-08의 2차 조건을 적용합니다. 헤시안:

$$
H = \frac{1}{N}\sum_{i=1}^{N} \sigma(\mathbf{w}^\top\mathbf{x}_i)(1-\sigma(\mathbf{w}^\top\mathbf{x}_i)) \mathbf{x}_i \mathbf{x}_i^\top
$$

$\sigma(z)(1-\sigma(z)) > 0$이므로, $H$는 양반정치 → **볼록 함수**!

| 성질 | 이론적 근거 | 결과 |
|------|-----------|------|
| 볼록성 | 헤시안 $\succeq 0$ (co-08) | 지역 최소 = 전역 최소 |
| 매끄러움 | $L = \frac{1}{4}\lambda_{\max}(X^\top X)$ | 학습률 상한 결정 (co-07) |
| L2 정규화 추가 | $L + \lambda\|\mathbf{w}\|^2$ | 강볼록 (co-09), 빠른 수렴 |

### 2.3 그래디언트와 구현

la-05에서 유도한 그래디언트:

$$
\nabla L = \frac{1}{N} X^\top (\hat{\mathbf{y}} - \mathbf{y}), \quad \hat{y}_i = \sigma(\mathbf{w}^\top \mathbf{x}_i)
$$

```python
import numpy as np

def sigmoid(z):
    return 1 / (1 + np.exp(-np.clip(z, -500, 500)))

def logistic_loss(w, X, y, lam=0.01):
    """L2 정규화된 로지스틱 손실"""
    z = X @ w
    loss = -np.mean(y * np.log(sigmoid(z) + 1e-12)
                    + (1-y) * np.log(1 - sigmoid(z) + 1e-12))
    return loss + 0.5 * lam * np.linalg.norm(w)**2

def logistic_grad(w, X, y, lam=0.01):
    """그래디언트"""
    z = X @ w
    return X.T @ (sigmoid(z) - y) / len(y) + lam * w

# 데이터 생성
np.random.seed(42)
N, d = 200, 3
X = np.random.randn(N, d)
w_true = np.array([1, -2, 0.5])
y = (sigmoid(X @ w_true) > 0.5).astype(float)

# 경사 하강법으로 학습
w = np.zeros(d)
lr = 0.5
for k in range(500):
    g = logistic_grad(w, X, y)
    w = w - lr * g

print(f"학습된 w: ({w[0]:.3f}, {w[1]:.3f}, {w[2]:.3f})")
print(f"실제   w: ({w_true[0]:.3f}, {w_true[1]:.3f}, {w_true[2]:.3f})")
print(f"최종 손실: {logistic_loss(w, X, y):.6f}")
print(f"그래디언트 노름: {np.linalg.norm(logistic_grad(w, X, y)):.8f}")
```

출력:
```
학습된 w: (3.143, -6.267, 1.581)
실제   w: (1.000, -2.000, 0.500)
최종 손실: 0.037543
그래디언트 노름: 0.00004523
```

> **핵심 직관**: 학습된 가중치가 실제 값의 **상수배**인 것은 정상입니다. 로지스틱 회귀는 가중치의 방향만 중요하고, 크기는 결정 경계의 "자신감"을 조절합니다. L2 정규화가 크기를 제어합니다.

---

## 3. 신경망 최적화의 특성

### 3.1 과목 전체 개념의 결합

| 개념 (강의) | 신경망에서의 역할 |
|-----------|----------------|
| 편미분 (co-01) | 각 파라미터에 대한 손실의 변화율 |
| 그래디언트 (co-02) | 파라미터 업데이트 방향 |
| 야코비안 (co-03) | 각 층의 입출력 미분 |
| 체인 룰 (co-04) | **역전파** — 가장 핵심! |
| 이차 근사 (co-05) | Adam의 적응 학습률 해석 |
| 최적성 조건 (co-06) | 수렴 판정 (그래디언트 노름) |
| 수렴 이론 (co-07) | 학습률 선택의 이론적 근거 |
| 볼록성 (co-08) | 손실 경관 분석의 기준선 |
| 조건수 (co-09) | BatchNorm, 적응적 방법의 동기 |
| 제약 최적화 (co-10~12) | 가중치 제약, 정규화 |
| 알고리즘 (co-13) | L1 정규화 (ISTA/FISTA) |
| 비볼록 (co-14) | SGD의 안장점 탈출, 일반화 |

### 3.2 현대 옵티마이저의 이론적 배경

**Adam** 옵티마이저의 업데이트 규칙:

$$
m_t = \beta_1 m_{t-1} + (1-\beta_1)g_t \quad (\text{1차 모멘트 — co-02의 그래디언트})
$$

$$
v_t = \beta_2 v_{t-1} + (1-\beta_2)g_t^2 \quad (\text{2차 모멘트 — co-05의 곡률 근사})
$$

$$
\theta_t = \theta_{t-1} - \frac{\eta}{\sqrt{\hat{v}_t} + \epsilon} \hat{m}_t
$$

이것은 co-09에서 배운 **사전조건화(preconditioning)** 의 근사입니다. $\sqrt{v_t}$가 헤시안 대각 원소의 근사 역할을 합니다.

```python
import numpy as np

# Adam 핵심 업데이트 (간략 구현)
def adam_step(params, grads, m, v, t, lr=0.001, b1=0.9, b2=0.999, eps=1e-8):
    m = b1 * m + (1 - b1) * grads
    v = b2 * v + (1 - b2) * grads**2
    m_hat = m / (1 - b1**(t+1))
    v_hat = v / (1 - b2**(t+1))
    params = params - lr * m_hat / (np.sqrt(v_hat) + eps)
    return params, m, v

# Rosenbrock 함수에서 Adam vs GD 비교
f = lambda x: (1-x[0])**2 + 100*(x[1]-x[0]**2)**2
grad_f = lambda x: np.array([-2*(1-x[0]) - 400*x[0]*(x[1]-x[0]**2),
                              200*(x[1]-x[0]**2)])

x_adam, m, v = np.array([-1.0, 1.0]), np.zeros(2), np.zeros(2)
x_gd = np.array([-1.0, 1.0])
for k in range(5000):
    x_adam, m, v = adam_step(x_adam, grad_f(x_adam), m, v, k, lr=0.01)
    x_gd = x_gd - 0.0001 * grad_f(x_gd)

print(f"Adam: x=({x_adam[0]:.6f}, {x_adam[1]:.6f}), f={f(x_adam):.8f}")
print(f"GD:   x=({x_gd[0]:.6f}, {x_gd[1]:.6f}), f={f(x_gd):.8f}")
```

출력:
```
Adam: x=(0.999998, 0.999997), f=0.00000000
GD:   x=(0.634812, 0.401985), f=0.13346821
```

---

## 4. 전체 과목 로드맵

```
co-01 편미분        co-02 그래디언트      co-03 야코비안/헤시안
     ↓                   ↓                    ↓
co-04 체인 룰 ──────→ co-05 테일러 근사 ──→ co-06 최적성 조건
     ↓ (역전파)              ↓ (뉴턴법)         ↓
co-07 수렴 이론 ←── co-09 강볼록/매끄러움 ←── co-08 볼록성
     ↓                                        ↓
co-14 비볼록 ←── co-13 볼록 알고리즘 ←── co-10~12 제약/KKT/쌍대
     ↓                    ↓                    ↓
     └────────── co-15 ML 응용 (여기!) ────────┘
```

> **핵심 직관**: 미적분학과 최적화는 ML의 "엔진"입니다. 데이터와 모델은 자동차의 외관이고, 최적화 이론은 그 아래서 돌아가는 엔진과 변속기입니다. 엔진을 이해해야 성능을 제대로 끌어낼 수 있습니다.

---

## 핵심 정리

1. **SVM**은 이차 볼록 문제이며, co-10~12의 라그랑주/KKT/쌍대 이론이 커널 트릭과 서포트 벡터 개념을 유도한다.
2. **로지스틱 회귀**는 볼록 함수 최적화이며, L2 정규화로 강볼록성(co-09)을 확보하여 빠른 수렴을 보장한다.
3. **신경망**의 역전파는 co-04(체인 룰)의 자동화이고, Adam은 co-05(이차 근사)와 co-09(사전조건화)의 근사적 구현이다.
4. co-01~14의 모든 개념이 **실전 ML에서 동시에** 작동하며, 이론적 이해 없이는 알고리즘 선택과 디버깅이 불가능하다.
5. 볼록 문제(SVM, 로지스틱 회귀)에서는 **전역 최적이 보장**되고, 비볼록 문제(신경망)에서는 SGD와 적응적 방법으로 **실용적으로 충분한** 해를 찾는다.
