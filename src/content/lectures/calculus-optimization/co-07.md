# 경사 하강법의 수렴 이론

## 왜 경사 하강법의 수렴 이론을 배워야 하는가

co-02에서 경사 하강법의 기본 형태를 소개했습니다. "그래디언트 반대 방향으로 이동"이라는 직관은 간단하지만, 실전에서는 수많은 질문이 생깁니다. 학습률을 얼마로 설정해야 하는가? 몇 번 반복해야 수렴하는가? 어떤 함수에서는 수렴이 보장되는가?

이번 강의에서는 이 질문들에 **이론적 근거**를 제공합니다. 학습률의 상한, 수렴 속도, 그리고 함수의 성질(볼록성, 매끄러움)이 수렴에 어떤 영향을 미치는지를 엄밀하게 다룹니다.

---

## 1. 경사 하강법 복습

$$
\mathbf{x}_{k+1} = \mathbf{x}_k - \eta \nabla f(\mathbf{x}_k)
$$

학습률 $\eta > 0$의 선택이 수렴 성패를 결정합니다.

```
학습률에 따른 행동:

η 너무 작음         η 적절함           η 너무 큼
    ·                  ·                  ·
   ·                   ·                    ·
  ·                    ·                ·
 ·                     ·             ·
·                      ·          ·       ·
느린 수렴             빠른 수렴          발산!
(수렴하지만 느림)    (목표 도달)     (진동 또는 발산)
```

---

## 2. Lipschitz 연속 그래디언트

그래디언트가 "너무 급격히 변하지 않는다"는 조건을 수학적으로 표현합니다.

**정의**: $\nabla f$가 **$L$-Lipschitz 연속**이란:

$$
\|\nabla f(\mathbf{x}) - \nabla f(\mathbf{y})\| \leq L \|\mathbf{x} - \mathbf{y}\|, \quad \forall \mathbf{x}, \mathbf{y}
$$

동치 조건: $f$가 $C^2$이면, 모든 $\mathbf{x}$에서 $\|H(\mathbf{x})\|_2 \leq L$

| 함수 | $L$ 값 | 의미 |
|------|--------|------|
| $f(x) = \frac{1}{2}x^2$ | $1$ | 그래디언트가 천천히 변함 |
| $f(x) = \frac{1}{2}ax^2$ | $a$ | 곡률에 비례 |
| $f(\mathbf{x}) = \frac{1}{2}\mathbf{x}^\top A\mathbf{x}$ | $\lambda_{\max}(A)$ | 최대 고유값 |

> **핵심 직관**: $L$은 함수의 "최대 곡률"입니다. 곡률이 클수록 함수가 급격히 변하므로, 더 작은 스텝을 밟아야 합니다.

---

## 3. 충분한 감소 조건

$\eta \leq \frac{1}{L}$이면, 매 스텝에서 함수값이 반드시 감소합니다.

**정리 (Descent Lemma)**: $\nabla f$가 $L$-Lipschitz이면:

$$
f(\mathbf{y}) \leq f(\mathbf{x}) + \nabla f(\mathbf{x})^\top(\mathbf{y} - \mathbf{x}) + \frac{L}{2}\|\mathbf{y} - \mathbf{x}\|^2
$$

$\mathbf{y} = \mathbf{x} - \eta \nabla f(\mathbf{x})$를 대입하면:

$$
f(\mathbf{x}_{k+1}) \leq f(\mathbf{x}_k) - \eta\left(1 - \frac{L\eta}{2}\right)\|\nabla f(\mathbf{x}_k)\|^2
$$

$\eta \leq \frac{1}{L}$이면 괄호 안이 $\geq \frac{1}{2}$이므로:

$$
f(\mathbf{x}_{k+1}) \leq f(\mathbf{x}_k) - \frac{\eta}{2}\|\nabla f(\mathbf{x}_k)\|^2
$$

> **핵심 직관**: 학습률의 안전한 상한은 $1/L$입니다. 이보다 크면 함수값이 증가할 수 있고, 이보다 작으면 수렴은 하지만 느립니다.

---

## 4. 수렴 속도: 일반 매끄러운 함수

**정리**: $f$가 하한을 가지고 $\nabla f$가 $L$-Lipschitz이면, $\eta = 1/L$로 $k$번 반복 후:

$$
\min_{i=0,\dots,k-1} \|\nabla f(\mathbf{x}_i)\|^2 \leq \frac{2L(f(\mathbf{x}_0) - f^*)}{k}
$$

이것은 $O(1/k)$ 수렴 속도입니다. 즉, 그래디언트 노름을 $\epsilon$ 이하로 만들려면 $O(1/\epsilon^2)$번 반복이 필요합니다.

---

## 5. 볼록 함수에서의 수렴

$f$가 볼록이면 수렴 보장이 강화됩니다. (볼록성은 co-08에서 자세히 다룹니다.)

$$
f(\mathbf{x}_k) - f^* \leq \frac{L\|\mathbf{x}_0 - \mathbf{x}^*\|^2}{2k}
$$

이것은 함수값 차이가 $O(1/k)$로 줄어든다는 뜻입니다.

**강볼록(strongly convex)** 함수에서는 훨씬 빠릅니다 (co-09에서 상세히):

$$
f(\mathbf{x}_k) - f^* \leq \left(1 - \frac{\mu}{L}\right)^k (f(\mathbf{x}_0) - f^*)
$$

| 함수 클래스 | 수렴 속도 | $\epsilon$-정확도 도달 반복 수 |
|-----------|----------|--------------------------|
| 일반 매끄러움 | $O(1/k)$ (그래디언트 노름) | $O(1/\epsilon^2)$ |
| 볼록 + 매끄러움 | $O(1/k)$ (함수값) | $O(1/\epsilon)$ |
| 강볼록 + 매끄러움 | $O((1-\mu/L)^k)$ 선형 수렴 | $O(\kappa \log(1/\epsilon))$ |

여기서 $\kappa = L/\mu$는 **조건수(condition number)**입니다.

> **핵심 직관**: 함수의 성질이 좋을수록(볼록 → 강볼록), 수렴이 기하급수적으로 빨라집니다. 조건수 $\kappa$가 작을수록 수렴이 빠릅니다.

---

## 6. Python 실험: 학습률과 수렴

```python
import numpy as np

# f(x) = 0.5 * x^T A x, A의 최대 고유값 = L
A = np.array([[10, 0], [0, 1]])  # L = 10, μ = 1, κ = 10
f = lambda x: 0.5 * x @ A @ x
grad_f = lambda x: A @ x
L = 10.0

x0 = np.array([5.0, 5.0])
learning_rates = [0.5/L, 1.0/L, 1.5/L, 2.1/L]
labels = ['η=0.5/L (보수적)', 'η=1/L (최적)', 'η=1.5/L (공격적)', 'η=2.1/L (발산)']

for eta, label in zip(learning_rates, labels):
    x = x0.copy()
    values = []
    for _ in range(50):
        values.append(f(x))
        x = x - eta * grad_f(x)
    print(f"{label:25s} | 50스텝 후 f = {values[-1]:12.4f} | "
          f"수렴? {'예' if values[-1] < 1e-3 else '아니오'}")
```

출력:
```
η=0.5/L (보수적)           | 50스텝 후 f =       0.0000 | 수렴? 예
η=1/L (최적)               | 50스텝 후 f =       0.0000 | 수렴? 예
η=1.5/L (공격적)           | 50스텝 후 f =       0.0001 | 수렴? 예
η=2.1/L (발산)             | 50스텝 후 f = 7065929.5498 | 수렴? 아니오
```

---

## 7. 실전에서의 학습률 전략

이론적 최적 학습률 $1/L$을 실전에서 직접 사용하기 어려운 이유와 대안을 정리합니다.

| 전략 | 방법 | 장점 |
|------|------|------|
| 고정 학습률 | $\eta$ 고정 | 단순함, 이론 분석 용이 |
| 학습률 감쇠 | $\eta_k = \eta_0 / (1 + \alpha k)$ | 초기엔 빠르게, 후기엔 정밀하게 |
| 선형 탐색 | 매 스텝 최적 $\eta$ 탐색 | 이론적으로 최적 |
| 적응적 | Adam, AdaGrad 등 | 파라미터별 학습률 자동 조정 |

> **핵심 직관**: 이론의 $\eta = 1/L$은 "최악의 경우"를 대비한 보수적 선택입니다. 실전에서는 Adam 같은 적응적 방법이 각 파라미터의 국소 곡률에 맞춰 학습률을 조정합니다.

---

## 핵심 정리

1. $\nabla f$가 **$L$-Lipschitz 연속**이면, 학습률 $\eta \leq 1/L$에서 매 스텝 함수값 감소가 보장된다.
2. 일반 매끄러운 함수에서 경사 하강법은 $O(1/k)$ 속도로 정류점에 접근하며, 이는 $O(1/\epsilon^2)$ 반복이다.
3. **볼록 함수**에서는 $O(1/k)$ 함수값 수렴, **강볼록**에서는 $O((1-\mu/L)^k)$ **선형 수렴**이 보장된다.
4. **조건수** $\kappa = L/\mu$가 클수록 수렴이 느려지며, 이는 등고선이 길쭉하게 찌그러진 것에 대응한다.
5. 실전에서는 $1/L$을 직접 구하기 어려우므로, **적응적 학습률**(Adam 등)이나 학습률 스케줄링을 사용한다.
