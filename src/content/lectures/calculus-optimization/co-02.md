# 그래디언트와 방향 도함수

## 왜 그래디언트와 방향 도함수를 배워야 하는가

경사하강법(gradient descent)은 머신러닝 최적화의 핵심 엔진입니다. "그래디언트의 반대 방향으로 한 걸음 이동한다"는 단순한 규칙이 수백만 파라미터를 학습시킵니다. 그런데 **왜** 반대 방향이어야 할까요? 그래디언트가 정확히 무엇을 가리키며, 다른 방향으로 가면 어떤 일이 벌어질까요?

이 질문에 답하려면 **방향 도함수(directional derivative)**와 그것이 최대가 되는 방향인 **그래디언트 벡터**를 정확히 이해해야 합니다. co-01에서 정의한 편미분을 임의의 방향으로 확장하는 것이 이번 강의의 핵심입니다.

---

## 1. 그래디언트 벡터의 정의

co-01에서 편미분을 모은 벡터를 이미 만났습니다. 이를 **그래디언트(gradient)**라 합니다.

$$
\nabla f(\mathbf{x}) = \begin{pmatrix} \frac{\partial f}{\partial x_1} \\ \frac{\partial f}{\partial x_2} \\ \vdots \\ \frac{\partial f}{\partial x_n} \end{pmatrix} \in \mathbb{R}^n
$$

la-05에서 손실 함수의 그래디언트를 구해 본 경험이 있으므로, 여기서는 기하학적 의미에 집중합니다.

> **핵심 직관**: 그래디언트는 "함수가 가장 가파르게 올라가는 방향과 그 기울기의 크기"를 동시에 담고 있는 벡터입니다.

---

## 2. 방향 도함수

단위 벡터 $\mathbf{u} \in \mathbb{R}^n$, $\|\mathbf{u}\| = 1$ 방향으로의 **방향 도함수**는 다음과 같이 정의됩니다.

$$
D_{\mathbf{u}} f(\mathbf{a}) = \lim_{t \to 0} \frac{f(\mathbf{a} + t\mathbf{u}) - f(\mathbf{a})}{t}
$$

$f$가 전미분 가능하면, 방향 도함수는 그래디언트와의 내적으로 계산됩니다.

$$
D_{\mathbf{u}} f(\mathbf{a}) = \nabla f(\mathbf{a})^\top \mathbf{u}
$$

| 방향 $\mathbf{u}$ | 방향 도함수 값 | 의미 |
|---|---|---|
| $\mathbf{e}_i$ (표준 기저) | $\frac{\partial f}{\partial x_i}$ | 편미분과 일치 |
| $\frac{\nabla f}{\|\nabla f\|}$ | $\|\nabla f\|$ | **최대 상승 방향** |
| $-\frac{\nabla f}{\|\nabla f\|}$ | $-\|\nabla f\|$ | **최대 하강 방향** |
| $\nabla f$에 수직인 $\mathbf{u}$ | $0$ | 등고선 접선 방향 |

---

## 3. 최대 상승 방향의 증명

코시-슈바르츠 부등식에 의해:

$$
D_{\mathbf{u}} f = \nabla f^\top \mathbf{u} \leq \|\nabla f\| \cdot \|\mathbf{u}\| = \|\nabla f\|
$$

등호가 성립하는 조건은 $\mathbf{u} = \frac{\nabla f}{\|\nabla f\|}$, 즉 그래디언트 방향일 때입니다.

> **핵심 직관**: 경사하강법이 $-\nabla f$ 방향을 택하는 이유는 단순합니다. 모든 단위 벡터 중 함수값을 **가장 빠르게 줄이는** 방향이기 때문입니다.

---

## 4. 등고선과 그래디언트의 직교성

$f(\mathbf{x}) = c$인 등고선(또는 등위면) 위의 곡선 $\mathbf{r}(t)$를 생각합니다.

$$
f(\mathbf{r}(t)) = c \implies \frac{d}{dt}f(\mathbf{r}(t)) = \nabla f(\mathbf{r}(t))^\top \mathbf{r}'(t) = 0
$$

따라서 $\nabla f$는 등고선의 접선 벡터 $\mathbf{r}'(t)$에 수직입니다.

```
등고선과 그래디언트의 관계

        y
        |        ╭──── f = 3
        |     ╭──┤
        |   ╭─┤  │
        |   │ │  │  ← f = 2
        |   │ │╭─╯
        |   │ ╰┤ ──→ ∇f  (등고선에 수직)
        |   ╰──╯ ← f = 1
        |
        +──────────────→ x
```

이 성질은 ML에서 매우 중요합니다. 손실 등고선 위를 이동하면 손실이 변하지 않고, 등고선을 **관통**해야 손실이 줄어듭니다. 그래디언트는 정확히 그 관통 방향을 가리킵니다.

---

## 5. 경사 하강법의 기본 형태

그래디언트의 성질을 바탕으로, 경사 하강법의 업데이트 규칙은 자연스럽게 유도됩니다.

$$
\mathbf{x}_{k+1} = \mathbf{x}_k - \eta \nabla f(\mathbf{x}_k)
$$

여기서 $\eta > 0$는 **학습률(learning rate)**입니다. co-07에서 학습률 선택과 수렴 이론을 본격적으로 다룹니다.

```python
import numpy as np

def gradient_descent(f, grad_f, x0, lr=0.01, n_iter=100):
    """기본 경사 하강법"""
    x = x0.copy()
    history = [x.copy()]
    for _ in range(n_iter):
        g = grad_f(x)
        x = x - lr * g
        history.append(x.copy())
    return x, np.array(history)

# 예제: f(x, y) = x^2 + 4y^2 (타원형 등고선)
f = lambda x: x[0]**2 + 4*x[1]**2
grad_f = lambda x: np.array([2*x[0], 8*x[1]])

x_opt, path = gradient_descent(f, grad_f, x0=np.array([4.0, 2.0]), lr=0.1, n_iter=50)
print(f"최적점: ({x_opt[0]:.6f}, {x_opt[1]:.6f})")
print(f"함수값: {f(x_opt):.8f}")
```

출력:
```
최적점: (0.000054, 0.000000)
함수값: 0.00000000
```

---

## 6. 그래디언트의 크기와 정보량

그래디언트의 **노름** $\|\nabla f(\mathbf{x})\|$은 해당 점에서 함수가 얼마나 급격히 변하는지를 나타냅니다.

$$
\|\nabla f(\mathbf{x})\| = 0 \iff \text{정류점(stationary point)}
$$

정류점은 극소, 극대, 또는 **안장점(saddle point)** 중 하나이며, 이를 구분하는 것은 co-06에서 헤시안으로 판별합니다.

```python
# 학습 과정에서 그래디언트 노름 추적
norms = [np.linalg.norm(grad_f(p)) for p in path]
print(f"초기 그래디언트 노름: {norms[0]:.4f}")
print(f"10스텝 후:           {norms[10]:.4f}")
print(f"50스텝 후:           {norms[50]:.8f}")
```

출력:
```
초기 그래디언트 노름: 17.8885
10스텝 후:           0.2367
50스텝 후:           0.00010862
```

> **핵심 직관**: 학습이 진행될수록 그래디언트 노름이 줄어드는 것은 최적점에 가까워지고 있다는 신호입니다. 실무에서 그래디언트 노름을 모니터링하는 이유가 여기에 있습니다.

---

## 7. 편미분 vs 그래디언트 vs 방향 도함수 비교

| 개념 | 입력 | 출력 | 의미 |
|------|------|------|------|
| 편미분 $\frac{\partial f}{\partial x_i}$ | 함수 + 좌표축 | 스칼라 | $i$번째 축 방향 변화율 |
| 그래디언트 $\nabla f$ | 함수 | 벡터 ($\mathbb{R}^n$) | 최대 상승 방향 + 크기 |
| 방향 도함수 $D_\mathbf{u} f$ | 함수 + 단위 벡터 | 스칼라 | 임의 방향 변화율 |

세 개념은 다음 관계로 엮입니다:

$$
D_{\mathbf{u}} f = \nabla f^\top \mathbf{u} = \sum_{i=1}^{n} \frac{\partial f}{\partial x_i} u_i
$$

---

## 핵심 정리

1. **그래디언트** $\nabla f$는 편미분을 모은 벡터이며, 함수가 가장 빠르게 증가하는 방향을 가리킨다.
2. **방향 도함수** $D_\mathbf{u} f = \nabla f^\top \mathbf{u}$는 그래디언트와의 내적이며, 코시-슈바르츠에 의해 그래디언트 방향에서 최대가 된다.
3. 그래디언트는 **등고선에 항상 수직**이며, 이것이 경사하강법이 등고선을 관통하여 손실을 줄이는 이유이다.
4. $\|\nabla f\| = 0$인 **정류점**은 극소·극대·안장점 중 하나이며, 구분에는 이차 정보(헤시안)가 필요하다.
5. 경사하강법 $\mathbf{x}_{k+1} = \mathbf{x}_k - \eta \nabla f(\mathbf{x}_k)$는 그래디언트의 "최대 하강 방향" 성질에 기반한다.
