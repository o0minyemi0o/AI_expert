# 다변수 함수와 편미분

## 왜 다변수 함수와 편미분을 배워야 하는가

머신러닝에서 다루는 함수는 거의 대부분 **다변수 함수**입니다. 신경망의 손실 함수는 수백만 개의 파라미터에 동시에 의존하고, 이 함수를 최적화하려면 각 변수에 대해 "얼마나 민감한지"를 측정할 수 있어야 합니다. 그 도구가 바로 편미분(partial derivative)입니다.

la-05(행렬 미적분)에서 그래디언트와 편미분을 간략히 소개한 바 있습니다. 이번 강의에서는 처음부터 엄밀하게, 다변수 함수의 정의 → 편미분 → 전미분 → 그래프·등고선 해석까지 체계적으로 다룹니다.

---

## 1. 다변수 함수의 정의

**다변수 함수(multivariable function)**란 $\mathbb{R}^n$에서 $\mathbb{R}$로의 매핑입니다.

$$
f : \mathbb{R}^n \to \mathbb{R}, \quad \mathbf{x} = (x_1, x_2, \dots, x_n) \mapsto f(\mathbf{x})
$$

| 예시 | 함수 | 변수 개수 |
|------|------|-----------|
| 이차 형식 | $f(x_1, x_2) = x_1^2 + 3x_1 x_2 + x_2^2$ | 2 |
| 선형 회귀 손실 | $L(\mathbf{w}) = \frac{1}{N}\sum_{i=1}^{N}(y_i - \mathbf{w}^\top \mathbf{x}_i)^2$ | $d$ (특성 수) |
| 소프트맥스 입력 | $z_k(\mathbf{x}) = \mathbf{w}_k^\top \mathbf{x} + b_k$ | $d + 1$ |

> **핵심 직관**: 스칼라 함수 $y = f(x)$의 그래프는 평면 위의 곡선이었습니다. $z = f(x_1, x_2)$의 그래프는 3차원 공간 속의 **곡면**이 됩니다. 변수가 $n$개이면 $(n+1)$차원 공간 속의 초곡면(hypersurface)입니다.

---

## 2. 편미분의 정의

$f(\mathbf{x})$의 $x_i$에 대한 **편미분(partial derivative)**은 다른 변수를 모두 고정한 채 $x_i$ 방향으로만 변화율을 측정합니다.

$$
\frac{\partial f}{\partial x_i}(\mathbf{a}) = \lim_{h \to 0} \frac{f(a_1, \dots, a_i + h, \dots, a_n) - f(a_1, \dots, a_i, \dots, a_n)}{h}
$$

**예제**: $f(x, y) = x^2 y + \sin(xy)$

$$
\frac{\partial f}{\partial x} = 2xy + y\cos(xy), \qquad \frac{\partial f}{\partial y} = x^2 + x\cos(xy)
$$

> **핵심 직관**: 편미분은 "다른 모든 축을 얼어붙게 한 뒤, 한 축 방향으로만 기울기를 재는 것"입니다. 단변수 미적분의 자연스러운 확장입니다.

---

## 3. 전미분과 선형 근사

편미분이 모든 방향에서 존재한다고 해서 함수가 "잘 행동"하는 것은 아닙니다. **전미분 가능(total differentiability)**이란, 한 점 근처에서 함수가 선형 함수로 잘 근사된다는 뜻입니다.

$$
f(\mathbf{a} + \mathbf{h}) = f(\mathbf{a}) + \nabla f(\mathbf{a})^\top \mathbf{h} + o(\|\mathbf{h}\|)
$$

여기서 $\nabla f(\mathbf{a}) = \left(\frac{\partial f}{\partial x_1}, \dots, \frac{\partial f}{\partial x_n}\right)^\top$가 **그래디언트 벡터**이며, la-05에서 이미 만나본 개념입니다.

| 개념 | 조건 | 의미 |
|------|------|------|
| 편미분 가능 | 각 $\partial f / \partial x_i$ 존재 | 축 방향 변화율 측정 가능 |
| 전미분 가능 | 선형 근사 오차가 $o(\|\mathbf{h}\|)$ | **모든 방향**에서 근사 가능 |
| 연속 편도함수 | 각 $\partial f / \partial x_i$가 연속 | 전미분 가능을 보장 ($C^1$ 함수) |

> **핵심 직관**: 전미분 가능성은 "함수가 접평면을 갖는다"와 동치입니다. ML에서 사용하는 대부분의 함수는 $C^1$이므로 자동으로 전미분 가능합니다.

---

## 4. 등고선과 기하학적 해석

2변수 함수 $f(x, y)$의 **등고선(level curve)**은 $f(x, y) = c$를 만족하는 점들의 집합입니다.

```
등고선 예시: f(x, y) = x² + y²

        y
        |    /  c=4  \
        |  /  c=1  \   \
        | |  c=0.25  |   |
        | |    ·     |   |     · = 최솟값 (0,0)
        | |          |   |
        |  \        /   /
        |    \     /  /
        +------------------→ x
```

등고선은 손실 함수의 지형을 2차원으로 시각화할 때 핵심 도구입니다. 다음 강의(co-02)에서 보겠지만, **그래디언트 벡터는 항상 등고선에 수직**입니다.

---

## 5. 고차 편미분

이차 편미분은 곡률 정보를 담고 있으며, co-03에서 다룰 **헤시안 행렬**의 원소가 됩니다.

$$
\frac{\partial^2 f}{\partial x_i \partial x_j}
$$

**슈바르츠 정리(Clairaut's theorem)**: $f$의 이차 편도함수가 연속이면 미분 순서를 바꿀 수 있습니다.

$$
\frac{\partial^2 f}{\partial x \partial y} = \frac{\partial^2 f}{\partial y \partial x}
$$

이 대칭성 덕분에 헤시안 행렬은 **대칭 행렬**이 되며, la-03(고유값 분해)에서 배운 스펙트럼 정리를 적용할 수 있습니다.

---

## 6. Python으로 편미분 확인하기

수치 미분과 해석적 미분을 비교하여 편미분이 올바른지 검증할 수 있습니다.

```python
import numpy as np

def f(x, y):
    return x**2 * y + np.sin(x * y)

def df_dx_analytic(x, y):
    return 2 * x * y + y * np.cos(x * y)

def df_dy_analytic(x, y):
    return x**2 + x * np.cos(x * y)

# 수치 편미분 (중앙 차분)
def numerical_partial(f, x, y, var='x', h=1e-7):
    if var == 'x':
        return (f(x + h, y) - f(x - h, y)) / (2 * h)
    else:
        return (f(x, y + h) - f(x, y - h)) / (2 * h)

# 검증
x0, y0 = 1.0, 2.0
print(f"df/dx 해석적: {df_dx_analytic(x0, y0):.8f}")
print(f"df/dx 수치적: {numerical_partial(f, x0, y0, 'x'):.8f}")
print(f"df/dy 해석적: {df_dy_analytic(x0, y0):.8f}")
print(f"df/dy 수치적: {numerical_partial(f, x0, y0, 'y'):.8f}")
```

출력:
```
df/dx 해석적: 3.16770634
df/dx 수치적: 3.16770634
df/dy 해석적: 1.58385317
df/dy 수치적: 1.58385317
```

> **핵심 직관**: 수치 미분은 구현 검증(gradient checking)의 핵심 도구입니다. 역전파로 구한 그래디언트가 올바른지 확인할 때 반드시 사용합니다.

---

## 핵심 정리

1. **다변수 함수** $f: \mathbb{R}^n \to \mathbb{R}$의 그래프는 $(n+1)$차원 초곡면이며, ML의 손실 함수가 대표적이다.
2. **편미분**은 한 변수 방향으로만 변화율을 측정한 것이며, 단변수 미분의 자연스러운 확장이다.
3. **전미분 가능성**은 함수가 접평면(선형 근사)을 갖는다는 뜻이며, $C^1$ 조건이면 충분하다.
4. **등고선**은 $f(\mathbf{x}) = c$인 점의 집합이고, 손실 경관을 시각화하는 핵심 도구이다.
5. 이차 편도함수의 **교환 대칭** 덕분에 헤시안은 대칭 행렬이 되어 스펙트럼 분해가 가능하다.
