# 야코비안과 헤시안

## 왜 야코비안과 헤시안을 배워야 하는가

la-05(행렬 미적분)에서 야코비안과 헤시안을 처음 만났습니다. 야코비안은 벡터 함수의 "국소 선형 근사"이고, 헤시안은 이차 곡률 정보를 담고 있다고 배웠습니다. 이번 강의에서는 그 개념들을 **더 깊이** 확장합니다.

최적화 이론에서 야코비안은 체인 룰과 역전파의 핵심이고, 헤시안은 극소점 판별과 뉴턴 방법의 기초입니다. 이 두 행렬을 이해하지 않으면, 1차 최적화와 2차 최적화의 근본적 차이를 알 수 없습니다.

---

## 1. 야코비안 행렬의 정의

벡터 함수 $\mathbf{f} : \mathbb{R}^n \to \mathbb{R}^m$, 즉 $\mathbf{f}(\mathbf{x}) = (f_1(\mathbf{x}), \dots, f_m(\mathbf{x}))^\top$의 **야코비안(Jacobian)**은 $m \times n$ 행렬입니다.

$$
J = \frac{\partial \mathbf{f}}{\partial \mathbf{x}} = \begin{pmatrix} \frac{\partial f_1}{\partial x_1} & \cdots & \frac{\partial f_1}{\partial x_n} \\ \vdots & \ddots & \vdots \\ \frac{\partial f_m}{\partial x_1} & \cdots & \frac{\partial f_m}{\partial x_n} \end{pmatrix}
$$

각 행은 $f_i$의 그래디언트 $\nabla f_i^\top$입니다.

> **핵심 직관**: 야코비안의 $(i, j)$ 원소는 "출력 $f_i$가 입력 $x_j$에 얼마나 민감한가"를 나타냅니다. 신경망의 한 층이 바로 이 매핑에 해당합니다.

---

## 2. 야코비안의 기하학적 의미

야코비안은 비선형 함수의 **국소 선형 근사** 행렬입니다.

$$
\mathbf{f}(\mathbf{x} + \mathbf{h}) \approx \mathbf{f}(\mathbf{x}) + J(\mathbf{x}) \mathbf{h}
$$

| 함수 유형 | 야코비안 크기 | 예시 |
|-----------|-------------|------|
| $f: \mathbb{R}^n \to \mathbb{R}$ | $1 \times n$ (= $\nabla f^\top$) | 손실 함수 |
| $\mathbf{f}: \mathbb{R}^n \to \mathbb{R}^m$ | $m \times n$ | 신경망 한 층 |
| $\mathbf{f}: \mathbb{R}^n \to \mathbb{R}^n$ | $n \times n$ (정방 행렬) | 좌표 변환 |

정방 야코비안의 **행렬식** $\det(J)$은 부피 변화율을 나타냅니다. la-02(행렬식과 역행렬)에서 배운 행렬식의 기하학적 의미가 여기서 다시 등장합니다.

---

## 3. 헤시안 행렬의 정의

스칼라 함수 $f : \mathbb{R}^n \to \mathbb{R}$의 **헤시안(Hessian)** 행렬은 이차 편도함수로 이루어진 $n \times n$ 대칭 행렬입니다.

$$
H = \nabla^2 f = \begin{pmatrix} \frac{\partial^2 f}{\partial x_1^2} & \frac{\partial^2 f}{\partial x_1 \partial x_2} & \cdots & \frac{\partial^2 f}{\partial x_1 \partial x_n} \\ \frac{\partial^2 f}{\partial x_2 \partial x_1} & \frac{\partial^2 f}{\partial x_2^2} & \cdots & \frac{\partial^2 f}{\partial x_2 \partial x_n} \\ \vdots & \vdots & \ddots & \vdots \\ \frac{\partial^2 f}{\partial x_n \partial x_1} & \frac{\partial^2 f}{\partial x_n \partial x_2} & \cdots & \frac{\partial^2 f}{\partial x_n^2} \end{pmatrix}
$$

co-01에서 증명한 슈바르츠 정리에 의해 $H_{ij} = H_{ji}$이므로, 헤시안은 **대칭 행렬**입니다.

---

## 4. 이차 형식과 곡률

헤시안의 핵심 역할은 **이차 형식(quadratic form)**을 통해 함수의 곡률을 결정하는 것입니다.

$$
f(\mathbf{x} + \mathbf{h}) \approx f(\mathbf{x}) + \nabla f(\mathbf{x})^\top \mathbf{h} + \frac{1}{2} \mathbf{h}^\top H(\mathbf{x}) \mathbf{h}
$$

정류점($\nabla f = \mathbf{0}$)에서 이차항 $\frac{1}{2}\mathbf{h}^\top H \mathbf{h}$의 부호가 극값의 종류를 결정합니다.

| 헤시안의 성질 | 이차 형식 | 정류점의 종류 |
|-------------|----------|-------------|
| 양정치 (모든 고유값 > 0) | 항상 양 | **극소점** |
| 음정치 (모든 고유값 < 0) | 항상 음 | **극대점** |
| 부정치 (양·음 고유값 혼재) | 부호 변함 | **안장점** |
| 반정치 (고유값에 0 포함) | 판별 불가 | 추가 분석 필요 |

> **핵심 직관**: la-04(이차 형식)에서 배운 양정치 판별법이 여기서 극값 판별로 직결됩니다. 고유값의 부호가 곧 곡률의 방향입니다.

---

## 5. Python으로 야코비안과 헤시안 계산

```python
import numpy as np

# 예제: f(x, y) = x^3 - 3xy + y^3
def f(x, y):
    return x**3 - 3*x*y + y**3

# 해석적 그래디언트와 헤시안
def grad_f(x, y):
    return np.array([3*x**2 - 3*y, -3*x + 3*y**2])

def hessian_f(x, y):
    return np.array([
        [6*x, -3],
        [-3, 6*y]
    ])

# 정류점 찾기: ∇f = 0  →  (0, 0)과 (1, 1)
critical_points = [(0, 0), (1, 1)]

for pt in critical_points:
    x, y = pt
    H = hessian_f(x, y)
    eigenvalues = np.linalg.eigvalsh(H)
    print(f"\n정류점 ({x}, {y}):")
    print(f"  헤시안 = \n{H}")
    print(f"  고유값 = {eigenvalues}")
    if all(e > 0 for e in eigenvalues):
        print(f"  → 양정치 → 극소점")
    elif all(e < 0 for e in eigenvalues):
        print(f"  → 음정치 → 극대점")
    else:
        print(f"  → 부정치 → 안장점")
```

출력:
```
정류점 (0, 0):
  헤시안 =
[[ 0 -3]
 [-3  0]]
  고유값 = [-3.  3.]
  → 부정치 → 안장점

정류점 (1, 1):
  헤시안 =
[[ 6 -3]
 [-3  6]]
  고유값 = [3. 9.]
  → 양정치 → 극소점
```

---

## 6. 수치 야코비안 계산

역전파 구현을 검증할 때 수치 야코비안이 유용합니다.

```python
def numerical_jacobian(f_vec, x, h=1e-7):
    """벡터 함수의 수치 야코비안 (중앙 차분)"""
    n = len(x)
    f0 = f_vec(x)
    m = len(f0)
    J = np.zeros((m, n))
    for j in range(n):
        e_j = np.zeros(n)
        e_j[j] = h
        J[:, j] = (f_vec(x + e_j) - f_vec(x - e_j)) / (2 * h)
    return J

# 예제: f(x) = [x1^2 + x2, x1*x2]
f_vec = lambda x: np.array([x[0]**2 + x[1], x[0]*x[1]])
x0 = np.array([2.0, 3.0])

J_num = numerical_jacobian(f_vec, x0)
J_exact = np.array([[2*x0[0], 1], [x0[1], x0[0]]])

print(f"수치 야코비안:\n{J_num}")
print(f"해석 야코비안:\n{J_exact}")
print(f"최대 오차: {np.max(np.abs(J_num - J_exact)):.2e}")
```

출력:
```
수치 야코비안:
[[4. 1.]
 [3. 2.]]
해석 야코비안:
[[4. 1.]
 [3. 2.]]
최대 오차: 4.97e-10
```

> **핵심 직관**: 야코비안의 각 열은 "입력의 $j$번째 성분을 미세하게 흔들었을 때 출력이 어떻게 변하는가"를 담고 있습니다. 이것이 역전파에서 각 파라미터의 기여도를 추적하는 원리입니다.

---

## 핵심 정리

1. **야코비안** $J \in \mathbb{R}^{m \times n}$은 벡터 함수의 국소 선형 근사 행렬이며, 역전파에서 각 층의 미분을 표현한다.
2. **헤시안** $H \in \mathbb{R}^{n \times n}$은 이차 편도함수의 대칭 행렬이며, 함수의 곡률 정보를 담고 있다.
3. 정류점에서 헤시안의 **고유값 부호**가 극소(양정치), 극대(음정치), 안장점(부정치)을 결정한다.
4. 이차 형식 $\mathbf{h}^\top H \mathbf{h}$는 이차 근사의 핵심이며, co-05(테일러 근사)에서 더 자세히 다룬다.
5. **수치 야코비안**은 해석적 미분의 검증 도구이며, gradient checking의 기반이다.
