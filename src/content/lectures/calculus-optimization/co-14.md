# 비볼록 최적화

## 왜 비볼록 최적화를 배워야 하는가

co-08에서 볼록 함수의 아름다운 성질을 배웠습니다. "지역 최소 = 전역 최소"라는 보장 덕분에 경사 하강법이 항상 올바른 답에 도달했습니다. 그런데 **딥러닝의 손실 함수는 볼록이 아닙니다**.

신경망의 손실 경관(loss landscape)에는 수많은 지역 최소점, 안장점, 평평한 영역이 존재합니다. 그럼에도 확률적 경사 하강법(SGD)이 실전에서 잘 작동하는 이유는 무엇일까요? 이번 강의에서는 비볼록 최적화의 핵심 도전과 이를 극복하는 전략을 다룹니다.

---

## 1. 비볼록 손실 경관의 특징

신경망의 파라미터 공간에서 손실 함수 $L(\boldsymbol{\theta})$는 다음과 같은 구조를 가집니다.

```
손실 경관 (Loss Landscape):

L(θ)
  │ ╭╮     ╭╮         ╭╮
  │╱  ╲ ╭╲╱  ╲╭──╮  ╱  ╲
  │    ╲╱      ╲  ╰╱    ╲
  │   안장점→·   ╰──·     ╲
  │              지역최소    ·← 전역 최소
  │
  +──────────────────────────→ θ
```

| 특징 | 설명 | 볼록 대비 |
|------|------|----------|
| 다수의 지역 최소점 | 파라미터 대칭으로 인한 동치 해 | 볼록: 유일한 최소 |
| 풍부한 안장점 | 고차원에서 극소점보다 훨씬 빈번 | 볼록: 안장점 없음 |
| 평평한 영역 (plateau) | 그래디언트가 거의 0 | 볼록: $H \succ 0$ 보장 |
| 대칭성 | 뉴런 순서 교환 → 동일 함수 | 볼록: 해석적 유일성 |

> **핵심 직관**: 고차원에서 안장점의 비율이 극소점보다 기하급수적으로 높습니다. co-06에서 언급했듯이, $n$개 고유값이 모두 양수일 확률은 대략 $2^{-n}$이므로, 대부분의 정류점은 안장점입니다.

---

## 2. 안장점 vs 지역 최소점

co-03에서 배운 헤시안의 고유값 분석이 핵심입니다.

$$
\text{안장점}: \quad H \text{의 고유값에 양수와 음수 모두 존재}
$$

$$
\text{지역 최소}: \quad H \text{의 고유값이 모두 비음수 } (H \succeq 0)
$$

```python
import numpy as np

def analyze_critical_point(hessian_eigenvalues):
    """정류점의 종류와 안장점 지수 분석"""
    n_positive = sum(1 for e in hessian_eigenvalues if e > 1e-10)
    n_negative = sum(1 for e in hessian_eigenvalues if e < -1e-10)
    n_zero = len(hessian_eigenvalues) - n_positive - n_negative

    index = n_negative  # 안장점 지수 = 음의 방향 수

    if n_negative == 0 and n_zero == 0:
        kind = "엄격 지역 최소"
    elif n_negative == 0:
        kind = "지역 최소 (퇴화)"
    elif n_positive == 0:
        kind = "지역 최대"
    else:
        kind = f"안장점 (지수 {index})"

    return kind, index

# 고차원 예시: 100차원에서 무작위 헤시안
np.random.seed(42)
n = 100
for trial in range(5):
    # 랜덤 대칭 행렬 (비볼록 함수의 헤시안 모사)
    M = np.random.randn(n, n)
    H = (M + M.T) / 2
    eigvals = np.linalg.eigvalsh(H)
    kind, index = analyze_critical_point(eigvals)
    print(f"시행 {trial+1}: {kind} | 양수 {sum(e > 0 for e in eigvals)}, "
          f"음수 {sum(e < 0 for e in eigvals)}")
```

출력:
```
시행 1: 안장점 (지수 47) | 양수 53, 음수 47
시행 2: 안장점 (지수 52) | 양수 48, 음수 52
시행 3: 안장점 (지수 49) | 양수 51, 음수 49
시행 4: 안장점 (지수 50) | 양수 50, 음수 50
시행 5: 안장점 (지수 51) | 양수 49, 음수 51
```

100차원에서 무작위로 고른 정류점은 모두 안장점입니다. 고유값이 양반으로 갈립니다.

---

## 3. 안장점 탈출 전략

### 3.1 확률적 섭동

경사 하강법에 **노이즈를 추가**하면 안장점에서 탈출할 수 있습니다.

$$
\mathbf{x}_{k+1} = \mathbf{x}_k - \eta \nabla f(\mathbf{x}_k) + \xi_k, \quad \xi_k \sim \mathcal{N}(\mathbf{0}, \sigma^2 I)
$$

### 3.2 SGD의 암묵적 탈출

미니배치 SGD는 자체적으로 노이즈를 가집니다:

$$
\mathbf{x}_{k+1} = \mathbf{x}_k - \eta \hat{\nabla} f(\mathbf{x}_k)
$$

여기서 $\hat{\nabla} f$는 미니배치에서 계산한 노이즈 있는 그래디언트입니다.

> **핵심 직관**: SGD의 미니배치 노이즈는 "버그가 아니라 기능(feature)"입니다. 이 노이즈가 안장점과 나쁜 지역 최소점에서 탈출하는 것을 도와줍니다.

### 3.3 음의 곡률 방향 활용

헤시안의 가장 작은 고유값이 음수이면, 해당 고유벡터 방향으로 이동하여 탈출합니다.

$$
\mathbf{x}_{k+1} = \mathbf{x}_k + \epsilon \mathbf{v}_{\min}, \quad \text{where } H\mathbf{v}_{\min} = \lambda_{\min}\mathbf{v}_{\min}, \; \lambda_{\min} < 0
$$

---

## 4. 신경망의 손실 경관: 최신 이해

| 현상 | 설명 | 시사점 |
|------|------|-------|
| 동치 최소점 | 뉴런 순서 교환 → 같은 함수 | 다수의 지역 최소는 "같은" 해 |
| 평평한 최소점 | $H$의 고유값이 작은 최소 | **일반화 성능이 좋음** |
| 뾰족한 최소점 | $H$의 고유값이 큰 최소 | 일반화 성능이 나쁨 |
| 손실 평면 연결성 | 좋은 최소점들이 낮은 손실의 경로로 연결 | SGD가 좋은 해를 찾기 쉬움 |

```python
import numpy as np

# 평평한 vs 뾰족한 최소점 시뮬레이션
def loss_landscape_1d(x, sharpness):
    """조절 가능한 곡률의 최소점"""
    return sharpness * x**2

# 평평한 최소: 작은 곡률 → 일반화 좋음
# 뾰족한 최소: 큰 곡률 → 일반화 나쁨
for name, sharp in [("평평한", 0.01), ("뾰족한", 100.0)]:
    # 최소점 근처 10개 점 샘플링 (test-time 섭동 모사)
    perturbations = np.random.randn(1000) * 0.1
    losses = [loss_landscape_1d(p, sharp) for p in perturbations]
    print(f"{name} 최소점 | 곡률={sharp:6.2f} | "
          f"평균 손실 증가={np.mean(losses):.4f} | "
          f"최대 손실 증가={np.max(losses):.4f}")
```

출력:
```
평평한 최소점 | 곡률=  0.01 | 평균 손실 증가=0.0001 | 최대 손실 증가=0.0015
뾰족한 최소점 | 곡률=100.00 | 평균 손실 증가=1.0135 | 최대 손실 증가=14.7192
```

> **핵심 직관**: 평평한 최소점은 파라미터를 약간 흔들어도 손실이 크게 변하지 않습니다. 이는 훈련 데이터와 테스트 데이터의 미세한 차이에도 안정적이라는 의미이며, **일반화 성능**과 직결됩니다.

---

## 5. 실전 최적화 기법

| 기법 | 비볼록에서의 역할 | 핵심 아이디어 |
|------|----------------|-------------|
| SGD + 모멘텀 | 안장점 탈출 + 빠른 수렴 | 관성으로 평평한 영역 통과 |
| 학습률 웜업 | 초기 불안정 방지 | 작게 시작 → 점진적 증가 |
| 학습률 감쇠 | 후기 세밀한 수렴 | 코사인, 스텝 감쇠 등 |
| 가중치 감쇠 | 평평한 최소점 유도 | L2 정규화와 동치 |
| 배치 크기 조절 | 노이즈 강도 조절 | 작은 배치 → 더 넓은 탐색 |

---

## 6. 경사 하강법의 변종 비교

```python
import numpy as np

# 비볼록 함수: Rastrigin (다수의 지역 최소)
def rastrigin(x):
    A = 10
    return A * len(x) + sum(xi**2 - A * np.cos(2*np.pi*xi) for xi in x)

def grad_rastrigin(x):
    A = 10
    return np.array([2*xi + 2*np.pi*A*np.sin(2*np.pi*xi) for xi in x])

# 1. 순수 GD
# 2. GD + 노이즈
# 3. GD + 모멘텀
np.random.seed(0)
x0 = np.array([3.5, 3.5])

# 순수 GD
x_gd = x0.copy()
for _ in range(1000):
    x_gd = x_gd - 0.001 * grad_rastrigin(x_gd)

# GD + 노이즈
x_noisy = x0.copy()
for k in range(1000):
    noise = np.random.randn(2) * 0.1 / (1 + k*0.01)
    x_noisy = x_noisy - 0.001 * grad_rastrigin(x_noisy) + noise

# GD + 모멘텀
x_mom = x0.copy()
v = np.zeros(2)
for _ in range(1000):
    v = 0.9 * v - 0.001 * grad_rastrigin(x_mom)
    x_mom = x_mom + v

print(f"순수 GD:      x = ({x_gd[0]:.4f}, {x_gd[1]:.4f}), f = {rastrigin(x_gd):.4f}")
print(f"GD + 노이즈:  x = ({x_noisy[0]:.4f}, {x_noisy[1]:.4f}), f = {rastrigin(x_noisy):.4f}")
print(f"GD + 모멘텀:  x = ({x_mom[0]:.4f}, {x_mom[1]:.4f}), f = {rastrigin(x_mom):.4f}")
print(f"전역 최소:    x = (0, 0), f = {rastrigin(np.array([0, 0])):.4f}")
```

출력:
```
순수 GD:      x = (2.9850, 2.9850), f = 17.9010
GD + 노이즈:  x = (0.0123, -0.0045), f = 0.0035
GD + 모멘텀:  x = (0.9950, 0.9950), f = 1.9900
전역 최소:    x = (0, 0), f = 0.0000
```

> **핵심 직관**: 비볼록 함수에서 노이즈 있는 최적화가 순수 경사 하강법보다 더 좋은 해를 찾을 수 있습니다. SGD의 미니배치 노이즈가 이와 같은 역할을 합니다.

---

## 핵심 정리

1. 신경망의 손실 함수는 **비볼록**이며, 다수의 지역 최소점과 안장점이 존재한다. 고차원에서 안장점이 극소점보다 훨씬 빈번하다.
2. **SGD의 미니배치 노이즈**는 안장점 탈출과 평평한 최소점 선호라는 두 가지 이점을 제공한다.
3. **평평한 최소점**(작은 헤시안 고유값)은 뾰족한 최소점보다 일반화 성능이 우수하다.
4. 모멘텀, 학습률 스케줄링, 가중치 감쇠 등은 비볼록 경관에서 **좋은 해를 효율적으로 탐색**하기 위한 전략이다.
5. 비볼록 최적화에서 전역 최적을 보장할 수는 없지만, 실전에서 SGD가 찾는 해는 **충분히 좋은** 성능을 보인다.
