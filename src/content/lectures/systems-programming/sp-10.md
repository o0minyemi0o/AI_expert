# AI 인프라 실전

## 왜 AI 인프라 실전 지식이 중요한가

지금까지 배운 메모리 계층, 프로세스, GPU, 네트워크, 분산 시스템, 컨테이너의 개념들이 실제 AI 시스템에서 어떻게 결합되는지 이해해야 합니다. 대규모 모델 학습은 수천 개의 GPU, 페타바이트급 스토리지, 초고속 네트워크가 유기적으로 동작해야 하는 복합 엔지니어링 문제입니다. 이 강의에서는 앞선 모든 개념을 통합하여 실전 AI 인프라를 설계합니다.

> **핵심 직관**: AI 인프라는 "가장 느린 구성 요소가 전체 성능을 결정"하는 파이프라인입니다. GPU 연산이 아무리 빨라도 데이터 로딩이 느리면 GPU는 놀게 되고, 네트워크가 느리면 분산 학습의 동기화가 병목이 됩니다.

## 1. 학습 클러스터 설계

대규모 모델 학습을 위한 클러스터 설계의 핵심 요소입니다.

| 구성 요소 | 설계 고려사항 | 예시 (1000 GPU 클러스터) |
|-----------|-------------|------------------------|
| GPU | 세대, 메모리, 상호연결 | A100 80GB 또는 H100 80GB |
| 네트워크 | GPU 간 대역폭, 토폴로지 | InfiniBand HDR 200Gb/s |
| 스토리지 | IOPS, 대역폭, 용량 | 분산 파일시스템 (Lustre, GPFS) |
| CPU/메모리 | 데이터 전처리 용량 | 노드당 64코어, 512GB RAM |
| 냉각/전력 | 전력 밀도, PUE | 액냉식, 노드당 10kW |

```
┌──────────────────────────────────────────────┐
│          학습 클러스터 토폴로지               │
├──────────────────────────────────────────────┤
│                                              │
│  노드 내부 (DGX 스타일):                     │
│  ┌──────────────────────────┐               │
│  │ GPU0 ══ NVLink ══ GPU1   │               │
│  │  ║                  ║    │               │
│  │ GPU2 ══ NVLink ══ GPU3   │               │
│  │  ║                  ║    │               │
│  │ GPU4 ══ NVLink ══ GPU5   │               │
│  │  ║                  ║    │               │
│  │ GPU6 ══ NVLink ══ GPU7   │               │
│  └──────────┬───────────────┘               │
│             │ InfiniBand                     │
│  노드 간:   │                                │
│  ┌──────────┴───────────────┐               │
│  │     Leaf Switch          │               │
│  │  ┌───┬───┬───┬───┐      │               │
│  │  │N0 │N1 │N2 │N3 │ ...  │  (랙 내)      │
│  │  └───┴───┴───┴───┘      │               │
│  └──────────┬───────────────┘               │
│             │                                │
│  ┌──────────┴───────────────┐               │
│  │     Spine Switch         │               │
│  │  ┌────┬────┬────┐       │               │
│  │  │Rack│Rack│Rack│ ...   │  (랙 간)      │
│  │  │ 0  │ 1  │ 2  │       │               │
│  │  └────┴────┴────┘       │               │
│  └──────────────────────────┘               │
│                                              │
└──────────────────────────────────────────────┘
```

## 2. 분산 학습 통신

분산 학습에서 GPU 간 통신은 가장 중요한 성능 요소입니다(dl-09 분산 학습 참조).

| 통신 패턴 | 용도 | 구현 |
|-----------|------|------|
| AllReduce | 그래디언트 동기화 | NCCL (NVIDIA) |
| AllGather | 모델 파라미터 수집 | NCCL |
| ReduceScatter | 분산 그래디언트 합산 | NCCL |
| Point-to-Point | 파이프라인 스테이지 간 | NCCL / MPI |

```python
# PyTorch 분산 학습 기본 설정
import torch
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP

def setup_distributed(rank, world_size):
    """분산 학습 초기화"""
    dist.init_process_group(
        backend='nccl',       # GPU 통신에 최적화된 NCCL
        init_method='env://', # 환경 변수로 랑데부
        rank=rank,
        world_size=world_size
    )
    torch.cuda.set_device(rank % torch.cuda.device_count())

def train(rank, world_size):
    setup_distributed(rank, world_size)

    model = MyModel().cuda(rank)
    model = DDP(model, device_ids=[rank])

    optimizer = torch.optim.Adam(model.parameters())

    for batch in dataloader:
        loss = model(batch.cuda(rank))
        loss.backward()       # 그래디언트 계산
        # DDP가 자동으로 AllReduce 수행 (NCCL)
        optimizer.step()
        optimizer.zero_grad()
```

> **핵심 직관**: AllReduce의 통신량은 모델 파라미터 수에 비례합니다. 10B 파라미터 모델을 fp16으로 AllReduce하면 매 스텝마다 20GB의 데이터가 GPU 간에 교환됩니다. 네트워크 대역폭이 이 속도를 따라가지 못하면, GPU는 통신을 기다리며 놀게 됩니다.

## 3. 통신과 연산 오버랩

분산 학습의 핵심 최적화는 통신과 연산을 겹치는 것입니다.

```python
# 그래디언트 버켓팅과 통신-연산 오버랩
# DDP는 자동으로 이를 수행하지만, 원리를 이해하는 것이 중요합니다

# 개념적 구현
class OverlappedAllReduce:
    def __init__(self, model, num_buckets=4):
        self.buckets = self._create_buckets(model.parameters(), num_buckets)

    def backward_hook(self, bucket_id):
        """각 버켓의 그래디언트가 준비되면 즉시 AllReduce 시작"""
        # 레이어 N의 그래디언트 AllReduce를 시작하면서
        # 레이어 N-1의 backward 연산을 동시에 수행
        dist.all_reduce_async(self.buckets[bucket_id])

# FSDP (Fully Sharded Data Parallel) - 더 큰 모델을 위한 전략
from torch.distributed.fsdp import FullyShardedDataParallel as FSDP

model = FSDP(
    MyLargeModel(),
    sharding_strategy=ShardingStrategy.FULL_SHARD,
    mixed_precision=MixedPrecision(
        param_dtype=torch.float16,
        reduce_dtype=torch.float16,
        buffer_dtype=torch.float16,
    ),
)
```

## 4. 스토리지 최적화

대규모 학습 데이터와 체크포인트의 효율적 관리입니다(sp-06 직렬화, mo-02 데이터 파이프라인 참조).

| 스토리지 유형 | 용도 | 성능 특성 |
|-------------|------|----------|
| NVMe SSD (로컬) | 학습 데이터 캐시 | IOPS 최고, 용량 제한 |
| 분산 파일시스템 | 공유 데이터셋 | 대역폭 높음, 지연 중간 |
| 오브젝트 스토리지 | 체크포인트, 아카이브 | 용량 무제한, 지연 높음 |

```python
# 다계층 스토리지 전략
class TieredDataLoader:
    def __init__(self, remote_path, local_cache_path):
        self.remote = remote_path       # S3/GCS
        self.local = local_cache_path   # NVMe SSD

    async def prefetch(self, batch_indices):
        """다음 배치를 로컬 SSD에 프리페치"""
        for idx in batch_indices:
            local_file = f"{self.local}/{idx}.bin"
            if not os.path.exists(local_file):
                await self._download(
                    f"{self.remote}/{idx}.bin", local_file
                )

    def load_batch(self, batch_indices):
        """로컬 SSD에서 데이터 로딩 (빠름)"""
        return np.stack([
            np.fromfile(f"{self.local}/{idx}.bin", dtype=np.float32)
            for idx in batch_indices
        ])

# 체크포인트 비동기 저장
async def async_checkpoint(model, optimizer, step, path):
    """학습을 멈추지 않고 체크포인트 저장"""
    state = {
        'model': model.state_dict(),
        'optimizer': optimizer.state_dict(),
        'step': step
    }
    # CPU로 복사 후 백그라운드 저장 (GPU 학습 계속)
    cpu_state = {k: v.cpu() for k, v in state['model'].items()}
    await save_to_storage(cpu_state, f"{path}/ckpt_{step}.pt")
```

## 5. 장애 복구

수천 GPU 규모의 학습에서는 하드웨어 장애가 **확률이 아니라 확실**합니다(sp-08 분산 시스템 참조).

```
┌───────────────────────────────────────────┐
│        장애 복구 전략 의사결정              │
├───────────────────────────────────────────┤
│                                           │
│   장애 감지 (heartbeat 미수신)             │
│       │                                   │
│       ▼                                   │
│   장애 유형 판별                           │
│       │                                   │
│  ┌────┴────────┐                          │
│  ▼             ▼                          │
│ 일시적         영구적                      │
│ (네트워크)     (GPU 고장)                  │
│  │              │                         │
│  ▼              ▼                         │
│ 재시도         노드 교체                   │
│ (exponential    │                         │
│  backoff)      ▼                          │
│              최근 체크포인트               │
│              로드                         │
│                │                          │
│                ▼                          │
│              학습 재개                     │
│              (elastic training)           │
│                                           │
└───────────────────────────────────────────┘
```

```python
# 탄력적 학습 (Elastic Training) 설정
# torchrun으로 실행: 노드 장애 시 자동 재시작

# 체크포인트 기반 복구
class ResilientTrainer:
    def __init__(self, model, ckpt_dir, ckpt_interval=1000):
        self.model = model
        self.ckpt_dir = ckpt_dir
        self.ckpt_interval = ckpt_interval

    def train(self):
        start_step = self._load_latest_checkpoint()

        for step in range(start_step, total_steps):
            try:
                loss = self._train_step()

                if step % self.ckpt_interval == 0:
                    self._save_checkpoint(step)

            except RuntimeError as e:
                if "NCCL" in str(e):
                    # 통신 장애: 체크포인트에서 재시작
                    self._reinitialize_distributed()
                    self._load_latest_checkpoint()
                else:
                    raise

    def _save_checkpoint(self, step):
        """롤링 체크포인트: 최근 3개만 유지"""
        path = f"{self.ckpt_dir}/ckpt_{step}.pt"
        torch.save(self.model.state_dict(), path)
        self._cleanup_old_checkpoints(keep=3)
```

## 6. 실전 시나리오

**시나리오 1: LLM 사전학습 클러스터 설계**

70B 파라미터 LLM을 학습하기 위해 256개의 H100 GPU(32노드 x 8GPU)를 사용합니다. 노드 내부는 NVLink로 900GB/s, 노드 간은 InfiniBand 400Gb/s로 연결합니다. 텐서 병렬은 노드 내(8-way), 파이프라인 병렬은 노드 간(4-way), 데이터 병렬은 8-way로 구성합니다. 체크포인트는 500스텝마다 분산 파일시스템에 저장하고, MTBF(평균 고장 간격)가 약 2시간이므로 체크포인트 간격을 이에 맞춥니다(dl-09 분산 학습 참조).

**시나리오 2: 추론 서빙 인프라**

실시간 추론 서비스를 위해 Kubernetes 위에 Triton Inference Server를 배포합니다. 오토스케일러가 요청 큐 길이를 모니터링하여 GPU 파드를 자동 확장/축소합니다. 모델 가중치는 S3에 저장하고, 파드 시작 시 로컬 NVMe에 캐시하여 콜드 스타트 시간을 줄입니다(mo-04 서빙 인프라 참조).

```python
# 종합: 학습 파이프라인 성능 분석
def analyze_training_bottleneck(
    gpu_util, gpu_mem_util, network_util, disk_io_util
):
    """학습 파이프라인 병목 진단"""
    if gpu_util < 0.7:
        if disk_io_util > 0.8:
            return "스토리지 병목: 데이터 로딩 최적화 필요 (sp-01, sp-06)"
        elif network_util > 0.8:
            return "네트워크 병목: 통신-연산 오버랩 확인 (sp-07)"
        else:
            return "CPU 병목: 데이터 전처리 병렬화 필요 (sp-02)"
    elif gpu_mem_util > 0.95:
        return "GPU 메모리 병목: mixed precision, 체크포인팅 적용 (sp-04)"
    else:
        return "GPU 연산 병목: 배치 크기 증가 또는 커널 최적화 (sp-04)"
```

> **핵심 직관**: 대규모 AI 인프라 설계의 핵심은 "모든 자원이 동시에 최대 효율로 동작하게 만드는 것"입니다. 하나라도 놀고 있다면 전체 시스템에 비효율이 있다는 신호입니다. 프로파일링과 모니터링이 이 비효율을 찾는 유일한 방법입니다.

## 핵심 정리

- 학습 클러스터 설계는 GPU, 네트워크(NVLink/InfiniBand), 스토리지, CPU/메모리의 균형이 핵심이며, 가장 느린 구성 요소가 전체 성능을 결정합니다
- 분산 학습의 통신(AllReduce)은 모델 크기에 비례하는 데이터를 교환하며, 통신과 연산의 오버랩(그래디언트 버켓팅)이 GPU 활용률을 높이는 핵심 기법입니다
- 다계층 스토리지 전략(NVMe 캐시 + 분산 파일시스템 + 오브젝트 스토리지)으로 학습 데이터 로딩과 체크포인트 저장을 최적화해야 합니다
- 대규모 클러스터에서 하드웨어 장애는 불가피하므로, 주기적 체크포인트, 탄력적 학습, 자동 재시작 메커니즘이 필수적입니다
- AI 인프라의 성능 병목은 GPU 연산, 데이터 로딩, 네트워크 통신, 메모리 중 하나이며, 체계적인 프로파일링으로 병목을 식별하고 해당 영역을 집중 최적화하는 것이 실전 엔지니어링의 핵심입니다
