# 프로세스와 스레드

## 왜 프로세스와 스레드가 중요한가

AI 시스템은 데이터 전처리, 모델 학습, 추론 서빙 등 다양한 작업을 동시에 수행해야 합니다. 이 동시성을 구현하는 두 가지 기본 단위가 **프로세스**와 **스레드**입니다. Python의 GIL이라는 독특한 제약 아래에서 올바른 병렬화 전략을 선택하는 것은 AI 엔지니어에게 필수적인 역량입니다.

> **핵심 직관**: 프로세스는 독립된 집을 가진 가족이고, 스레드는 하나의 집을 공유하는 가족 구성원입니다. 집이 분리되면 안전하지만 소통 비용이 크고, 같은 집에 살면 소통은 쉽지만 충돌 위험이 있습니다.

## 1. 프로세스 모델

운영체제에서 프로세스는 실행 중인 프로그램의 인스턴스입니다. 각 프로세스는 독립된 자원을 가집니다.

| 자원 | 프로세스 간 공유 | 스레드 간 공유 |
|------|-----------------|---------------|
| 가상 메모리 공간 | 독립 | 공유 |
| 파일 디스크립터 | 독립 | 공유 |
| 힙 메모리 | 독립 | 공유 |
| 스택 | 독립 | 독립 |
| 레지스터 | 독립 | 독립 |
| PID | 고유 | 부모 프로세스와 동일 |

```python
import os
import multiprocessing as mp

def worker(name):
    print(f"워커 {name}: PID={os.getpid()}, 부모 PID={os.getppid()}")

# 프로세스 생성 - 각각 독립된 메모리 공간
if __name__ == "__main__":
    processes = [mp.Process(target=worker, args=(i,)) for i in range(4)]
    for p in processes:
        p.start()
    for p in processes:
        p.join()
```

## 2. 스레드와 GIL

Python의 **Global Interpreter Lock(GIL)**은 한 번에 하나의 스레드만 Python 바이트코드를 실행할 수 있게 하는 뮤텍스입니다. 이는 CPU 바운드 작업에서 멀티스레딩의 성능 이점을 크게 제한합니다.

```
┌──────────────────────────────────────────────┐
│         Python GIL과 실행 모델 선택            │
├──────────────────────────────────────────────┤
│                                              │
│        작업 유형 판별                          │
│            │                                 │
│     ┌──────┴──────┐                          │
│     ▼             ▼                          │
│  CPU 바운드    I/O 바운드                      │
│     │             │                          │
│     ▼             ▼                          │
│  ┌──────┐    ┌──────────┐                    │
│  │multi-│    │threading │                    │
│  │process│   │또는 asyncio│                   │
│  └──┬───┘    └────┬─────┘                    │
│     │             │                          │
│     ▼             ▼                          │
│  GIL 우회     GIL 해제 구간                    │
│  (별도 인터프리터) (I/O 대기 중)                │
│                                              │
└──────────────────────────────────────────────┘
```

> **핵심 직관**: GIL은 CPU 바운드 작업에서만 병목입니다. I/O 대기 중에는 GIL이 해제되므로, 네트워크 요청이나 파일 읽기 같은 I/O 바운드 작업에서는 멀티스레딩이 효과적입니다.

```python
import threading
import time

# I/O 바운드 - 스레딩이 효과적
def io_bound_task(url_id):
    import urllib.request
    # GIL은 I/O 대기 중 해제됨
    time.sleep(0.1)  # 네트워크 요청 시뮬레이션
    return url_id

# CPU 바운드 - multiprocessing 사용 권장
def cpu_bound_task(n):
    return sum(i * i for i in range(n))
```

## 3. 컨텍스트 스위칭

운영체제가 실행 중인 프로세스나 스레드를 전환할 때 **컨텍스트 스위칭**이 발생합니다. 이 비용은 무시할 수 없습니다.

| 전환 유형 | 대략적 비용 | 주요 오버헤드 |
|-----------|-----------|-------------|
| 스레드 간 전환 | 1-10 us | 레지스터 저장/복원, 스택 전환 |
| 프로세스 간 전환 | 10-100 us | TLB 플러시, 페이지 테이블 전환 |
| 가상머신 전환 | 100+ us | 전체 머신 상태 저장/복원 |

```python
# 시나리오: 과도한 스레드 생성으로 인한 컨텍스트 스위칭 오버헤드
import concurrent.futures

# 나쁜 예: 10,000개의 스레드 생성
# with concurrent.futures.ThreadPoolExecutor(max_workers=10000) as e:
#     futures = [e.submit(task) for _ in range(10000)]

# 좋은 예: CPU 코어 수에 맞춘 워커 풀
with concurrent.futures.ThreadPoolExecutor(max_workers=os.cpu_count()) as e:
    futures = [e.submit(io_bound_task, i) for i in range(100)]
```

## 4. 동기화 원시 타입

공유 자원에 대한 동시 접근을 안전하게 관리하기 위한 동기화 메커니즘이 필요합니다(py-07 동시성 프로그래밍 참조).

| 원시 타입 | 용도 | Python 예시 |
|-----------|------|------------|
| Mutex/Lock | 상호 배제 | `threading.Lock()` |
| Semaphore | 제한된 동시 접근 | `threading.Semaphore(n)` |
| Event | 스레드 간 신호 전달 | `threading.Event()` |
| Condition | 조건부 대기/통지 | `threading.Condition()` |
| Barrier | 동기화 지점 | `threading.Barrier(n)` |

```python
import threading

# Lock을 사용한 안전한 공유 자원 접근
counter = 0
lock = threading.Lock()

def safe_increment(n):
    global counter
    for _ in range(n):
        with lock:  # 컨텍스트 매니저로 자동 해제 보장
            counter += 1

threads = [threading.Thread(target=safe_increment, args=(100000,)) for _ in range(4)]
for t in threads:
    t.start()
for t in threads:
    t.join()
print(f"최종 카운터: {counter}")  # 정확히 400000
```

## 5. 프로세스 간 통신 (IPC)

멀티프로세싱 환경에서 프로세스 간 데이터를 교환하는 메커니즘입니다(mo-05 분산 처리 참조).

```python
import multiprocessing as mp
import numpy as np

# 공유 메모리를 통한 제로카피 데이터 공유
def process_chunk(shm_name, shape, dtype, start, end):
    from multiprocessing import shared_memory
    shm = shared_memory.SharedMemory(name=shm_name)
    arr = np.ndarray(shape, dtype=dtype, buffer=shm.buf)
    arr[start:end] *= 2  # 직접 수정 - 복사 없음
    shm.close()

# 대규모 배열을 공유 메모리에 할당
data = np.ones(1_000_000, dtype=np.float64)
shm = mp.shared_memory.SharedMemory(create=True, size=data.nbytes)
shared_arr = np.ndarray(data.shape, dtype=data.dtype, buffer=shm.buf)
shared_arr[:] = data[:]  # 초기 복사 1회
```

## 6. 실전 시나리오

**시나리오 1: 데이터 로딩 파이프라인**

PyTorch의 `DataLoader`는 `num_workers` 파라미터로 데이터 전처리를 별도 프로세스에서 수행합니다. CPU 바운드인 이미지 디코딩과 증강을 멀티프로세싱으로 병렬화하여 GPU가 쉬지 않게 데이터를 공급합니다(dl-03 학습 파이프라인 참조).

**시나리오 2: 모델 서빙에서의 스레드 풀**

FastAPI 기반 추론 서버에서 동시 요청을 처리할 때, I/O 바운드인 요청 수신은 asyncio로, CPU 바운드인 모델 추론은 프로세스 풀로 분리하면 최적의 처리량을 달성할 수 있습니다(se-06 API 설계 참조).

```python
# FastAPI + ProcessPoolExecutor 조합 예시
from concurrent.futures import ProcessPoolExecutor

pool = ProcessPoolExecutor(max_workers=4)

async def predict(request):
    loop = asyncio.get_event_loop()
    result = await loop.run_in_executor(pool, model_inference, request.data)
    return result
```

> **핵심 직관**: AI 시스템에서 최적의 병렬화 전략은 하나가 아닙니다. 데이터 로딩은 멀티프로세싱, API 서빙은 asyncio, GPU 연산은 CUDA 스트림 등 작업 특성에 따라 적절한 도구를 조합해야 합니다.

## 핵심 정리

- 프로세스는 독립된 메모리 공간을 가져 안전하지만 통신 비용이 크고, 스레드는 메모리를 공유하여 효율적이지만 동기화가 필요합니다
- Python의 GIL은 CPU 바운드 멀티스레딩을 제한하므로, CPU 집약 작업에는 multiprocessing을, I/O 집약 작업에는 threading이나 asyncio를 사용해야 합니다
- 컨텍스트 스위칭 비용은 스레드 간 1-10us, 프로세스 간 10-100us이므로, 워커 수는 하드웨어 자원에 맞게 제한해야 합니다
- Lock, Semaphore, Event 등의 동기화 원시 타입은 공유 자원의 경쟁 조건을 방지하며, Python의 컨텍스트 매니저와 함께 사용하면 안전합니다
- 실전 AI 시스템에서는 데이터 로딩(멀티프로세싱), 서빙(asyncio), 연산(GPU 병렬)을 적절히 조합하여 전체 파이프라인의 처리량을 극대화해야 합니다
