# 메모리 계층과 캐시

## 왜 메모리 계층이 중요한가

AI 모델의 학습과 추론 속도는 연산 능력만으로 결정되지 않습니다. 실제 병목은 **데이터가 연산 유닛에 얼마나 빠르게 도달하느냐**에 있습니다. CPU가 아무리 빨라도 메모리에서 데이터를 가져오는 데 수백 사이클이 걸린다면, 프로세서는 대부분의 시간을 기다리며 낭비하게 됩니다. 메모리 계층을 이해하면 동일한 하드웨어에서 10배 이상의 성능 차이를 만들어낼 수 있습니다.

> **핵심 직관**: 프로세서 속도는 매년 급격히 증가했지만, 메모리 접근 속도는 그에 비해 더디게 개선되었습니다. 이 간극을 "Memory Wall"이라 부르며, 캐시는 이 벽을 넘기 위한 핵심 전략입니다.

## 1. 메모리 계층 구조

현대 컴퓨터의 메모리는 속도와 용량의 트레이드오프에 따라 계층적으로 구성됩니다.

| 계층 | 용량 | 지연 시간 (대략) | 대역폭 | 사이클 |
|------|------|-----------------|--------|--------|
| 레지스터 | ~1 KB | ~0.3 ns | - | 1 |
| L1 캐시 | 32-64 KB | ~1 ns | ~1 TB/s | 3-4 |
| L2 캐시 | 256 KB-1 MB | ~3-5 ns | ~500 GB/s | 10-12 |
| L3 캐시 | 8-64 MB | ~10-20 ns | ~200 GB/s | 30-40 |
| DRAM | 16-512 GB | ~50-100 ns | ~50 GB/s | 100-200 |
| SSD | TB급 | ~10-100 us | ~5 GB/s | 10,000+ |

> **핵심 직관**: L1 캐시에서 데이터를 읽는 것과 DRAM에서 읽는 것은 약 100배의 지연 시간 차이가 납니다. 이것은 책상 위 메모(L1)와 도서관까지 걸어가는 것(DRAM)의 차이와 같습니다.

## 2. 캐시 동작 원리

캐시는 두 가지 핵심 원리에 기반합니다.

- **시간적 지역성(Temporal Locality)**: 최근 접근한 데이터는 곧 다시 접근될 가능성이 높습니다
- **공간적 지역성(Spatial Locality)**: 접근한 데이터 근처의 데이터도 곧 접근될 가능성이 높습니다

```
┌─────────────────────────────────────────┐
│           캐시 히트/미스 판정 흐름         │
├─────────────────────────────────────────┤
│                                         │
│   CPU가 주소 요청                        │
│        │                                │
│        ▼                                │
│   ┌─────────┐    히트    ┌──────────┐   │
│   │ L1 확인  │──────────▶│ 데이터 반환│   │
│   └────┬────┘           └──────────┘   │
│        │ 미스                           │
│        ▼                                │
│   ┌─────────┐    히트    ┌──────────┐   │
│   │ L2 확인  │──────────▶│ L1에 적재 │   │
│   └────┬────┘           └──────────┘   │
│        │ 미스                           │
│        ▼                                │
│   ┌─────────┐    히트    ┌──────────┐   │
│   │ L3 확인  │──────────▶│ L2에 적재 │   │
│   └────┬────┘           └──────────┘   │
│        │ 미스                           │
│        ▼                                │
│   ┌─────────┐           ┌──────────┐   │
│   │ DRAM 접근│──────────▶│ L3에 적재 │   │
│   └─────────┘           └──────────┘   │
└─────────────────────────────────────────┘
```

## 3. 캐시 라인과 캐시 친화적 코드

캐시는 개별 바이트가 아니라 **캐시 라인**(보통 64바이트) 단위로 데이터를 가져옵니다. 이 사실이 코드 성능에 결정적인 영향을 미칩니다.

```python
import numpy as np
import time

# 시나리오: 행 우선 vs 열 우선 순회
n = 10000
matrix = np.random.rand(n, n)

# 행 우선 순회 (캐시 친화적 - C order)
start = time.perf_counter()
row_sum = np.sum(matrix, axis=1)  # 연속된 메모리 접근
row_time = time.perf_counter() - start

# 열 우선 순회 (캐시 비친화적)
start = time.perf_counter()
col_sum = np.sum(matrix, axis=0)  # 불연속 메모리 접근 (stride가 큼)
col_time = time.perf_counter() - start

print(f"행 우선: {row_time:.4f}s, 열 우선: {col_time:.4f}s")
```

## 4. NumPy 메모리 레이아웃

NumPy 배열은 두 가지 메모리 레이아웃을 지원합니다. 이는 수치 계산 성능에 직접적인 영향을 미칩니다(nm-01 수치 계산 기초 참조).

| 속성 | C order (행 우선) | Fortran order (열 우선) |
|------|-------------------|------------------------|
| NumPy 기본값 | 예 | 아니오 |
| 행 연산 최적 | 예 | 아니오 |
| 열 연산 최적 | 아니오 | 예 |
| BLAS 호환 | 변환 필요할 수 있음 | 직접 호환 |

```python
import numpy as np

# 메모리 레이아웃 확인
a = np.array([[1, 2, 3], [4, 5, 6]], order='C')
b = np.array([[1, 2, 3], [4, 5, 6]], order='F')

print(a.strides)  # (24, 8) - 행 간 24바이트, 열 간 8바이트
print(b.strides)  # (8, 16) - 행 간 8바이트, 열 간 16바이트

# 실전: 딥러닝 텐서는 보통 C order (py-05 NumPy 고급 참조)
# PyTorch: row-major (C order) 기본
# TensorFlow: row-major (C order) 기본
```

## 5. 캐시 오염과 프리페칭

대용량 데이터를 순회할 때 캐시가 불필요한 데이터로 채워지는 **캐시 오염** 문제가 발생합니다.

```python
import ctypes

# 시나리오: 대규모 모델 추론에서 캐시 효율 높이기
# ctypes로 시스템 수준 메모리 힌트 제공 (리눅스)
libc = ctypes.CDLL("libc.so.6")

# madvise를 통한 커널 힌트
MADV_SEQUENTIAL = 2  # 순차 접근 패턴 힌트
MADV_WILLNEED = 3    # 곧 필요한 데이터 프리페치 요청

# 실전에서는 numpy의 memmap과 조합하여 사용
data = np.memmap('large_weights.bin', dtype='float32', mode='r')
# 운영체제가 순차 접근을 예측하여 프리페치 최적화
```

> **핵심 직관**: 캐시를 직접 제어할 수는 없지만, 데이터 접근 패턴을 제어하면 캐시가 우리 편에서 일하게 만들 수 있습니다. 핵심은 **순차적이고 예측 가능한 접근 패턴**을 설계하는 것입니다.

## 6. 실전 시나리오

**시나리오 1: 대규모 임베딩 테이블 조회**

추천 시스템에서 수백만 개의 임베딩 벡터를 조회할 때, 임베딩 테이블이 L3 캐시보다 크면 거의 매번 캐시 미스가 발생합니다. 해결 전략으로는 자주 접근되는 임베딩을 연속된 메모리에 배치하거나, 임베딩 차원을 줄여 캐시 라인에 맞추는 방법이 있습니다(dl-04 임베딩 참조).

**시나리오 2: 배치 처리에서의 메모리 정렬**

ML 파이프라인에서 배치 크기를 캐시 라인 크기의 배수로 설정하면, 불필요한 캐시 라인 분할(cache line split)을 방지하여 데이터 로딩 속도를 높일 수 있습니다(mo-03 파이프라인 설계 참조).

```python
# 메모리 정렬된 배열 할당
aligned = np.empty(1024, dtype=np.float32)
print(f"주소 64바이트 정렬 여부: {aligned.ctypes.data % 64 == 0}")
```

## 핵심 정리

- 메모리 계층은 레지스터 → L1 → L2 → L3 → DRAM → SSD 순으로 용량은 커지고 속도는 느려지며, 각 단계마다 약 10배의 지연 시간 차이가 존재합니다
- 캐시는 64바이트 캐시 라인 단위로 동작하며, 시간적 지역성과 공간적 지역성을 활용하여 메모리 접근 속도를 높입니다
- NumPy의 C order(행 우선) 레이아웃에서 행 방향 연산이 열 방향보다 빠른 이유는 연속된 메모리 접근이 캐시 히트율을 높이기 때문입니다
- 캐시 친화적 코드의 핵심은 순차적이고 예측 가능한 데이터 접근 패턴을 설계하는 것이며, 이는 AI 학습/추론 성능에 직접적인 영향을 줍니다
- 프리페칭, 메모리 정렬, 데이터 레이아웃 최적화는 동일한 하드웨어에서 수배의 성능 향상을 가져올 수 있는 시스템 수준의 최적화 기법입니다
