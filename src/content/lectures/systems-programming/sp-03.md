# 비동기 I/O와 이벤트 루프

## 왜 비동기 I/O가 중요한가

AI 서비스는 수천 개의 동시 요청을 처리해야 합니다. 각 요청마다 스레드를 생성하면 메모리와 컨텍스트 스위칭 비용이 기하급수적으로 증가합니다. **비동기 I/O**는 단일 스레드에서 수만 개의 동시 연결을 처리할 수 있게 해주며, 이는 현대 AI 서빙 인프라의 핵심 기반입니다.

> **핵심 직관**: 식당에서 웨이터가 한 테이블의 음식이 나올 때까지 기다리지 않고 다른 테이블 주문을 받는 것이 비동기 I/O입니다. 한 명의 웨이터(단일 스레드)가 수십 개의 테이블(연결)을 효율적으로 관리할 수 있습니다.

## 1. 동기 vs 비동기 I/O 모델

| 모델 | 동작 방식 | 동시 연결 | 메모리/연결 | 적합한 작업 |
|------|----------|----------|------------|-----------|
| 블로킹 I/O | 요청 완료까지 대기 | 스레드 수에 제한 | ~1 MB (스택) | 단순 스크립트 |
| 논블로킹 폴링 | 반복 확인 | 제한적 | 낮음 | 레거시 시스템 |
| I/O 멀티플렉싱 | OS가 준비 알림 | 수만 개 | ~수 KB | 서버 애플리케이션 |
| 비동기 I/O (AIO) | 커널 완료 통지 | 수만 개 | ~수 KB | 고성능 서버 |

## 2. epoll과 kqueue

운영체제 수준의 I/O 멀티플렉싱 메커니즘은 비동기 프레임워크의 기반입니다.

```
┌───────────────────────────────────────────┐
│       이벤트 루프 동작 흐름 (단일 스레드)     │
├───────────────────────────────────────────┤
│                                           │
│  ┌─────────────┐                          │
│  │ 이벤트 대기   │◀─────────────────┐      │
│  │ (epoll_wait) │                  │      │
│  └──────┬──────┘                  │      │
│         │ 이벤트 발생               │      │
│         ▼                         │      │
│  ┌─────────────┐                  │      │
│  │ 준비된 fd 목록│                  │      │
│  │ 순회 처리     │                  │      │
│  └──────┬──────┘                  │      │
│         │                         │      │
│    ┌────┴────┐                    │      │
│    ▼         ▼                    │      │
│  읽기 가능  쓰기 가능               │      │
│    │         │                    │      │
│    ▼         ▼                    │      │
│  콜백 실행  콜백 실행               │      │
│    │         │                    │      │
│    └────┬────┘                    │      │
│         └─────────────────────────┘      │
│                                           │
└───────────────────────────────────────────┘
```

```python
import select
import socket

# 저수준 epoll 사용 예시 (리눅스)
server = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
server.setblocking(False)
server.bind(('0.0.0.0', 8080))
server.listen()

# macOS에서는 kqueue, 리눅스에서는 epoll
# Python의 selectors 모듈이 플랫폼별 최적 구현을 자동 선택
import selectors
sel = selectors.DefaultSelector()  # epoll/kqueue 자동 선택
sel.register(server, selectors.EVENT_READ, data=None)
```

## 3. asyncio 내부 구조

Python의 asyncio는 이벤트 루프 위에 구축된 고수준 비동기 프레임워크입니다(py-07 동시성 프로그래밍 참조).

```python
import asyncio

# 코루틴 정의 - async def는 코루틴 함수를 만듭니다
async def fetch_prediction(session, model_id):
    """모델 서버에서 예측 결과를 비동기로 가져옵니다"""
    async with session.get(f'/predict/{model_id}') as resp:
        return await resp.json()

# 다수의 모델 서버에 동시 요청
async def ensemble_predict(data):
    import aiohttp
    async with aiohttp.ClientSession() as session:
        tasks = [fetch_prediction(session, mid) for mid in range(10)]
        results = await asyncio.gather(*tasks)  # 10개 동시 실행
        return aggregate(results)
```

> **핵심 직관**: `await`는 "여기서 I/O 대기가 발생하니, 다른 코루틴에게 실행을 양보하겠다"는 선언입니다. 이벤트 루프는 이 양보 지점들을 활용하여 단일 스레드에서 수천 개의 작업을 동시에 진행합니다.

## 4. 코루틴 스케줄링

asyncio의 스케줄러는 협력적(cooperative) 방식입니다. 코루틴이 스스로 제어권을 반환해야 합니다.

```python
import asyncio
import time

# 나쁜 예: CPU 바운드 작업이 이벤트 루프를 블로킹
async def bad_cpu_work():
    # 이 동안 다른 코루틴이 전혀 실행되지 않음!
    result = sum(i * i for i in range(10_000_000))
    return result

# 좋은 예: CPU 바운드 작업을 별도 스레드/프로세스로 분리
async def good_cpu_work():
    loop = asyncio.get_event_loop()
    result = await loop.run_in_executor(None, heavy_computation)
    return result

# 좋은 예: 주기적으로 제어권 반환
async def cooperative_work():
    result = 0
    for i in range(10_000_000):
        result += i * i
        if i % 100_000 == 0:
            await asyncio.sleep(0)  # 제어권 반환
    return result
```

## 5. 비동기 패턴

실전에서 자주 사용되는 비동기 패턴들입니다.

```python
import asyncio

# 패턴 1: 생산자-소비자 (데이터 파이프라인)
async def producer(queue):
    for batch in load_batches():
        await queue.put(batch)
    await queue.put(None)  # 종료 신호

async def consumer(queue):
    while True:
        batch = await queue.get()
        if batch is None:
            break
        await process_batch(batch)

# 패턴 2: 세마포어로 동시성 제한 (API rate limiting)
sem = asyncio.Semaphore(10)  # 동시 10개 제한

async def rate_limited_request(url):
    async with sem:
        return await fetch(url)

# 패턴 3: 타임아웃 처리 (모델 추론 타임아웃)
async def predict_with_timeout(data, timeout=5.0):
    try:
        return await asyncio.wait_for(model.predict(data), timeout=timeout)
    except asyncio.TimeoutError:
        return fallback_prediction(data)
```

## 6. 실전 시나리오

**시나리오 1: AI 게이트웨이 서버**

여러 모델 서버에 동시에 요청을 보내고 결과를 집계하는 앙상블 게이트웨이에서, asyncio를 사용하면 단일 프로세스로 수천 개의 동시 클라이언트 요청을 처리할 수 있습니다(se-06 API 설계, mo-04 서빙 인프라 참조).

**시나리오 2: 실시간 스트리밍 추론**

LLM의 토큰 스트리밍 응답을 구현할 때, 비동기 제너레이터를 활용하면 여러 사용자에게 동시에 토큰을 스트리밍할 수 있습니다.

```python
async def stream_tokens(prompt):
    """LLM 토큰 스트리밍 - 비동기 제너레이터"""
    async for token in model.generate_stream(prompt):
        yield token
        # 각 yield에서 이벤트 루프가 다른 클라이언트도 처리

# FastAPI SSE 엔드포인트
from fastapi.responses import StreamingResponse

async def sse_endpoint(request):
    return StreamingResponse(
        stream_tokens(request.prompt),
        media_type="text/event-stream"
    )
```

> **핵심 직관**: 비동기 I/O의 진정한 가치는 "빠르게 하는 것"이 아니라 "효율적으로 기다리는 것"입니다. CPU가 I/O를 기다리는 유휴 시간을 다른 작업에 활용하여 전체 시스템의 처리량을 극대화합니다.

## 핵심 정리

- 비동기 I/O는 단일 스레드에서 수만 개의 동시 연결을 처리할 수 있으며, 이는 epoll(리눅스)/kqueue(macOS) 같은 OS 수준 I/O 멀티플렉싱에 기반합니다
- asyncio의 이벤트 루프는 협력적 스케줄링으로 동작하므로, CPU 바운드 작업은 반드시 `run_in_executor`로 분리하여 이벤트 루프 블로킹을 방지해야 합니다
- `await` 키워드는 I/O 대기 지점에서 제어권을 이벤트 루프에 반환하는 협력적 양보 지점이며, 이를 통해 동시성이 구현됩니다
- 생산자-소비자, 세마포어 기반 제한, 타임아웃 처리 등의 비동기 패턴은 AI 서빙 시스템에서 안정성과 성능을 동시에 확보하는 핵심 도구입니다
- 비동기 프로그래밍의 본질은 속도 향상이 아니라 자원 효율성이며, I/O 대기 시간을 유용한 작업으로 채워 전체 시스템의 처리량(throughput)을 극대화하는 것입니다
