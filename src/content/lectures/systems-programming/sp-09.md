# 컨테이너와 가상화

## 왜 컨테이너와 가상화가 중요한가

AI 모델은 개발 환경에서 잘 작동하지만, 프로덕션에 배포하면 "제 컴퓨터에서는 됐는데요"라는 문제가 빈번합니다. CUDA 버전, Python 패키지 의존성, 시스템 라이브러리 차이 등이 원인입니다. 컨테이너는 이 문제를 근본적으로 해결하며, AI/ML 워크로드의 재현 가능한 실행 환경을 보장합니다.

> **핵심 직관**: 가상머신이 "건물 전체를 복제"하는 것이라면, 컨테이너는 "방 하나를 칸막이로 분리"하는 것입니다. 같은 운영체제 커널을 공유하면서도 격리된 실행 환경을 제공하므로, 가상머신 대비 훨씬 가볍고 빠릅니다.

## 1. 가상화 기술 비교

| 특성 | 가상머신 (VM) | 컨테이너 | 서버리스 |
|------|-------------|---------|---------|
| 격리 수준 | 완전 (하이퍼바이저) | 프로세스 수준 | 함수 수준 |
| 시작 시간 | 분 단위 | 초 단위 | 밀리초 단위 |
| 이미지 크기 | GB 단위 | MB-GB | MB 이하 |
| 오버헤드 | 높음 (게스트 OS) | 낮음 | 매우 낮음 |
| GPU 지원 | 제한적 (패스스루) | 네이티브 (NVIDIA Container Toolkit) | 제한적 |
| 적합한 용도 | 강한 격리 필요 | 마이크로서비스, ML 서빙 | 간헐적 추론 |

## 2. 리눅스 네임스페이스

컨테이너의 격리는 리눅스 커널의 **네임스페이스**로 구현됩니다. 각 네임스페이스는 시스템 자원의 독립된 뷰를 제공합니다.

| 네임스페이스 | 격리 대상 | 효과 |
|------------|----------|------|
| PID | 프로세스 ID | 컨테이너 내 PID 1부터 시작 |
| NET | 네트워크 스택 | 독립된 IP, 포트, 라우팅 |
| MNT | 파일 시스템 마운트 | 독립된 파일 시스템 뷰 |
| UTS | 호스트명, 도메인명 | 독립된 호스트 이름 |
| IPC | 프로세스 간 통신 | 독립된 세마포어, 메시지 큐 |
| USER | 사용자, 그룹 ID | 컨테이너 내 root 매핑 |

```python
# 네임스페이스를 직접 생성하는 저수준 예시 (리눅스)
import ctypes
import os

CLONE_NEWPID = 0x20000000
CLONE_NEWNS  = 0x00020000
CLONE_NEWNET = 0x40000000

libc = ctypes.CDLL("libc.so.6", use_errno=True)

# unshare: 현재 프로세스에 새 네임스페이스 적용
# 이것이 docker run의 내부에서 일어나는 일의 핵심입니다
# libc.unshare(CLONE_NEWPID | CLONE_NEWNS)
```

## 3. cgroups (Control Groups)

cgroups는 프로세스 그룹의 **자원 사용량을 제한**합니다.

```
┌───────────────────────────────────────────┐
│        cgroups 자원 제어 구조               │
├───────────────────────────────────────────┤
│                                           │
│  cgroup 계층                              │
│  ├── /sys/fs/cgroup/                      │
│  │   ├── cpu/                             │
│  │   │   └── docker/container_id/         │
│  │   │       ├── cpu.shares    (CPU 비율) │
│  │   │       └── cpu.cfs_quota (CPU 상한) │
│  │   ├── memory/                          │
│  │   │   └── docker/container_id/         │
│  │   │       ├── memory.limit  (메모리 상한)│
│  │   │       └── memory.usage  (현재 사용) │
│  │   └── devices/                         │
│  │       └── docker/container_id/         │
│  │           └── devices.allow (GPU 접근) │
│  │                                        │
│  │  GPU 컨테이너:                          │
│  │  devices.allow = "c 195:* rwm"         │
│  │  (NVIDIA 디바이스 접근 허용)             │
│  │                                        │
└───────────────────────────────────────────┘
```

```bash
# Docker에서 cgroup 제한 설정 예시
# docker run --cpus=4 --memory=16g --gpus '"device=0,1"' my-training-image
```

## 4. Docker 내부 동작

Docker 이미지는 **레이어 파일 시스템**으로 구성됩니다(mo-05 배포 자동화 참조).

```dockerfile
# AI 추론 서버 Dockerfile (멀티스테이지 빌드)
# 스테이지 1: 빌드 환경
FROM python:3.11-slim AS builder
WORKDIR /app
COPY requirements.txt .
RUN pip install --no-cache-dir --prefix=/install -r requirements.txt

# 스테이지 2: 실행 환경 (최소 이미지)
FROM nvidia/cuda:12.1-runtime-ubuntu22.04
COPY --from=builder /install /usr/local
COPY model/ /app/model/
COPY serve.py /app/

# 비root 사용자로 실행 (보안)
RUN useradd -m appuser
USER appuser

EXPOSE 8080
CMD ["python", "/app/serve.py"]
```

```python
# Docker SDK로 컨테이너 프로그래매틱 제어
import docker

client = docker.from_env()

# GPU 컨테이너 실행
container = client.containers.run(
    "my-inference-server:v2",
    detach=True,
    ports={'8080/tcp': 8080},
    device_requests=[
        docker.types.DeviceRequest(
            count=1,  # GPU 1개
            capabilities=[['gpu']]
        )
    ],
    mem_limit='16g',
    cpus=4.0,
    environment={
        'MODEL_PATH': '/app/model/weights.bin',
        'BATCH_SIZE': '32'
    }
)
print(f"컨테이너 ID: {container.short_id}")
```

> **핵심 직관**: Docker 이미지의 레이어 시스템은 캐시의 원리와 같습니다. 변경이 없는 하위 레이어(OS, 기본 패키지)는 재사용하고, 변경된 상위 레이어(모델 코드)만 다시 빌드합니다. Dockerfile에서 자주 변경되는 내용을 아래에 배치하는 것이 빌드 시간 최적화의 핵심입니다.

## 5. OCI 표준과 컨테이너 런타임

| 컴포넌트 | 역할 | 구현체 |
|---------|------|--------|
| OCI Image Spec | 이미지 포맷 표준 | Docker, Podman |
| OCI Runtime Spec | 실행 표준 | runc, crun, gVisor |
| OCI Distribution Spec | 레지스트리 API | Docker Hub, ECR, GCR |

```
┌───────────────────────────────────────────┐
│        컨테이너 실행 스택                    │
├───────────────────────────────────────────┤
│                                           │
│  kubectl / docker CLI                     │
│       │                                   │
│       ▼                                   │
│  kubelet / dockerd (고수준 런타임)          │
│       │                                   │
│       ▼                                   │
│  containerd / CRI-O (컨테이너 관리)        │
│       │                                   │
│       ▼                                   │
│  runc (OCI 런타임 - 실제 컨테이너 생성)     │
│       │                                   │
│       ▼                                   │
│  Linux Kernel (namespaces + cgroups)      │
│       │                                   │
│       ▼                                   │
│  NVIDIA Container Toolkit (GPU 접근)      │
│                                           │
└───────────────────────────────────────────┘
```

## 6. 실전 시나리오

**시나리오 1: ML 실험 재현성 보장**

연구팀에서 6개월 전의 실험을 재현해야 할 때, Docker 이미지에 정확한 CUDA 버전, PyTorch 버전, 커스텀 라이브러리를 모두 포함시켜 두면 `docker run` 하나로 동일한 환경을 즉시 복원할 수 있습니다. 이미지 태그에 실험 ID를 포함시키면 추적도 용이합니다(mo-06 실험 관리 참조).

**시나리오 2: GPU 자원 격리와 멀티테넌시**

여러 팀이 공유 GPU 클러스터를 사용할 때, NVIDIA MPS(Multi-Process Service)와 cgroups를 조합하여 각 팀의 GPU 메모리와 연산 자원을 격리합니다. 한 팀의 OOM(Out of Memory)이 다른 팀에 영향을 주지 않도록 보장합니다.

```python
# Kubernetes에서 GPU 리소스 요청 (YAML 개념)
pod_spec = {
    "containers": [{
        "name": "training",
        "image": "my-training:v1",
        "resources": {
            "limits": {
                "nvidia.com/gpu": 4,      # GPU 4개
                "memory": "64Gi",
                "cpu": "16"
            },
            "requests": {
                "nvidia.com/gpu": 4,
                "memory": "32Gi",
                "cpu": "8"
            }
        }
    }]
}
```

> **핵심 직관**: 컨테이너는 "실행 환경의 버전 관리"입니다. 코드를 Git으로 관리하듯, 실행 환경을 Docker 이미지로 관리하면 "어디서나 동일하게 실행"이라는 목표를 달성할 수 있습니다.

## 핵심 정리

- 컨테이너는 리눅스 네임스페이스(격리)와 cgroups(자원 제한)라는 커널 기능으로 구현되며, 가상머신 대비 수십 배 가벼운 오버헤드로 프로세스 수준 격리를 제공합니다
- Docker 이미지의 레이어 시스템은 변경되지 않은 레이어를 캐시하여 빌드와 배포 속도를 높이며, 멀티스테이지 빌드로 최종 이미지 크기를 최소화해야 합니다
- NVIDIA Container Toolkit은 GPU 디바이스를 컨테이너에 노출시키며, cgroups의 devices 서브시스템으로 GPU 접근을 제어합니다
- OCI 표준은 컨테이너 이미지와 런타임의 호환성을 보장하며, runc, containerd, kubelet 등의 스택이 계층적으로 협력합니다
- AI 워크로드에서 컨테이너는 실험 재현성, 환경 일관성, GPU 자원 격리를 동시에 달성하는 핵심 인프라이며, Kubernetes와 결합하여 대규모 클러스터 관리에 활용됩니다
