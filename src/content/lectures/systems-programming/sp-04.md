# GPU 아키텍처와 CUDA 기초

## 왜 GPU 아키텍처가 중요한가

딥러닝의 폭발적 성장은 GPU의 병렬 연산 능력에 의해 가능했습니다. 그러나 GPU를 효과적으로 활용하려면 CPU와는 근본적으로 다른 실행 모델을 이해해야 합니다. 동일한 모델도 GPU 아키텍처에 맞게 최적화하면 학습 시간을 수배에서 수십 배까지 단축할 수 있습니다.

> **핵심 직관**: CPU는 소수의 강력한 코어로 복잡한 작업을 빠르게 처리하는 "소수 정예"이고, GPU는 수천 개의 단순한 코어로 대량의 동일한 작업을 동시에 처리하는 "대규모 병력"입니다.

## 1. GPU vs CPU 아키텍처 비교

| 특성 | CPU | GPU |
|------|-----|-----|
| 코어 수 | 8-128 | 수천-수만 |
| 클럭 속도 | 3-5 GHz | 1-2 GHz |
| 캐시 크기 | 수십 MB | 수 MB |
| 메모리 대역폭 | ~50 GB/s | ~2 TB/s (HBM) |
| 분기 예측 | 정교함 | 제한적 |
| 설계 철학 | 지연 시간 최소화 | 처리량 최대화 |

## 2. CUDA 실행 모델

CUDA에서 연산은 **커널(kernel)**이라는 함수로 정의되며, 수천 개의 스레드가 동시에 실행합니다.

```
┌─────────────────────────────────────────────┐
│            CUDA 실행 계층 구조                │
├─────────────────────────────────────────────┤
│                                             │
│  Grid (커널 1회 실행)                        │
│  ┌─────────┬─────────┬─────────┐           │
│  │ Block   │ Block   │ Block   │ ...       │
│  │ (0,0)   │ (1,0)   │ (2,0)   │           │
│  ├─────────┼─────────┼─────────┤           │
│  │ Block   │ Block   │ Block   │ ...       │
│  │ (0,1)   │ (1,1)   │ (2,1)   │           │
│  └─────────┴─────────┴─────────┘           │
│                                             │
│  Block 내부:                                │
│  ┌───┬───┬───┬───┬───┬───┬───┬───┐        │
│  │ T0│ T1│ T2│...│T31│   Warp 0  │        │
│  ├───┼───┼───┼───┼───┤           │        │
│  │T32│T33│T34│...│T63│   Warp 1  │        │
│  └───┴───┴───┴───┴───┴───────────┘        │
│                                             │
└─────────────────────────────────────────────┘
```

```python
# CUDA 커널의 개념적 구조 (의사 코드)
# __global__ void vector_add(float *a, float *b, float *c, int n) {
#     int idx = blockIdx.x * blockDim.x + threadIdx.x;
#     if (idx < n) {
#         c[idx] = a[idx] + b[idx];
#     }
# }

# Python에서 PyCUDA로 동일한 작업
import numpy as np

# Numba CUDA를 사용한 실제 실행 가능 코드
from numba import cuda

@cuda.jit
def vector_add(a, b, c):
    idx = cuda.grid(1)  # 글로벌 스레드 인덱스
    if idx < a.size:
        c[idx] = a[idx] + b[idx]

n = 1_000_000
a = cuda.to_device(np.ones(n, dtype=np.float32))
b = cuda.to_device(np.ones(n, dtype=np.float32))
c = cuda.device_array(n, dtype=np.float32)

threads_per_block = 256
blocks_per_grid = (n + threads_per_block - 1) // threads_per_block
vector_add[blocks_per_grid, threads_per_block](a, b, c)
```

## 3. GPU 메모리 계층

GPU의 메모리 계층은 성능 최적화의 핵심입니다(sp-01 메모리 계층과 캐시 참조).

| 메모리 유형 | 범위 | 속도 | 용량 | 용도 |
|------------|------|------|------|------|
| 레지스터 | 스레드 | 가장 빠름 | ~256 KB/SM | 지역 변수 |
| 공유 메모리 | 블록 | 매우 빠름 | 48-164 KB/SM | 블록 내 협력 |
| L1 캐시 | SM | 빠름 | 128 KB/SM | 자동 캐싱 |
| L2 캐시 | 전체 GPU | 보통 | 6-50 MB | 전역 데이터 캐싱 |
| 글로벌 메모리 (HBM) | 전체 GPU | 느림 | 16-80 GB | 주 데이터 저장 |

```python
# 공유 메모리를 활용한 행렬 곱셈 (개념적 예시)
@cuda.jit
def matmul_shared(A, B, C):
    # 공유 메모리 선언 - 블록 내 스레드가 공유
    sA = cuda.shared.array(shape=(32, 32), dtype=np.float32)
    sB = cuda.shared.array(shape=(32, 32), dtype=np.float32)

    tx, ty = cuda.threadIdx.x, cuda.threadIdx.y
    row = cuda.blockIdx.y * 32 + ty
    col = cuda.blockIdx.x * 32 + tx

    acc = 0.0
    for tile in range(A.shape[1] // 32):
        sA[ty, tx] = A[row, tile * 32 + tx]
        sB[ty, tx] = B[tile * 32 + ty, col]
        cuda.syncthreads()  # 블록 내 동기화
        for k in range(32):
            acc += sA[ty, k] * sB[k, tx]
        cuda.syncthreads()
    C[row, col] = acc
```

## 4. Warp와 Occupancy

> **핵심 직관**: Warp는 32개의 스레드가 **같은 명령어를 동시에 실행**하는 SIMT(Single Instruction, Multiple Threads) 단위입니다. Warp 내 분기(if-else)가 발생하면, 양쪽 경로를 모두 실행하고 결과를 마스킹하는 **warp divergence**가 생겨 성능이 절반으로 떨어질 수 있습니다.

```python
# Warp divergence 예시 (피해야 할 패턴)
@cuda.jit
def bad_kernel(data, result):
    idx = cuda.grid(1)
    if idx % 2 == 0:      # Warp 내 절반이 다른 경로
        result[idx] = data[idx] * 2
    else:
        result[idx] = data[idx] + 1

# 개선: 연속된 스레드가 같은 경로를 타도록 설계
@cuda.jit
def good_kernel(data, result):
    idx = cuda.grid(1)
    warp_id = idx // 32
    if warp_id % 2 == 0:  # Warp 단위로 분기 → divergence 없음
        result[idx] = data[idx] * 2
    else:
        result[idx] = data[idx] + 1
```

**Occupancy**는 SM이 동시에 실행할 수 있는 warp 수 대비 실제 활성 warp의 비율입니다.

| Occupancy 영향 요소 | 높이는 방법 |
|--------------------|-----------|
| 레지스터 사용량 | 커널당 레지스터 수 줄이기 |
| 공유 메모리 사용량 | 타일 크기 조정 |
| 블록 크기 | 32의 배수로 설정 (128, 256, 512) |

## 5. 호스트-디바이스 데이터 전송

CPU(호스트)와 GPU(디바이스) 간 데이터 전송은 PCIe 버스를 통하며, 이것이 종종 성능 병목이 됩니다(dl-06 학습 최적화 참조).

```python
import torch

# 나쁜 패턴: 반복적인 소량 전송
for batch in dataloader:
    x = batch.cuda()     # 매번 CPU → GPU 전송
    y = model(x)
    loss = y.cpu()       # 매번 GPU → CPU 전송

# 좋은 패턴: 비동기 전송과 핀드 메모리
dataloader = DataLoader(dataset, pin_memory=True)  # DMA 전송 가능

for batch in dataloader:
    x = batch.cuda(non_blocking=True)  # 비동기 전송
    y = model(x)
    # GPU 연산과 다음 배치 전송을 오버랩
```

## 6. 실전 시나리오

**시나리오 1: 커스텀 CUDA 커널 vs PyTorch 내장 연산**

특수한 어텐션 메커니즘(예: Flash Attention)은 GPU 메모리 계층을 세밀하게 활용하는 커스텀 CUDA 커널로 구현하여 표준 어텐션 대비 2-4배 빠르고 메모리를 O(N)으로 줄였습니다(dl-05 트랜스포머 참조).

**시나리오 2: 멀티 GPU 학습에서의 메모리 관리**

A100 80GB GPU 8장으로 대규모 모델을 학습할 때, GPU 메모리 계층을 이해하면 gradient checkpointing, mixed precision, model parallelism 등의 전략을 효과적으로 조합할 수 있습니다(dl-09 분산 학습 참조).

> **핵심 직관**: GPU 프로그래밍의 핵심은 "더 많은 연산"이 아니라 "더 효율적인 메모리 접근"입니다. 연산은 풍부하고, 메모리 대역폭이 진짜 병목입니다. 이를 Memory-bound라 합니다.

## 핵심 정리

- GPU는 수천 개의 단순 코어로 대량의 동일한 연산을 병렬 처리하며, CPU와 달리 처리량(throughput) 최대화에 최적화된 아키텍처입니다
- CUDA의 실행 계층은 Grid → Block → Thread로 구성되며, 32개 스레드 단위의 Warp가 실제 하드웨어 실행 단위입니다
- GPU 메모리 계층(레지스터 → 공유 메모리 → L2 → 글로벌 메모리)을 활용한 데이터 재사용이 성능 최적화의 핵심이며, 공유 메모리를 통한 타일링 기법이 대표적입니다
- Warp divergence를 최소화하고, occupancy를 높이며, 호스트-디바이스 전송을 비동기화하는 것이 GPU 활용률을 높이는 세 가지 핵심 전략입니다
- 현대 딥러닝에서 GPU 성능 병목은 대부분 메모리 대역폭(memory-bound)이며, Flash Attention 같은 혁신은 GPU 메모리 계층에 대한 깊은 이해에서 비롯됩니다
