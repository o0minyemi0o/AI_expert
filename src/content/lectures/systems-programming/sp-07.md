# 네트워크와 RPC

## 왜 네트워크와 RPC가 중요한가

현대 AI 시스템은 단일 서버에서 동작하지 않습니다. 모델 서빙, 특성 저장소, 데이터 파이프라인, 모니터링 등 수십 개의 서비스가 네트워크를 통해 협력합니다. 네트워크 통신의 원리와 효율적인 RPC(Remote Procedure Call) 설계를 이해하는 것은 지연 시간과 처리량 모두에 직접적인 영향을 미칩니다.

> **핵심 직관**: 네트워크 호출은 함수 호출과 완전히 다릅니다. 로컬 함수 호출은 나노초 단위이지만, 네트워크 호출은 밀리초 단위입니다. 이 1,000,000배의 차이를 항상 인식하고 설계해야 합니다.

## 1. TCP와 HTTP 기본

| 프로토콜 | 계층 | 특성 | AI 시스템 사용 예 |
|---------|------|------|-----------------|
| TCP | L4 (전송) | 신뢰성 보장, 순서 보장 | 기본 연결 |
| HTTP/1.1 | L7 (응용) | 텍스트 헤더, 연결당 1요청 | REST API |
| HTTP/2 | L7 (응용) | 바이너리, 멀티플렉싱, 헤더 압축 | gRPC |
| HTTP/3 | L7 (응용) | QUIC 기반, 0-RTT | 차세대 서빙 |

```python
# TCP 소켓 프로그래밍 기본
import socket

# 간단한 추론 서버 (저수준)
def simple_inference_server():
    server = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
    server.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
    server.bind(('0.0.0.0', 9090))
    server.listen(128)  # backlog 크기

    while True:
        conn, addr = server.accept()
        data = conn.recv(4096)
        result = inference(data)  # 모델 추론
        conn.sendall(result)
        conn.close()
```

## 2. REST vs gRPC

AI 서비스 간 통신에서 가장 흔히 비교되는 두 가지 방식입니다(se-06 API 설계 참조).

| 비교 항목 | REST (JSON/HTTP) | gRPC (Protobuf/HTTP2) |
|-----------|-----------------|----------------------|
| 직렬화 | JSON (텍스트) | Protocol Buffers (바이너리) |
| 전송 | HTTP/1.1 or 2 | HTTP/2 |
| 스키마 | OpenAPI (선택) | .proto (필수) |
| 스트리밍 | SSE, WebSocket | 양방향 내장 |
| 코드 생성 | 선택적 | 자동 |
| 브라우저 지원 | 네이티브 | grpc-web 필요 |
| 지연 시간 | 보통 | 낮음 (2-10x 빠름) |
| 적합한 상황 | 외부 API, 웹 클라이언트 | 내부 마이크로서비스 |

```
┌─────────────────────────────────────────────┐
│        API 프로토콜 선택 가이드               │
├─────────────────────────────────────────────┤
│                                             │
│     클라이언트 유형?                          │
│         │                                   │
│    ┌────┴─────┐                             │
│    ▼          ▼                             │
│  브라우저    서버/내부                        │
│    │          │                             │
│    ▼          ▼                             │
│   REST      지연 시간 중요?                  │
│    │          │                             │
│    │     ┌────┴────┐                        │
│    │     ▼         ▼                        │
│    │    Yes        No                       │
│    │     │         │                        │
│    │     ▼         ▼                        │
│    │   gRPC      REST                       │
│    │     │                                  │
│    │     ▼                                  │
│    │  스트리밍 필요?                         │
│    │     │                                  │
│    │  ┌──┴──┐                               │
│    │  ▼     ▼                               │
│    │ Yes    No                              │
│    │  │     │                               │
│    │  ▼     ▼                               │
│    │ gRPC  gRPC Unary                       │
│    │ Stream                                 │
│                                             │
└─────────────────────────────────────────────┘
```

## 3. gRPC 구현

gRPC는 Protocol Buffers 기반의 고성능 RPC 프레임워크입니다(sp-06 직렬화 참조).

```protobuf
// model_service.proto
syntax = "proto3";

service ModelService {
    // 단일 추론
    rpc Predict(PredictRequest) returns (PredictResponse);
    // 스트리밍 추론 (LLM 토큰 생성)
    rpc StreamPredict(PredictRequest) returns (stream TokenResponse);
    // 양방향 스트리밍 (실시간 특성 업데이트)
    rpc UpdateFeatures(stream FeatureUpdate) returns (stream AckResponse);
}

message PredictRequest {
    repeated float features = 1;
    string model_version = 2;
}

message PredictResponse {
    repeated float predictions = 1;
    float latency_ms = 2;
}

message TokenResponse {
    string token = 1;
    bool is_finished = 2;
}
```

```python
# gRPC 서버 구현
import grpc
from concurrent import futures
import model_service_pb2_grpc as service_grpc
import model_service_pb2 as service_pb2

class ModelServicer(service_grpc.ModelServiceServicer):
    def Predict(self, request, context):
        result = model.predict(request.features)
        return service_pb2.PredictResponse(
            predictions=result.tolist(),
            latency_ms=elapsed
        )

    def StreamPredict(self, request, context):
        """LLM 토큰 스트리밍"""
        for token in model.generate(request.features):
            yield service_pb2.TokenResponse(
                token=token, is_finished=False
            )
        yield service_pb2.TokenResponse(is_finished=True)

server = grpc.server(futures.ThreadPoolExecutor(max_workers=10))
service_grpc.add_ModelServiceServicer_to_server(ModelServicer(), server)
server.add_insecure_port('[::]:50051')
server.start()
```

> **핵심 직관**: gRPC의 양방향 스트리밍은 LLM 서빙에 이상적입니다. 토큰이 생성될 때마다 클라이언트에 즉시 전송할 수 있으며, HTTP/2의 멀티플렉싱으로 하나의 연결에서 수백 개의 동시 스트림을 처리할 수 있습니다.

## 4. 로드밸런싱

다수의 추론 서버에 트래픽을 분산하는 전략입니다(mo-04 서빙 인프라 참조).

| 전략 | 동작 | 장점 | 단점 |
|------|------|------|------|
| Round Robin | 순서대로 분배 | 단순함 | 서버 부하 무시 |
| Least Connections | 연결 적은 서버 우선 | 부하 균형 | 상태 추적 필요 |
| Weighted | 가중치 기반 분배 | GPU 성능 차이 반영 | 가중치 설정 필요 |
| Consistent Hash | 해시 기반 고정 라우팅 | 캐시 친화적 | 편향 가능성 |

```python
# 간단한 가중치 기반 로드밸런서
import random

class WeightedLoadBalancer:
    def __init__(self, backends):
        # backends: [("gpu-a100:50051", 8), ("gpu-v100:50051", 4)]
        self.backends = backends
        self.weights = [w for _, w in backends]
        self.total = sum(self.weights)

    def select(self):
        """가중치에 비례하여 백엔드 선택"""
        r = random.uniform(0, self.total)
        cumulative = 0
        for (host, weight) in self.backends:
            cumulative += weight
            if r <= cumulative:
                return host
```

## 5. 서비스 디스커버리

동적으로 변하는 서비스 인스턴스를 자동으로 찾는 메커니즘입니다(mo-07 인프라 관리 참조).

```python
# Consul을 이용한 서비스 등록 및 발견 (개념적 예시)
import consul

c = consul.Consul()

# 서비스 등록
c.agent.service.register(
    name='inference-service',
    service_id='inference-1',
    address='10.0.1.5',
    port=50051,
    tags=['gpu', 'v100'],
    check=consul.Check.grpc('10.0.1.5:50051', interval='10s')
)

# 서비스 발견
_, services = c.health.service('inference-service', passing=True)
healthy_endpoints = [(s['Service']['Address'], s['Service']['Port'])
                     for s in services]
```

## 6. 실전 시나리오

**시나리오 1: 멀티모달 추론 파이프라인**

이미지 인코더, 텍스트 인코더, 크로스어텐션 모델이 각각 별도 서비스로 배포된 환경에서, gRPC 스트리밍으로 파이프라인을 구성하면 중간 결과를 바로 전달하여 전체 지연 시간을 최소화할 수 있습니다(dl-05 트랜스포머 참조).

**시나리오 2: A/B 테스트를 위한 트래픽 분배**

Consistent Hashing으로 사용자 ID 기반 라우팅을 구현하면, 동일 사용자가 항상 같은 모델 버전에 연결되어 일관된 A/B 테스트 결과를 보장할 수 있습니다(mo-06 실험 관리 참조).

```python
# gRPC 클라이언트 - 스트리밍 추론
import grpc
import model_service_pb2 as pb2
import model_service_pb2_grpc as pb2_grpc

async def stream_inference():
    async with grpc.aio.insecure_channel('localhost:50051') as channel:
        stub = pb2_grpc.ModelServiceStub(channel)
        request = pb2.PredictRequest(features=[0.1, 0.2], model_version="v2")

        async for response in stub.StreamPredict(request):
            if response.is_finished:
                break
            print(response.token, end='', flush=True)
```

> **핵심 직관**: AI 시스템의 네트워크 설계에서 가장 중요한 원칙은 "네트워크 호출 횟수를 최소화"하는 것입니다. 배치 요청, 연결 풀링, 멀티플렉싱은 모두 이 원칙의 실현 방법입니다.

## 핵심 정리

- REST는 외부 클라이언트와 브라우저 호환에, gRPC는 내부 마이크로서비스 간 고성능 통신에 적합하며, AI 서빙 시스템에서는 두 가지를 상황에 맞게 혼용합니다
- gRPC는 Protocol Buffers와 HTTP/2 기반으로 REST 대비 2-10배 빠른 직렬화와 멀티플렉싱을 제공하며, 양방향 스트리밍은 LLM 토큰 생성에 이상적입니다
- 로드밸런싱 전략은 GPU 서버의 성능 차이를 반영하는 가중치 기반이 AI 인프라에 적합하며, 헬스체크와 결합하여 장애 서버를 자동 제거해야 합니다
- 서비스 디스커버리는 동적으로 확장/축소되는 추론 서버를 자동으로 등록하고 발견하게 하여, 수동 설정 없이 인프라를 관리할 수 있게 합니다
- 네트워크 통신 최적화의 핵심은 호출 횟수 최소화, 배치 처리, 연결 재사용이며, 이는 AI 서비스의 p99 지연 시간을 결정짓는 핵심 요소입니다
