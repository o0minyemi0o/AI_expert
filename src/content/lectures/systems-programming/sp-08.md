# 분산 시스템 기초

## 왜 분산 시스템이 중요한가

단일 서버로는 대규모 AI 모델의 학습과 서빙을 감당할 수 없습니다. 수천 개의 GPU에 걸쳐 모델을 학습하고, 수만 QPS(queries per second)의 추론 요청을 처리하려면 분산 시스템에 대한 근본적인 이해가 필요합니다. 분산 시스템의 이론적 한계와 실용적 전략을 아는 것이 안정적인 AI 인프라 설계의 출발점입니다.

> **핵심 직관**: 분산 시스템의 핵심 난제는 "여러 기계가 하나의 기계처럼 동작하게 만드는 것"입니다. 네트워크 지연, 부분 장애, 시계 불일치라는 세 가지 근본 문제가 이를 어렵게 만듭니다.

## 1. CAP 정리

분산 시스템에서 **Consistency(일관성)**, **Availability(가용성)**, **Partition Tolerance(분할 허용성)** 세 가지를 동시에 만족시키는 것은 불가능하다는 정리입니다.

| 속성 | 의미 | 예시 |
|------|------|------|
| Consistency (C) | 모든 노드가 동일한 데이터를 봄 | 모델 버전이 모든 서버에서 동일 |
| Availability (A) | 모든 요청이 응답을 받음 | 추론 요청이 항상 결과를 반환 |
| Partition Tolerance (P) | 네트워크 분할에도 동작 | 데이터센터 간 연결 끊김 대응 |

```
┌───────────────────────────────────────────┐
│            CAP 정리와 시스템 선택            │
├───────────────────────────────────────────┤
│                                           │
│           ┌─── C ───┐                     │
│          /           \                    │
│     CP 시스템     CA 시스템                 │
│     (ZooKeeper)   (단일 노드 DB)           │
│        /               \                  │
│       P ─── AP 시스템 ─── A               │
│            (Cassandra,                    │
│             DynamoDB)                     │
│                                           │
│  네트워크 분할은 불가피 → P는 필수          │
│  → 실제 선택: CP vs AP                    │
│                                           │
│  AI 시스템에서:                            │
│  - 모델 레지스트리 → CP (일관성 우선)       │
│  - 특성 저장소 → AP (가용성 우선)          │
│  - 추론 캐시 → AP (가용성 우선)            │
│                                           │
└───────────────────────────────────────────┘
```

## 2. 합의 알고리즘

분산 노드들이 하나의 값에 동의하는 프로토콜입니다.

| 알고리즘 | 복잡도 | 장애 허용 | 사용 사례 |
|---------|--------|----------|----------|
| Paxos | 높음 | f < n/2 | 이론적 기반 |
| Raft | 중간 | f < n/2 | etcd, Consul |
| ZAB | 중간 | f < n/2 | ZooKeeper |
| PBFT | 매우 높음 | f < n/3 | 비잔틴 장애 허용 |

```python
# Raft 합의의 핵심 개념 (의사 코드)
class RaftNode:
    def __init__(self, node_id, peers):
        self.state = 'follower'  # follower, candidate, leader
        self.term = 0            # 현재 임기
        self.voted_for = None
        self.log = []            # 복제된 로그

    def on_timeout(self):
        """리더로부터 heartbeat 미수신 시"""
        self.state = 'candidate'
        self.term += 1
        votes = self.request_votes()  # 다른 노드에 투표 요청
        if votes > len(self.peers) // 2:
            self.state = 'leader'
            self.send_heartbeats()    # 주기적 heartbeat 시작

    def append_entry(self, command):
        """리더가 로그 항목을 추가하고 복제"""
        if self.state != 'leader':
            return redirect_to_leader()
        entry = LogEntry(term=self.term, command=command)
        self.log.append(entry)
        acks = self.replicate(entry)  # 팔로워에 복제
        if acks > len(self.peers) // 2:
            self.commit(entry)  # 과반수 확인 후 커밋
```

> **핵심 직관**: 합의 알고리즘의 핵심 아이디어는 "과반수(majority)가 동의하면 전체가 동의한 것으로 간주"하는 것입니다. 5개 노드 중 3개가 동의하면 나머지 2개가 장애여도 시스템은 동작합니다.

## 3. 복제 전략

데이터의 내구성과 가용성을 위해 여러 노드에 복제합니다.

| 복제 방식 | 일관성 | 지연 시간 | 데이터 손실 위험 |
|-----------|--------|----------|----------------|
| 동기 복제 | 강함 | 높음 | 매우 낮음 |
| 비동기 복제 | 약함 | 낮음 | 있음 |
| 반동기 복제 | 중간 | 중간 | 낮음 |

```python
# 시나리오: 모델 체크포인트 복제 전략
class CheckpointManager:
    def __init__(self, primary_store, replicas):
        self.primary = primary_store
        self.replicas = replicas

    async def save_checkpoint(self, model_state, sync_replicas=1):
        """
        1. 로컬(primary)에 저장
        2. sync_replicas개 복제본에 동기 복제 (내구성 보장)
        3. 나머지는 비동기 복제 (성능 유지)
        """
        await self.primary.save(model_state)

        # 동기 복제: sync_replicas개가 확인될 때까지 대기
        sync_tasks = [r.save(model_state) for r in self.replicas[:sync_replicas]]
        await asyncio.gather(*sync_tasks)

        # 비동기 복제: 나머지는 백그라운드에서
        for r in self.replicas[sync_replicas:]:
            asyncio.create_task(r.save(model_state))
```

## 4. 파티셔닝 (Sharding)

데이터를 여러 노드에 분산 저장하는 전략입니다(mo-02 데이터 파이프라인 참조).

```
┌───────────────────────────────────────────┐
│          파티셔닝 전략 비교                  │
├───────────────────────────────────────────┤
│                                           │
│  해시 파티셔닝:                            │
│  ┌───────┐  hash(key)%3=0  ┌──────────┐ │
│  │ key_a │ ──────────────▶ │ 노드 0   │ │
│  │ key_b │ ──hash(key)%3=1▶│ 노드 1   │ │
│  │ key_c │ ──hash(key)%3=2▶│ 노드 2   │ │
│  └───────┘                 └──────────┘ │
│  → 균등 분배, 범위 쿼리 어려움              │
│                                           │
│  범위 파티셔닝:                            │
│  ┌───────┐  A-H   ┌──────────┐          │
│  │ 전체   │ ─────▶ │ 노드 0   │          │
│  │ 데이터  │ I-P  ▶ │ 노드 1   │          │
│  │        │ Q-Z  ▶ │ 노드 2   │          │
│  └───────┘        └──────────┘          │
│  → 범위 쿼리 가능, 핫스팟 위험              │
│                                           │
└───────────────────────────────────────────┘
```

```python
# Consistent Hashing 구현 (특성 저장소 샤딩)
import hashlib
import bisect

class ConsistentHash:
    def __init__(self, nodes, virtual_nodes=100):
        self.ring = []
        self.node_map = {}
        for node in nodes:
            for i in range(virtual_nodes):
                key = hashlib.md5(f"{node}:{i}".encode()).hexdigest()
                hash_val = int(key, 16)
                self.ring.append(hash_val)
                self.node_map[hash_val] = node
        self.ring.sort()

    def get_node(self, key):
        hash_val = int(hashlib.md5(key.encode()).hexdigest(), 16)
        idx = bisect.bisect_right(self.ring, hash_val) % len(self.ring)
        return self.node_map[self.ring[idx]]
```

## 5. 일관성 모델

분산 시스템에서 클라이언트가 관찰하는 데이터 일관성의 수준입니다.

| 일관성 모델 | 보장 수준 | 성능 | AI 시스템 적용 |
|------------|----------|------|--------------|
| 강한 일관성 | 최신 데이터 보장 | 낮음 | 모델 배포 상태 관리 |
| 인과적 일관성 | 인과 관계 순서 보장 | 중간 | 실험 추적 |
| 결과적 일관성 | 최종적으로 동일해짐 | 높음 | 특성 캐시, 로그 |
| 읽기 후 쓰기 일관성 | 자신의 쓰기를 읽을 수 있음 | 중간 | 사용자 설정 |

## 6. 실전 시나리오

**시나리오 1: 분산 특성 저장소 설계**

실시간 추천 시스템에서 수십억 개의 사용자-아이템 특성을 저장하고 밀리초 이내에 조회해야 합니다. Consistent Hashing으로 데이터를 샤딩하고, 결과적 일관성 모델을 채택하며, 각 샤드에 3개의 복제본을 유지합니다. 쓰기는 1개 복제본 확인 후 응답하고(W=1), 읽기는 1개에서 읽되(R=1), W+R > N(복제본 수)을 만족하지 않으므로 결과적 일관성입니다(dl-04 임베딩 참조).

**시나리오 2: 학습 체크포인트 관리**

수백 개의 GPU에서 분산 학습 중 체크포인트를 저장할 때, 리더 노드가 반동기 복제로 최소 2개 스토리지 노드에 동기 복사한 후 나머지는 비동기로 처리합니다. 이를 통해 체크포인트 저장 시간을 최소화하면서도 데이터 손실 위험을 줄입니다(dl-09 분산 학습 참조).

```python
# 분산 카운터 - CRDT (Conflict-free Replicated Data Type)
class GCounter:
    """충돌 없는 분산 증가 카운터"""
    def __init__(self, node_id, num_nodes):
        self.node_id = node_id
        self.counts = [0] * num_nodes

    def increment(self):
        self.counts[self.node_id] += 1

    def value(self):
        return sum(self.counts)

    def merge(self, other):
        """동기화: 각 노드의 최대값 취함 → 충돌 불가"""
        for i in range(len(self.counts)):
            self.counts[i] = max(self.counts[i], other.counts[i])
```

> **핵심 직관**: 분산 시스템에서 "완벽한 일관성"을 추구하면 성능과 가용성을 희생합니다. 실전에서는 각 컴포넌트에 필요한 최소한의 일관성 수준을 파악하고, 그에 맞는 전략을 선택하는 것이 핵심입니다.

## 핵심 정리

- CAP 정리에 의해 네트워크 분할 상황에서 일관성과 가용성 중 하나를 선택해야 하며, AI 시스템에서는 컴포넌트별로 CP 또는 AP를 전략적으로 결정합니다
- Raft 같은 합의 알고리즘은 과반수 동의를 통해 분산 환경에서 단일 진실(single source of truth)을 유지하며, 리더 선출과 로그 복제가 핵심 메커니즘입니다
- 복제 전략은 동기/비동기/반동기 중 선택하며, 데이터 중요도에 따라 체크포인트는 반동기, 로그는 비동기로 차등 적용합니다
- Consistent Hashing은 노드 추가/제거 시 최소한의 데이터만 재배치하여 샤딩의 유연성을 보장하며, 특성 저장소나 캐시 분산에 적합합니다
- 일관성 모델은 강한 일관성부터 결과적 일관성까지 스펙트럼이 있으며, 각 서비스의 요구사항에 맞는 최소한의 일관성 수준을 선택하는 것이 성능과 정확성의 균형점입니다
