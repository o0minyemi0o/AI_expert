# 직렬화와 데이터 포맷

## 왜 직렬화와 데이터 포맷이 중요한가

AI 시스템에서 데이터는 끊임없이 변환되고 전송됩니다. 모델 가중치 저장, 학습 데이터 로딩, 마이크로서비스 간 통신 등 모든 과정에서 직렬화가 발생합니다. 잘못된 포맷 선택은 I/O 병목을 만들고, 올바른 선택은 파이프라인 전체 성능을 수배 향상시킵니다.

> **핵심 직관**: 데이터를 책에 비유하면, JSON은 "누구나 읽을 수 있는 일반 책"이고, Protocol Buffers는 "속기로 쓴 노트"이며, Arrow는 "이미 정리된 칠판 위의 표"입니다. 용도에 따라 최적의 형태가 다릅니다.

## 1. 직렬화 포맷 비교

| 포맷 | 유형 | 사람 가독성 | 크기 | 속도 | 스키마 | 주요 용도 |
|------|------|-----------|------|------|--------|----------|
| JSON | 텍스트 | 예 | 큼 | 느림 | 없음 | API 응답 |
| CSV | 텍스트 | 예 | 중간 | 느림 | 없음 | 표 형식 데이터 |
| Protocol Buffers | 바이너리 | 아니오 | 작음 | 빠름 | 필수 | RPC 통신 |
| MessagePack | 바이너리 | 아니오 | 작음 | 빠름 | 없음 | 캐시, 로그 |
| Arrow | 바이너리 | 아니오 | 중간 | 매우 빠름 | 내장 | 분석 엔진 |
| Parquet | 바이너리 | 아니오 | 매우 작음 | 빠름 | 내장 | 데이터 레이크 |

## 2. Protocol Buffers

Google이 개발한 언어 중립적 직렬화 프레임워크입니다(sp-07 네트워크와 RPC에서 gRPC와 함께 사용).

```protobuf
// inference_request.proto
syntax = "proto3";

message InferenceRequest {
    string model_id = 1;
    repeated float features = 2;
    map<string, string> metadata = 3;
    RequestConfig config = 4;
}

message RequestConfig {
    int32 top_k = 1;
    float temperature = 2;
    int32 max_tokens = 3;
}

message InferenceResponse {
    repeated float logits = 1;
    float latency_ms = 2;
}
```

```python
# protobuf 사용 예시
# protoc --python_out=. inference_request.proto

from inference_request_pb2 import InferenceRequest, RequestConfig

# 직렬화
request = InferenceRequest(
    model_id="gpt-small",
    features=[0.1, 0.2, 0.3],
    config=RequestConfig(top_k=5, temperature=0.7)
)
binary = request.SerializeToString()  # 바이너리 직렬화
print(f"크기: {len(binary)} bytes")   # JSON 대비 50-80% 작음

# 역직렬화
parsed = InferenceRequest()
parsed.ParseFromString(binary)
```

## 3. Apache Arrow와 제로카피

Arrow는 **인메모리 컬럼 포맷**으로, 프로세스 간 제로카피 데이터 공유를 가능하게 합니다.

```
┌───────────────────────────────────────────┐
│        행 지향 vs 열 지향 레이아웃           │
├───────────────────────────────────────────┤
│                                           │
│  행 지향 (CSV, JSON):                     │
│  ┌────────────────────────┐              │
│  │ id=1, name="A", val=10 │ 행 1         │
│  │ id=2, name="B", val=20 │ 행 2         │
│  │ id=3, name="C", val=30 │ 행 3         │
│  └────────────────────────┘              │
│                                           │
│  열 지향 (Arrow, Parquet):                │
│  ┌──────────┐                            │
│  │ 1, 2, 3  │ id 열    → 연속 메모리      │
│  ├──────────┤                            │
│  │ A, B, C  │ name 열  → 연속 메모리      │
│  ├──────────┤                            │
│  │ 10,20,30 │ val 열   → 연속 메모리      │
│  └──────────┘                            │
│                                           │
│  열 연산 (SUM, MEAN) → 캐시 친화적!       │
│                                           │
└───────────────────────────────────────────┘
```

```python
import pyarrow as pa
import pyarrow.parquet as pq
import numpy as np

# Arrow 배열 생성 - 제로카피 변환
np_arr = np.random.randn(1_000_000).astype(np.float32)
arrow_arr = pa.array(np_arr)  # 제로카피: 메모리 복사 없음

# Arrow → NumPy 제로카피 변환
back_to_np = arrow_arr.to_numpy(zero_copy_only=True)

# Arrow 테이블 생성
table = pa.table({
    'embedding': pa.array(np.random.randn(1000).tolist()),
    'label': pa.array(np.random.randint(0, 10, 1000).tolist()),
    'score': pa.array(np.random.rand(1000).tolist()),
})
```

> **핵심 직관**: 제로카피(zero-copy)란 데이터를 복사하지 않고 포인터(참조)만 전달하는 기법입니다. 1GB 데이터를 복사하면 수백 ms가 걸리지만, 포인터만 전달하면 나노초면 됩니다.

## 4. Parquet: 분석 최적화 저장 포맷

Parquet은 Arrow의 디스크 저장 형태로, 칼럼 압축과 술어 푸시다운을 지원합니다(mo-02 데이터 파이프라인 참조).

```python
import pyarrow.parquet as pq
import pandas as pd

# Parquet 저장 - 압축과 파티셔닝
df = pd.DataFrame({
    'date': pd.date_range('2024-01-01', periods=1_000_000, freq='s'),
    'model': np.random.choice(['gpt', 'bert', 'llama'], 1_000_000),
    'latency_ms': np.random.exponential(50, 1_000_000),
})

# 파티셔닝: 필요한 파티션만 읽어서 I/O 절약
pq.write_to_dataset(
    pa.Table.from_pandas(df),
    root_path='inference_logs/',
    partition_cols=['model'],  # model별 디렉토리 분리
)

# 술어 푸시다운: 필요한 행만 읽기
filters = [('model', '=', 'gpt'), ('latency_ms', '>', 100)]
result = pq.read_table('inference_logs/', filters=filters)
```

| 기능 | CSV | JSON | Parquet |
|------|-----|------|---------|
| 컬럼 선택 읽기 | 불가 | 불가 | 가능 |
| 압축 | 외부 필요 | 외부 필요 | 내장 (Snappy/Zstd) |
| 스키마 진화 | 불가 | 수동 | 지원 |
| 분석 쿼리 속도 | 느림 | 매우 느림 | 빠름 |

## 5. 메모리 매핑 (mmap)

디스크의 파일을 가상 메모리에 매핑하여 마치 메모리처럼 접근하는 기법입니다(sp-01 메모리 계층 참조).

```python
import numpy as np
import mmap

# NumPy memmap: 대용량 데이터를 디스크에 두고 필요한 부분만 로딩
# 100GB 임베딩 테이블도 메모리 부족 없이 접근 가능
embeddings = np.memmap(
    'embeddings.bin', dtype='float32', mode='r',
    shape=(50_000_000, 256)  # 5천만 x 256차원
)

# 필요한 인덱스만 접근 - OS가 해당 페이지만 메모리에 로딩
batch_indices = [100, 5000, 999999]
batch_embeddings = embeddings[batch_indices]  # 지연 로딩
```

## 6. 실전 시나리오

**시나리오 1: 학습 데이터 포맷 전환**

CSV로 저장된 10TB 학습 데이터를 Parquet으로 전환하면 저장 공간이 3TB로 줄고, 특정 컬럼만 읽는 특성 추출 작업이 10배 빨라집니다. Arrow를 중간 포맷으로 사용하면 Pandas, Spark, DuckDB 간 제로카피 데이터 교환이 가능합니다.

**시나리오 2: 모델 서빙 통신 최적화**

JSON으로 통신하던 추론 API를 Protocol Buffers로 전환하면 직렬화/역직렬화 시간이 5-10배 단축되고, 네트워크 전송량이 60-80% 감소합니다(se-06 API 설계 참조).

```python
import json
import time

# JSON vs msgpack 비교 (간단한 벤치마크)
import msgpack

data = {"features": list(range(1000)), "model": "bert", "version": 3}

json_bytes = json.dumps(data).encode()
msgpack_bytes = msgpack.packb(data)

print(f"JSON: {len(json_bytes)} bytes")     # ~5000 bytes
print(f"MsgPack: {len(msgpack_bytes)} bytes")  # ~3000 bytes
```

> **핵심 직관**: AI 시스템에서 데이터 포맷 선택은 "저장-전송-처리" 전체 라이프사이클을 고려해야 합니다. 저장에 최적인 포맷(Parquet)과 전송에 최적인 포맷(Protobuf)과 처리에 최적인 포맷(Arrow)은 서로 다릅니다.

## 핵심 정리

- 직렬화 포맷은 용도에 따라 선택해야 하며, API 통신에는 Protocol Buffers, 분석 저장에는 Parquet, 인메모리 처리에는 Arrow가 각각 최적입니다
- Apache Arrow의 컬럼 지향 인메모리 포맷은 제로카피 데이터 공유를 가능하게 하여 프로세스 간 데이터 전달 비용을 극적으로 줄입니다
- Parquet은 컬럼 압축, 술어 푸시다운, 파티셔닝을 통해 대규모 데이터셋에서 필요한 데이터만 효율적으로 읽을 수 있게 합니다
- 메모리 매핑(mmap)은 디스크의 대용량 데이터를 가상 메모리로 접근하여 물리 메모리 한계를 넘어서는 데이터 처리를 가능하게 합니다
- 제로카피, 컬럼 지향, 지연 로딩은 AI 데이터 파이프라인에서 I/O 병목을 해소하는 세 가지 핵심 원칙이며, 이들을 조합하면 처리량을 수배에서 수십 배 향상시킬 수 있습니다
