# 오프라인 RL

## 왜 오프라인으로 학습하는가

지금까지의 RL은 환경과 **실시간 상호작용**하며 학습했습니다. 하지만 의료(환자에게 실험 불가), 자율주행(사고 위험), 로봇(하드웨어 손상) 등 **탐색이 위험하거나 비용이 큰** 도메인에서는 실시간 상호작용이 불가능합니다. **오프라인 RL**은 이미 수집된 **고정된 데이터셋**에서만 정책을 학습합니다. 지도학습의 데이터 효율과 RL의 최적화를 결합하는 패러다임입니다.

> **핵심 직관**: 오프라인 RL의 핵심 도전은 **"분포 이탈(distribution shift)"**입니다. 데이터에 없는 (s, a) 조합에서 Q 값이 과대추정됩니다. 데이터에 "앞으로 가기"만 있고 "왼쪽으로 가기"가 없으면, "왼쪽"의 Q가 부정확합니다. 이 부정확한 Q에 의한 잘못된 정책이 핵심 문제입니다.

## 1. 오프라인 RL의 문제

```
설정:
  데이터셋 D = {(s, a, r, s')} — 이전 정책 β로 수집
  D에서만 학습 (추가 상호작용 없음!)

  Online RL: 학습 ↔ 데이터 수집 (반복)
  Offline RL: 데이터 수집(완료) → 학습(D만 사용)

  왜 단순 Q-Learning이 실패하는가:

  Q(s, a) ← r + γ max_{a'} Q(s', a')

  문제: max_{a'} Q(s', a')에서
  데이터에 없는 a'를 선택할 수 있음!
  → Q(s', a')가 학습된 적 없음 → 임의의 값
  → 종종 과대추정 → 이 행동을 선호
  → 더 과대추정 → 발산!

  Extrapolation Error:
  데이터 분포 밖(Out-of-Distribution)의 Q 추정이 부정확
  → max가 이 부정확한 값을 선택
  → 부트스트래핑이 에러를 증폭
  → 정책이 데이터와 완전히 다른 영역으로 이동

  예시:
  데이터: 전문가가 안전하게 운전한 기록
  학습된 Q: "급가속"의 Q가 매우 높음 (학습 안 됨 → 랜덤 높은 값)
  결과: 에이전트가 급가속을 선택 → 사고!
```

## 2. 보수적 Q-Learning (CQL)

```
CQL (Kumar et al., 2020):
  Conservative Q-Learning

  핵심 아이디어:
  데이터에 없는 행동의 Q를 낮추고,
  데이터에 있는 행동의 Q를 높여서
  보수적인 Q 추정을 만든다

  CQL 손실:
  L_CQL = α × (E_{a~π}[Q(s,a)] - E_{a~D}[Q(s,a)])
          + 기본 TD 손실

  첫 항: 정책이 선택할 행동의 Q를 낮춤 (페널티)
  둘 항: 데이터에 있는 행동의 Q를 높임 (보상)

  → 데이터 밖 행동의 Q가 자연스럽게 낮아짐
  → 보수적: 실제 Q보다 낮은 하한(lower bound) 추정
  → 과대추정 문제 해결!

  α: 보수성 정도
  α 크면: 매우 보수적 → 데이터 정책에 가까움
  α 작으면: 덜 보수적 → 더 나은 정책 탐색 가능

  CQL의 이론적 보장:
  Q_CQL(s,a) ≤ Q^π(s,a) (높은 확률로)
  → 과대추정이 아닌 과소추정
  → 안전한 정책 평가

  | 방법 | OOD Q | 보수성 | 성능 |
  |------|-------|--------|------|
  | 기본 Q-Learning | 과대추정 | 없음 | 실패 |
  | CQL | 과소추정 | α로 제어 | 안정적 |
```

## 3. 정책 제약 방법

```
행동 제약 (Behavior Constraining):
  학습된 정책이 데이터 정책과 크게 다르지 않도록 제약

  BCQ (Batch-Constrained Q-Learning, Fujimoto et al., 2019):
  ├─ 생성 모델 G로 데이터 분포 학습
  ├─ G가 생성한 행동 후보 중에서만 Q 최대화
  │   a = argmax_{a_i ~ G(s)} Q(s, a_i)
  └─ 데이터 밖 행동을 원천 차단!

  TD3+BC (Fujimoto & Gu, 2021):
  놀라울 정도로 간단한 방법

  L = Q-Learning 손실 + λ ||π(s) - a||²
  → BC(Behavior Cloning) 항을 추가
  → 정책이 데이터 행동에서 크게 벗어나지 않도록

  λ = α / (1/N × Σ|Q(s,a)|)
  → Q 스케일에 맞게 자동 조절

  → 10줄 코드 변경으로 강력한 오프라인 RL!

  IQL (Implicit Q-Learning, Kostrikov et al., 2022):
  ├─ OOD 행동의 Q를 아예 계산하지 않음
  ├─ Expectile Regression으로 V 학습
  │   L_V = |τ - 1(Q(s,a) - V(s) < 0)| × (Q(s,a) - V(s))²
  │   τ = 0.7~0.9: 상위 quantile 추정
  ├─ max Q 대신 V를 사용하여 타겟 계산
  └─ max 연산 없이 오프라인 RL!

  | 방법 | 핵심 아이디어 | 복잡도 | 성능 |
  |------|-------------|--------|------|
  | BCQ | 생성 모델로 행동 제약 | 높음 | 좋음 |
  | CQL | Q 페널티 (보수적) | 중간 | 좋음 |
  | TD3+BC | BC 정규화 | 매우 낮음 | 좋음 |
  | IQL | max 제거 (Expectile) | 낮음 | 매우 좋음 |
```

> **핵심 직관**: 오프라인 RL의 모든 방법은 결국 **"데이터 밖으로 나가지 마라"**를 다른 방식으로 구현합니다. CQL은 Q를 낮춰서, BCQ는 행동을 제한해서, TD3+BC는 정규화로, IQL은 max를 제거해서. "데이터에 있는 행동만 신뢰하라"는 공통 원칙입니다.

## 4. Decision Transformer

```
Decision Transformer (Chen et al., 2021):
  "RL을 시퀀스 모델링으로 재정의"

  핵심 아이디어:
  벨만 방정식을 풀지 않고,
  GPT처럼 다음 행동을 예측!

  입력 시퀀스:
  (R̂₁, s₁, a₁, R̂₂, s₂, a₂, R̂₃, s₃, a₃, ...)

  R̂_t = 목표 리턴 (이 시점부터 원하는 누적 보상)

  → 리턴, 상태, 행동의 인터리빙 시퀀스

  학습:
  기존 RL 궤적에서 (s, a, r) 시퀀스 추출
  GPT(Transformer Decoder)로 다음 행동 예측

  L = E[||a_t - Transformer(R̂_{≤t}, s_{≤t}, a_{<t})||²]

  추론:
  1. 원하는 리턴 R̂ 설정 (예: 1000점)
  2. 현재 상태 s 관찰
  3. Transformer가 행동 a 예측
  4. 행동 실행, 다음 상태 관찰
  5. R̂ 업데이트: R̂_{t+1} = R̂_t - r_t
  6. 반복

  왜 작동하는가:
  "높은 리턴" 조건에서 학습된 행동 패턴 =
  높은 보상을 얻는 전략
  → 리턴 조건이 "어떤 수준의 정책"을 요청하는 것

  장점:
  ├─ 벨만 방정식, TD 에러, Q 함수 불필요!
  ├─ Transformer의 시퀀스 모델링 능력 활용
  ├─ 리턴 조건으로 정책 수준 제어
  └─ 분포 이탈 문제가 자연스럽게 완화

  한계:
  ├─ 데이터에 높은 리턴의 궤적이 있어야
  ├─ "더 나은" 행동을 발견하지 못함 (데이터 이상 불가)
  └─ 긴 시야 과제에서 어려움

  | 속성 | CQL | DT |
  |------|-----|-----|
  | 패러다임 | Q-Learning | 시퀀스 모델링 |
  | 핵심 | 보수적 Q | 조건부 생성 |
  | 데이터 이상 | 가능 | 어려움 |
  | 구현 | RL 지식 필요 | Transformer 지식 |
  | 조건 | 없음 | 리턴 조건 |
```

## 5. 오프라인-온라인 전환

```
순수 오프라인의 한계:
  데이터 품질이 성능 상한을 결정
  데이터에 없는 전략은 학습 불가

  해결: 오프라인 → 온라인 파인튜닝

  Cal-QL (Nakamoto et al., 2023):
  ├─ CQL로 오프라인 사전학습
  ├─ 온라인으로 전환 시 CQL 제약 점진적 완화
  └─ 안전한 오프라인 + 온라인 개선

  RLPD (Ball et al., 2023):
  ├─ 오프라인 데이터를 Replay Buffer에 초기 저장
  ├─ 온라인 데이터를 점진적으로 추가
  ├─ 온라인과 오프라인 데이터 비율 조절
  └─ 매우 간단하면서 효과적

  실전 워크플로우:
  1. 기존 데이터 수집 (로그, 전문가 시연)
  2. 오프라인 RL로 초기 정책 학습
  3. 안전한 환경에서 온라인 파인튜닝
  4. 점진적으로 실환경 배포

  데이터 품질과 성능:

  | 데이터 유형 | 품질 | 오프라인 RL 성능 |
  |-----------|------|---------------|
  | 전문가 | 최고 | 매우 좋음 |
  | 중간 수준 | 중간 | 좋음 |
  | 무작위 | 낮음 | 나쁨 |
  | 혼합 | 다양 | 좋음 (다양성이 핵심) |

  → 다양한 수준의 데이터가 오히려 유리!
  → 전문가 데이터만 있으면 BC로도 충분
```

## 6. 오프라인 RL 실전

```
실전 적용:

  로봇:
  ├─ 수천 번의 시연 데이터 수집
  ├─ 오프라인 RL로 안전한 초기 정책
  ├─ 시뮬레이션에서 파인튜닝
  └─ RT-2, Gato 등에 활용

  의료:
  ├─ 환자 치료 기록에서 최적 치료 전략 학습
  ├─ 투약량 조절, 치료 순서 결정
  └─ 오프라인만 (환자에 직접 시험 불가)

  추천 시스템:
  ├─ 사용자 클릭/구매 로그에서 학습
  ├─ 새로운 추천 전략 오프라인 평가
  └─ A/B 테스트 전 오프라인 검증

  자율주행:
  ├─ 운전 로그 데이터에서 학습
  ├─ 위험 상황 데이터 포함 (사고 방지)
  └─ 오프라인 → 시뮬레이션 → 실차

  오프라인 평가 (OPE):
  학습된 정책을 실제 배포 전에 평가
  ├─ FQE (Fitted Q Evaluation): Q를 학습하여 평가
  ├─ IS (Importance Sampling): 데이터 재가중
  └─ OPE의 정확도가 실전 적용의 관건

  선택 가이드:
  ├─ 간단+강건: TD3+BC 또는 IQL
  ├─ 이론적 보장: CQL
  ├─ 시퀀스 데이터: Decision Transformer
  └─ 온라인 전환: Cal-QL 또는 RLPD
```

## 핵심 정리

- **오프라인 RL**은 고정된 데이터셋에서만 정책을 학습하며, **분포 이탈**로 인한 Q 과대추정이 핵심 문제입니다
- **CQL**은 데이터 밖 행동의 Q를 페널티하여 보수적 하한을 추정하고, **TD3+BC**는 BC 정규화 한 줄로 간단히 구현됩니다
- **IQL**은 max 연산을 제거하고 Expectile Regression으로 V를 학습하여 OOD 문제를 근본적으로 회피합니다
- **Decision Transformer**는 RL을 조건부 시퀀스 생성으로 재정의하여, 목표 리턴을 조건으로 행동을 예측하며 벨만 방정식이 불필요합니다
- 실전에서는 **오프라인 사전학습 → 온라인 파인튜닝**이 가장 효과적이며, 다양한 수준의 데이터가 전문가 데이터만보다 오히려 유리합니다
