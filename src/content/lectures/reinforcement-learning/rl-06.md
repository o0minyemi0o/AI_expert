# Policy Gradient

## 왜 정책을 직접 학습하는가

rl-05의 DQN은 Q 함수를 학습하고 argmax로 행동을 선택합니다. 하지만 로봇의 관절 토크처럼 **연속 행동**에서는 argmax가 불가능하고, 가위바위보처럼 **확률적 정책**이 최적인 경우도 있습니다. **Policy Gradient**는 Q를 거치지 않고 **정책 π를 직접 최적화**합니다. 이것이 A2C(rl-07), PPO(rl-08), 그리고 RLHF(rl-13)의 기초입니다.

> **핵심 직관**: Value-Based(DQN)는 **"각 행동의 가치를 계산하고 최선을 선택"**하는 간접적 접근입니다. Policy Gradient는 **"좋은 결과를 낳은 행동의 확률을 높인다"**는 직접적 접근입니다. 좋은 보상 → 해당 행동 확률 증가, 나쁜 보상 → 확률 감소. 이것이 정책 경사의 전부입니다.

## 1. Policy Gradient 정리

```
정책 파라미터화:
  π_θ(a|s): 파라미터 θ를 가진 확률적 정책
  → 신경망: 상태 s → softmax → 행동 확률 분포

  연속 행동: 가우시안 정책
  π_θ(a|s) = N(μ_θ(s), σ_θ(s))
  → 신경망이 평균과 분산을 출력

목적 함수:
  J(θ) = E_{τ~π_θ}[Σ_t γ^t r_t] = E_{τ~π_θ}[R(τ)]

  τ = (s₀, a₀, r₀, s₁, a₁, r₁, ...): 궤적(trajectory)
  R(τ) = Σ_t γ^t r_t: 궤적의 누적 보상

  목표: J(θ)를 최대화하는 θ를 찾기
  → 경사 상승: θ ← θ + α ∇_θ J(θ)

Policy Gradient Theorem:
  ∇_θ J(θ) = E_{τ~π_θ}[Σ_t ∇_θ log π_θ(a_t|s_t) · Q^π(s_t, a_t)]

  놀라운 점:
  ├─ 환경의 전이 확률 P가 포함되지 않음!
  ├─ 정책의 로그 확률의 기울기 × Q 값
  └─ 모델 프리로 정책 최적화 가능

  직관적 이해:
  ∇_θ log π_θ(a|s) = "이 행동의 확률을 높이는 방향"
  Q^π(s, a) = "이 행동이 얼마나 좋은가"

  → 좋은 행동(Q 높음)의 확률을 높이는 방향으로 업데이트
  → 나쁜 행동(Q 낮음)의 확률을 낮추는 방향으로 업데이트
```

## 2. REINFORCE

```
REINFORCE (Williams, 1992):
  가장 단순한 Policy Gradient 알고리즘

  Q^π(s_t, a_t)를 실제 리턴 G_t로 대체:
  ∇_θ J ≈ Σ_t ∇_θ log π_θ(a_t|s_t) · G_t

  알고리즘:
  1. 정책 π_θ로 에피소드 생성
     τ = (s₀, a₀, r₀, ..., s_T)
  2. 각 시점의 리턴 계산
     G_t = Σ_{k=0}^{T-t-1} γ^k r_{t+k}
  3. 정책 업데이트
     θ ← θ + α Σ_t ∇_θ log π_θ(a_t|s_t) · G_t

  → MC 방식: 에피소드 끝까지 진행 후 업데이트
  → 비편향: 실제 G_t 사용

  문제: 높은 분산
  G_t의 분산이 매우 큼
  → 학습이 느리고 불안정

  왜 분산이 높은가:
  G_t = r₀ + γr₁ + γ²r₂ + ...
  → 많은 확률 변수의 합 → 분산 누적
  → 에피소드가 길수록 분산 증가
```

## 3. 분산 감소: Baseline

```
Baseline 기법:
  가장 효과적인 분산 감소 방법

  ∇_θ J = E[Σ_t ∇_θ log π_θ(a_t|s_t) · (G_t - b(s_t))]

  b(s_t): 상태에만 의존하는 베이스라인
  → E[∇_θ log π_θ · b(s_t)] = 0 (기대값 불변!)
  → 편향을 추가하지 않으면서 분산을 줄임

  최적 베이스라인: b(s) = V^π(s)

  G_t - V^π(s_t) = "이 에피소드가 평균보다 얼마나 좋았나"
  = Advantage A^π(s_t, a_t) (이점 함수)

  직관:
  보상이 항상 양수이면 (예: 게임 점수):
  without baseline: 모든 행동의 확률 증가 (좋든 나쁘든)
  with baseline: 평균 이상은 증가, 미만은 감소
  → 상대적 평가!

  Baseline의 효과:
  상황: 모든 보상이 +90~+100 사이
  without: G=95 → log π · 95 (큰 양수 → 모든 행동 강화)
  with V=95: G-V=0 → 변화 없음 (평균적 행동)
  with V=95: G-V=+5 → 약간 강화 (평균 이상)
  with V=95: G-V=-5 → 약간 약화 (평균 이하)

  | 방법 | G_t 사용 | 편향 | 분산 |
  |------|---------|------|------|
  | REINFORCE | G_t | 없음 | 높음 |
  | + Baseline | G_t - b | 없음 | 중간 |
  | + V(s) | G_t - V(s) | 없음 | 낮음 |
  | Actor-Critic | r + γV(s') - V(s) | 있음 | 매우 낮음 |
```

> **핵심 직관**: Baseline의 원리는 **"절대 점수가 아닌 상대 점수로 평가"**하는 것입니다. 시험에서 90점을 받아도 평균이 95점이면 나쁜 결과이고, 평균이 80점이면 좋은 결과입니다. G_t에서 V(s)를 빼면 "이 행동이 평균보다 좋았는가?"의 상대적 평가가 되어, 의미 있는 방향으로만 정책을 업데이트합니다.

## 4. 연속 행동과 가우시안 정책

```
연속 행동 공간에서의 Policy Gradient:

  정책: π_θ(a|s) = N(μ_θ(s), σ_θ(s)²)
  → 신경망이 평균 μ와 표준편차 σ 출력

  log π = -1/(2σ²) (a - μ)² - log σ - 1/2 log(2π)

  ∇_μ log π = (a - μ) / σ²
  → 행동 a가 평균보다 높으면 → 평균을 올리는 방향
  → G가 양수이면 → 이 방향이 강화됨

  ∇_σ log π = (a - μ)² / σ³ - 1/σ
  → σ를 조절하여 탐색 범위 제어

  로봇 제어 예시:
  상태: 관절 각도와 속도
  행동: 각 관절의 토크 (연속)
  정책: 각 관절에 가우시안 → 토크 샘플

  학습 중 σ가 자연스럽게 감소:
  초기: σ 큼 → 넓은 탐색
  후기: σ 작음 → 정밀한 제어
  → ε-greedy의 ε 감소와 유사한 효과

다차원 행동:
  π(a|s) = N(μ(s), Σ(s))
  보통 대각 공분산: Σ = diag(σ₁², σ₂², ...)
  → 각 행동 차원이 독립
  → 구현이 간단하고 충분히 효과적
```

## 5. Policy Gradient vs Value-Based

```
비교:

  Value-Based (DQN):
  ├─ Q(s,a) 학습 → argmax로 행동
  ├─ 이산 행동만
  ├─ 결정론적 정책 (argmax)
  ├─ 오프 정책 가능 (Experience Replay)
  ├─ 샘플 효율적
  └─ 수렴 보장 어려움 (함수 근사 시)

  Policy-Based (REINFORCE):
  ├─ π(a|s) 직접 학습
  ├─ 연속 행동 가능
  ├─ 확률적 정책 (자연스러운 탐색)
  ├─ 온 정책 (데이터 재사용 어려움)
  ├─ 샘플 비효율적 (높은 분산)
  └─ 로컬 최적에 수렴 보장

  Actor-Critic (rl-07):
  ├─ π(Actor) + Q/V(Critic) 결합
  ├─ 두 방법의 장점 통합
  └─ 현대 RL의 주류

  | 속성 | Value | Policy | Actor-Critic |
  |------|-------|--------|-------------|
  | 행동 공간 | 이산 | 연속+이산 | 연속+이산 |
  | 정책 유형 | 결정론적 | 확률적 | 확률적 |
  | 분산 | 낮음 | 높음 | 중간 |
  | 편향 | 있음 | 없음 | 있음 |
  | 샘플 효율 | 높음 | 낮음 | 중간 |
  | 대표 | DQN | REINFORCE | PPO, SAC |
```

## 6. Policy Gradient의 도전과 실전

```
Policy Gradient의 실전 도전:

  1. 높은 분산:
     → Baseline, GAE(rl-07), 다수 환경 병렬 실행
     → 에피소드가 짧을수록 유리

  2. 샘플 비효율:
     정책이 바뀌면 이전 데이터 사용 불가 (on-policy)
     → Importance Sampling으로 일부 재사용 (PPO)
     → 환경 병렬화로 보상

  3. 학습률 민감:
     너무 크면: 정책이 급변 → 성능 붕괴
     너무 작으면: 학습이 매우 느림
     → TRPO/PPO의 Trust Region (rl-08)

  4. 로컬 최적:
     Policy Gradient는 경사 상승 → 로컬 최적 수렴
     → 전역 최적 보장 없음
     → 실전에서는 충분히 좋은 해를 찾는 것이 목표

구현 팁:
  ├─ 보상 정규화: (G_t - mean) / std
  │   → 학습률에 덜 민감
  ├─ 그래디언트 클리핑: ||∇|| > max_norm이면 스케일
  │   → 큰 업데이트 방지
  ├─ 엔트로피 보너스: H(π) = -Σ π log π
  │   → 정책의 엔트로피를 높게 유지 → 충분한 탐색
  └─ 병렬 환경: 여러 환경 동시 실행 → 분산 감소

  L = -E[log π(a|s) · Â] - β H(π)
  → 첫 항: 정책 경사 (이점이 큰 행동 강화)
  → 둘째 항: 엔트로피 보너스 (너무 결정적이면 벌칙)
```

## 핵심 정리

- **Policy Gradient**는 정책 π_θ를 직접 최적화하며, ∇J = E[∇log π · Q]로 좋은 행동의 확률을 높이고 나쁜 행동의 확률을 낮춥니다
- **REINFORCE**는 실제 리턴 G_t를 사용하는 MC 기반 Policy Gradient로 비편향이지만, G_t의 높은 분산으로 학습이 느립니다
- **Baseline** b(s) = V(s)를 빼면 편향 없이 분산을 크게 줄이며, G_t - V(s)는 **이점 함수(Advantage)**로 "평균보다 얼마나 좋은가"를 측정합니다
- **가우시안 정책**은 연속 행동 공간에서 자연스럽게 작동하며, 신경망이 평균과 분산을 출력하고 σ가 자연스럽게 감소하여 탐색→활용을 전환합니다
- Policy Gradient는 **높은 분산, 샘플 비효율, 학습률 민감도**가 도전이며, Actor-Critic(rl-07)과 PPO(rl-08)가 이를 해결합니다
