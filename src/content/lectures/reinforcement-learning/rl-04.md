# 시간차 학습

## 왜 TD가 RL의 핵심인가

rl-02의 DP는 모델이 필요하고, rl-03의 MC는 에피소드 끝까지 기다려야 합니다. **시간차 학습(Temporal Difference, TD)**은 이 두 방법의 장점만 취합니다: 모델 없이(MC처럼), 매 스텝 업데이트(DP처럼). TD에서 파생된 **Q-Learning**은 최초의 off-policy model-free 알고리즘이며, rl-05 DQN의 직접적 기초입니다.

> **핵심 직관**: TD의 핵심은 **"내일의 추정으로 오늘을 교정"**하는 것입니다. MC는 에피소드 끝까지 가서 실제 G_t를 보지만, TD는 한 스텝 후의 **추정치** V(s')를 사용합니다. 부정확한 추정이지만 매 스텝 업데이트가 가능하므로, 실전에서 MC보다 훨씬 효율적입니다.

## 1. TD(0): 한 스텝 예측

```
TD(0) 업데이트:

  MC: V(s) ← V(s) + α[G_t - V(s)]
  TD: V(s) ← V(s) + α[r + γV(s') - V(s)]

  TD Target: r + γV(s')
  TD Error: δ = r + γV(s') - V(s)

  → MC의 G_t를 r + γV(s')로 대체!
  → 에피소드 끝까지 기다릴 필요 없음

  알고리즘:
  매 스텝 (s, a, r, s')에서:
  V(s) ← V(s) + α × δ
  δ = r + γV(s') - V(s)

  직관:
  V(s) = "이 상태의 예상 가치"
  r + γV(s') = "실제 보상 + 다음 상태의 예상 가치"
  δ = "예상보다 좋았나/나빴나" → 놀람(surprise)

  TD vs MC:
  V(s)가 10이었는데
  MC: 에피소드 끝까지 가서 G_t = 12 → δ = 2
  TD: 한 스텝에서 r=3, V(s')=8 → r+γV(s') = 3+0.9×8 = 10.2 → δ = 0.2

  왜 TD가 작동하는가:
  r + γV(s')는 G_t의 편향된(biased) 추정이지만
  반복을 통해 V가 정확해지면 편향도 사라짐
  → 점근적으로 V^π에 수렴 (일정 조건 하에)

  | 속성 | DP | MC | TD |
  |------|----|----|-----|
  | 모델 필요 | O | X | X |
  | 부트스트랩 | O | X | O |
  | 에피소드 완료 | X | O | X |
  | 편향 | 없음 | 없음 | 있음(감소) |
  | 분산 | 낮음 | 높음 | 낮음 |
  | 온라인 학습 | 가능 | 불가 | 가능 |
```

## 2. SARSA: On-Policy TD 제어

```
SARSA (State-Action-Reward-State-Action):
  TD(0)를 Q 함수로 확장한 on-policy 제어

  이름의 유래: (S_t, A_t, R_t, S_{t+1}, A_{t+1}) 사용

  업데이트:
  Q(s, a) ← Q(s, a) + α[r + γQ(s', a') - Q(s, a)]

  a'는 π(s')에 따라 선택 (예: ε-greedy)
  → 실제로 선택할 행동 a'의 Q 값 사용

  알고리즘:
  1. 상태 s에서 행동 a 선택 (ε-greedy)
  2. r, s' 관찰
  3. s'에서 행동 a' 선택 (ε-greedy)
  4. Q(s,a) ← Q(s,a) + α[r + γQ(s',a') - Q(s,a)]
  5. s ← s', a ← a'
  6. 반복

  특성:
  ├─ On-policy: 행동하는 정책의 Q를 학습
  ├─ ε-greedy가 행동 정책이자 목표 정책
  ├─ 안전한 탐색: 위험한 행동의 Q가 낮아짐
  └─ 탐색적이기 때문에 최적이 아닌 ε-soft 정책으로 수렴

  Cliff Walking 예시:
  ┌──────────────────┐
  │                  │
  │ S · · · · · · G  │   S: 시작, G: 목표
  │ C C C C C C C C  │   C: 절벽 (-100)
  └──────────────────┘

  SARSA는 절벽에서 멀리 떨어진 안전한 경로를 학습
  → ε-greedy의 랜덤 행동이 절벽으로 떨어질 수 있으므로
  → 절벽 근처의 Q가 낮아짐
```

## 3. Q-Learning: Off-Policy TD 제어

```
Q-Learning (Watkins, 1989):
  가장 중요한 RL 알고리즘 중 하나

  업데이트:
  Q(s, a) ← Q(s, a) + α[r + γ max_{a'} Q(s', a') - Q(s, a)]

  SARSA와의 차이:
  SARSA: r + γQ(s', a')         ← a'는 실제 선택한 행동
  Q-Learning: r + γ max Q(s', a') ← 최선의 행동

  → 실제로 a'를 선택할 필요 없이 max만 계산!
  → Off-policy: 어떤 정책으로 행동하든 Q*를 학습

  알고리즘:
  1. 상태 s에서 행동 a 선택 (ε-greedy 등)
  2. r, s' 관찰
  3. Q(s,a) ← Q(s,a) + α[r + γ max_{a'} Q(s',a') - Q(s,a)]
  4. s ← s'
  5. 반복

  왜 Off-Policy인가:
  행동 정책(b): ε-greedy (탐색적)
  목표 정책(π): greedy (max로 내재)
  → b와 π가 다르지만 Q*를 학습 가능!
  → MC의 Importance Sampling이 불필요!

  수렴 보장:
  모든 (s, a)를 충분히 방문하고
  학습률이 적절하면 (Σα = ∞, Σα² < ∞)
  Q → Q* 수렴 (테이블 방식에서)

  Cliff Walking에서:
  Q-Learning은 절벽 바로 옆의 최단 경로를 학습
  → max가 탐욕적 정책의 Q를 사용하므로
  → 절벽 근처를 피할 이유가 없음 (탐욕적으로는 최적)
  → but: ε-greedy 행동 시 실제로는 절벽에 빠질 수 있음

SARSA vs Q-Learning 비교:
  | 속성 | SARSA | Q-Learning |
  |------|-------|-----------|
  | 정책 | On-policy | Off-policy |
  | 타겟 | r + γQ(s',a') | r + γ max Q(s',a') |
  | 안전성 | 보수적 | 공격적 |
  | 수렴 | ε-soft 최적 | Q* (진정한 최적) |
  | Cliff | 안전한 경로 | 최단 경로 |
```

> **핵심 직관**: Q-Learning의 max가 SARSA의 a'와 다른 것이 **off-policy의 핵심**입니다. SARSA는 "실제로 내가 할 행동(a')의 가치"를 사용하지만, Q-Learning은 "내가 할 수 있는 최선의 행동(max)의 가치"를 사용합니다. 탐색적으로 행동하면서도 최적의 가치를 학습하는 것이 가능합니다.

## 4. Expected SARSA와 n-step TD

```
Expected SARSA:
  SARSA의 분산을 줄인 방법

  SARSA: r + γQ(s', a')  (a'를 실제로 샘플)
  Expected: r + γ Σ_{a'} π(a'|s') Q(s', a')  (기대값 사용)

  → a'를 샘플링하지 않고 기대값을 직접 계산
  → SARSA보다 분산 낮음

  특수 경우:
  π가 탐욕적이면: Expected SARSA = Q-Learning!
  → Q-Learning은 Expected SARSA의 특수 경우

n-step TD:
  TD(0)와 MC 사이의 스펙트럼

  1-step (TD): G_t^(1) = r_t + γV(s_{t+1})
  2-step:      G_t^(2) = r_t + γr_{t+1} + γ²V(s_{t+2})
  n-step:      G_t^(n) = Σ_{k=0}^{n-1} γ^k r_{t+k} + γ^n V(s_{t+n})
  ∞-step (MC): G_t^(∞) = Σ_{k=0}^{T-t-1} γ^k r_{t+k}

  V(s) ← V(s) + α[G_t^(n) - V(s)]

  n이 커질수록:
  ├─ MC에 가까움 → 편향 감소, 분산 증가
  └─ TD에 가까움 → 편향 증가, 분산 감소

  최적의 n은 문제에 따라 다름
  보통 n=5~10이 좋은 성능

  | n | 편향 | 분산 | 특성 |
  |---|------|------|------|
  | 1 | 높음 | 낮음 | TD(0) |
  | 5 | 중간 | 중간 | 균형 |
  | ∞ | 없음 | 높음 | MC |

TD(λ):
  모든 n-step을 가중 결합

  G_t^λ = (1-λ) Σ_{n=1}^∞ λ^{n-1} G_t^(n)

  λ = 0: TD(0)
  λ = 1: MC
  0 < λ < 1: 가중 평균

  Eligibility Trace로 효율적 구현:
  e_t(s) = γλ e_{t-1}(s) + 1(s_t = s)
  V(s) ← V(s) + α δ_t e_t(s)
  → 최근 방문한 상태에 더 많이 업데이트
```

## 5. 테이블 방식의 한계

```
테이블 Q-Learning의 한계:

  상태가 이산적이고 적을 때: 잘 동작!
  그리드 월드, 간단한 게임

  상태가 많거나 연속일 때: 불가능!
  ├─ 바둑: 10^170 상태 → 테이블 저장 불가
  ├─ Atari: 이미지 → 256^(210×160×3) 상태
  ├─ 로봇: 연속 관절 각도 → 무한 상태
  └─ 방문하지 않은 상태의 Q = 알 수 없음 (일반화 없음)

  함수 근사의 필요성:
  Q(s, a) ≈ Q̂(s, a; θ)  (신경망으로 근사)
  → 유사한 상태는 유사한 Q 값 (일반화)
  → DQN (rl-05)으로 이어짐

  테이블 → 신경망:
  ├─ Q 테이블: |S| × |A| 크기의 표
  ├─ Q 네트워크: s → Q̂(s, a₁), Q̂(s, a₂), ... (신경망 출력)
  ├─ 손실: (r + γ max Q̂(s'; θ) - Q̂(s, a; θ))²
  └─ 경사하강으로 θ 업데이트

  주의: 함수 근사 + TD → 수렴 보장이 사라짐!
  "치명적 삼각형": 함수 근사 + 부트스트래핑 + Off-policy
  → 세 가지가 결합되면 발산 가능
  → DQN의 Experience Replay와 Target Network가 이를 완화
```

## 6. TD 정리와 비교

```
알고리즘 총정리:

  | 알고리즘 | 타겟 | 정책 | 용도 |
  |---------|------|------|------|
  | TD(0) | r + γV(s') | - | V 예측 |
  | SARSA | r + γQ(s',a') | On | Q 제어 |
  | Q-Learning | r + γ max Q(s',a') | Off | Q 제어 |
  | Expected SARSA | r + γ E[Q(s',·)] | 모두 | Q 제어 |
  | n-step TD | Σγ^k r + γ^n V(s') | - | V/Q 예측 |

  각 알고리즘의 선택 기준:

  "안전한 행동이 중요하다" → SARSA
  (자율주행: 위험 회피가 중요)

  "최적 정책을 빠르게 찾고 싶다" → Q-Learning
  (게임: 최고 점수가 목표)

  "분산을 줄이고 싶다" → Expected SARSA
  (안정적 학습이 중요)

  "편향-분산을 조절하고 싶다" → n-step TD / TD(λ)
  (문제에 맞는 n 선택)

하이퍼파라미터 가이드:
  학습률 α:
  ├─ 0.01~0.1: 일반적
  ├─ 크면: 빠르지만 불안정
  └─ 작으면: 안정적이지만 느림

  할인율 γ:
  ├─ 0.99: 장기적 과제
  ├─ 0.9: 중기 과제
  └─ 0.0: 즉각적 보상만

  탐색 ε:
  ├─ 초기 0.5~1.0 → 감쇠
  └─ 최종 0.01~0.1

  → rl-05에서 이 모든 것을 신경망으로 확장
```

## 핵심 정리

- **TD(0)**는 한 스텝의 보상 + 다음 상태의 추정치(r + γV(s'))로 업데이트하며, MC의 모델 불필요와 DP의 부트스트래핑을 결합합니다
- **SARSA**는 on-policy TD 제어로 실제 선택 행동의 Q를 사용하며, ε-greedy의 탐색적 행동까지 반영하여 **안전한** 정책을 학습합니다
- **Q-Learning**은 off-policy TD 제어로 max Q(s',a')를 사용하여 탐색 정책과 무관하게 **최적** Q*를 학습하며, DQN의 직접적 기초입니다
- **n-step TD**는 TD(0)과 MC의 스펙트럼으로 편향-분산 트레이드오프를 조절하며, **TD(λ)**는 Eligibility Trace로 모든 n-step을 가중 결합합니다
- 테이블 방식은 상태가 많으면 불가능하여 **함수 근사**(신경망)가 필요하지만, 함수 근사 + 부트스트래핑 + off-policy의 "치명적 삼각형"이 수렴을 위협합니다
