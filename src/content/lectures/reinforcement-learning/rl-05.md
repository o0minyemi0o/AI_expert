# Deep Q-Networks

## 왜 DQN이 혁명적인가

rl-04의 Q-Learning은 테이블에 Q 값을 저장하므로 Atari 게임(210×160 픽셀)에는 적용할 수 없습니다. **DQN(Deep Q-Network)**은 Q 테이블을 **신경망**으로 대체하여 이미지에서 직접 Q 값을 예측하고, **Experience Replay**와 **Target Network**로 학습을 안정화했습니다. 2015년 Nature 논문으로 Deep RL의 시대를 열었으며, Atari 49개 게임 중 29개에서 인간을 능가했습니다.

> **핵심 직관**: DQN의 핵심 혁신은 **"치명적 삼각형을 해결"**한 것입니다. 함수 근사 + 부트스트래핑 + off-policy는 발산의 위험이 있지만, Experience Replay(상관관계 제거)와 Target Network(타겟 안정화)라는 두 가지 트릭으로 안정적 학습을 달성했습니다.

## 1. DQN의 구조

```
DQN (Mnih et al., 2015):
  Deep Q-Network — Q-Learning + CNN

  입력: 84×84×4 (4프레임 스택, 그레이스케일)
  → 움직임 정보를 위해 4 프레임 연속 입력

  네트워크:
  Conv(8×8, 32, stride 4) → ReLU
  Conv(4×4, 64, stride 2) → ReLU
  Conv(3×3, 64, stride 1) → ReLU
  FC(512) → ReLU
  FC(|A|) → Q(s, a₁), Q(s, a₂), ..., Q(s, a_n)

  → 상태(이미지)를 입력하면 모든 행동의 Q 값을 출력
  → argmax로 행동 선택

  Q-Learning 손실:
  L(θ) = E[(r + γ max_{a'} Q(s', a'; θ⁻) - Q(s, a; θ))²]

  θ: 현재 네트워크 파라미터
  θ⁻: 타겟 네트워크 파라미터 (고정)

  → TD 타겟(r + γ max Q)과 현재 Q의 MSE를 최소화
```

## 2. Experience Replay

```
문제: 순차적 데이터의 상관관계
  (s₁, a₁, r₁, s₂), (s₂, a₂, r₂, s₃), ...
  → 연속 샘플이 강하게 상관
  → SGD는 i.i.d. 가정 → 상관 데이터로 학습 불안정

해결: Experience Replay

  Replay Buffer D = {(s, a, r, s')} — 최대 N개 저장
  (보통 N = 100만)

  학습:
  1. 환경에서 (s, a, r, s') 경험 수집
  2. D에 저장
  3. D에서 랜덤 미니배치 (32~64개) 샘플
  4. 미니배치로 Q 네트워크 업데이트

  효과:
  ├─ 상관관계 제거: 랜덤 샘플 → i.i.d.에 가까움
  ├─ 데이터 효율: 하나의 경험을 여러 번 재사용
  ├─ 과거 경험 활용: 다양한 정책의 데이터 혼합
  └─ off-policy 자연스러움: 과거 정책의 데이터 사용

  한계:
  ├─ 메모리 비용: 100만 전이 저장 필요
  └─ 오래된 데이터: 현재 정책과 무관할 수 있음
```

## 3. Target Network

```
문제: 이동하는 타겟
  L = (r + γ max Q(s'; θ) - Q(s, a; θ))²

  타겟도 θ에 의존 → θ 업데이트 시 타겟도 변함
  → "쫓아가는 목표가 계속 움직인다"
  → 학습이 진동하거나 발산

해결: Target Network (θ⁻)

  타겟 계산에 별도 네트워크 θ⁻ 사용:
  L = (r + γ max Q(s'; θ⁻) - Q(s, a; θ))²

  θ⁻는 C 스텝마다 θ로 복사:
  매 C 스텝: θ⁻ ← θ  (hard update, C=10,000)
  또는 매 스텝: θ⁻ ← τθ + (1-τ)θ⁻  (soft update, τ=0.005)

  효과:
  ├─ 타겟이 C 스텝 동안 고정 → 안정적 학습
  ├─ 자기 참조 루프 차단
  └─ 수렴 크게 개선

DQN 전체 알고리즘:
  1. Replay Buffer D 초기화
  2. Q 네트워크 θ, Target 네트워크 θ⁻ 초기화
  3. 반복:
     a. ε-greedy로 행동 a 선택
     b. (s, a, r, s') 경험 → D에 저장
     c. D에서 미니배치 샘플
     d. y = r + γ max_{a'} Q(s'; θ⁻) (종료면 y=r)
     e. (y - Q(s, a; θ))² 최소화로 θ 업데이트
     f. 매 C 스텝: θ⁻ ← θ

  | 하이퍼파라미터 | DQN 값 | 역할 |
  |--------------|--------|------|
  | Replay 크기 | 1M | 경험 저장량 |
  | 미니배치 | 32 | 업데이트 당 샘플 |
  | γ | 0.99 | 할인율 |
  | ε 시작/끝 | 1.0→0.1 | 탐색 감소 |
  | ε 감쇠 프레임 | 1M | 감쇠 기간 |
  | 타겟 업데이트 | 10K 스텝 | 타겟 고정 기간 |
  | 학습률 | 0.00025 | Adam |
```

> **핵심 직관**: Experience Replay와 Target Network는 **"데이터의 i.i.d.와 타겟의 안정성"**이라는 SGD의 두 가지 전제를 회복합니다. 순차 데이터를 랜덤 샘플로 만들고(i.i.d.), 움직이는 타겟을 고정시킵니다(안정성). 이 두 트릭이 없으면 DQN은 작동하지 않습니다.

## 4. Double DQN과 Dueling

```
DQN의 문제: Q 값 과대추정

  max 연산의 편향:
  Q(s', a') = Q_true(s', a') + noise
  max_{a'} Q(s', a') ≥ max_{a'} Q_true(s', a')
  → max가 노이즈까지 선택 → 체계적 과대추정

  실험적으로: DQN의 Q 추정이 실제보다 훨씬 높음

Double DQN (van Hasselt et al., 2016):
  행동 선택과 가치 평가를 분리

  DQN: y = r + γ Q(s', argmax_{a'} Q(s'; θ⁻); θ⁻)
  → 같은 네트워크로 선택 + 평가

  Double DQN: y = r + γ Q(s', argmax_{a'} Q(s'; θ); θ⁻)
  → θ로 행동 선택, θ⁻로 가치 평가

  → 선택과 평가를 분리하여 과대추정 감소
  → 구현 한 줄 변경으로 큰 개선!

Dueling DQN (Wang et al., 2016):
  Q를 V(상태 가치)와 A(이점 함수)로 분해

  Q(s, a) = V(s) + A(s, a)

  V(s): 상태 자체의 가치 (행동 무관)
  A(s, a): 이 행동의 상대적 이점

  네트워크 구조:
  CNN → 특징 →┬→ FC → V(s)     (1개 출력)
               └→ FC → A(s, a₁...a_n) (|A|개 출력)

  결합: Q(s,a) = V(s) + (A(s,a) - mean_a A(s,a))
  → mean을 빼서 V와 A의 고유한 분해 보장

  장점:
  "이 상태가 좋다/나쁘다"(V)를 행동과 독립적으로 학습
  → 행동의 차이가 작은 상태에서 빠른 학습
  → 중요하지 않은 행동의 Q도 V를 통해 정확
```

## 5. PER과 Rainbow

```
Prioritized Experience Replay (PER, Schaul et al., 2016):
  "놀라운 경험에서 더 많이 배우자"

  기본 Replay: 균일 랜덤 샘플
  PER: TD 에러가 큰 경험을 우선 샘플

  우선순위: p_i = |δ_i| + ε  (δ = TD 에러)
  샘플링 확률: P(i) = p_i^α / Σ p_j^α

  α = 0: 균일 (기본 Replay)
  α = 1: 완전 우선순위

  Importance Sampling 보정:
  PER는 비균일 샘플 → 편향 발생
  w_i = (1/(N·P(i)))^β로 보정
  β: 0 → 1로 점진적 증가

  → 학습 초반: 약한 보정 (빠른 학습)
  → 학습 후반: 강한 보정 (수렴 보장)

Rainbow (Hessel et al., 2018):
  6가지 개선을 모두 결합

  1. Double DQN: 과대추정 감소
  2. Dueling: V + A 분해
  3. PER: 우선순위 리플레이
  4. Multi-step: n-step 리턴 (n=3)
  5. NoisyNet: 파라미터 잡음으로 탐색
  6. Distributional: Q의 분포(C51) 학습

  NoisyNet:
  ε-greedy 대신 네트워크 가중치에 학습 가능한 잡음 추가
  → 상태 의존적 탐색 (ε-greedy보다 스마트)

  C51 (Distributional):
  Q(s,a) = E[Z(s,a)]  (기대값만 학습)
  → Z(s,a)의 전체 분포를 학습!
  → 51개 원자(atom)로 분포를 이산화

  | 구성 요소 | 단독 기여 | 제거 시 손실 |
  |----------|----------|------------|
  | PER | 높음 | 높음 |
  | Multi-step | 높음 | 높음 |
  | Distributional | 높음 | 중간 |
  | Dueling | 중간 | 낮음 |
  | NoisyNet | 중간 | 낮음 |
  | Double | 낮음 | 낮음 |

  Rainbow은 각 구성 요소의 합 이상의 시너지
  → Atari에서 사람의 ~200% 수준
```

## 6. DQN의 한계와 넘어서기

```
DQN 계열의 근본적 한계:

  1. 이산 행동만:
     Q(s, a₁), Q(s, a₂), ... → argmax
     → 연속 행동 (로봇 토크, 자동차 조향) 불가
     → Policy Gradient (rl-06)가 해결

  2. 결정론적 정책:
     항상 argmax_a Q(s, a) → 확률적 정책 불가
     → 확률적 환경에서 최적이 아닐 수 있음

  3. 고차원 행동 공간:
     |A| = 1000이면 max 계산 비쌈
     |A|가 연속이면 max 계산 불가

  4. 학습 불안정:
     함수 근사 + TD + off-policy → 여전히 불안정할 수 있음
     하이퍼파라미터에 민감

Value-Based → Policy-Based:
  DQN (rl-05): Q 학습 → argmax → 행동
  Policy Gradient (rl-06): π 직접 학습 → 행동 샘플
  Actor-Critic (rl-07): Q(Critic) + π(Actor) 결합
  PPO (rl-08): 안정적 정책 업데이트

  → 이 흐름이 현대 RL의 주류

DQN의 위치:
  ├─ 이산 행동: 여전히 강력 (보드 게임, Atari)
  ├─ Rainbow: Atari 벤치마크의 표준
  ├─ 개념적 중요성: Experience Replay, Target Net
  │   → PPO, SAC 등에서도 사용
  └─ 역사적 의의: Deep RL의 시작
```

## 핵심 정리

- **DQN**은 CNN으로 Q(s,a)를 근사하여 이미지 입력에서 직접 행동을 선택하며, Atari 49개 게임 중 29개에서 인간을 능가했습니다
- **Experience Replay**는 경험을 버퍼에 저장하고 랜덤 샘플하여 데이터 상관관계를 제거하고, **Target Network**는 타겟 Q를 C 스텝 동안 고정하여 학습을 안정화합니다
- **Double DQN**은 행동 선택과 가치 평가를 분리하여 Q 과대추정을 줄이고, **Dueling**은 V+A 분해로 상태 가치를 독립적으로 학습합니다
- **PER**은 TD 에러가 큰 경험을 우선 리플레이하고, **Rainbow**는 6가지 개선(Double, Dueling, PER, Multi-step, NoisyNet, C51)을 결합하여 시너지를 달성합니다
- DQN은 **이산 행동에만 적용** 가능하며, 연속 행동과 확률적 정책을 위해 rl-06의 Policy Gradient로 확장됩니다
