# 몬테카를로 방법

## 왜 몬테카를로인가

rl-02의 DP는 환경 모델 P(s'|s,a)를 완전히 알아야 합니다. 하지만 현실 세계는 모델을 모릅니다. **몬테카를로(MC) 방법**은 모델 없이 **실제 경험(에피소드)**에서 직접 가치를 추정합니다. 에피소드를 끝까지 진행하여 실제 누적 보상 G_t를 관찰하고, 이것의 평균으로 V(s)를 추정합니다. 단순하지만 강력한 첫 번째 **model-free** 알고리즘입니다.

> **핵심 직관**: MC의 핵심은 **"경험에서 배운다"**입니다. "이 상태를 100번 방문했는데 평균 보상이 5.3이었다 → V(s) ≈ 5.3." 전이 확률을 알 필요 없이 실제 경험의 평균이 기대값으로 수렴합니다. pt-02의 큰 수의 법칙이 이론적 근거입니다.

## 1. MC 예측 (Policy Evaluation)

```
MC 예측: 정책 π의 V^π(s)를 에피소드에서 추정

  에피소드: s₀, a₀, r₀, s₁, a₁, r₁, ..., s_T (종료)
  누적 보상: G_t = r_t + γ r_{t+1} + γ² r_{t+2} + ... + γ^(T-t-1) r_{T-1}

  V^π(s) = E[G_t | s_t = s] ≈ (1/N) Σ G_t

  First-Visit MC:
  에피소드에서 상태 s를 처음 방문할 때만 카운트
  1. 여러 에피소드 생성 (정책 π에 따라)
  2. 각 에피소드에서 s를 처음 방문한 시점의 G_t 계산
  3. V(s) = 이 G_t들의 평균

  Every-Visit MC:
  에피소드에서 상태 s를 방문할 때마다 카운트
  → 같은 에피소드에서 여러 번 방문하면 모두 사용

  | 방법 | 편향 | 분산 | 수렴 |
  |------|------|------|------|
  | First-Visit | 비편향 | 낮음 | O(1/√N) |
  | Every-Visit | 편향(점근적 비편향) | 더 낮음 | O(1/√N) |

  점진적 평균 (Incremental Mean):
  V(s) ← V(s) + (1/N(s)) × (G_t - V(s))

  또는 고정 학습률 α:
  V(s) ← V(s) + α × (G_t - V(s))
  → 비정상 환경에서 유리 (오래된 경험 잊기)
  → α가 작으면 안정적, 크면 빠르지만 불안정

  MC vs DP:
  DP:  V(s) ← Σ P(s'|s,a)[R + γ V(s')]  (모델 필요)
  MC:  V(s) ← V(s) + α(G_t - V(s))       (경험만 필요)
  → MC는 P를 몰라도 됨!
  → 대신 에피소드 끝까지 기다려야 함
```

## 2. MC 제어 (Finding Optimal Policy)

```
MC 제어: 최적 정책 π*를 찾기

  문제: MC로 V^π를 구해도 정책 개선에 P가 필요
  V 기반 개선: π'(s) = argmax_a Σ P(s'|s,a)[R + γ V(s')]
  → P를 모르면 불가능!

  해결: V 대신 Q 사용!
  Q 기반 개선: π'(s) = argmax_a Q(s, a)
  → P가 필요 없음! Q 값만으로 행동 선택

  MC 제어 알고리즘:
  1. 에피소드 생성 (현재 정책 π에 따라)
  2. 에피소드의 각 (s, a) 쌍에 대해 G_t 계산
  3. Q(s, a) 업데이트 (평균으로)
  4. 정책 개선: π(s) = argmax_a Q(s, a)
  5. 반복

  문제: 탐색 부족 (Exploration)
  탐욕적 정책으로만 행동하면
  → 특정 (s, a)를 한 번도 방문하지 않을 수 있음
  → Q(s, a) 추정 불가능
  → 최적이 아닌 행동을 시도해봐야 함!
```

## 3. 탐색-활용 딜레마

```
탐색 vs 활용 (Exploration vs Exploitation):
  RL의 근본적 딜레마

  활용(Exploitation): 현재 최선으로 아는 행동 선택
  탐색(Exploration): 새로운 행동을 시도하여 더 나은 것 찾기

  예시 - 레스토랑 선택:
  활용: 항상 좋아하는 식당에 감 (안전)
  탐색: 새로운 식당을 시도 (더 좋은 곳 발견 가능)
  → 적절한 균형이 필요!

ε-Greedy:
  가장 기본적인 탐색 전략

  π(a|s) = {
    1-ε+ε/|A|  if a = argmax Q(s,a)  (최선 행동)
    ε/|A|       otherwise              (랜덤 탐색)
  }

  ε = 0.1: 90% 활용, 10% 탐색
  ε = 1.0: 완전 랜덤
  ε = 0.0: 완전 탐욕적

  ε 감소 (Decay):
  초기에 많이 탐색 → 점차 활용 증가
  ε_t = max(ε_min, ε_start × decay^t)
  → 학습 초기: 넓게 탐색
  → 학습 후기: 최선 행동에 집중

GLIE (Greedy in the Limit with Infinite Exploration):
  수렴을 보장하는 조건
  1. 모든 (s, a)를 무한히 방문
  2. 정책이 결국 탐욕적으로 수렴
  → ε = 1/k (k번째 에피소드)가 GLIE 조건 만족

  | 전략 | 탐색 | 수렴 | 실전 |
  |------|------|------|------|
  | 탐욕적 | 없음 | 보장 안 됨 | 비추천 |
  | ε-greedy (고정) | 일정 | 최적 아님 | 간단 |
  | ε-greedy (감소) | 감소 | GLIE 시 보장 | 가장 일반적 |
  | Softmax/Boltzmann | 가치 비례 | 조건부 | Q 차이가 클 때 |
```

> **핵심 직관**: 탐색-활용 딜레마는 **"정보의 가치"**와 관련됩니다. 탐색은 즉각적 보상은 낮지만 **정보**를 얻습니다. 이 정보가 미래의 더 좋은 결정을 가능하게 합니다. 최적의 탐색량은 "아직 모르는 것이 얼마나 있는가"에 달려있으며, 학습 초기에는 많이 탐색하고 후기에는 활용에 집중합니다.

## 4. On-Policy vs Off-Policy MC

```
On-Policy MC:
  행동하는 정책과 개선하는 정책이 같음

  π로 데이터 수집 → π의 Q를 추정 → π를 개선
  → ε-greedy 정책이 행동 정책이자 목표 정책

  문제: ε > 0이면 최적 정책이 아닌 ε-soft 정책으로 수렴
  → ε을 0으로 줄여야 진정한 최적

Off-Policy MC:
  행동하는 정책(b)과 학습하는 정책(π)이 다름

  행동 정책(Behavior Policy) b: 탐색적 (ε-greedy)
  목표 정책(Target Policy) π: 탐욕적 (최적)

  b로 데이터 수집 → π의 Q를 추정 → π를 개선
  → b가 탐색하면서 π는 순수 탐욕적일 수 있음!

  Importance Sampling:
  b에서 수집한 데이터로 π의 기대값을 추정

  E_π[G_t] = E_b[ρ_t × G_t]

  ρ_t = ∏_{k=t}^{T-1} π(a_k|s_k) / b(a_k|s_k)
  → 중요도 비율: π와 b의 확률 비율

  문제: ρ의 분산이 매우 큼!
  T가 길면 ρ = 곱셈 → 지수적으로 커지거나 작아짐

  Weighted Importance Sampling:
  V(s) = Σ ρ_i G_i / Σ ρ_i
  → 분산을 줄이지만 약간의 편향 도입
  → 실전에서 일반 IS보다 훨씬 안정적

  | 속성 | On-Policy | Off-Policy |
  |------|-----------|-----------|
  | 행동/학습 정책 | 같음 | 다름 |
  | 탐색 | 제한적 | 자유로움 |
  | 데이터 효율 | 낮음 | 높을 수 있음 |
  | 구현 | 간단 | IS 필요 |
  | 분산 | 낮음 | 높음 (IS) |
```

## 5. MC의 장단점과 적용

```
MC의 장점:
  ├─ 모델 불필요: P(s'|s,a)를 알 필요 없음
  ├─ 편향 없음: 실제 G_t를 사용 → 비편향 추정
  ├─ 에피소딕 문제에 자연스러움
  ├─ 구현이 간단
  └─ 부트스트래핑 없음 → 초기 추정에 덜 민감

MC의 단점:
  ├─ 높은 분산: G_t의 분산이 큼
  │   G_t = r₀ + γr₁ + γ²r₂ + ...
  │   → 많은 확률적 요소의 합 → 분산 ↑
  ├─ 에피소드 완료 필요: 지속적 과제에 부적합
  ├─ 느린 수렴: 높은 분산 → 많은 에피소드 필요
  └─ 온라인 학습 불가: 에피소드 중간에 업데이트 불가

  분산 감소 기법:
  ├─ Baseline 사용: G_t 대신 G_t - b(s_t)
  ├─ 여러 에피소드 평균
  └─ rl-06의 Policy Gradient에서 더 상세히

MC vs DP 요약:

  DP:
  V(s) ← E_{P}[R + γV(s')]  (모델 기반, 부트스트랩)
  ├─ 전이 확률 P 필요
  ├─ 한 스텝 업데이트 (부트스트래핑)
  └─ 편향 있을 수 있음 (V 추정치 사용)

  MC:
  V(s) ← V(s) + α(G_t - V(s))  (모델 프리, 샘플 기반)
  ├─ 경험만 필요
  ├─ 에피소드 끝까지 기다림
  └─ 비편향 (실제 G_t 사용)

  → rl-04의 TD가 이 둘의 장점을 결합!
```

## 6. MC 실전 예시

```
블랙잭 MC:
  Sutton & Barto의 대표적 MC 예시

  상태: (플레이어 합, 딜러 오픈 카드, 에이스 유무)
  행동: Hit(카드 받기) 또는 Stick(멈추기)
  보상: 승리 +1, 패배 -1, 무승부 0

  MC 학습:
  1. 100만 번 블랙잭 플레이 (무작위 정책)
  2. 각 에피소드에서 방문한 (s, a)의 G_t 기록
  3. Q(s, a)를 평균으로 추정
  4. π(s) = argmax_a Q(s, a)

  결과:
  "합이 20 이상이면 Stick" 같은 직관적 정책 학습
  딜러 카드가 작을 때 더 보수적으로 행동

MC가 적합한 문제:
  ├─ 에피소딕 (종료가 있는)
  ├─ 모델을 모르거나 복잡한
  ├─ 에피소드가 짧은 (분산 관리)
  ├─ 비편향 추정이 중요한
  └─ 시뮬레이션이 가능한

  DP → MC → TD 진화:
  ├─ DP: 모델 필요, 부트스트랩 O
  ├─ MC: 모델 불필요, 부트스트랩 X (에피소드 끝 필요)
  ├─ TD: 모델 불필요, 부트스트랩 O (한 스텝만!)
  └─ TD가 MC와 DP의 장점을 결합 → rl-04에서
```

## 핵심 정리

- **몬테카를로 방법**은 환경 모델 없이 실제 에피소드의 누적 보상 G_t를 평균하여 가치를 추정하며, 큰 수의 법칙으로 수렴이 보장됩니다
- **First-Visit MC**는 상태의 첫 방문만 카운트하여 비편향 추정을 제공하고, **Every-Visit MC**는 모든 방문을 사용하여 더 낮은 분산을 달성합니다
- **ε-greedy**는 ε 확률로 랜덤 탐색을 하는 가장 기본적인 탐색 전략이며, ε을 점차 줄여 **GLIE 조건**을 만족시키면 최적 수렴이 보장됩니다
- **Off-Policy MC**는 행동 정책(b)과 목표 정책(π)을 분리하여 자유로운 탐색을 가능하게 하지만, **Importance Sampling**의 높은 분산이 문제입니다
- MC는 비편향이지만 **높은 분산과 에피소드 완료 요구**가 단점이며, rl-04의 **TD 학습**이 MC(비편향)와 DP(부트스트래핑)의 장점을 결합합니다
