# RLHF와 LLM 정렬

## 왜 RL이 LLM에 필요한가

nlp-08에서 RLHF를 NLP 관점으로 다루었다면, 이 강의에서는 **RL의 관점**에서 깊이 분석합니다. 사전학습된 LLM은 "다음 토큰 예측"에는 뛰어나지만, "도움이 되고, 정직하고, 안전한" 응답을 생성하는 것은 별개의 문제입니다. **RLHF(Reinforcement Learning from Human Feedback)**는 인간의 선호를 보상 함수로 변환하고, rl-08의 PPO로 LLM을 정렬합니다.

> **핵심 직관**: RLHF의 핵심은 **"인간의 판단을 자동화"**하는 것입니다. "이 응답이 더 좋다"라는 인간의 판단은 수식으로 정의하기 어렵습니다. 보상 모델이 이 판단을 학습하면, PPO가 수백만 번의 업데이트로 LLM을 개선할 수 있습니다. 인간은 수천 개의 비교만 하면 됩니다.

## 1. RLHF 파이프라인

```
RLHF 3단계 파이프라인 (Ouyang et al., 2022):

  1단계: SFT (Supervised Fine-Tuning)
  ├─ 인간이 작성한 시연 데이터 수집
  ├─ 프롬프트 → 이상적 응답 쌍
  ├─ 지도학습으로 파인튜닝
  └─ 결과: SFT 모델 (기본 정렬)

  2단계: 보상 모델 (Reward Model) 학습
  ├─ SFT 모델로 여러 응답 생성 (프롬프트당 K개)
  ├─ 인간이 응답 쌍을 비교 (y_w ≻ y_l)
  ├─ Bradley-Terry 모델:
  │   P(y_w ≻ y_l) = σ(r(x, y_w) - r(x, y_l))
  ├─ 손실: L = -log σ(r(x, y_w) - r(x, y_l))
  └─ 결과: 보상 모델 r(x, y)

  3단계: PPO로 LLM 최적화
  ├─ LLM이 정책 π_θ (상태=프롬프트+이전 토큰, 행동=다음 토큰)
  ├─ 보상: r(x, y) (보상 모델의 점수)
  ├─ KL 페널티: -β KL(π_θ || π_SFT)
  │   → SFT 모델에서 너무 벗어나지 않도록
  │   → 언어 품질/다양성 유지
  ├─ 최종 보상: R = r(x, y) - β log(π_θ(y|x)/π_SFT(y|x))
  └─ PPO로 R 최대화

RL로서의 RLHF:
  상태(State): 프롬프트 + 지금까지 생성된 토큰
  행동(Action): 다음 토큰 선택 (어휘 크기 ~50K)
  전이(Transition): 결정론적 (선택한 토큰 추가)
  보상(Reward): 생성 완료 시 보상 모델 점수 (희소!)
  정책(Policy): LLM π_θ(next_token | prompt + tokens)
  에피소드: 하나의 응답 생성
```

## 2. 보상 모델의 역할

```
보상 모델 학습 (Reward Modeling):

  데이터: {(x, y_w, y_l)} — 프롬프트, 선호/비선호 응답
  모델: SFT 모델의 마지막 레이어를 스칼라 출력으로 교체

  학습:
  L_RM = -E[log σ(r_θ(x, y_w) - r_θ(x, y_l))]

  Bradley-Terry 모델의 가정:
  인간의 선호가 보상 차이의 시그모이드 함수
  → rl-01의 가치 함수가 인간 선호를 반영

  보상 모델의 도전:

  1. 보상 해킹 (Reward Hacking):
     정책이 보상 모델의 약점을 악용
     예: 매우 길고 반복적인 답변 → 높은 보상
     실제로는 나쁜 응답

     완화:
     ├─ KL 페널티: SFT에서 크게 벗어나지 않도록
     ├─ 보상 모델 앙상블
     └─ 보상 정규화/클리핑

  2. 주석자 간 불일치:
     같은 쌍에 대해 주석자마다 다른 선호
     → 노이즈가 보상 모델에 전파
     → 다수 주석자 + 합의 기반 필터링

  3. 분포 이탈:
     PPO가 보상 모델의 학습 분포를 벗어남
     → 보상 모델이 잘못된 높은 점수 부여
     → KL 페널티로 완화

  | 보상 모델 크기 | 보상 정확도 | LLM 품질 |
  |--------------|-----------|---------|
  | 소형 (1B) | 낮음 | 보통 |
  | 중형 (6B) | 중간 | 좋음 |
  | 대형 (13B+) | 높음 | 매우 좋음 |
  → 보상 모델 크기가 최종 성능에 큰 영향
```

## 3. PPO for LLM

```
LLM에서 PPO의 구현:

  환경: 프롬프트 → LLM 생성 → 보상 모델 점수

  PPO 구성 요소:
  Actor: LLM π_θ (응답 생성)
  Critic: 별도 가치 모델 V_φ (응답의 가치 예측)
  Reference: SFT 모델 π_ref (KL 계산용, 동결)

  보상 계산:
  각 토큰 t에서:
  r_t = 0 (중간 토큰)
  r_T = r_RM(x, y) - β log(π_θ(y_T)/π_ref(y_T))  (마지막 토큰)

  또는 토큰 레벨 KL:
  r_t = -β log(π_θ(y_t|y_{<t}, x)/π_ref(y_t|y_{<t}, x))
  r_T += r_RM(x, y)

  → KL 페널티를 매 토큰에 분산

  PPO 업데이트:
  GAE로 Advantage 계산 (rl-07)
  Clipped Objective (rl-08)
  Critic 업데이트
  → 표준 PPO와 동일!

  실전 하이퍼파라미터 (InstructGPT 기준):
  ├─ β (KL 계수): 0.02~0.2
  ├─ PPO 에폭: 1~4
  ├─ 미니배치: 64~512
  ├─ 클리핑 ε: 0.2
  ├─ 학습률: 1e-6 ~ 5e-6
  ├─ 생성 길이: 최대 512~2048 토큰
  └─ Value Head: LLM에 추가된 선형 레이어

  PPO for LLM의 어려움:
  ├─ 매우 큰 행동 공간 (어휘 크기 50K+)
  ├─ 희소 보상 (생성 완료 시에만)
  ├─ 학습 불안정 (LLM이 매우 큼)
  ├─ 연산 비용: Actor + Critic + Reference + Reward = 4개 모델
  └─ 보상 해킹 방지 필요
```

> **핵심 직관**: RLHF에서 PPO가 어려운 이유는 **"보상이 에피소드 끝에만 있다"**는 것입니다. 100 토큰의 응답에서 마지막에만 보상을 받으므로, 어떤 토큰이 좋았고 어떤 토큰이 나빴는지(신용 할당)가 극도로 어렵습니다. rl-01에서 다룬 신용 할당 문제의 극단적 사례입니다.

## 4. DPO: PPO 없는 정렬

```
DPO (Rafailov et al., 2023):
  Direct Preference Optimization

  핵심: 보상 모델 + PPO를 하나로 합침!

  RLHF: 선호 데이터 → 보상 모델 학습 → PPO 최적화
  DPO:  선호 데이터 → 직접 정책 최적화

  수학적 유도:
  RLHF의 최적 정책:
  π*(y|x) = π_ref(y|x) exp(r(x,y)/β) / Z(x)

  이를 뒤집으면:
  r(x,y) = β log(π*(y|x)/π_ref(y|x)) + β log Z(x)

  Bradley-Terry에 대입:
  P(y_w ≻ y_l) = σ(β log(π_θ(y_w|x)/π_ref(y_w|x))
                   - β log(π_θ(y_l|x)/π_ref(y_l|x)))

  DPO 손실:
  L_DPO = -E[log σ(β(log π_θ(y_w)/π_ref(y_w)
                     - log π_θ(y_l)/π_ref(y_l)))]

  → 보상 모델을 명시적으로 학습하지 않고
  → 선호 데이터에서 직접 정책을 학습!

  장점:
  ├─ 보상 모델 학습 불필요
  ├─ PPO 최적화 불필요 (지도학습만!)
  ├─ 구현이 매우 간단
  ├─ 학습이 안정적
  └─ 보상 해킹 리스크 감소

  한계:
  ├─ 온라인 탐색 없음 → 데이터 외 개선 제한
  ├─ β 설정에 민감
  └─ 이론적으로 RLHF와 동등하지만 실전 차이 있음

  DPO 변형:
  ├─ IPO: σ 대신 제곱 손실 → 과적합 감소
  ├─ KTO: 쌍(pair) 대신 개별 좋음/나쁨 판단
  ├─ ORPO: SFT + DPO를 하나로 결합
  └─ SimPO: Reference 모델 불필요

  | 방법 | 보상 모델 | PPO | 데이터 | 안정성 |
  |------|---------|-----|-------|--------|
  | RLHF | 필요 | 필요 | 쌍 비교 | 낮음 |
  | DPO | 불필요 | 불필요 | 쌍 비교 | 높음 |
  | KTO | 불필요 | 불필요 | 개별 | 높음 |
  | ORPO | 불필요 | 불필요 | 쌍 비교 | 높음 |
```

## 5. 반복적 정렬과 RLVR

```
반복적 RLHF/DPO:

  Online DPO (Iterative DPO):
  1. 현재 정책으로 응답 생성
  2. 보상 모델(또는 인간)으로 선호 데이터 생성
  3. DPO로 정책 업데이트
  4. 반복
  → 자기 생성 데이터로 지속 개선

  Self-Play Fine-Tuning (SPIN):
  ├─ 현재 모델 vs SFT 데이터로 DPO
  ├─ 반복할수록 SFT 데이터에 가까워짐
  └─ 추가 선호 데이터 불필요

RLVR (RL from Verifiable Rewards):
  검증 가능한 보상으로 직접 학습

  수학 문제:
  보상 = 정답 여부 (자동 검증 가능!)
  → 인간 피드백 불필요

  코딩 문제:
  보상 = 테스트 케이스 통과 여부
  → 자동으로 보상 생성

  DeepSeek-R1, OpenAI o1:
  ├─ 추론 능력 강화에 RLVR 활용
  ├─ 수학/코딩에서 정답 여부로 보상
  ├─ Chain-of-Thought 추론을 RL로 학습
  └─ 인간 피드백보다 확장 가능!

  | 정렬 방법 | 보상 원천 | 확장성 | 적합한 영역 |
  |----------|---------|--------|-----------|
  | RLHF | 인간 비교 | 낮음 | 일반 대화 |
  | DPO | 인간 비교 | 중간 | 일반 대화 |
  | RLVR | 자동 검증 | 높음 | 수학, 코딩 |
  | Constitutional AI | AI 피드백 | 높음 | 안전성 |
```

## 6. 정렬의 RL 관점 분석

```
RLHF를 RL 프레임워크로:

  MDP 관점:
  상태: (프롬프트, 지금까지 토큰)
  행동: 다음 토큰 (|A| ≈ 50K)
  전이: 결정론적 (토큰 추가)
  보상: 희소 (완료 시 보상 모델)
  정책: LLM

  KL 페널티의 RL 해석:
  R = r_RM - β KL(π || π_ref)
  → 보상 최대화 + 참조 정책 근처 유지
  → rl-08의 Trust Region과 동일한 원리!
  → RLHF = 보상 최적화 with Trust Region

  Alignment Tax:
  정렬 후 벤치마크 성능이 약간 하락
  → KL 제약이 능력을 일부 제한
  → 안전성과 유용성의 트레이드오프

  RL의 교훈이 정렬에 주는 시사점:
  ├─ 보상 해킹 = rl-01의 잘못된 보상 설계
  ├─ KL 페널티 = rl-08의 Trust Region
  ├─ DPO = rl-12의 오프라인 RL (데이터에서만 학습)
  ├─ RLVR = rl-01의 밀집 보상 (자동 검증)
  └─ 반복적 정렬 = rl-10의 자기 대국 (자기 개선)

  → RLHF/DPO는 RL의 특수 사례이며,
     RL의 원칙이 그대로 적용됩니다.
```

## 핵심 정리

- **RLHF**는 인간 선호 데이터로 보상 모델을 학습하고, PPO로 LLM을 최적화하며, KL 페널티가 SFT 모델에서의 이탈을 제한합니다
- **보상 해킹**(정책이 보상 모델의 약점 악용)이 RLHF의 핵심 도전이며, KL 페널티, 보상 앙상블, 보상 정규화로 완화합니다
- **DPO**는 보상 모델과 PPO 없이 선호 데이터에서 직접 정책을 최적화하는 지도학습 방식이며, 간단하고 안정적이지만 온라인 탐색이 없습니다
- **RLVR**은 수학/코딩 등 자동 검증 가능한 보상으로 학습하여 인간 피드백보다 확장 가능하며, DeepSeek-R1과 o1의 추론 능력 강화에 활용됩니다
- RLHF의 KL 페널티는 PPO의 **Trust Region**과, DPO는 **오프라인 RL**과, RLVR은 **밀집 보상**과 동일한 원리이며, 정렬은 RL의 특수 사례입니다
