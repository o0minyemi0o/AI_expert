# PPO와 TRPO

## 왜 업데이트 크기를 제한하는가

rl-06의 Policy Gradient는 학습률에 극도로 민감합니다. 학습률이 조금만 크면 정책이 급변하여 성능이 붕괴하고, 회복이 불가능할 수 있습니다. **TRPO**는 "정책 변화를 신뢰 영역 내로 제한"하여 이 문제를 해결했고, **PPO**는 이를 간단한 클리핑으로 구현하여 실전 표준이 되었습니다. PPO는 RLHF(rl-13)에서 LLM 정렬에도 사용됩니다.

> **핵심 직관**: PPO의 핵심은 **"조금씩, 안전하게 개선"**하는 것입니다. 정책이 크게 바뀌면 이전 데이터가 무효화되고 학습이 불안정합니다. "현재 정책에서 너무 멀리 가지 마라"는 제약이 안정적이면서 꾸준한 개선을 보장합니다. ms-07의 정규화와 유사한 "과도한 변화 방지"의 원리입니다.

## 1. TRPO: 신뢰 영역 정책 최적화

```
TRPO (Schulman et al., 2015):
  Trust Region Policy Optimization

  Policy Gradient의 문제:
  θ ← θ + α ∇J(θ)
  → α가 크면 정책이 급변 → 성능 붕괴
  → "한 발짝"이 너무 크면 절벽에서 떨어짐

  대리 목적 함수 (Surrogate Objective):
  L(θ) = E[π_θ(a|s)/π_{θ_old}(a|s) · Â(s, a)]
       = E[r_t(θ) · Â(s, a)]

  r_t(θ) = π_θ(a|s) / π_{θ_old}(a|s): 확률 비율

  r_t = 1: 새 정책 = 이전 정책
  r_t > 1: 이 행동의 확률 증가
  r_t < 1: 이 행동의 확률 감소

  TRPO의 제약:
  maximize L(θ)
  subject to KL(π_{θ_old} || π_θ) ≤ δ

  → KL 발산으로 정책 변화 크기를 제한
  → δ = 0.01이 일반적

  풀이:
  ├─ 자연 그래디언트 (Natural Gradient)
  ├─ 피셔 정보 행렬 F의 역행렬 필요
  ├─ Conjugate Gradient로 근사적 풀이
  └─ 라인 서치로 제약 만족 확인

  수렴 보장:
  단조 개선 (Monotonic Improvement) 보장
  → 각 업데이트가 성능을 떨어뜨리지 않음

  문제: 구현이 복잡!
  ├─ 피셔 행렬, Conjugate Gradient
  ├─ 라인 서치
  └─ 코드가 길고 디버깅 어려움
  → PPO가 이를 간단히 해결
```

## 2. PPO: 근위 정책 최적화

```
PPO (Schulman et al., 2017):
  Proximal Policy Optimization

  TRPO의 아이디어를 클리핑으로 간소화

  PPO-Clip 목적 함수:
  L^CLIP(θ) = E[min(r_t · Â, clip(r_t, 1-ε, 1+ε) · Â)]

  r_t = π_θ(a|s) / π_{θ_old}(a|s)
  ε = 0.2 (보통)

  clip(r_t, 1-ε, 1+ε):
  r_t를 [0.8, 1.2] 범위로 제한

  min의 역할:
  Â > 0 (좋은 행동):
    r_t가 1.2를 넘으면 클리핑 → 더 이상 증가 안 함
    "좋은 행동이라고 너무 확률을 높이지 마라"

  Â < 0 (나쁜 행동):
    r_t가 0.8 아래로 가면 클리핑 → 더 이상 감소 안 함
    "나쁜 행동이라고 너무 확률을 낮추지 마라"

  → 정책 변화를 ε 범위로 자연스럽게 제한!
  → TRPO의 Trust Region을 간단한 클리핑으로 근사

  시각적 이해:
  Â > 0:
  L = min(r·Â, 1.2·Â)
       ↗ r < 1.2까지는 증가
       — r ≥ 1.2에서 평탄 (더 이상 증가 안 함)

  Â < 0:
  L = min(r·Â, 0.8·Â)
       ↘ r > 0.8까지는 감소
       — r ≤ 0.8에서 평탄 (더 이상 감소 안 함)
```

## 3. PPO 전체 알고리즘

```
PPO 알고리즘:

  하이퍼파라미터:
  ├─ N: 병렬 환경 수 (8~64)
  ├─ T: 롤아웃 길이 (128~2048)
  ├─ K: 에폭 수 (3~10)
  ├─ M: 미니배치 수 (4~32)
  ├─ ε: 클리핑 계수 (0.1~0.3)
  ├─ γ: 할인율 (0.99)
  ├─ λ: GAE 계수 (0.95)
  └─ c₁, c₂: Critic, 엔트로피 계수

  알고리즘:
  for iteration = 1, 2, ...:
    1. 데이터 수집:
       N개 환경에서 T 스텝씩 롤아웃
       → N×T 개의 (s, a, r, s', log π_old) 수집

    2. Advantage 계산:
       GAE로 Â_t 계산 (rl-07)
       V_target = Â_t + V(s_t)

    3. 미니배치 업데이트:
       for epoch = 1 to K:
         데이터를 M개 미니배치로 나눔
         for each minibatch:
           r_t = π_θ(a|s) / π_{θ_old}(a|s)
           L_actor = -min(r·Â, clip(r,1-ε,1+ε)·Â)
           L_critic = (V_θ(s) - V_target)²
           L_entropy = -H(π_θ)
           L = L_actor + c₁·L_critic - c₂·L_entropy
           θ ← θ - α∇L

    4. θ_old ← θ

  핵심 포인트:
  같은 데이터를 K 에폭 재사용!
  → 샘플 효율성 향상 (A2C 대비)
  → 클리핑이 과도한 업데이트를 방지하므로 재사용 안전

  | 하이퍼파라미터 | Atari | MuJoCo | 일반적 |
  |--------------|-------|--------|--------|
  | N (환경 수) | 8 | 1 | 8~64 |
  | T (롤아웃) | 128 | 2048 | 128~2048 |
  | K (에폭) | 4 | 10 | 3~10 |
  | ε (클리핑) | 0.1 | 0.2 | 0.1~0.3 |
  | 학습률 | 2.5e-4 | 3e-4 | 1e-4~3e-4 |
  | 배치 크기 | 256 | 64 | 64~256 |
```

> **핵심 직관**: PPO가 같은 데이터를 여러 에폭 재사용할 수 있는 이유는 **클리핑이 "안전 장치"** 역할을 하기 때문입니다. r_t가 [1-ε, 1+ε] 밖으로 나가면 그래디언트가 0이 되어 추가 업데이트가 멈춥니다. 이 메커니즘이 "같은 데이터에서 너무 많이 배우는 것"을 방지합니다.

## 4. PPO 변형과 실전 트릭

```
PPO-Penalty (대안 버전):
  클리핑 대신 KL 페널티

  L = E[r_t · Â - β KL(π_old || π)]

  β를 적응적으로 조절:
  KL > KL_target × 1.5: β ← 2β (제약 강화)
  KL < KL_target / 1.5: β ← β/2 (제약 완화)

  → 실전에서 PPO-Clip이 더 선호 (하이퍼파라미터 적음)

실전 트릭 (구현 디테일이 성능에 큰 영향):

  1. 가치 함수 클리핑:
     V_loss = max((V-V_target)², (clip(V, V_old±ε)-V_target)²)
     → Critic도 급격한 변화 방지

  2. Advantage 정규화:
     Â = (Â - mean) / (std + 1e-8)
     → 미니배치 내에서 정규화 → 학습 안정

  3. 그래디언트 클리핑:
     max_grad_norm = 0.5
     → 큰 그래디언트 방지

  4. 학습률 감쇠:
     선형 또는 코사인 감쇠
     → 학습 후반부 안정성

  5. 직교 초기화:
     가중치를 직교 행렬로 초기화
     → 학습 초기 안정성

  6. 보상 정규화/클리핑:
     보상을 [-10, 10] 범위로
     → 극단적 보상의 영향 제한

  "구현 사항이 성능의 절반":
  같은 PPO라도 이 트릭들의 유무로
  성능이 2배 이상 차이날 수 있음!
```

## 5. PPO의 위치와 비교

```
현대 RL 알고리즘 비교:

  On-Policy:
  ├─ A2C: 간단, 기본 AC
  ├─ PPO: 안정적, 범용 (사실상 표준)
  └─ TRPO: 이론적, 구현 복잡

  Off-Policy:
  ├─ DQN/Rainbow: 이산 행동 (Atari)
  ├─ SAC: 연속 행동 (로봇)
  └─ TD3: 연속 행동 (SAC 대안)

  PPO가 표준인 이유:
  ├─ 구현이 간단 (50줄 핵심 코드)
  ├─ 하이퍼파라미터에 강건
  ├─ 이산/연속 행동 모두 지원
  ├─ 대규모 병렬화에 적합
  ├─ 다양한 환경에서 일관적 성능
  └─ OpenAI, Google 등에서 광범위 사용

  PPO의 한계:
  ├─ 샘플 효율이 off-policy보다 낮음
  │   → 같은 성능에 더 많은 환경 상호작용 필요
  ├─ 시뮬레이션이 비싸면 비효율적
  └─ 매우 긴 시야(long-horizon) 과제에서 어려움

  선택 가이드:
  | 상황 | 추천 | 이유 |
  |------|------|------|
  | 범용 | PPO | 안정적, 간단 |
  | 이산+Atari | Rainbow | 샘플 효율 |
  | 연속+샘플 효율 | SAC | Off-policy |
  | RLHF/LLM | PPO | 표준 |
  | 빠른 프로토타입 | PPO | 구현 용이 |
```

## 6. PPO 코드와 디버깅

```
PPO 핵심 코드 (의사 코드):

  # 롤아웃 수집
  for step in range(T):
    action, log_prob = actor.sample(state)
    next_state, reward, done = env.step(action)
    buffer.add(state, action, reward, log_prob, done)

  # GAE 계산
  advantages = compute_gae(buffer, critic, gamma, lam)
  returns = advantages + values

  # PPO 업데이트
  for epoch in range(K):
    for batch in buffer.sample_minibatches(M):
      ratio = exp(actor.log_prob(batch.action, batch.state)
                  - batch.old_log_prob)
      surr1 = ratio * batch.advantage
      surr2 = clip(ratio, 1-eps, 1+eps) * batch.advantage
      actor_loss = -min(surr1, surr2).mean()
      critic_loss = (critic(batch.state) - batch.returns)^2
      entropy = actor.entropy(batch.state).mean()
      loss = actor_loss + 0.5*critic_loss - 0.01*entropy
      optimizer.step(loss)

디버깅 체크리스트:
  ├─ □ 보상 곡선이 상승하는가?
  ├─ □ 정책 엔트로피가 적절히 감소하는가?
  ├─ □ 가치 함수 손실이 감소하는가?
  ├─ □ KL 발산이 너무 크지 않은가? (< 0.05)
  ├─ □ clip 비율이 적절한가? (대부분 1 근처)
  ├─ □ Advantage의 분포가 합리적인가?
  └─ □ 그래디언트 크기가 안정적인가?

  문제 해결:
  보상이 안 오르면 → 환경/보상 확인, 학습률 조절
  불안정하면 → ε 줄이기, 그래디언트 클리핑 강화
  느리면 → 환경 수(N) 늘리기, 롤아웃(T) 조절
```

## 핵심 정리

- **TRPO**는 KL 발산으로 정책 변화를 제한하여 단조 개선을 보장하지만, 자연 그래디언트와 Conjugate Gradient로 구현이 복잡합니다
- **PPO-Clip**은 확률 비율 r_t를 [1-ε, 1+ε]로 클리핑하여 TRPO의 Trust Region을 간단히 구현하며, ε=0.2가 일반적입니다
- PPO는 **같은 데이터를 K 에폭 재사용**하여 A2C보다 샘플 효율이 높으며, 클리핑이 과도한 업데이트를 방지합니다
- 실전 PPO에서는 **Advantage 정규화, 그래디언트 클리핑, 학습률 감쇠** 등 구현 디테일이 성능의 절반을 차지합니다
- PPO는 **구현 간단, 하이퍼파라미터 강건, 이산/연속 행동 지원**으로 사실상 on-policy RL의 표준이며, RLHF/LLM 정렬에도 사용됩니다
