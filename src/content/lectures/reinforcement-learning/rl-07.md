# Actor-Critic

## 왜 Actor와 Critic을 결합하는가

rl-06의 REINFORCE는 비편향이지만 분산이 높고, rl-05의 DQN은 분산이 낮지만 연속 행동을 다룰 수 없습니다. **Actor-Critic**은 이 두 방법을 결합합니다: **Actor(정책)**가 행동을 선택하고, **Critic(가치)**이 행동을 평가합니다. Critic의 평가가 REINFORCE의 높은 분산을 해결하며, 현대 RL(PPO, SAC)의 기본 프레임워크입니다.

> **핵심 직관**: Actor-Critic은 **"배우와 비평가"**의 관계입니다. 배우(Actor)가 연기하고, 비평가(Critic)가 "좋았다/나빴다"를 평가합니다. REINFORCE에서 Critic은 관객의 환호(G_t)였지만, Actor-Critic에서 Critic은 **전문 비평가 V(s)**입니다. 전문가의 평가가 관객의 반응보다 일관적이므로 분산이 낮습니다.

## 1. Actor-Critic 기본 구조

```
Actor-Critic 아키텍처:

  Actor (정책 π_θ):
  상태 s → 신경망 → π_θ(a|s)
  → 행동의 확률 분포 출력
  → 이 분포에서 행동 a 샘플

  Critic (가치 V_w 또는 Q_w):
  상태 s → 신경망 → V_w(s) 또는 Q_w(s, a)
  → 상태/행동의 가치 추정

  업데이트:

  Critic 업데이트 (TD 학습):
  δ = r + γV_w(s') - V_w(s)  (TD 에러)
  w ← w + α_w · δ · ∇_w V_w(s)

  Actor 업데이트 (Policy Gradient):
  θ ← θ + α_θ · δ · ∇_θ log π_θ(a|s)

  → δ(TD 에러)가 Advantage의 추정!
  → δ > 0: 예상보다 좋았음 → 행동 확률 증가
  → δ < 0: 예상보다 나빴음 → 행동 확률 감소

  REINFORCE vs Actor-Critic:
  REINFORCE: θ ← θ + α · (G_t - V(s)) · ∇ log π
  A-C:       θ ← θ + α · (r + γV(s') - V(s)) · ∇ log π

  G_t → r + γV(s'): MC → TD로 전환
  ├─ 분산 감소: V(s')는 G_t보다 안정적
  ├─ 온라인 학습: 에피소드 끝까지 기다릴 필요 없음
  └─ 편향 도입: V(s')가 부정확하면 편향 → 학습으로 개선

네트워크 구조 선택:

  분리 구조:
  Actor: s → π(a|s)  (별도 네트워크)
  Critic: s → V(s)   (별도 네트워크)
  → 각각 독립적으로 학습 가능

  공유 구조:
  s → 공유 특징 → Actor Head: π(a|s)
                 → Critic Head: V(s)
  → 특징 추출 공유 → 계산 효율적
  → 학습 간 간섭 가능성

  | 구조 | 장점 | 단점 | 사용 |
  |------|------|------|------|
  | 분리 | 안정적 | 비효율적 | 연구 |
  | 공유 | 효율적 | 간섭 가능 | 실전 (대부분) |
```

## 2. A2C: Advantage Actor-Critic

```
A2C (Advantage Actor-Critic):
  동기적(synchronous) 병렬 Actor-Critic

  핵심: Advantage 함수 사용

  A(s, a) = Q(s, a) - V(s)
  ≈ r + γV(s') - V(s) = δ (TD 에러)

  → V만 학습하면 Advantage 추정 가능!
  → Q를 별도로 학습할 필요 없음

  손실 함수:
  L = L_actor + c₁ L_critic + c₂ L_entropy

  L_actor = -E[log π_θ(a|s) · Â(s, a)]
  L_critic = E[(V_w(s) - V_target)²]
  L_entropy = -E[H(π_θ(·|s))]

  c₁ = 0.5 (Critic 가중치)
  c₂ = 0.01 (엔트로피 가중치)

  엔트로피 보너스:
  H(π) = -Σ_a π(a) log π(a)
  → 정책이 너무 결정적이 되는 것을 방지
  → 충분한 탐색 유지

병렬 환경:
  N개의 환경을 동시에 실행

  Env 1: s₁ → a₁ → r₁, s'₁
  Env 2: s₂ → a₂ → r₂, s'₂
  ...
  Env N: s_N → a_N → r_N, s'_N

  → N개의 경험을 동시에 수집
  → 배치 업데이트로 분산 감소
  → 벡터화로 GPU 효율적 활용

  보통 N = 8~64개 환경 동시 실행
```

## 3. A3C: 비동기 Actor-Critic

```
A3C (Mnih et al., 2016):
  Asynchronous Advantage Actor-Critic

  구조:
  글로벌 네트워크 (θ, w)
    ↕ (파라미터 동기화)
  워커 1: 자체 환경에서 학습
  워커 2: 자체 환경에서 학습
  ...
  워커 K: 자체 환경에서 학습

  각 워커의 알고리즘:
  1. 글로벌 → 로컬 파라미터 복사
  2. n 스텝 동안 에피소드 진행
  3. n-step 리턴 계산
  4. 로컬 그래디언트 계산
  5. 글로벌 파라미터에 비동기 업데이트
  6. 반복

  비동기의 장점:
  ├─ 락(lock) 없이 병렬 학습 → 빠름
  ├─ 각 워커가 다른 환경 상태 탐색 → 다양한 경험
  ├─ Experience Replay 불필요! (다양성이 자연 확보)
  └─ CPU만으로도 효율적 학습

  A3C vs A2C:
  A3C: 비동기 → 각 워커가 독립 업데이트
  A2C: 동기 → 모든 워커가 동시에 한 배치 업데이트

  실전에서 A2C가 더 선호:
  ├─ 구현이 간단 (GPU 배치 처리)
  ├─ 성능이 A3C와 동등
  ├─ 디버깅이 쉬움 (결정론적)
  └─ GPU 활용에 적합
```

## 4. GAE: 일반화 이점 추정

```
GAE (Generalized Advantage Estimation, Schulman et al., 2016):
  Advantage 추정의 편향-분산 트레이드오프를 제어

  문제:
  1-step Advantage: Â = r + γV(s') - V(s)
  → 낮은 분산, 높은 편향 (V가 부정확하면)

  MC Advantage: Â = G_t - V(s)
  → 높은 분산, 낮은 편향

  → rl-04의 n-step TD와 같은 딜레마!

  GAE: TD(λ)의 Advantage 버전

  δ_t = r_t + γV(s_{t+1}) - V(s_t)  (1-step TD 에러)

  Â_t^GAE(γ,λ) = Σ_{l=0}^∞ (γλ)^l δ_{t+l}
               = δ_t + (γλ)δ_{t+1} + (γλ)²δ_{t+2} + ...

  λ = 0: Â = δ_t (1-step, 낮은 분산, 높은 편향)
  λ = 1: Â = G_t - V(s_t) (MC, 높은 분산, 낮은 편향)

  보통 λ = 0.95~0.99가 최적

  효율적 계산 (역방향):
  Â_T = δ_T
  Â_t = δ_t + γλ Â_{t+1}
  → 에피소드 끝에서 시작하여 역방향으로 계산

  | λ | 편향 | 분산 | 적합한 상황 |
  |---|------|------|-----------|
  | 0 | 높음 | 낮음 | V가 정확할 때 |
  | 0.95 | 중간 | 중간 | 일반적 |
  | 0.99 | 낮음 | 중높음 | 에피소드 짧을 때 |
  | 1.0 | 없음 | 높음 | MC와 동일 |

  → PPO(rl-08)에서 GAE가 표준으로 사용됨
```

> **핵심 직관**: GAE는 **"가까운 미래는 믿고, 먼 미래는 의심"**하는 것입니다. δ_t(한 스텝 후)는 V(s')를 사용하므로 편향이 있지만 분산이 낮습니다. δ_{t+2}(세 스텝 후)는 정보가 많지만 불확실합니다. (γλ)^l이 먼 미래의 기여를 지수적으로 줄여 이 균형을 맞춥니다.

## 5. Actor-Critic 변형들

```
SAC (Soft Actor-Critic, Haarnoja et al., 2018):
  엔트로피 최대화 Actor-Critic

  목적: 보상 최대화 + 엔트로피 최대화
  J = E[Σ γ^t (r_t + α H(π(·|s_t)))]

  → "보상도 높고, 행동도 다양하게"
  → 탐색-활용의 자동 균형!

  특징:
  ├─ Off-policy (Replay Buffer 사용)
  ├─ 자동 엔트로피 조절 (α 자동 학습)
  ├─ 쌍 Q 네트워크 (Double Q로 과추정 방지)
  └─ 연속 행동에 최적화

  → 로봇 학습에서 가장 인기 있는 알고리즘

TD3 (Twin Delayed DDPG, Fujimoto et al., 2018):
  결정론적 정책의 Actor-Critic

  DDPG 기반 + 3가지 트릭:
  ├─ Twin Q: 두 Q 중 작은 값 사용 (과추정 방지)
  ├─ Delayed Actor: Actor를 Critic보다 덜 자주 업데이트
  └─ Target Policy Smoothing: 타겟에 잡음 추가

  | 알고리즘 | 정책 | Off/On | 행동 | 특징 |
  |---------|------|--------|------|------|
  | A2C | 확률적 | On | 이산/연속 | 간단, 병렬 |
  | PPO | 확률적 | On | 이산/연속 | 안정적 |
  | SAC | 확률적 | Off | 연속 | 샘플 효율 |
  | TD3 | 결정론적 | Off | 연속 | 안정적 |
  | DDPG | 결정론적 | Off | 연속 | 초기 연속 AC |
```

## 6. Actor-Critic 실전

```
구현 가이드:

  공유 네트워크 (Atari 등 이미지 입력):
  이미지 → CNN → 공유 특징
    → Actor Head: FC → softmax → π(a|s)
    → Critic Head: FC → V(s)

  분리 네트워크 (연속 제어):
  Actor: s → FC(256) → FC(256) → μ, log σ
  Critic: s → FC(256) → FC(256) → V(s)

  하이퍼파라미터:
  ├─ 학습률: Actor 3e-4, Critic 1e-3
  ├─ γ: 0.99
  ├─ GAE λ: 0.95
  ├─ 엔트로피 계수: 0.01
  ├─ 병렬 환경: 8~64개
  └─ 업데이트 주기: 매 128~2048 스텝

  디버깅 팁:
  ├─ Critic 먼저 확인: V(s) 수렴하는지
  ├─ 정책 엔트로피 모니터링: 너무 빨리 떨어지면 문제
  ├─ 보상 스케일링: 보상 정규화
  └─ 그래디언트 크기: Actor와 Critic 모두 확인

  Actor-Critic → PPO:
  A2C에서 한 가지만 바꾸면 PPO:
  "Policy Gradient의 업데이트 크기를 제한"
  → rl-08에서 상세히
```

## 핵심 정리

- **Actor-Critic**은 Actor(정책)와 Critic(가치)을 결합하여 REINFORCE의 높은 분산을 해결하며, TD 에러 δ = r + γV(s') - V(s)가 Advantage의 추정치입니다
- **A2C**는 동기적 병렬 환경에서 Actor-Critic을 실행하며, 정책 손실 + 가치 손실 + 엔트로피 보너스의 결합 손실로 학습합니다
- **A3C**는 비동기 병렬 학습으로 Experience Replay 없이 다양한 경험을 수집하지만, 실전에서는 GPU 효율이 좋은 A2C가 더 선호됩니다
- **GAE**는 λ로 Advantage의 편향-분산 트레이드오프를 제어하며, λ=0.95가 일반적이고 PPO의 표준 구성 요소입니다
- **SAC**는 엔트로피 최대화로 탐색-활용을 자동 균형하는 off-policy Actor-Critic이며, 연속 행동의 로봇 학습에서 가장 인기 있습니다
