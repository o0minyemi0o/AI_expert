# 다중 에이전트 RL

## 왜 여러 에이전트가 필요한가

지금까지는 **하나의 에이전트**가 환경과 상호작용했습니다. 하지만 현실은 **여러 에이전트가 동시에 존재**합니다. 축구에서 11명이 협력하고, 주식 시장에서 수천 명이 경쟁하며, 자율주행 차량들이 도로를 공유합니다. **다중 에이전트 RL(MARL)**은 여러 에이전트가 상호작용하는 환경에서 학습하며, 협력, 경쟁, 통신의 새로운 도전을 다룹니다.

> **핵심 직관**: MARL의 근본적 어려움은 **"다른 에이전트도 학습한다"**는 것입니다. 단일 에이전트 RL에서 환경은 고정(정적)이지만, MARL에서는 다른 에이전트가 환경의 일부이므로 **환경 자체가 변합니다**. 내가 정책을 바꾸면 상대도 바꾸고, 상대가 바꾸면 나도 바꿔야 합니다. 이 비정상성(non-stationarity)이 MARL의 핵심 난제입니다.

## 1. MARL의 기본 프레임워크

```
다중 에이전트 환경 유형:

  협력 (Cooperative):
  ├─ 모든 에이전트가 같은 보상을 공유
  ├─ 팀 스포츠, 로봇 협업, 드론 편대
  └─ 목표: 팀 보상 최대화

  경쟁 (Competitive):
  ├─ 한 에이전트의 이익 = 다른 에이전트의 손실
  ├─ 제로섬 게임: 바둑, 체스
  └─ 목표: 상대를 이기기

  혼합 (Mixed):
  ├─ 협력과 경쟁이 공존
  ├─ 교통: 각자 목적지 + 충돌 회피
  └─ 경매: 자신의 이익 + 사회적 효율

  수학적 프레임워크:

  Dec-POMDP (Decentralized POMDP):
  ├─ N개의 에이전트
  ├─ 각 에이전트 i: 관찰 o_i, 행동 a_i
  ├─ 공동 행동 a = (a₁, ..., a_N)
  ├─ 전이: P(s'|s, a₁, ..., a_N)
  └─ 보상: R(s, a₁, ..., a_N)

  마르코프 게임 (Stochastic Game):
  ├─ 각 에이전트가 독립적 보상 r_i
  ├─ 각자의 정책 π_i를 최적화
  └─ 내쉬 균형이 목표

  핵심 난제:
  1. 비정상성: 다른 에이전트도 학습 → 환경 변화
  2. 부분 관찰: 다른 에이전트의 행동/의도 모름
  3. 조합 폭발: 공동 행동 공간 |A|^N
  4. 신용 할당: 팀 보상에서 내 기여는?
```

## 2. 독립 학습과 CTDE

```
Independent Learning:
  각 에이전트가 독립적으로 단일 에이전트 RL

  Independent Q-Learning (IQL):
  각 에이전트 i가 자기 Q_i(o_i, a_i)만 학습
  다른 에이전트를 환경의 일부로 취급

  문제:
  ├─ 비정상성: 다른 에이전트가 정책을 바꾸면
  │   환경이 변한 것처럼 보임 → Q 값 의미 상실
  ├─ 조정 실패: "둘 다 왼쪽" or "둘 다 오른쪽"이 최적인데
  │   독립 학습은 조정 불가
  └─ 신용 할당: 팀이 졌는데 내 탓인가 동료 탓인가?

  그래도 놀랍게도 잘 작동하는 경우가 많음!
  → 간단하고 확장 가능

CTDE (Centralized Training, Decentralized Execution):
  "학습은 중앙에서, 실행은 분산으로"

  학습 시: 모든 에이전트의 정보를 활용 (전역 상태 등)
  실행 시: 각 에이전트가 자기 관찰만으로 행동

  왜 CTDE인가:
  ├─ 학습: 시뮬레이션 → 전역 정보 사용 가능
  ├─ 실행: 실세계 → 통신 제한, 분산 필요
  └─ 학습의 이점을 실행에 전이

  대표 알고리즘:

  QMIX (Rashid et al., 2018):
  ├─ 각 에이전트: Q_i(o_i, a_i) 학습
  ├─ 혼합 네트워크: Q_tot = f(Q₁, Q₂, ..., Q_N)
  ├─ 단조성 제약: ∂Q_tot/∂Q_i ≥ 0
  │   → 개인 Q 증가 = 전체 Q 증가
  ├─ Hypernetwork로 혼합 가중치 생성
  └─ argmax Q_tot = 각 에이전트의 argmax Q_i

  → 개인 최선 = 팀 최선이 보장!
  → 분산 실행 가능 (각자 argmax Q_i)

  | 방법 | 학습 | 실행 | 조정 |
  |------|------|------|------|
  | IQL | 독립 | 독립 | 약함 |
  | QMIX | 중앙 | 분산 | 중간 |
  | MAPPO | 중앙 | 분산 | 강함 |
```

## 3. MAPPO와 정책 기반 MARL

```
MAPPO (Multi-Agent PPO, Yu et al., 2022):
  PPO를 다중 에이전트로 확장

  구조:
  각 에이전트: Actor π_i(a_i|o_i) + Critic V(s)
  → Actor는 개인 관찰, Critic은 전역 상태 사용!

  Centralized Critic:
  V(s) 또는 V(o₁, o₂, ..., o_N)
  → 전역 정보로 가치 추정 → 더 정확한 Advantage
  → 학습 시에만 사용, 실행 시 불필요

  파라미터 공유:
  모든 에이전트가 같은 네트워크 사용
  입력에 에이전트 ID 추가
  → 파라미터 효율적, 확장 가능
  → 에이전트가 100명이어도 동작!

  MAPPO가 의외로 강한 이유:
  ├─ 간단한 구현
  ├─ 하이퍼파라미터에 강건
  ├─ PPO의 안정적 업데이트
  └─ 많은 환경에서 복잡한 방법과 동등

  StarCraft Multi-Agent Challenge (SMAC):
  ├─ 유닛 조합의 미시적 제어
  ├─ 협력: 팀이 적을 이기기
  ├─ MAPPO, QMIX 등의 표준 벤치마크
  └─ MAPPO가 많은 맵에서 SOTA

  | 알고리즘 | 유형 | Critic | 특징 |
  |---------|------|--------|------|
  | IQL | 값 기반 | 개인 | 간단 |
  | QMIX | 값 기반 | Q 혼합 | 단조성 |
  | MADDPG | AC | 중앙 | 연속 행동 |
  | MAPPO | AC | 중앙 | 범용 |
```

## 4. 통신 학습

```
에이전트 간 통신:
  에이전트가 메시지를 주고받아 협력

  CommNet (Sukhbaatar et al., 2016):
  ├─ 각 에이전트의 숨은 상태를 평균하여 메시지로
  ├─ h_i^{l+1} = σ(W h_i^l + C mean(h_j^l))
  └─ 간단한 평균 통신

  TarMAC (Das et al., 2019):
  ├─ 어텐션 기반 통신
  ├─ 각 에이전트가 "누구의 말을 들을지" 결정
  ├─ Key-Value Attention으로 메시지 선택
  └─ 동적으로 통신 구조 학습

  NeurComm:
  ├─ 자연어와 유사한 프로토콜 자동 학습
  ├─ 이산 메시지 → 언어의 탄생?
  └─ 흥미로운 연구 방향

  통신의 도전:
  ├─ 대역폭 제한: 실세계에서 무한 통신 불가
  ├─ 학습 어려움: 메시지 의미를 자동으로 학습해야
  ├─ 확장성: 에이전트 수 증가 시 통신 폭발
  └─ 부분 관찰: 어떤 정보를 공유할지 결정

  실전에서는:
  통신 없는 MAPPO/QMIX가 대부분 충분
  → 통신은 "통신 대역폭이 핵심"인 문제에서만
```

> **핵심 직관**: 다중 에이전트 통신은 **"인간 팀의 커뮤니케이션"**과 같습니다. 축구에서 "패스해!"라고 외치는 것처럼, 에이전트도 간결한 메시지로 협력합니다. 하지만 효과적인 통신 프로토콜을 **자동으로 학습**하는 것이 도전이며, 이것은 언어의 진화와 유사한 문제입니다.

## 5. 경쟁과 자기 대국

```
경쟁적 MARL:

  자기 대국 (Self-Play):
  rl-10에서 다룬 AlphaZero의 핵심 기법
  ├─ 자신의 복사본과 대전
  ├─ 자동 커리큘럼: 실력에 맞는 상대
  └─ 내쉬 균형으로 수렴 가능

  Population-Based Self-Play:
  하나의 상대가 아닌 에이전트 풀과 대전
  ├─ 다양한 전략에 대한 강건성
  ├─ 순환 이기기(A>B>C>A) 방지
  └─ OpenAI Five, AlphaStar

  OpenAI Five (2019):
  ├─ Dota 2: 5v5 팀 게임
  ├─ PPO + 자기 대국
  ├─ 256 GPU, 10개월 학습
  ├─ 프로팀을 2-0으로 승리
  └─ 초당 180년 분량의 게임 경험

  AlphaStar (2019):
  ├─ StarCraft II: 실시간 전략
  ├─ 리그(League) 기반 자기 대국
  │   → Main Agent + Exploiter + League Exploiter
  ├─ Grandmaster 수준 달성
  └─ 불완전 정보 + 실시간 + 대규모 행동 공간

  Cicero (Meta, 2022):
  ├─ Diplomacy: 협상 + 전략 게임
  ├─ RL + LLM (자연어 협상)
  ├─ 인간 수준의 전략적 대화
  └─ MARL + NLP의 융합
```

## 6. MARL의 실전 응용

```
응용 분야:

  자율주행:
  ├─ 다수 차량의 교통 제어
  ├─ 교차로 통과 순서 조정
  ├─ 협력: 차간 통신으로 안전 향상
  └─ 혼합: 자기 이익 + 전체 흐름

  로봇 팀:
  ├─ 드론 편대: 대형 유지, 영역 탐색
  ├─ 물류 로봇: 창고 내 경로 조정
  ├─ 구조 로봇: 탐색 영역 분배
  └─ 분산 실행이 필수 (통신 제한)

  네트워크:
  ├─ 트래픽 신호 제어: 교차로별 에이전트
  ├─ 네트워크 라우팅: 패킷 경로 최적화
  └─ 자원 할당: 기지국별 주파수 할당

  경제/금융:
  ├─ 시장 메이킹: 여러 딜러의 경쟁
  ├─ 경매 설계: 입찰 전략 학습
  └─ 게임 이론과의 교집합

  MARL 선택 가이드:
  | 상황 | 추천 | 이유 |
  |------|------|------|
  | 완전 협력 | MAPPO | 간단, 강건 |
  | 팀 기반 | QMIX | 분산 실행 |
  | 경쟁 | Self-Play PPO | AlphaZero 스타일 |
  | 대규모 | Mean Field RL | 확장성 |
  | 혼합 | MADDPG | 연속 행동 |
```

## 핵심 정리

- **MARL**의 핵심 난제는 **비정상성**(다른 에이전트도 학습 → 환경 변화)이며, 협력/경쟁/혼합 환경에서 여러 에이전트가 동시에 학습합니다
- **CTDE**(중앙 학습, 분산 실행)는 학습 시 전역 정보를 활용하고 실행 시 개인 관찰만 사용하며, **QMIX**의 단조 혼합이 개인 최선 = 팀 최선을 보장합니다
- **MAPPO**는 PPO + Centralized Critic으로 놀랍게도 많은 환경에서 복잡한 방법과 동등하며, 파라미터 공유로 100+ 에이전트까지 확장됩니다
- **통신 학습**(CommNet, TarMAC)은 에이전트 간 메시지 전달을 자동으로 학습하지만, 실전에서는 통신 없는 MAPPO/QMIX가 대부분 충분합니다
- **OpenAI Five**(Dota 2)와 **AlphaStar**(StarCraft II)는 자기 대국 + 리그 기반 MARL로 인간 프로 수준을 달성하며, 복잡한 전략 게임에서 MARL의 가능성을 증명했습니다
