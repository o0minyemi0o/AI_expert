# MDP와 벨만 방정식

## 왜 강화학습인가

지도학습은 "정답이 주어진" 상황에서 학습합니다. 하지만 바둑, 로봇 제어, 게임 플레이에서는 **즉각적인 정답이 없습니다**. 대신 행동의 결과가 **시간이 지나서야** 보상으로 돌아옵니다. **강화학습(Reinforcement Learning, RL)**은 에이전트가 환경과 상호작용하며 **누적 보상을 최대화하는 정책**을 학습합니다. 이 모든 것의 수학적 기초가 **Markov Decision Process(MDP)**와 **벨만 방정식**입니다.

> **핵심 직관**: 강화학습의 핵심 난제는 **"신용 할당 문제(credit assignment)"**입니다. 바둑에서 100수 후에 이겼다면, 어느 수가 승리에 기여했는가? 즉각적 피드백이 아닌 지연된 보상에서 학습해야 하며, MDP의 벨만 방정식이 이 문제를 수학적으로 분해합니다.

## 1. 강화학습의 기본 구성

```
에이전트-환경 상호작용:

  에이전트(Agent) ←──→ 환경(Environment)

  시간 t에서:
  1. 에이전트가 상태 s_t를 관찰
  2. 행동 a_t를 선택 (정책 π에 따라)
  3. 환경이 보상 r_t와 다음 상태 s_{t+1}을 반환
  4. 반복

  s₀ → a₀ → r₀, s₁ → a₁ → r₁, s₂ → ...

  핵심 요소:
  ├─ 상태(State, S): 환경의 현재 상황
  │   바둑: 바둑판 배치, 로봇: 관절 각도
  ├─ 행동(Action, A): 에이전트의 선택
  │   바둑: 돌 놓기, 로봇: 토크 적용
  ├─ 보상(Reward, R): 즉각적 피드백 (스칼라)
  │   바둑: 승리 +1, 패배 -1
  ├─ 정책(Policy, π): 상태 → 행동 매핑
  │   π(a|s): 상태 s에서 행동 a를 선택할 확률
  └─ 가치(Value): 미래 보상의 기대값
      "이 상태가 얼마나 좋은가?"

  RL vs 지도학습 vs 비지도학습:
  | 패러다임 | 신호 | 데이터 | 목표 |
  |---------|------|--------|------|
  | 지도학습 | 정답 라벨 | 고정 | 정확한 예측 |
  | 비지도학습 | 없음 | 고정 | 구조 발견 |
  | 강화학습 | 보상 (지연) | 상호작용 | 누적 보상 최대화 |
```

## 2. Markov Decision Process (MDP)

```
MDP의 정의:
  M = (S, A, P, R, γ) — 5-튜플

  S: 상태 공간 (유한 또는 연속)
  A: 행동 공간 (유한 또는 연속)
  P: 전이 확률 P(s'|s, a)
     상태 s에서 행동 a를 하면 s'로 갈 확률
  R: 보상 함수 R(s, a, s')
     상태 전이에 따른 즉각적 보상
  γ: 할인율 (0 ≤ γ < 1)
     미래 보상의 현재 가치

  마르코프 성질 (Markov Property):
  P(s_{t+1} | s_t, a_t, s_{t-1}, a_{t-1}, ...) = P(s_{t+1} | s_t, a_t)
  → "미래는 현재에만 의존하고 과거에는 무관"
  → 현재 상태가 의사결정에 필요한 모든 정보를 포함

  이것이 왜 중요한가:
  마르코프 성질 덕분에 벨만 방정식이 성립하고,
  동적 프로그래밍으로 최적 정책을 찾을 수 있습니다.
  → 과거 전체가 아닌 현재 상태만 보면 됨!

  할인율 γ의 역할:
  G_t = r_t + γ r_{t+1} + γ² r_{t+2} + ...
      = Σ_{k=0}^∞ γ^k r_{t+k}

  γ = 0: 근시안적 (즉각적 보상만)
  γ = 0.99: 원시안적 (먼 미래까지 고려)
  γ < 1이어야 하는 이유:
  ├─ 무한 합이 수렴 (G_t < R_max / (1-γ))
  ├─ 불확실한 미래를 덜 중요시
  └─ 수학적 편의성

  예시 - 그리드 월드:
  ┌───┬───┬───┬───┐
  │ S │   │   │ G │   S: 시작, G: 목표(+1)
  ├───┼───┼───┼───┤
  │   │ X │   │   │   X: 장애물
  ├───┼───┼───┼───┤
  │   │   │   │ T │   T: 함정(-1)
  └───┴───┴───┴───┘
  상태: 격자 위치, 행동: 상하좌우
  전이: 80% 의도 방향, 10%씩 양쪽으로 미끄러짐
```

## 3. 가치 함수

```
상태 가치 함수 V^π(s):
  "정책 π를 따를 때, 상태 s의 기대 누적 보상"

  V^π(s) = E_π[G_t | s_t = s]
          = E_π[r_t + γ r_{t+1} + γ² r_{t+2} + ... | s_t = s]

  → "이 상태에서 시작하여 π를 따르면 얼마나 좋은가?"

행동 가치 함수 Q^π(s, a):
  "상태 s에서 행동 a를 하고, 이후 π를 따를 때의 기대 보상"

  Q^π(s, a) = E_π[G_t | s_t = s, a_t = a]

  → "이 상태에서 이 행동을 하면 얼마나 좋은가?"

  V와 Q의 관계:
  V^π(s) = Σ_a π(a|s) Q^π(s, a)
  → 상태 가치 = 모든 행동의 가치를 정책으로 가중 평균

  Q^π(s, a) = Σ_{s'} P(s'|s,a)[R(s,a,s') + γ V^π(s')]
  → 행동 가치 = 즉각 보상 + 할인된 다음 상태 가치

최적 가치 함수:
  V*(s) = max_π V^π(s)
  → 모든 정책 중 최고의 상태 가치

  Q*(s, a) = max_π Q^π(s, a)
  → 모든 정책 중 최고의 행동 가치

  최적 정책 π*:
  π*(s) = argmax_a Q*(s, a)
  → 최적 Q를 알면 탐욕적으로 행동 선택!
  → Q*를 구하는 것이 강화학습의 핵심 목표
```

> **핵심 직관**: Q 함수는 **"행동의 장기적 결과를 하나의 숫자로 요약"**합니다. 바둑에서 특정 수의 Q 값이 높다는 것은, 그 수를 두면 **이후 최선을 다했을 때** 좋은 결과가 기대된다는 뜻입니다. 즉각적 보상이 아닌 **장기적 누적 보상**의 기대값입니다.

## 4. 벨만 기대 방정식

```
벨만 기대 방정식 (Bellman Expectation Equation):
  가치 함수의 재귀적 관계

  V^π(s) = Σ_a π(a|s) Σ_{s'} P(s'|s,a)[R(s,a,s') + γ V^π(s')]

  "현재 가치 = 즉각 보상 + 할인된 미래 가치"

  풀어 쓰면:
  V^π(s) = E_π[r_t + γ V^π(s_{t+1}) | s_t = s]

  이것이 왜 강력한가:
  ├─ 무한 합 G_t를 1-스텝 재귀로 분해!
  ├─ 동적 프로그래밍의 기초 (rl-02)
  └─ TD 학습의 이론적 근거 (rl-04)

  Q 함수 버전:
  Q^π(s,a) = Σ_{s'} P(s'|s,a)[R(s,a,s') + γ Σ_{a'} π(a'|s') Q^π(s',a')]

  → "현재 행동의 가치 = 즉각 보상 + 할인된 다음 상태에서의 기대 가치"

벨만 방정식의 행렬 형태:
  V^π = R^π + γ P^π V^π
  → (I - γ P^π) V^π = R^π
  → V^π = (I - γ P^π)^{-1} R^π

  |S| × |S| 역행렬 계산
  → 상태 수가 적으면 직접 풀 수 있음!
  → 상태가 많으면 반복적 방법 필요 (rl-02)

예시 계산:
  두 상태 s₁, s₂, 하나의 행동:
  s₁ → r=1, s₂(확률 1)
  s₂ → r=2, s₁(확률 1)
  γ = 0.9

  V(s₁) = 1 + 0.9 V(s₂)
  V(s₂) = 2 + 0.9 V(s₁)

  V(s₁) = 1 + 0.9(2 + 0.9 V(s₁))
  V(s₁) = 1 + 1.8 + 0.81 V(s₁)
  0.19 V(s₁) = 2.8
  V(s₁) = 14.74
  V(s₂) = 2 + 0.9 × 14.74 = 15.26
```

## 5. 벨만 최적 방정식

```
벨만 최적 방정식 (Bellman Optimality Equation):

  V*(s) = max_a Σ_{s'} P(s'|s,a)[R(s,a,s') + γ V*(s')]
  Q*(s,a) = Σ_{s'} P(s'|s,a)[R(s,a,s') + γ max_{a'} Q*(s',a')]

  기대 방정식과의 차이:
  기대: Σ_a π(a|s) [...]  (정책에 따른 평균)
  최적: max_a [...]         (최선의 행동 선택)

  → "최적 정책은 매 상태에서 최적 행동을 선택"
  → 비선형(max)이므로 닫힌 해가 없음
  → 반복적 방법으로 풀어야 함 (rl-02)

벨만 방정식 정리:

  | 방정식 | 형태 | 의미 |
  |--------|------|------|
  | V 기대 | Σ_a π(a|s)[...] | π를 따를 때의 가치 |
  | Q 기대 | Σ_{s'} P[R + γ Σ_{a'} π Q'] | π를 따를 때의 행동 가치 |
  | V 최적 | max_a Σ_{s'} P[R + γ V*] | 최적 상태 가치 |
  | Q 최적 | Σ_{s'} P[R + γ max_{a'} Q*] | 최적 행동 가치 |

  모든 RL 알고리즘은 이 방정식의 근사적 풀이:
  ├─ DP (rl-02): P를 알 때 직접 풀기
  ├─ MC (rl-03): 샘플 평균으로 추정
  ├─ TD (rl-04): 부트스트래핑으로 추정
  ├─ DQN (rl-05): 신경망으로 Q* 근사
  └─ PPO (rl-08): 정책을 직접 최적화
```

> **핵심 직관**: 벨만 방정식은 **"큰 문제를 작은 문제로 분해"**합니다. "이 상태의 가치"를 "즉각 보상 + 다음 상태의 가치"로 재귀적으로 표현합니다. co-12의 동적 프로그래밍에서 다룬 **최적 부분 구조**와 정확히 같은 원리입니다. 큰 의사결정을 한 스텝씩 분해하여 풀 수 있게 합니다.

## 6. MDP의 변형과 확장

```
MDP의 변형:

  POMDP (Partially Observable MDP):
  에이전트가 상태를 완전히 관찰하지 못함
  ├─ 관찰(Observation) o ≠ 상태 s
  ├─ 포커: 상대 카드를 모름
  ├─ 로봇: 센서 노이즈, 가려진 영역
  └─ 믿음 상태(Belief State): 상태의 확률 분포

  Multi-Agent MDP:
  여러 에이전트가 동시에 행동
  ├─ 협력: 팀 게임
  ├─ 경쟁: 제로섬 게임
  └─ rl-11에서 상세히

  Continuous MDP:
  상태/행동이 연속 공간
  ├─ 로봇 관절: 연속 각도와 토크
  ├─ 테이블 방식 불가 → 함수 근사 필요
  └─ Policy Gradient (rl-06) 계열이 자연스러운 접근

  에피소딕 vs 지속적:
  에피소딕: 종료 상태 존재 (게임, 에피소드)
  지속적: 영원히 계속 (서버 관리, 트레이딩)
  → γ < 1이 지속적 과제에서 특히 중요

실전에서 MDP 설계:

  보상 설계 (Reward Shaping):
  ├─ 희소 보상: 게임 승리 시 +1 (학습 어려움)
  ├─ 밀집 보상: 매 스텝마다 작은 보상 (빠른 학습)
  ├─ 중간 보상: 하위 목표 달성 시 보상
  └─ 주의: 잘못된 보상 → 의도치 않은 행동!
     예: "멀리 가기" 보상 → 원 그리며 돌기

  상태 설계:
  ├─ 충분한 정보: 마르코프 성질 확보
  ├─ 과도하지 않게: 차원의 저주
  └─ 특징 엔지니어링 또는 원시 입력(DQN)

  | 설계 요소 | 좋은 예 | 나쁜 예 |
  |----------|--------|--------|
  | 상태 | 관련 정보 포함 | 불필요한 정보 과다 |
  | 행동 | 의미 있는 선택지 | 너무 세밀한 제어 |
  | 보상 | 목표와 정렬 | 의도치 않은 단축 유도 |
  | 할인율 | 과제에 맞는 시야 | 임의 설정 |
```

## 핵심 정리

- **강화학습**은 에이전트가 환경과 상호작용하며 지연된 보상에서 학습하는 패러다임으로, **신용 할당 문제**가 핵심 난제입니다
- **MDP**(S, A, P, R, γ)는 순차적 의사결정의 수학적 프레임워크이며, **마르코프 성질**(미래는 현재에만 의존)이 벨만 방정식을 가능하게 합니다
- **V(s)**는 상태의 장기적 가치, **Q(s,a)**는 행동의 장기적 가치이며, 최적 정책은 π*(s) = argmax_a Q*(s,a)로 Q*만 알면 도출됩니다
- **벨만 기대 방정식**은 "현재 가치 = 즉각 보상 + 할인된 미래 가치"의 재귀 관계이고, **벨만 최적 방정식**은 max를 사용하여 최적 가치를 정의합니다
- 모든 RL 알고리즘(DP, MC, TD, DQN, PPO)은 **벨만 방정식의 근사적 풀이**이며, 보상/상태/행동의 설계가 실전 성능을 좌우합니다
