# 탐색과 계획

## 왜 AlphaGo가 중요한가

바둑은 10^170개의 가능한 상태를 가진 게임입니다. 완전 탐색은 불가능하고, 인간의 직관도 컴퓨터에게는 없습니다. **Monte Carlo Tree Search(MCTS)**는 유망한 수만 선택적으로 탐색하며, **AlphaGo**는 이를 딥러닝과 결합하여 2016년 인간 세계 챔피언을 이겼습니다. **AlphaZero**는 인간 지식 없이 자기 대국만으로 바둑, 체스, 쇼기를 정복했고, **MuZero**는 게임 규칙조차 없이 학습합니다.

> **핵심 직관**: MCTS의 핵심은 **"선택과 집중"**입니다. 모든 수를 탐색하는 것은 불가능하므로, "유망한 수"를 더 깊이 탐색합니다. 하지만 유망해 보이지 않는 수도 가끔 시도하여 놓친 좋은 수를 찾습니다. 이것이 rl-03의 탐색-활용 딜레마를 트리 탐색에 적용한 것입니다.

## 1. Monte Carlo Tree Search

```
MCTS: 4단계 반복

  1. Selection (선택):
     루트에서 시작, UCB로 자식 노드 선택하며 내려감
     → 리프 노드에 도달할 때까지

  2. Expansion (확장):
     리프 노드에서 새로운 자식 노드 추가
     → 탐색 트리 확장

  3. Simulation (시뮬레이션/Rollout):
     새 노드에서 랜덤 플레이로 게임 끝까지 진행
     → 결과(승/패) 획득

  4. Backpropagation (역전파):
     결과를 루트까지 역방향으로 전파
     → 방문 횟수 N(s,a)와 가치 Q(s,a) 업데이트

  UCB1 (Upper Confidence Bound):
  UCB(s, a) = Q(s, a) + c √(ln N(s) / N(s, a))

  첫 항: 활용 (가치가 높은 행동)
  둘 항: 탐색 (덜 방문한 행동, N(s,a) 작으면 큰 값)
  c: 탐색 계수 (보통 √2)

  → 방문 적은 행동에 보너스 → 골고루 탐색
  → 가치 높은 행동에 집중 → 좋은 수 깊이 탐색

  MCTS의 장점:
  ├─ 비대칭 탐색: 유망한 가지를 더 깊이
  ├─ 언제든 중단 가능: 시간이 더 있으면 더 정확
  ├─ 게임 규칙만 있으면 작동 (도메인 지식 불필요)
  └─ 이론적으로 최적에 수렴

  한계:
  ├─ Rollout이 무작위 → 비효율적
  ├─ 큰 분기 계수에서 탐색 부족
  └─ 평가 함수 없이는 깊은 전략 학습 어려움
```

## 2. AlphaGo: 딥러닝 + MCTS

```
AlphaGo (Silver et al., 2016):
  MCTS + 정책 네트워크 + 가치 네트워크

  구성 요소:

  1. Policy Network p_σ (SL):
     인간 고수의 기보에서 지도학습
     바둑판 → CNN → 각 위치의 확률
     → "인간이라면 여기에 둘 것"
     → MCTS의 Selection에서 유망한 수 안내

  2. Policy Network p_ρ (RL):
     SL 정책을 자기 대국으로 강화학습
     → SL보다 더 강한 정책

  3. Value Network v_θ:
     바둑판 → CNN → 승률 예측 [-1, 1]
     → "이 상황에서 이길 확률"
     → MCTS의 Rollout을 대체

  MCTS + 신경망:
  Selection: p_σ로 유망한 수를 우선 탐색
  Evaluation: v_θ(리프) + Rollout 결과를 결합
  → 무작위 Rollout 대신 신경망 평가!

  PUCT (Polynomial UCT):
  UCB(s, a) = Q(s, a) + c_puct × P(s, a) × √N(s) / (1 + N(s, a))

  P(s, a): 정책 네트워크의 사전 확률
  → 정책이 높은 확률을 부여한 수를 우선 탐색
  → 탐색 효율 극대화

  결과:
  AlphaGo vs 이세돌: 4-1 승리 (2016)
  → "인간의 직관"을 신경망으로 대체
  → 30억 개 이상의 바둑 상태 탐색
```

## 3. AlphaZero: 인간 지식 없이

```
AlphaZero (Silver et al., 2017):
  자기 대국만으로 바둑/체스/쇼기 정복

  AlphaGo와의 차이:
  ├─ 인간 기보 불필요! (SL 단계 제거)
  ├─ Rollout 불필요! (가치 네트워크만 사용)
  ├─ 하나의 네트워크가 정책 + 가치 동시 출력
  └─ 바둑/체스/쇼기에 동일 알고리즘

  단일 네트워크 f_θ:
  바둑판 s → ResNet → (p, v)
  p: 정책 (각 수의 확률)
  v: 가치 (승률 예측)

  자기 대국 학습:
  1. MCTS로 자기 대국 (현재 네트워크 사용)
     MCTS 탐색 결과 → 개선된 정책 π_MCTS
  2. 게임 종료 → 승패 결과 z
  3. 네트워크 학습:
     L = (z - v)² - π_MCTS · log p + c||θ||²
     → 가치가 결과를 예측하도록
     → 정책이 MCTS 결과를 따르도록

  핵심 통찰: MCTS가 "정책 개선 연산자"
  현재 네트워크의 정책 p → MCTS로 더 나은 정책 π
  → π로 네트워크를 학습 → 더 나은 p' → 더 나은 MCTS → ...
  → 자기 강화 루프!

  결과:
  ├─ 바둑: 인간 지식 없이 AlphaGo를 100-0으로 압도
  ├─ 체스: 4시간 학습으로 Stockfish(기존 최강)를 압도
  ├─ 쇼기: 2시간 학습으로 최강 프로그램 압도
  └─ 하나의 알고리즘으로 세 게임 모두!

  | 모델 | 인간 지식 | Rollout | 결과 |
  |------|----------|---------|------|
  | AlphaGo Fan | SL 기보 | 있음 | 프로 5단 |
  | AlphaGo Lee | SL 기보 | 있음 | 이세돌 |
  | AlphaGo Zero | 없음 | 없음 | AlphaGo Lee 100-0 |
  | AlphaZero | 없음 | 없음 | 모든 게임 최강 |
```

> **핵심 직관**: AlphaZero가 인간 지식 없이 인간을 초월한 비결은 **"탐색 + 학습의 선순환"**입니다. 네트워크가 MCTS를 안내하고, MCTS의 결과가 네트워크를 개선합니다. 이 루프에서 인간의 편견 없이 바둑의 원리를 **처음부터** 발견합니다. 인간이 "나쁜 수"라고 가르친 수가 실제로는 좋은 수일 수 있습니다.

## 4. MuZero: 규칙도 필요 없다

```
MuZero (Schrittwieser et al., 2020):
  게임 규칙을 모르고도 계획

  AlphaZero의 한계:
  MCTS에 게임 규칙(시뮬레이터)이 필요
  → 실제 세계에서는 규칙을 모르는 경우가 대부분

  MuZero의 해결:
  게임 규칙 대신 "학습된 모델"로 MCTS

  세 가지 네트워크:

  1. Representation h_θ:
     관찰 o → 숨은 상태 s = h(o)
     → 원시 입력을 잠재 공간으로

  2. Dynamics g_θ:
     (s, a) → (r, s') = g(s, a)
     → 잠재 공간에서의 전이 모델
     → 게임 규칙을 대체!

  3. Prediction f_θ:
     s → (p, v) = f(s)
     → AlphaZero와 동일 (정책 + 가치)

  MCTS in Latent Space:
  실제 환경 대신 Dynamics 모델로 상상의 전이
  → 게임 규칙 불필요!

  학습:
  실제 궤적 o₁, a₁, r₁, o₂, a₂, r₂, ...에서:
  ├─ 보상 예측: r̂_t ≈ r_t
  ├─ 가치 예측: v̂_t ≈ n-step return
  ├─ 정책 예측: p̂_t ≈ π_MCTS(o_t)
  └─ 모든 예측을 k 스텝 앞까지 펼쳐서 학습

  결과:
  ├─ 바둑: AlphaZero와 동등 (규칙 없이!)
  ├─ Atari: SOTA (이미지 입력, 규칙 모름)
  └─ 범용성: 보드 게임 + 비디오 게임 모두

  MuZero Reanalyze:
  ├─ 과거 데이터를 최신 모델로 재분석
  ├─ 더 나은 MCTS 결과로 학습 → 샘플 효율 향상
  └─ Off-policy 학습과 유사

  | 속성 | AlphaGo | AlphaZero | MuZero |
  |------|---------|-----------|--------|
  | 인간 지식 | 필요 | 불필요 | 불필요 |
  | 게임 규칙 | 필요 | 필요 | 불필요 |
  | 도메인 | 바둑 | 보드 게임 | 보드+Atari |
  | 모델 | 없음 | 없음 | 학습된 모델 |
```

## 5. 자기 대국과 탐색 알고리즘

```
자기 대국 (Self-Play):
  에이전트가 자신의 복사본과 대전하여 학습

  왜 효과적인가:
  ├─ 자동 커리큘럼: 실력이 올라가면 상대도 강해짐
  ├─ 무한 데이터: 언제든 새 게임 생성
  ├─ 균형: 특정 전략에 과적합하지 않음
  └─ 안정성: 이전 버전들과도 대전 (League)

  Population-Based Training:
  여러 에이전트를 동시에 학습
  ├─ 에이전트 풀에서 상대 선택
  ├─ 다양한 전략이 공존
  └─ OpenAI Five (Dota 2), AlphaStar (StarCraft II)

탐색 알고리즘 비교:

  | 알고리즘 | 탐색 방식 | 도메인 지식 | 적합한 문제 |
  |---------|----------|-----------|-----------|
  | Minimax | 완전 탐색 | 평가 함수 | 작은 게임 |
  | Alpha-Beta | 가지치기 Minimax | 평가 함수 | 체스(전통) |
  | MCTS | 샘플 기반 | 규칙만 | 큰 분기 계수 |
  | MCTS+NN | 신경망 안내 | 학습 | 바둑, 체스 |
  | MuZero | 학습된 모델 | 없음 | 범용 |
```

## 6. AlphaGo의 영향과 확장

```
AlphaGo 계열의 실세계 응용:

  과학:
  ├─ AlphaFold: 단백질 구조 예측 (MCTS 영감)
  ├─ GNoME: 신소재 탐색
  └─ FunSearch: 수학 문제 탐색 (LLM + 진화)

  산업:
  ├─ 칩 설계: 트랜지스터 배치 최적화
  ├─ 데이터센터: 냉각 에너지 최적화 (30% 절감)
  ├─ 플라즈마 제어: 핵융합 토카막 제어
  └─ 코드 최적화: 행렬 곱셈 알고리즘 개선

  핵심 교훈:
  1. 탐색 + 학습의 결합이 핵심
     → 학습만으로는 한계, 탐색만으로도 한계
     → 둘의 시너지가 초인적 성능

  2. 자기 대국/자기 개선이 가능
     → 데이터 병목 없이 무한 학습
     → 인간 편향 없는 새로운 발견

  3. 범용 알고리즘의 가치
     → 바둑 전용이 아닌 범용 원리
     → MuZero가 보여준 범용성

  한계와 열린 문제:
  ├─ 불완전 정보 게임 (포커 등): MCTS 직접 적용 어려움
  ├─ 연속 행동 공간: MCTS의 분기 폭발
  ├─ 다인 게임: 탐색 복잡도 폭발
  └─ 실세계: 시뮬레이터의 정확도 문제
```

## 핵심 정리

- **MCTS**는 Selection(UCB)→Expansion→Simulation→Backpropagation의 4단계로 유망한 수를 선택적으로 탐색하며, 탐색-활용의 트리 버전입니다
- **AlphaGo**는 정책/가치 네트워크를 MCTS와 결합하여 Rollout을 신경망 평가로 대체했고, **AlphaZero**는 인간 기보 없이 자기 대국만으로 모든 보드 게임에서 최강을 달성했습니다
- **MuZero**는 게임 규칙 대신 학습된 Dynamics 모델로 잠재 공간에서 MCTS를 수행하여, 규칙을 모르는 Atari에서도 SOTA를 달성합니다
- MCTS가 "**정책 개선 연산자**" 역할을 하여, 네트워크→MCTS→개선된 정책→네트워크 학습의 **자기 강화 루프**가 초인적 성능의 핵심입니다
- AlphaGo 계열의 **탐색+학습 원리**는 단백질 구조(AlphaFold), 칩 설계, 에너지 최적화 등 과학/산업에 광범위하게 확장되고 있습니다
