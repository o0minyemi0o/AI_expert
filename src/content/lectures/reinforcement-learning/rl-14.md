# 안전한 RL과 미래 방향

## 왜 안전이 중요한가

rl-01~13에서 다룬 RL 알고리즘은 **보상 최대화**만을 목표로 합니다. 하지만 로봇이 목표에 도달하기 위해 사람을 밀치거나, 자율주행차가 빨리 가려고 신호를 무시하면 안 됩니다. **안전한 RL(Safe RL)**은 보상을 최대화하면서도 **제약 조건을 만족**시키는 정책을 학습합니다. 현실에서 RL을 배포하려면, "얼마나 잘하는가"뿐 아니라 **"무엇을 하면 안 되는가"**를 반드시 다뤄야 합니다.

> **핵심 직관**: 안전한 RL의 핵심은 **"보상과 제약의 분리"**입니다. 일반 RL은 "빨리 목적지에 도착"이라는 보상에 모든 것을 맡기지만, 안전한 RL은 "빨리 도착하되, 속도 제한을 지키고, 보행자와 거리를 유지하라"는 **별도의 제약**을 명시합니다. 안전을 보상에 녹이면 가중치 조정이 어렵지만, 제약으로 분리하면 "반드시 지켜야 할 것"을 보장할 수 있습니다.

## 1. 제약 기반 RL (Constrained MDP)

```
Constrained MDP (CMDP):
  표준 MDP + 비용 함수

  목적:
  max_π E[Σ γ^t r_t]          (보상 최대화)
  subject to E[Σ γ^t c_t] ≤ d  (비용 제약)

  r_t: 보상 (빨리 도착, 작업 완수)
  c_t: 비용 (위험한 행동, 제약 위반)
  d: 허용 가능한 총 비용 상한

  예시: 자율주행
  보상: 목적지 도착 속도
  비용: 차선 이탈, 급제동, 안전거리 위반
  제약: 에피소드당 안전 위반 ≤ 0.01

  왜 보상에 안전을 포함하면 안 되는가:
  r' = r - λc 로 합치면?
  ├─ λ 조정이 매우 어려움
  ├─ λ = 100이면 아무것도 안 함 (과도한 보수)
  ├─ λ = 0.1이면 위험 행동 (안전 무시)
  └─ 환경마다 λ를 재조정해야 함

  → CMDP로 분리하면 d만 설정하면 됨!

라그랑주 방법:
  CMDP를 제약 없는 문제로 변환

  L(π, λ) = E[Σ γ^t r_t] - λ(E[Σ γ^t c_t] - d)

  Primal: max_π L(π, λ)   (정책 최적화)
  Dual: min_λ L(π, λ)     (라그랑주 승수 최적화)
  λ ≥ 0

  → λ를 자동으로 조절!
  비용 > d: λ 증가 → 안전 강조
  비용 < d: λ 감소 → 성능 추구
  → 보상-안전 균형을 자동으로 학습

  PPO-Lagrangian:
  PPO + 라그랑주 승수
  ├─ 매 업데이트: 정책 PPO 업데이트
  ├─ λ 업데이트: λ ← λ + η(J_c - d)
  │   J_c: 현재 비용 기대값
  │   d: 비용 상한
  └─ 간단하지만 진동 가능 (λ가 불안정)
```

## 2. 안전한 탐색

```
탐색 중의 안전:
  학습 과정에서도 안전 제약을 위반하면 안 됨!

  문제: rl-03의 탐색-활용 딜레마
  탐색 = 새로운 행동 시도 = 위험할 수 있음
  로봇이 "이 행동은 해본 적 없으니 시도"
  → 로봇이 넘어지거나 부서질 수 있음!

  1. 보수적 안전 탐색 (Conservative Safety):

  CPO (Constrained Policy Optimization, Achiam et al., 2017):
  ├─ TRPO(rl-08)의 안전 버전
  ├─ KL 제약 + 비용 제약을 동시에
  ├─ 매 업데이트가 안전 제약을 만족하도록 보장
  └─ 이론적: π_{k+1}도 안전 (높은 확률로)

  정책 업데이트 영역:
  ┌─────────────────────────┐
  │    KL 신뢰 영역         │
  │   ┌──────────┐          │
  │   │ 안전 영역 │ ← 여기서만 │
  │   │  (c ≤ d)  │   업데이트 │
  │   └──────────┘          │
  └─────────────────────────┘
  → KL과 안전 제약의 교집합에서 최적화

  2. 안전 층 (Safety Layer):

  행동 수정 접근:
  ├─ 정책이 행동 a 제안
  ├─ 안전 필터가 a를 검사
  ├─ 위험하면 가장 가까운 안전 행동 ã로 수정
  └─ ã = argmin ||a - ã|| s.t. c(s, ã) ≤ d

  Control Barrier Function (CBF):
  ├─ 안전 집합 C = {s : h(s) ≥ 0}을 정의
  ├─ h(s)가 항상 양수를 유지하도록 행동 제약
  ├─ ḣ(s) + α h(s) ≥ 0 → 안전 보장
  └─ 로봇 제어에서 활발히 사용

  3. 불확실성 기반 안전:

  rl-09의 앙상블 모델 활용
  ├─ 모델 앙상블의 불일치 = 불확실성
  ├─ 불확실성 높은 상태 회피
  ├─ "모르는 곳은 위험하다" 가정
  └─ 보수적이지만 안전한 탐색

  | 방법 | 안전 보장 | 성능 | 실용성 |
  |------|----------|------|--------|
  | PPO-Lagrangian | 평균적 | 좋음 | 높음 |
  | CPO | 매 업데이트 | 중간 | 중간 |
  | Safety Layer | 매 행동 | 제한적 | 높음 |
  | CBF | 수학적 | 좋음 | 도메인 필요 |
```

> **핵심 직관**: 안전한 탐색의 핵심 딜레마는 **"안전하게 탐색하면 학습이 느리고, 자유롭게 탐색하면 위험하다"**는 것입니다. CPO는 "한 발씩만 조심스럽게"로, Safety Layer는 "위험한 행동을 마지막에 차단"으로, CBF는 "안전 경계를 수학적으로 보장"하여 이 딜레마에 접근합니다. 실전에서는 시뮬레이션에서 자유롭게 탐색하고, 실제 환경에서는 보수적으로 행동하는 **이중 전략**이 일반적입니다.

## 3. Sim-to-Real Transfer

```
시뮬레이션 → 실세계 전이:
  시뮬레이션에서 학습한 정책을 실세계에 배포

  왜 필요한가:
  ├─ 시뮬레이션: 무한 데이터, 안전, 빠름
  ├─ 실세계: 데이터 부족, 위험, 느림
  └─ 시뮬레이션에서 학습 → 실세계에 적용!

  Reality Gap (현실 격차):
  시뮬레이션 ≠ 실세계
  ├─ 물리 엔진 오차: 마찰, 관성, 접촉
  ├─ 시각 차이: 조명, 텍스처, 카메라
  ├─ 동역학 차이: 모터 지연, 센서 노이즈
  └─ 시뮬레이션에서 완벽 → 실세계에서 실패!

  Domain Randomization:
  시뮬레이션의 파라미터를 무작위로 변경하여 학습

  무작위화 대상:
  ├─ 물리: 마찰 계수, 질량, 관성
  ├─ 시각: 조명, 색상, 텍스처, 카메라 위치
  ├─ 동역학: 지연, 노이즈, 액추에이터 강도
  └─ 환경: 물체 크기, 위치, 형태

  → 다양한 시뮬레이션에서 학습
  → 실세계는 "또 하나의 변형"으로 간주
  → 충분히 다양하면 실세계에도 일반화!

  OpenAI의 Rubik's Cube (2019):
  ├─ 시뮬레이션에서만 학습
  ├─ 물리 파라미터 극단적 무작위화
  │   → 마찰: 0.1~2.0, 질량: 0.5~2.0x
  │   → 중력 방향도 약간 변경!
  ├─ 실제 로봇 손으로 루빅스 큐브 풀기 성공
  └─ Zero-shot transfer (추가 학습 없이!)

  System Identification:
  실세계 파라미터를 추정하여 시뮬레이션 보정

  ├─ 실세계 데이터 소량 수집
  ├─ 시뮬레이션 파라미터 최적화
  │   → 시뮬레이션 궤적 ≈ 실세계 궤적
  └─ 보정된 시뮬레이션에서 학습

  Adaptive Methods:
  ├─ SimOpt: 시뮬레이션 분포를 실세계에 맞게 조정
  ├─ RCAN: 실세계 이미지를 시뮬레이션 스타일로 변환
  └─ RMA (Rapid Motor Adaptation): 온라인으로 환경 적응

  RMA (Kumar et al., 2021):
  ├─ Base Policy: 관찰 + 환경 임베딩 → 행동
  ├─ Adaptation Module: 최근 관찰 이력 → 환경 임베딩
  ├─ 시뮬레이션에서 다양한 환경으로 학습
  ├─ 실세계에서 적응 모듈이 환경을 자동 추정
  └─ 울퉁불퉁한 지형에서 사족보행 로봇 성공

  | 방법 | 실세계 데이터 | 적응성 | 적용 사례 |
  |------|-------------|--------|----------|
  | Domain Rand. | 불필요 | 고정 | Rubik's Cube |
  | Sys. ID | 소량 | 보정 | 산업 로봇 |
  | RMA | 불필요 | 온라인 적응 | 사족보행 |
  | Fine-tuning | 소량~중량 | 높음 | 범용 |
```

## 4. 실세계 RL 성공 사례

```
로봇 보행:

  Cassie/Digit (Agility Robotics):
  ├─ PPO + Domain Randomization
  ├─ 시뮬레이션 학습 → 실세계 이족보행
  └─ 걷기, 달리기, 장애물 넘기

  ANYmal (ETH Zürich):
  ├─ Teacher-Student 학습
  │   Teacher: 시뮬레이션에서 특권 정보로 학습
  │   Student: Teacher를 모방하되 센서 입력만 사용
  ├─ 험한 지형(계단, 돌밭, 진흙) 보행
  └─ 2023: 스키, 파쿠르까지!

  사족보행 최근 성과:
  ├─ 2020: 평지 걷기 (Sim-to-Real 기본)
  ├─ 2022: 험한 지형 (RMA, 적응형)
  ├─ 2023: 파쿠르, 점프 (고급 제어)
  └─ 2024: 축구, 복잡한 조작 (범용화)

로봇 조작:

  Dexterous Manipulation:
  ├─ Shadow Hand: 시뮬레이션 학습 → 실세계
  ├─ 루빅스 큐브, 펜 돌리기 등
  └─ 수천 개의 무작위 환경에서 학습

  Mobile Manipulation:
  ├─ 로봇이 이동 + 물체 조작
  ├─ Google RT-2: Vision-Language-Action 모델
  │   → 이미지+언어 명령 → 로봇 행동
  │   → rl-12의 오프라인 RL + LLM
  ├─ 범용 로봇: 다양한 작업을 하나의 정책으로
  └─ Foundation Model for Robotics의 시작

  | 과제 | 알고리즘 | Sim-to-Real | 성과 |
  |------|---------|-------------|------|
  | 이족보행 | PPO+DR | 있음 | 험지 보행 |
  | 사족보행 | PPO+RMA | 있음 | 파쿠르 |
  | 조작 | PPO+DR | 있음 | 루빅스큐브 |
  | 범용 조작 | RT-2 | 부분적 | 다양한 작업 |

핵융합 제어:

  DeepMind의 토카막 제어 (2022):
  ├─ 플라즈마 형상을 RL로 제어
  ├─ 시뮬레이션 학습 → 실제 토카막(TCV)
  ├─ 기존 PID 제어보다 유연한 형상 생성
  └─ 과학 연구에서 RL의 가능성

데이터센터 냉각:

  Google DeepMind:
  ├─ 데이터센터 냉각 에너지 40% 절감
  ├─ 온도, 습도, 외부 날씨 등 관찰
  ├─ 냉각 장비 설정을 RL로 최적화
  └─ 2016년부터 실 배포, 지속 운영 중
```

## 5. RL의 미해결 문제

```
샘플 효율성:
  현재 RL은 여전히 데이터 배고프다
  ├─ Atari: 수억 프레임 (인간의 수천 배)
  ├─ 로봇: 시뮬레이션 의존
  ├─ 인간: 수 번의 시행착오로 학습
  └─ 해결 방향: World Models(rl-09), 사전학습, 메타학습

일반화:
  학습 환경에 과적합
  ├─ 다른 맵, 다른 물체 → 성능 급락
  ├─ "레벨 1을 마스터 → 레벨 2에서 실패"
  ├─ 인간: 새 게임도 빠르게 적응
  └─ 해결 방향:
     ├─ Procedural Generation: 무한한 환경 변형
     ├─ Foundation Models: 대규모 사전학습
     └─ Contextual RL: 환경 컨텍스트 조건화

보상 설계:
  보상 함수를 만드는 것 자체가 어렵다
  ├─ 보상 해킹: 의도와 다른 방식으로 보상 최대화
  │   → 청소 로봇이 쓰레기를 숨김 (보이지 않으면 보상)
  │   → 게임 에이전트가 버그를 이용하여 점수 획득
  ├─ 복잡한 목표: "좋은 요리"를 보상으로 정의?
  ├─ 장기 목표: 중간 보상이 없으면 학습 불가
  └─ 해결 방향:
     ├─ RLHF(rl-13): 인간 선호에서 보상 학습
     ├─ Reward Modeling: 보상 함수를 신경망으로
     └─ LLM as Reward: LLM이 보상을 자연어로 평가

장기 추론과 계획:
  긴 시야(long-horizon) 문제
  ├─ 100스텝이 아닌 10000스텝의 과제
  ├─ 요리: 재료 준비 → 조리 → 담기 (수백 단계)
  ├─ γ^T → 0: 먼 미래 보상이 사라짐
  └─ 해결 방향:
     ├─ Hierarchical RL: 상위/하위 정책 분리
     ├─ Goal-Conditioned RL: 하위 목표 설정
     ├─ LLM Planning: LLM이 상위 계획, RL이 실행
     └─ Skill Discovery: 재사용 가능한 기술 자동 발견

  Hierarchical RL:
  ├─ 상위 정책: "주방으로 이동" → "칼을 잡아" → "양파를 써라"
  ├─ 하위 정책: 각 지시를 실행하는 전문 에이전트
  ├─ Options Framework (Sutton, 1999): 시간적 추상화
  └─ Goal-Conditioned: 상위가 목표 설정, 하위가 달성
```

> **핵심 직관**: RL의 미해결 문제들은 **"인간과의 격차"**를 보여줍니다. 인간은 몇 번의 경험으로 학습하고(샘플 효율), 새 환경에 적응하며(일반화), 직관적으로 목표를 이해하고(보상 설계), 복잡한 계획을 세울 수 있습니다(장기 추론). RL이 이 격차를 좁히기 위해서는 **기반 모델(Foundation Model)**의 세계 지식과 RL의 최적화 능력을 결합하는 방향이 유망합니다.

## 6. RL의 미래 방향

```
Foundation Models + RL:

  LLM + RL의 융합:
  ├─ LLM이 계획, RL이 실행
  │   → "이 과제를 어떻게 할까?" → LLM이 단계 분해
  │   → 각 단계를 RL 정책이 수행
  ├─ LLM이 보상 함수 생성
  │   → "좋은 코드란?" → LLM이 평가 기준 제시
  │   → rl-13의 RLHF보다 확장 가능
  ├─ RL이 LLM을 개선
  │   → RLHF, DPO, RLVR(rl-13)
  │   → 추론 능력 강화 (o1, DeepSeek-R1)
  └─ 양방향 시너지: LLM ↔ RL

  World Foundation Models:
  ├─ 대규모 비디오/물리 데이터에서 세계 모델 사전학습
  ├─ Genie(Google): 비디오에서 상호작용 가능한 환경 생성
  ├─ UniSim: 범용 시뮬레이터 학습
  ├─ SORA(OpenAI): 비디오 생성 = 세계 시뮬레이션?
  └─ rl-09의 Dreamer → 범용 세계 모델로 확장

  Foundation Policy:
  ├─ 하나의 정책으로 수천 가지 과제
  ├─ Gato(DeepMind): 텍스트, 이미지, 게임, 로봇을 하나의 모델로
  ├─ RT-2: 웹 지식 + 로봇 행동
  ├─ 범용 로봇 정책의 시작
  └─ "GPT for Robotics"의 비전

RL의 위치:

  현대 AI에서 RL의 역할:
  ├─ 사전학습: 지도학습 / 자기지도학습 (LLM, Vision)
  ├─ 정렬/미세조정: RLHF, DPO (rl-13)
  ├─ 추론 강화: RLVR, Process Reward (rl-13)
  ├─ 의사결정: 로봇, 게임, 최적화
  └─ 과학: 수학 증명, 신약 개발, 핵융합

  RL은 더 이상 "게임을 푸는 기술"이 아니라
  "AI가 행동하고 개선하는 근본 원리"

  RL 커리큘럼 전체 흐름:
  ├─ 기초: MDP, 벨만(rl-01~02)
  ├─ 테이블: MC, TD, Q-Learning(rl-03~04)
  ├─ Deep RL: DQN, PG, AC, PPO(rl-05~08)
  ├─ 고급: Model-Based, MCTS, MARL(rl-09~11)
  ├─ 실전: Offline RL, RLHF(rl-12~13)
  └─ 미래: Safe RL, Sim-to-Real, 열린 문제(rl-14)

  → 벨만 방정식에서 시작한 원리가
  → DQN으로 Atari를 정복하고
  → AlphaZero로 바둑을 초월하며
  → RLHF로 ChatGPT를 탄생시키고
  → 로봇이 현실을 걷게 만듭니다

  이 모든 것의 기반은 같습니다:
  "시행착오를 통해 더 나은 행동을 학습한다"
```

## 핵심 정리

- **CMDP**는 보상 최대화에 비용 제약을 추가하여 안전과 성능을 분리하며, **라그랑주 방법**이 보상-안전 균형을 자동으로 학습합니다
- **CPO**는 매 정책 업데이트가 안전 제약을 만족하도록 보장하고, **Safety Layer/CBF**는 위험한 행동을 실시간으로 차단합니다
- **Sim-to-Real**은 시뮬레이션에서 학습한 정책을 실세계에 배포하며, **Domain Randomization**과 **RMA**가 현실 격차를 극복합니다
- RL의 핵심 미해결 문제는 **샘플 효율**, **일반화**, **보상 설계**, **장기 추론**이며, Foundation Model과의 결합이 유망한 해결 방향입니다
- RL은 게임에서 시작하여 **LLM 정렬(RLHF)**, **로봇 제어**, **과학 발견**으로 확장되며, "시행착오를 통한 학습"이라는 근본 원리가 현대 AI의 핵심 축입니다
