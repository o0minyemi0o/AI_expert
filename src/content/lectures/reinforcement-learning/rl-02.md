# 동적 프로그래밍

## 왜 동적 프로그래밍으로 시작하는가

rl-01에서 벨만 방정식이 MDP의 가치를 재귀적으로 정의했습니다. **동적 프로그래밍(Dynamic Programming, DP)**은 환경 모델(전이 확률 P, 보상 R)을 **완전히 알 때** 벨만 방정식을 반복적으로 풀어 최적 정책을 구합니다. 실전에서 P를 아는 경우는 드물지만, DP는 모든 RL 알고리즘의 **이론적 기초**이며, MC(rl-03)와 TD(rl-04)는 DP의 샘플 기반 근사입니다.

> **핵심 직관**: DP의 핵심은 **"부트스트래핑(bootstrapping)"**입니다. V(s)를 추정할 때, 아직 정확하지 않은 V(s')의 현재 추정치를 사용합니다. "부정확한 추정으로 부정확한 추정을 개선"하는 것이 반복을 통해 정확한 값으로 수렴합니다. co-12의 동적 프로그래밍과 같은 원리입니다.

## 1. 정책 평가 (Policy Evaluation)

```
정책 평가: 주어진 정책 π의 가치 함수 V^π를 구하기

  벨만 기대 방정식:
  V^π(s) = Σ_a π(a|s) Σ_{s'} P(s'|s,a)[R(s,a,s') + γ V^π(s')]

  반복적 정책 평가 (Iterative Policy Evaluation):
  V₀(s) = 0  (모든 상태를 0으로 초기화)

  반복:
  V_{k+1}(s) = Σ_a π(a|s) Σ_{s'} P(s'|s,a)[R(s,a,s') + γ V_k(s')]

  k → ∞일 때 V_k → V^π (수렴 보장!)

  예시 - 4×4 그리드 (무작위 정책):
  π(a|s) = 0.25 for all a (상하좌우 동일)
  보상: 매 이동 -1, 종료 시 0

  k=0:   [0  0  0  0]
         [0  0  0  0]
         [0  0  0  0]
         [0  0  0  0]

  k=1:   [0  -1 -1 -1]
         [-1 -1 -1 -1]
         [-1 -1 -1 -1]
         [-1 -1 -1  0]

  k=∞:   [0  -14 -20 -22]
         [-14 -18 -20 -20]
         [-20 -20 -18 -14]
         [-22 -20 -14  0]

  수렴 판정: max_s |V_{k+1}(s) - V_k(s)| < θ

  시간 복잡도: O(|S|² × |A|) per iteration
  공간 복잡도: O(|S|)
```

## 2. 정책 개선 (Policy Improvement)

```
정책 개선 정리:
  V^π를 알면 더 좋은 정책 π'를 만들 수 있다!

  탐욕적 정책 개선:
  π'(s) = argmax_a Σ_{s'} P(s'|s,a)[R(s,a,s') + γ V^π(s')]

  → 각 상태에서 V^π 기반으로 최선의 행동 선택
  → π'는 π보다 항상 같거나 더 좋음 (V^π' ≥ V^π)

  증명 스케치:
  Q^π(s, π'(s)) = max_a Q^π(s, a) ≥ Q^π(s, π(s)) = V^π(s)
  → 한 스텝에서 개선 → 전체에서도 개선

  그리드 예시에서:
  V^π를 보면 -14(오른쪽)보다 0(종료)이 가까운 상태에서
  → 종료 방향으로 행동을 바꿈
  → 더 좋은 정책!

  개선이 멈추면:
  V^π(s) = max_a Q^π(s, a) for all s
  → 벨만 최적 방정식 만족 → π = π*!
```

## 3. 정책 반복 (Policy Iteration)

```
정책 반복: 평가와 개선을 번갈아 수행

  알고리즘:
  1. 초기 정책 π₀ (임의)
  2. 반복:
     a. 정책 평가: V^πₖ를 구함 (수렴까지 반복)
     b. 정책 개선: πₖ₊₁(s) = argmax_a Q^πₖ(s,a)
     c. πₖ₊₁ = πₖ이면 종료
  3. π* 반환

  평가 → 개선 → 평가 → 개선 → ... → 최적!

  π₀ → V^π₀ → π₁ → V^π₁ → ... → π* → V*

  수렴 보장:
  ├─ 정책 개선 정리: V^πₖ₊₁ ≥ V^πₖ
  ├─ 유한 MDP에서 정책 수는 유한 (|A|^|S|)
  ├─ 단조 증가 + 유한 → 반드시 수렴
  └─ 실제로는 매우 빠르게 수렴 (수 회 반복)

  문제:
  매 반복마다 정책 평가가 수렴까지 필요
  → 정책 평가 자체가 비쌀 수 있음

  해결: 절단된 정책 평가
  수렴까지 기다리지 않고 k 회만 평가
  → 극단적으로 k=1이면 → 가치 반복!
```

## 4. 가치 반복 (Value Iteration)

```
가치 반복: 평가와 개선을 한 스텝에 결합

  벨만 최적 방정식을 직접 반복:
  V_{k+1}(s) = max_a Σ_{s'} P(s'|s,a)[R(s,a,s') + γ V_k(s')]

  → 정책을 명시적으로 유지할 필요 없음!
  → max가 평가와 개선을 동시에 수행

  알고리즘:
  1. V₀(s) = 0 (모든 상태)
  2. 반복:
     V_{k+1}(s) = max_a Σ_{s'} P(s'|s,a)[R + γ V_k(s')]
  3. 수렴: max_s |V_{k+1}(s) - V_k(s)| < θ
  4. π*(s) = argmax_a [위 식] 으로 정책 추출

  정책 반복 vs 가치 반복:

  | 속성 | 정책 반복 | 가치 반복 |
  |------|----------|----------|
  | 내부 루프 | 정책 평가 (다중 반복) | 없음 (1 스텝) |
  | 외부 루프 | 적은 반복 | 많은 반복 |
  | 정책 유지 | 명시적 | 암묵적 |
  | 총 비용 | 보통 더 적음 | 보통 더 많음 |
  | 구현 | 복잡 | 간단 |

  수렴 속도:
  ├─ V 반복: O(log(1/ε) / log(1/γ)) 회 반복
  ├─ γ = 0.99: 약 460 회 반복 (ε=0.01)
  ├─ γ가 1에 가까울수록 느림
  └─ 각 반복: O(|S|² × |A|)

  비동기 DP:
  모든 상태를 동시에 업데이트할 필요 없음
  ├─ 우선순위 기반: 변화가 큰 상태부터
  ├─ 실시간: 에이전트가 방문하는 상태만
  └─ 수렴 보장은 유지 (모든 상태를 결국 업데이트)
```

> **핵심 직관**: 가치 반복은 **"등고선이 퍼져나가는 것"**과 같습니다. 초기에 V=0이지만, 목표 상태 근처에서 먼저 높은 가치가 계산되고, 반복마다 한 칸씩 퍼져나가 결국 모든 상태의 최적 가치를 채웁니다. 이것은 다익스트라 알고리즘의 가치 전파와 유사합니다.

## 5. DP의 한계와 확장

```
DP의 근본적 한계:

  1. 모델 필요:
     P(s'|s,a)와 R(s,a,s')를 알아야 함
     → 대부분의 실제 문제에서 모르거나 매우 복잡
     → 바둑: 상태 수 ≈ 10^170, 모든 전이를 저장 불가

  2. 차원의 저주:
     상태/행동이 연속이면 테이블 불가
     |S| = n이면 O(n²) 연산/반복
     → n이 크면 계산 불가능

  3. 상태 수 폭발:
     바둑: ~10^170 상태
     로봇: 연속 관절 각도 (무한)
     Atari: 210×160×3 = 100,800차원 이미지
     → 테이블로 저장/계산 불가

  해결 방향:
  ├─ 모델 모름 → Model-Free (MC, TD, Q-Learning)
  ├─ 상태 많음 → 함수 근사 (신경망)
  ├─ 연속 공간 → Policy Gradient
  └─ 이 모든 방법이 DP의 원리를 샘플 기반으로 구현

DP → RL 알고리즘 계보:

  DP (모델 있음, 전체 상태):
  ├─ 정책 평가 → MC 예측 (rl-03)
  ├─ 가치 반복 → Q-Learning (rl-04)
  └─ 정책 개선 → Policy Gradient (rl-06)

  근사 방법:
  ├─ V 테이블 → V 신경망
  ├─ Q 테이블 → Q 신경망 (DQN, rl-05)
  └─ π 테이블 → π 신경망 (PPO, rl-08)

  | 속성 | DP | MC | TD | DQN |
  |------|----|----|-----|------|
  | 모델 필요 | O | X | X | X |
  | 부트스트랩 | O | X | O | O |
  | 함수 근사 | X | X | X | O (신경망) |
  | 수렴 보장 | O | O | 조건부 | 이론적 미보장 |
```

## 6. DP 구현과 실전

```
구현 팁:

  In-Place 업데이트:
  별도의 V_old, V_new를 두지 않고
  하나의 V를 바로 업데이트
  → 메모리 절약, 수렴 속도 비슷 또는 더 빠름

  종료 상태 처리:
  V(terminal) = 0 (항상 고정)
  → 업데이트 대상에서 제외

  감마의 효과:
  γ = 0.9: ~10 스텝 앞까지 고려
  γ = 0.99: ~100 스텝 앞까지 고려
  γ = 0.999: ~1000 스텝 앞까지 고려
  → 에피소드 길이에 맞게 설정

  V vs Q 중 어느 것을 계산할까:
  V 반복: O(|S|² × |A|)
  Q 반복: O(|S|² × |A|²) — 더 비싸지만
  → Q를 구하면 정책 추출이 model-free
  → Q(s,a)만으로 argmax_a 가능 (P 불필요)

실전 적용 가능한 DP:

  작은 상태 공간:
  ├─ 보드 게임 (상태 이산화 후)
  ├─ 재고 관리 (이산적 재고 수준)
  ├─ 대기열 관리 (이산적 대기 상태)
  └─ 캐시 교체 정책 (Bélády)

  모델 학습 후 DP:
  환경에서 데이터 수집 → P̂, R̂ 추정 → DP 적용
  → Model-Based RL의 기초 (rl-09)

  DP의 교훈:
  "최적 정책을 찾는 것은 가치를 정확히 아는 것과 같다"
  → 좋은 가치 함수 = 좋은 정책
  → 이 원칙이 DQN, PPO 등 모든 현대 RL의 기초
```

## 핵심 정리

- **정책 평가**는 벨만 기대 방정식을 반복적으로 적용하여 주어진 정책의 가치 함수 V^π를 구하며, 수렴이 보장됩니다
- **정책 반복**은 평가(V^π 계산)와 개선(탐욕적 정책 추출)을 번갈아 수행하여 최적 정책을 찾으며, 유한 MDP에서 반드시 수렴합니다
- **가치 반복**은 벨만 최적 방정식을 직접 반복하여 평가와 개선을 한 스텝에 결합하며, 정책을 명시적으로 유지하지 않아도 됩니다
- DP의 핵심 원리인 **부트스트래핑**(현재 추정치로 다른 추정치를 개선)은 TD 학습과 DQN의 기초이며, DP의 한계(모델 필요, 상태 폭발)가 MC/TD/함수 근사를 동기 부여합니다
- 모든 현대 RL 알고리즘은 DP의 **"좋은 가치 = 좋은 정책"** 원칙을 샘플 기반, 함수 근사로 확장한 것이며, DP는 RL의 이론적 뼈대입니다
