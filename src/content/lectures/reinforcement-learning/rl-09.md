# Model-Based RL

## 왜 세계 모델을 학습하는가

rl-04~08의 model-free 방법은 환경과의 실제 상호작용에서만 배웁니다. 하지만 실세계(로봇, 자율주행)에서 수백만 번의 시행착오는 **위험하고 비용이 큽니다**. 인간은 행동 전에 **"이렇게 하면 이렇게 될 것이다"**를 상상합니다. **Model-Based RL**은 환경의 전이 모델을 학습하고, 이 **상상 속에서** 정책을 개선하여 실제 경험을 크게 줄입니다.

> **핵심 직관**: Model-Free는 **"직접 해봐야 안다"**, Model-Based는 **"상상해보고 결정한다"**입니다. 체스에서 수를 두기 전에 "이렇게 두면 상대가 이렇게 할 것이다"를 머릿속에서 시뮬레이션합니다. 이 "정신적 시뮬레이션"이 Model-Based RL의 핵심이며, 실제 경험 없이도 학습할 수 있게 합니다.

## 1. Dyna 아키텍처

```
Dyna (Sutton, 1991):
  "실제 경험 + 상상 경험을 함께 학습"

  구성 요소:
  ├─ 모델 M: (s, a) → (s', r) 예측
  ├─ 정책 π: 행동 선택
  └─ 가치 Q: 행동 가치

  Dyna-Q 알고리즘:
  매 스텝:
  1. 실제 경험:
     s에서 a 수행 → r, s' 관찰
  2. 모델 학습:
     M(s, a) ← (s', r) 저장/업데이트
  3. 직접 학습:
     Q(s, a) ← Q(s, a) + α[r + γ max Q(s', a') - Q(s, a)]
  4. 계획 (n 번):
     for k = 1 to n:
       랜덤 (s̃, ã) 선택 (이전 경험에서)
       (s̃', r̃) = M(s̃, ã)  (모델로 상상)
       Q(s̃, ã) ← Q(s̃, ã) + α[r̃ + γ max Q(s̃', a') - Q(s̃, ã)]

  n = 0: 순수 Q-Learning (model-free)
  n = 50: 실제 1스텝 + 상상 50스텝
  → 같은 실제 경험으로 50배 더 많이 학습!

  장점:
  ├─ 샘플 효율: 실제 경험 1회로 n번 학습
  ├─ 간단한 구현
  └─ Model-Free와 자연스럽게 결합

  한계:
  ├─ 모델이 부정확하면 잘못된 학습!
  ├─ 테이블 모델 → 복잡한 환경에 부적합
  └─ 모델 에러가 누적될 수 있음

  Dyna의 교훈:
  "적은 실제 경험 + 많은 상상 경험 = 효율적 학습"
  → 현대 Model-Based RL의 기본 원리
```

## 2. 학습된 환경 모델

```
환경 모델의 유형:

  1. 결정론적 모델:
     f_θ(s, a) → s'
     → MSE: ||s' - f_θ(s, a)||² 최소화

  2. 확률적 모델:
     f_θ(s, a) → (μ, σ²)  (가우시안)
     → NLL 최소화
     → 불확실성 추정 가능!

  3. 앙상블 모델:
     N개의 모델을 학습 (N=5~7)
     → 모델 간 불일치 = 인식론적 불확실성
     → 불확실한 영역에서 보수적 행동 가능

  MBPO (Model-Based Policy Optimization, Janner et al., 2019):
  모델 앙상블 + SAC

  알고리즘:
  1. 실제 환경에서 데이터 수집 → D_env
  2. D_env로 앙상블 모델 학습
  3. 모델에서 짧은 롤아웃으로 데이터 생성 → D_model
  4. D_env ∪ D_model로 SAC 업데이트
  5. 반복

  롤아웃 길이 제어:
  짧은 롤아웃(1~5스텝): 모델 에러 누적 적음
  긴 롤아웃: 더 다양한 경험, but 에러 누적

  → 학습 초기: 짧게 (모델 부정확)
  → 학습 후반: 길게 (모델 정확해짐)

  | 방법 | 실제 경험 | 성능 | 모델 |
  |------|---------|------|------|
  | SAC (Model-Free) | 100K | 기준 | 없음 |
  | MBPO | 10K | SAC 동등 | 앙상블 |
  → 10배 적은 실제 경험으로 동등한 성능!
```

## 3. World Models

```
World Models (Ha & Schmidhuber, 2018):
  "세계를 꿈꾸는 에이전트"

  구조:
  1. Vision (V): VAE로 이미지 → 잠재 벡터 z
  2. Memory (M): RNN으로 z의 시간적 진행 예측
  3. Controller (C): z와 h(RNN 상태)로 행동 선택

  학습:
  1단계: VAE 학습 (이미지 → z)
  2단계: RNN 학습 (z_t, a_t → z_{t+1})
  3단계: 컨트롤러 학습 (꿈 속에서!)

  "꿈 속 학습":
  실제 환경 없이 RNN이 생성하는
  상상의 시퀀스에서 정책 학습!
  → 매우 빠른 학습 (실제 환경 불필요)

Dreamer 시리즈:

  Dreamer v1 (Hafner et al., 2020):
  ├─ RSSM (Recurrent State Space Model)
  │   → 결정론적 h + 확률적 z로 상태 표현
  ├─ 학습된 모델에서 상상 롤아웃
  ├─ Actor-Critic을 상상 속에서 학습
  └─ 이미지 입력에서 직접 학습

  Dreamer v2 (Hafner et al., 2021):
  ├─ 이산 잠재 변수 (카테고리컬)
  ├─ KL 밸런싱
  └─ Atari 200M 벤치마크에서 인간 수준

  Dreamer v3 (Hafner et al., 2023):
  ├─ Symlog 예측: 보상 스케일 자동 처리
  ├─ 단일 하이퍼파라미터 세트로 다양한 도메인
  ├─ Minecraft Diamond 최초 달성 (인간 수준)
  └─ 150개+ 과제에서 일관적 성능

  RSSM 구조:
  h_t = f(h_{t-1}, z_{t-1}, a_{t-1})  (결정론적)
  z_t ~ q(z_t | h_t, o_t)              (후방, 관찰 사용)
  ẑ_t ~ p(ẑ_t | h_t)                  (전방, 관찰 없이)

  → 전방 모델로 상상, 후방 모델로 교정

  | 모델 | 입력 | 상상 학습 | 성능 |
  |------|------|----------|------|
  | World Models | 이미지 | VAE+RNN | 기초 증명 |
  | Dreamer v1 | 이미지 | RSSM | DMControl |
  | Dreamer v3 | 다양 | 개선 RSSM | Minecraft |
```

> **핵심 직관**: Dreamer의 "상상 속 학습"이 가능한 이유는 **"세계 모델이 충분히 정확하면 시뮬레이터를 대체"**하기 때문입니다. 물리 시뮬레이터는 정확하지만 느리고, 학습된 모델은 부정확할 수 있지만 빠릅니다. RSSM의 확률적 요소가 불확실성을 표현하여, 모델 에러에 대한 강건성을 제공합니다.

## 4. 모델 기반 계획

```
MPC (Model Predictive Control):
  학습된 모델로 미래를 시뮬레이션하여 최적 행동 선택

  매 시점:
  1. 현재 상태 s에서 시작
  2. 여러 행동 시퀀스를 시뮬레이션 (모델 사용)
  3. 누적 보상이 가장 높은 시퀀스의 첫 행동 선택
  4. 환경에서 실행, 다음 상태 관찰
  5. 반복

  CEM (Cross-Entropy Method):
  행동 시퀀스를 샘플링하는 방법
  1. N개 시퀀스를 가우시안에서 샘플
  2. 각 시퀀스를 모델로 롤아웃 → 보상 계산
  3. 상위 K%의 시퀀스를 선택
  4. 이들의 평균/분산으로 가우시안 업데이트
  5. 반복 → 점점 좋은 행동 시퀀스로 수렴

  MPPI (Model Predictive Path Integral):
  CEM의 개선, 소프트맥스 가중 평균
  → 로봇 제어에서 인기

  MPC의 장점:
  ├─ 정책 학습 불필요 (모델 + 계획만)
  ├─ 새 목표에 즉시 적응 (목표 함수만 변경)
  └─ 모델이 좋으면 매우 효과적

  MPC의 한계:
  ├─ 매 스텝 계획 → 계산 비용
  ├─ 수평선(horizon)이 길면 비효율적
  └─ 이전 경험 재사용 어려움
```

## 5. Model-Based vs Model-Free

```
비교:

  Model-Free:
  ├─ 모델 학습 불필요
  ├─ 모델 에러 없음
  ├─ 샘플 비효율적 (실제 경험만 사용)
  ├─ 구현 상대적으로 간단
  └─ 점근적 성능이 높을 수 있음

  Model-Based:
  ├─ 모델 학습 필요 (추가 복잡성)
  ├─ 모델 에러가 정책에 전파
  ├─ 샘플 효율적 (상상 경험 활용)
  ├─ 구현 복잡
  └─ 모델 정확도가 성능 상한 결정

  샘플 효율성 비교:
  | 방법 | 100K 스텝 | 1M 스텝 | 10M 스텝 |
  |------|----------|---------|---------|
  | PPO | 낮음 | 중간 | 높음 |
  | SAC | 중간 | 높음 | 높음 |
  | MBPO | 높음 | 높음 | 높음 |
  | Dreamer | 높음 | 높음 | 높음 |

  → 적은 경험에서 MB가 압도적 우위
  → 충분한 경험에서 MF도 수렴

  실전 선택:
  시뮬레이션 무료: PPO/SAC (간단, 충분한 데이터)
  실제 로봇: Dreamer/MBPO (적은 경험으로 학습)
  안전 필수: MPC + 모델 (계획 시 안전 제약)
```

## 6. Model-Based의 미래

```
최신 방향:

  TD-MPC2 (Hansen et al., 2024):
  ├─ 잠재 공간에서 모델 학습 + TD + MPC
  ├─ 80개 과제에서 단일 하이퍼파라미터
  ├─ 모델 기반의 실전 가능성 증명
  └─ Dreamer와 경쟁하는 범용 알고리즘

  Foundation World Models:
  ├─ 대규모 사전학습된 세계 모델
  ├─ Genie: 비디오에서 세계 모델 학습
  ├─ UniSim: 시뮬레이터를 학습
  └─ rl-10의 MuZero와 연결

  LLM as World Model:
  ├─ LLM이 텍스트 기반 환경의 모델 역할
  ├─ "방에 들어가면 무엇이 보이는가?"
  └─ 에이전트 계획의 새로운 접근 (nlp-14)

  Model-Based의 도전:
  ├─ 모델 에러의 누적 (compounding error)
  ├─ 고차원 상태의 모델링 어려움
  ├─ 불연속적 동역학 (접촉, 충돌)
  └─ 모델의 일반화 능력 제한
  → 짧은 롤아웃 + 앙상블이 실전적 해결책
```

## 핵심 정리

- **Dyna**는 실제 경험으로 모델을 학습하고 모델에서 상상 경험을 생성하여 학습 효율을 n배 높이며, Model-Based RL의 기본 패러다임입니다
- **MBPO**는 앙상블 모델 + 짧은 롤아웃으로 모델 에러를 제어하며, 10배 적은 실제 경험으로 model-free(SAC)와 동등한 성능을 달성합니다
- **Dreamer**는 RSSM으로 세계 모델을 학습하고 **상상 속에서 Actor-Critic을 학습**하여, Minecraft Diamond을 포함한 150개+ 과제에서 범용적 성능을 보여줍니다
- **MPC**는 학습된 모델로 미래를 시뮬레이션하여 매 스텝 최적 행동을 계획하며, CEM/MPPI로 행동 시퀀스를 최적화합니다
- Model-Based는 **샘플 효율이 핵심 장점**이지만 모델 에러 누적이 도전이며, 짧은 롤아웃과 앙상블이 실전적 해결책입니다
