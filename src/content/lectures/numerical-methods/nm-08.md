# 근사 이론

## 왜 근사 이론을 배워야 하는가

머신러닝의 핵심은 **근사(approximation)** 입니다. 데이터의 패턴을 함수로 근사하고, 복잡한 분포를 간단한 모델로 근사하며, 무한 차원의 함수 공간을 유한 파라미터로 근사합니다. 그렇다면 근본적인 질문이 떠오릅니다. "임의의 함수를 원하는 정밀도로 근사할 수 있는가?" 이 질문에 대한 답이 **Weierstrass 근사 정리**이고, 신경망에 대한 답이 **보편 근사 정리(Universal Approximation Theorem)** 입니다.

이번 강의에서는 고전 근사 이론부터 신경망의 이론적 토대까지를 연결합니다.

---

## 1. Weierstrass 근사 정리

**정리 (Weierstrass, 1885)**: 닫힌 구간 $[a, b]$에서 연속인 함수 $f$에 대해, 임의의 $\epsilon > 0$마다 다항식 $p_n$이 존재하여 다음을 만족한다.

$$
\max_{x \in [a,b]} |f(x) - p_n(x)| < \epsilon
$$

즉, **모든 연속 함수는 다항식으로 균등하게 근사할 수 있습니다.**

```python
import numpy as np

# Bernstein 다항식: Weierstrass 정리의 구성적 증명
def bernstein_approx(f, n, x):
    """n차 Bernstein 다항식으로 f를 근사"""
    result = np.zeros_like(x)
    for k in range(n + 1):
        # 이항 계수
        binom = np.math.factorial(n) / (
            np.math.factorial(k) * np.math.factorial(n - k))
        # Bernstein 기저
        basis = binom * x**k * (1 - x)**(n - k)
        result += f(k / n) * basis
    return result

# |x - 0.5| 같은 비매끄러운 함수도 근사 가능
f = lambda x: np.abs(x - 0.5)
x = np.linspace(0, 1, 200)

for n in [5, 20, 100]:
    approx = bernstein_approx(f, n, x)
    error = np.max(np.abs(f(x) - approx))
    print(f"n={n:3d}: 최대 오차 = {error:.6f}")
```

> **핵심 직관**: Weierstrass 정리는 "존재성"을 보장합니다. 다항식 하나로 어떤 연속 함수든 원하는 만큼 가까이 다가갈 수 있습니다. 하지만 "얼마나 빨리" 수렴하는지, 어떤 점에서 평가해야 하는지는 별개의 문제입니다.

---

## 2. Lagrange 보간

$n+1$개의 점 $(x_0, y_0), \ldots, (x_n, y_n)$을 정확히 지나는 $n$차 다항식입니다.

$$
p_n(x) = \sum_{i=0}^{n} y_i \prod_{j \neq i} \frac{x - x_j}{x_i - x_j}
$$

```python
import numpy as np

def lagrange_interp(x_nodes, y_nodes, x):
    """Lagrange 보간 다항식"""
    n = len(x_nodes)
    result = np.zeros_like(x, dtype=float)
    for i in range(n):
        basis = np.ones_like(x, dtype=float)
        for j in range(n):
            if j != i:
                basis *= (x - x_nodes[j]) / (x_nodes[i] - x_nodes[j])
        result += y_nodes[i] * basis
    return result

# Runge 현상: 등간격 점으로 고차 보간하면 발산!
f = lambda x: 1 / (1 + 25 * x**2)
x_eval = np.linspace(-1, 1, 500)

for n in [5, 10, 15]:
    x_eq = np.linspace(-1, 1, n + 1)  # 등간격
    p = lagrange_interp(x_eq, f(x_eq), x_eval)
    print(f"등간격 n={n:2d}: 최대 오차 = {np.max(np.abs(f(x_eval) - p)):.4f}")
```

### Runge 현상

등간격 점으로 Lagrange 보간의 차수를 높이면, 양 끝점에서 **진동이 폭발적으로 증가**합니다.

```
등간격 Lagrange 보간 (n=15, Runge 함수):

  ∧  진동!            진동!
  │ /\              /\
  │/  \    원래 함수   /  \
  │    \   /──\   /    \
──┼─────╲╱────╲╱──────╲──→
  │
```

---

## 3. Chebyshev 다항식과 최적 보간점

Runge 현상의 해결책은 **Chebyshev 점**을 사용하는 것입니다.

$$
x_k = \cos\left(\frac{2k+1}{2(n+1)}\pi\right), \quad k = 0, 1, \ldots, n
$$

Chebyshev 점은 양 끝점에 더 밀집되어 있어 Runge 현상을 억제합니다.

```python
import numpy as np

f = lambda x: 1 / (1 + 25 * x**2)
x_eval = np.linspace(-1, 1, 500)

for n in [5, 10, 15, 20]:
    # Chebyshev 점
    k = np.arange(n + 1)
    x_cheb = np.cos((2*k + 1) / (2*(n+1)) * np.pi)

    p = lagrange_interp(x_cheb, f(x_cheb), x_eval)
    err = np.max(np.abs(f(x_eval) - p))
    print(f"Chebyshev n={n:2d}: 최대 오차 = {err:.6f}")
```

| 보간점 | $n = 10$ 오차 | $n = 20$ 오차 | 수렴 |
|--------|-------------|-------------|------|
| 등간격 | ~2.0 | ~60.0 (발산!) | $\times$ |
| Chebyshev | ~0.11 | ~0.0015 | $\checkmark$ |

> **핵심 직관**: Chebyshev 점은 "양쪽 끝에 더 많은 점을 배치"하여 보간 오차를 최소화합니다. 이는 $[-1,1]$에서 최대 오차를 최소화하는 **최적 보간점**입니다.

---

## 4. 스플라인 근사

고차 다항식 대신, 구간마다 **저차 다항식을 이어 붙이는** 방법입니다.

$$
S(x) = S_i(x) = a_i + b_i(x - x_i) + c_i(x - x_i)^2 + d_i(x - x_i)^3, \quad x \in [x_i, x_{i+1}]
$$

**3차 스플라인 조건**:
- 각 구간에서 데이터를 지남: $S_i(x_i) = y_i$
- 연결점에서 연속: $S_i(x_{i+1}) = S_{i+1}(x_{i+1})$
- 1차, 2차 도함수 연속: $S_i'(x_{i+1}) = S_{i+1}'(x_{i+1})$, $S_i''(x_{i+1}) = S_{i+1}''(x_{i+1})$

```python
import numpy as np
from scipy.interpolate import CubicSpline

# Runge 함수도 스플라인으로는 안정
f = lambda x: 1 / (1 + 25 * x**2)

x_eval = np.linspace(-1, 1, 500)

for n in [5, 10, 20]:
    x_nodes = np.linspace(-1, 1, n + 1)
    cs = CubicSpline(x_nodes, f(x_nodes))
    err = np.max(np.abs(f(x_eval) - cs(x_eval)))
    print(f"3차 스플라인 n={n:2d}: 최대 오차 = {err:.6f}")
```

| 방법 | 전역/국소 | 매끄러움 | Runge 현상 | 사용처 |
|------|---------|---------|-----------|-------|
| Lagrange | 전역 | $C^\infty$ | 있음 (등간격) | 이론 |
| Chebyshev | 전역 | $C^\infty$ | 없음 | 수치 해석 |
| 3차 스플라인 | 국소 | $C^2$ | 없음 | 공학, 그래픽 |

---

## 5. 보편 근사 정리 (Universal Approximation Theorem)

**정리 (Cybenko, 1989; Hornik, 1991)**: 하나의 은닉층을 가진 신경망은 충분한 뉴런 수가 주어지면 컴팩트 집합 위의 임의의 연속 함수를 원하는 정밀도로 근사할 수 있다.

$$
f(\mathbf{x}) \approx \sum_{i=1}^{N} \alpha_i \sigma(\mathbf{w}_i^T \mathbf{x} + b_i)
$$

여기서 $\sigma$는 ReLU, sigmoid 등 비선형 활성화 함수입니다.

```python
import torch
import torch.nn as nn

# 보편 근사 정리 시연: 1 은닉층 신경망으로 함수 근사
target_fn = lambda x: torch.sin(3*x) * torch.exp(-x**2)
x_train = torch.linspace(-3, 3, 200).unsqueeze(1)
y_train = target_fn(x_train)

for width in [5, 20, 100]:
    model = nn.Sequential(nn.Linear(1, width), nn.ReLU(), nn.Linear(width, 1))
    opt = torch.optim.Adam(model.parameters(), lr=0.01)
    for _ in range(3000):
        loss = nn.functional.mse_loss(model(x_train), y_train)
        opt.zero_grad(); loss.backward(); opt.step()

    with torch.no_grad():
        x_test = torch.linspace(-3, 3, 500).unsqueeze(1)
        err = (model(x_test) - target_fn(x_test)).abs().max().item()
    print(f"뉴런 {width:3d}개: 최대 오차 = {err:.6f}")
```

> **핵심 직관**: 보편 근사 정리는 Weierstrass 정리의 **신경망 버전**입니다. "근사할 수 있다"는 존재성을 보장하지만, "어떻게 학습하는가"는 별도의 문제입니다. co-07에서 배운 경사 하강법이 실제 학습을 담당합니다.

---

## 6. 근사 이론이 ML에 주는 시사점

| 고전 근사 이론 | ML에서의 대응 |
|-------------|-------------|
| 다항식 차수 $n$ | 신경망 크기 (너비/깊이) |
| Weierstrass 정리 | 보편 근사 정리 |
| Runge 현상 (과적합) | 과적합 (overfitting) |
| Chebyshev 최적 보간점 | 데이터 분포의 중요성 |
| 스플라인 (국소 근사) | ReLU 신경망 (구간별 선형) |
| 근사 차수 vs 오차 | 모델 용량 vs 일반화 오차 |

실제로 ReLU 신경망은 **구간별 선형 스플라인**과 수학적으로 등가입니다. 깊은 ReLU 네트워크는 점점 더 복잡한 구간별 선형 함수를 표현하게 됩니다.

---

## 핵심 정리

1. **Weierstrass 근사 정리**: 모든 연속 함수는 다항식으로 균등 근사 가능하며, 이는 함수 근사의 이론적 토대이다.
2. **Runge 현상**: 등간격 고차 보간은 양 끝에서 발산하며, **Chebyshev 점**이 이를 해결한다.
3. **3차 스플라인**은 구간별 저차 다항식으로 Runge 현상 없이 안정적으로 근사한다.
4. **보편 근사 정리**: 1 은닉층 신경망도 충분한 뉴런이 있으면 임의의 연속 함수를 근사할 수 있다.
5. ReLU 신경망은 **구간별 선형 함수**로, 스플라인 근사의 현대적 확장이라 할 수 있다.
