# 수치 안정성과 오차 분석

## 왜 수치 안정성을 배워야 하는가

수학적으로 동일한 공식이라도 컴퓨터에서 계산하면 결과가 천차만별일 수 있습니다. 이차방정식의 근의 공식, 분산 계산, 행렬 역행렬 — 모두 "올바른 공식"임에도 구현 방식에 따라 결과가 완전히 틀어집니다. nm-01에서 배운 부동소수점 오차가 알고리즘을 통과하면서 어떻게 증폭되거나 억제되는지, 그리고 "좋은 알고리즘"과 "나쁜 알고리즘"을 구별하는 기준이 무엇인지 알아보겠습니다.

---

## 1. 전방 오차와 역방향 오차

함수 $f$의 정확한 값 $f(x)$와 수치 알고리즘이 계산한 근사값 $\hat{y}$의 관계를 두 가지 관점에서 분석할 수 있습니다.

$$
\text{전방 오차 (forward error)} = |\hat{y} - f(x)|
$$

$$
\text{역방향 오차 (backward error)} = \min\{|\Delta x| : f(x + \Delta x) = \hat{y}\}
$$

```
     정확한 입력 x ──── f ────→ 정확한 출력 f(x)
         │                            ↑
         │ Δx (역방향 오차)            │ 전방 오차
         ↓                            │
     x + Δx ──── f ────→ 근사 출력 ŷ = f(x + Δx)
```

> **핵심 직관**: 역방향 오차는 "우리의 답이 **약간 다른 입력**에 대한 정확한 답이 되는가?"라는 질문입니다. 역방향 오차가 작으면, 알고리즘 자체는 잘못이 없고 입력 데이터의 불확실성 범위 안에 있다고 해석할 수 있습니다.

| 오차 유형 | 의미 | 누가 제어하는가 |
|----------|------|----------------|
| **전방 오차** | 답이 얼마나 틀렸는가 | 문제 + 알고리즘 |
| **역방향 오차** | 어떤 "가상 입력"의 정확한 답인가 | 알고리즘만 |

---

## 2. 조건수 (Condition Number)

문제 자체가 입력의 작은 변화에 민감한 정도를 **조건수**로 측정합니다. la-09에서 행렬의 조건수를 다뤘지만, 여기서는 일반적인 정의를 살펴봅니다.

$$
\kappa = \lim_{\delta \to 0} \sup_{|\Delta x| \leq \delta} \frac{|f(x + \Delta x) - f(x)|/|f(x)|}{|\Delta x|/|x|}
$$

스칼라 함수 $f$에 대해 조건수는 다음과 같이 단순화됩니다.

$$
\kappa(f, x) = \left|\frac{x f'(x)}{f(x)}\right|
$$

행렬 $A$에 대한 조건수는 la-09에서 배운 대로 다음과 같습니다.

$$
\kappa(A) = \|A\| \cdot \|A^{-1}\| = \frac{\sigma_{\max}}{\sigma_{\min}}
$$

```python
import numpy as np

# 잘 조건화된 행렬 vs 나쁘게 조건화된 행렬
A_good = np.array([[2.0, 0.0],
                    [0.0, 1.0]])

A_bad = np.array([[1.0, 1.0],
                   [1.0, 1.0 + 1e-10]])

print(f"좋은 행렬 조건수: {np.linalg.cond(A_good):.1f}")      # 2.0
print(f"나쁜 행렬 조건수: {np.linalg.cond(A_bad):.2e}")        # ~2e+10

# 조건수가 큰 시스템: 작은 입력 변화 → 큰 출력 변화
b1 = np.array([2.0, 2.0 + 1e-10])
b2 = np.array([2.0, 2.0 + 2e-10])

x1 = np.linalg.solve(A_bad, b1)
x2 = np.linalg.solve(A_bad, b2)
print(f"b 변화: {np.linalg.norm(b2-b1)/np.linalg.norm(b1):.2e}")
print(f"x 변화: {np.linalg.norm(x2-x1)/np.linalg.norm(x1):.2e}")
```

> **핵심 직관**: 조건수가 $10^k$이면, 입력의 상대 오차가 결과에서 약 $k$자리만큼 추가로 손실됩니다. FP64의 16자리 정밀도에서 $\kappa = 10^{10}$이면 결과는 약 6자리만 신뢰할 수 있습니다.

---

## 3. 전방 오차와 조건수의 관계

전방 오차, 역방향 오차, 조건수는 다음 부등식으로 연결됩니다.

$$
\frac{|\hat{y} - f(x)|}{|f(x)|} \leq \kappa \cdot \frac{|\Delta x|}{|x|}
$$

$$
\text{(상대 전방 오차)} \leq \text{(조건수)} \times \text{(상대 역방향 오차)}
$$

이것은 수치 해석의 **황금률(golden rule)** 입니다.

| 조건수 | 역방향 오차 | 상황 |
|--------|-----------|------|
| 작음 | 작음 | 정확한 답 |
| 작음 | 큼 | 나쁜 알고리즘 |
| 큼 | 작음 | 어려운 문제 (알고리즘은 최선) |
| 큼 | 큼 | 최악 |

---

## 4. 치명적 소거 (Catastrophic Cancellation)

크기가 비슷한 두 수의 뺄셈에서 유효 자릿수가 급격히 줄어드는 현상입니다.

```python
import numpy as np

# 예시: 이차방정식 x^2 - 1e8 x + 1 = 0
# 정확한 근: x1 ≈ 1e8, x2 ≈ 1e-8
a, b, c = 1.0, -1e8, 1.0

# 불안정한 근의 공식 (작은 근에서 치명적 소거)
disc = np.sqrt(b**2 - 4*a*c)
x2_unstable = (-b - disc) / (2*a)

# 안정한 대안: 유리화 (rationalization)
x2_stable = (2*c) / (-b + disc)

print(f"불안정 계산: x2 = {x2_unstable:.15e}")
print(f"안정 계산:   x2 = {x2_stable:.15e}")
print(f"정확한 값:   x2 = {1e-8:.15e}")
```

**왜 소거가 발생하는가?**

$$
b = -10^8, \quad \sqrt{b^2 - 4ac} \approx 10^8 - 2 \times 10^{-8}
$$

$-b = 10^8$에서 $\sqrt{b^2 - 4ac} \approx 10^8$을 빼면, 유효 숫자 16자리 중 8자리가 상쇄되어 8자리만 남습니다.

> **핵심 직관**: 치명적 소거는 "비슷한 크기의 수를 빼면 유효 자릿수가 사라진다"는 현상입니다. 해결책은 수학적으로 동치인 다른 공식을 사용하여 뺄셈을 피하는 것입니다.

---

## 5. 안정한 알고리즘 vs 불안정한 알고리즘

### 분산 계산의 두 가지 방법

```python
import numpy as np

def var_naive(x):
    """불안정: E[X^2] - (E[X])^2"""
    n = len(x)
    mean_sq = np.sum(x**2) / n
    sq_mean = (np.sum(x) / n)**2
    return mean_sq - sq_mean  # 큰 수 - 큰 수 = 소거!

def var_welford(x):
    """안정: Welford의 온라인 알고리즘"""
    n = 0
    mean = 0.0
    M2 = 0.0
    for xi in x:
        n += 1
        delta = xi - mean
        mean += delta / n
        delta2 = xi - mean
        M2 += delta * delta2
    return M2 / n

# 큰 평균, 작은 분산 → 소거 위험
data = np.array([1e9 + 1.0, 1e9 + 2.0, 1e9 + 3.0])
true_var = np.var(data)

print(f"NumPy (정확): {true_var:.10f}")
print(f"Naive 공식:   {var_naive(data):.10f}")
print(f"Welford:      {var_welford(data):.10f}")
```

### 안정성 비교 요약

| 연산 | 불안정한 방법 | 안정한 대안 |
|------|-------------|------------|
| 분산 | $E[X^2] - (E[X])^2$ | Welford 알고리즘 |
| 이차방정식 작은 근 | 근의 공식 직접 사용 | 유리화 |
| $e^x$ 차이 | $e^x - 1$ 직접 계산 | `np.expm1(x)` |
| 로그 합 | $\log(e^{a} + e^{b})$ 직접 | log-sum-exp trick |
| 행렬 풀이 | $A^{-1}b$ | `np.linalg.solve(A, b)` |

---

## 6. 실전: 그래디언트 계산의 수치 검증

ML에서 자동 미분의 결과를 검증할 때, 유한 차분법(finite differences)과 비교합니다. 이때 스텝 크기 $h$의 선택이 중요합니다.

```python
import numpy as np

def f(x):
    return np.sin(x) * np.exp(-x**2)

def df_exact(x):
    return np.cos(x) * np.exp(-x**2) - 2*x * np.sin(x) * np.exp(-x**2)

x0 = 1.0
hs = [1e-2, 1e-4, 1e-6, 1e-8, 1e-10, 1e-12, 1e-14]

print(f"{'h':>10s}  {'전방차분 오차':>15s}  {'중심차분 오차':>15s}")
print("-" * 45)
for h in hs:
    fwd = (f(x0 + h) - f(x0)) / h
    ctr = (f(x0 + h) - f(x0 - h)) / (2*h)
    exact = df_exact(x0)
    print(f"{h:10.0e}  {abs(fwd - exact):15.2e}  {abs(ctr - exact):15.2e}")

# h가 너무 작으면 → 반올림 오차 지배
# h가 너무 크면 → 절단 오차 지배
# 최적 h: 전방차분 ~1e-8, 중심차분 ~1e-5
```

> **핵심 직관**: 유한 차분의 스텝 크기 $h$에는 **달콤한 지점(sweet spot)** 이 있습니다. $h$를 줄이면 절단 오차(truncation error)는 감소하지만 반올림 오차(rounding error)가 증가합니다. nm-03에서 배울 자동 미분은 이 트레이드오프 자체를 제거합니다.

---

## 핵심 정리

1. **전방 오차**는 답이 얼마나 틀렸는지, **역방향 오차**는 알고리즘이 얼마나 잘 작동하는지를 측정한다.
2. **조건수** $\kappa$는 문제 자체의 민감도이며, 상대 전방 오차 $\leq \kappa \times$ 상대 역방향 오차이다.
3. **치명적 소거**는 비슷한 크기의 수를 빼면 유효 자릿수가 급감하는 현상이다.
4. 수학적으로 동치인 공식이라도 **수치 안정성은 천차만별**이므로 구현을 신중히 선택해야 한다.
5. 유한 차분 검증 시 스텝 크기 $h$는 절단 오차와 반올림 오차의 균형점에서 선택해야 한다.
