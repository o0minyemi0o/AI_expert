# 수치 최적화 실전

## 왜 수치 최적화의 실전 기법을 배워야 하는가

co-07에서 경사 하강법의 수렴 이론을 배웠습니다. 그러나 실전에서는 "그래디언트 반대 방향으로 갈 때, 얼마나 갈 것인가?"라는 **스텝 크기(step size)** 문제가 핵심입니다. 스텝이 너무 크면 발산하고, 너무 작으면 수렴이 느립니다.

이번 강의에서는 단순한 고정 학습률을 넘어, **선 탐색(line search)**, **신뢰 영역(trust region)**, 그리고 곡률 정보를 활용하는 **뉴턴 방법과 준-뉴턴 방법**을 다룹니다. 이 기법들은 전통적 최적화에서 검증된 것이며, 현대 ML 최적화의 기반이 됩니다.

---

## 1. 선 탐색 (Line Search)

탐색 방향 $\mathbf{d}$가 정해진 후, 최적 스텝 크기 $\alpha$를 찾는 1차원 최적화 문제입니다.

$$
\alpha^* = \arg\min_{\alpha > 0} f(\mathbf{x} + \alpha \mathbf{d})
$$

정확한 최소화는 비현실적이므로, **충분한 감소**를 보장하는 조건을 사용합니다.

### Armijo 조건 (충분 감소)

$$
f(\mathbf{x} + \alpha \mathbf{d}) \leq f(\mathbf{x}) + c_1 \alpha \nabla f(\mathbf{x})^T \mathbf{d}
$$

$c_1 \in (0, 1)$, 보통 $c_1 = 10^{-4}$. 이 조건은 "적어도 선형 예측의 $c_1$ 비율만큼 감소해야 한다"를 의미합니다.

### Wolfe 조건 (곡률 조건 추가)

Armijo 조건만으로는 $\alpha$가 지나치게 작아질 수 있으므로, **곡률 조건**을 추가합니다.

$$
\nabla f(\mathbf{x} + \alpha \mathbf{d})^T \mathbf{d} \geq c_2 \nabla f(\mathbf{x})^T \mathbf{d}
$$

$c_2 \in (c_1, 1)$, 보통 $c_2 = 0.9$.

```python
import numpy as np

def backtracking_line_search(f, grad_f, x, d, alpha=1.0,
                              c1=1e-4, rho=0.5):
    """Armijo 조건을 만족할 때까지 스텝 크기를 반복 축소"""
    fx = f(x)
    gTd = grad_f(x) @ d

    while f(x + alpha * d) > fx + c1 * alpha * gTd:
        alpha *= rho  # 스텝 크기 절반으로 줄임

    return alpha

# Rosenbrock 함수: 최적화의 대표적 테스트 함수
def rosenbrock(x):
    return 100 * (x[1] - x[0]**2)**2 + (1 - x[0])**2

def rosenbrock_grad(x):
    dx0 = -400 * x[0] * (x[1] - x[0]**2) - 2 * (1 - x[0])
    dx1 = 200 * (x[1] - x[0]**2)
    return np.array([dx0, dx1])

# 경사 하강법 + Armijo 선 탐색
x = np.array([-1.0, 1.0])
for i in range(200):
    g = rosenbrock_grad(x)
    d = -g  # 최급강하 방향
    alpha = backtracking_line_search(rosenbrock, rosenbrock_grad, x, d)
    x = x + alpha * d
    if i % 50 == 0:
        print(f"iter {i:3d}: f = {rosenbrock(x):.6f}, "
              f"|grad| = {np.linalg.norm(g):.6f}, alpha = {alpha:.4f}")

print(f"최종 해: x = {x}, f(x) = {rosenbrock(x):.8f}")
```

> **핵심 직관**: Armijo 선 탐색은 "1에서 시작해서 조건을 만족할 때까지 절반씩 줄이는" 단순한 전략입니다. 이것만으로도 고정 학습률 대비 수렴 안정성이 크게 개선됩니다.

---

## 2. 신뢰 영역 (Trust Region)

선 탐색이 "방향을 먼저 정하고, 거리를 결정"한다면, 신뢰 영역은 "거리(반지름)를 먼저 정하고, 그 안에서 최적 방향을 결정"합니다.

$$
\min_{\mathbf{p}} m(\mathbf{p}) = f(\mathbf{x}) + \nabla f(\mathbf{x})^T \mathbf{p} + \frac{1}{2}\mathbf{p}^T B \mathbf{p} \quad \text{s.t.} \quad \|\mathbf{p}\| \leq \Delta
$$

여기서 $B$는 Hessian 또는 그 근사, $\Delta$는 신뢰 영역 반지름입니다.

```
선 탐색 vs 신뢰 영역:

선 탐색:                      신뢰 영역:
  ←──── 방향 고정 ────→         ╭───────╮
      ·──→──→──→·              │ 이 안에서 │
      방향 먼저, 거리 조절        │ 최적점  · │
                               │  탐색   │
                               ╰───────╯
                              반지름 먼저, 방향+거리 동시
```

신뢰 영역의 핵심은 **모델 예측과 실제 감소의 비율**로 반지름을 조절하는 것입니다.

$$
\rho = \frac{f(\mathbf{x}) - f(\mathbf{x} + \mathbf{p})}{m(\mathbf{0}) - m(\mathbf{p})} = \frac{\text{실제 감소}}{\text{예측 감소}}
$$

| $\rho$ 값 | 해석 | 행동 |
|-----------|------|------|
| $\rho < 0.25$ | 모델이 현실과 괴리 | 반지름 축소 |
| $0.25 \leq \rho \leq 0.75$ | 합리적 | 유지 |
| $\rho > 0.75$ | 모델이 정확 | 반지름 확대 |

> **핵심 직관**: 신뢰 영역은 "2차 근사가 신뢰할 수 있는 범위"를 적응적으로 조절합니다. 2차 근사가 좋으면 넓게 탐색하고, 나쁘면 좁게 탐색합니다. Adam 옵티마이저의 적응적 학습률도 비슷한 철학입니다.

---

## 3. 뉴턴 방법 (Newton's Method)

la-05에서 배운 Hessian을 활용하는 2차 최적화 방법입니다.

$$
\mathbf{x}^{(k+1)} = \mathbf{x}^{(k)} - [H(\mathbf{x}^{(k)})]^{-1} \nabla f(\mathbf{x}^{(k)})
$$

| 방법 | 수렴 차수 | 반복당 비용 | Hessian 필요 |
|------|----------|-----------|-------------|
| 경사 하강법 | 선형 | $O(n)$ | 아니오 |
| 뉴턴 방법 | **이차** (quadratic) | $O(n^3)$ | 예 |
| 준-뉴턴 | 초선형 | $O(n^2)$ | 아니오 (근사) |

```python
import numpy as np

def newton_method(f, grad_f, hess_f, x0, tol=1e-8, max_iter=100):
    """뉴턴 방법 (Hessian 직접 계산)"""
    x = x0.copy()
    for k in range(max_iter):
        g = grad_f(x)
        H = hess_f(x)

        # 뉴턴 방향: H·d = -g
        d = np.linalg.solve(H, -g)
        x = x + d

        if np.linalg.norm(g) < tol:
            print(f"뉴턴: {k+1}번 반복에서 수렴")
            return x
    return x

# Rosenbrock의 Hessian
def rosenbrock_hess(x):
    H = np.zeros((2, 2))
    H[0, 0] = -400 * (x[1] - 3*x[0]**2) + 2
    H[0, 1] = -400 * x[0]
    H[1, 0] = -400 * x[0]
    H[1, 1] = 200
    return H

x_newton = newton_method(rosenbrock, rosenbrock_grad, rosenbrock_hess,
                          np.array([-1.0, 1.0]))
print(f"뉴턴 해: {x_newton}")
print(f"f(x) = {rosenbrock(x_newton):.2e}")
```

---

## 4. 준-뉴턴 방법: BFGS와 L-BFGS

Hessian 계산은 $O(n^2)$ 저장, $O(n^3)$ 풀이가 필요합니다. **준-뉴턴 방법**은 Hessian을 **그래디언트 정보만으로 근사**합니다.

### BFGS 업데이트

$$
B_{k+1} = B_k + \frac{\mathbf{y}\mathbf{y}^T}{\mathbf{y}^T \mathbf{s}} - \frac{B_k \mathbf{s}\mathbf{s}^T B_k}{\mathbf{s}^T B_k \mathbf{s}}
$$

여기서 $\mathbf{s} = \mathbf{x}_{k+1} - \mathbf{x}_k$, $\mathbf{y} = \nabla f_{k+1} - \nabla f_k$입니다.

### L-BFGS: 대규모 문제를 위한 변형

$B_k$를 $n \times n$ 행렬로 저장하는 대신, 최근 $m$개(보통 $m = 10$~$20$)의 $(\mathbf{s}, \mathbf{y})$ 쌍만 저장하고, 행렬-벡터 곱을 암묵적으로 계산합니다.

```python
from scipy.optimize import minimize

# L-BFGS-B: 대규모 준-뉴턴
x0 = np.array([-1.0, 1.0])

# 경사 하강법
result_gd = minimize(rosenbrock, x0, jac=rosenbrock_grad,
                     method='CG', options={'maxiter': 500})

# L-BFGS-B
result_lbfgs = minimize(rosenbrock, x0, jac=rosenbrock_grad,
                        method='L-BFGS-B')

# 뉴턴-CG (Hessian-벡터 곱으로 뉴턴 방향 근사)
result_ncg = minimize(rosenbrock, x0, jac=rosenbrock_grad,
                      hess=rosenbrock_hess, method='Newton-CG')

print(f"{'방법':<12s} {'반복 횟수':>8s} {'함수 평가':>8s} {'f(x)':>12s}")
print("-" * 44)
print(f"{'CG':<12s} {result_gd.nit:>8d} {result_gd.nfev:>8d} "
      f"{result_gd.fun:>12.2e}")
print(f"{'L-BFGS-B':<12s} {result_lbfgs.nit:>8d} {result_lbfgs.nfev:>8d} "
      f"{result_lbfgs.fun:>12.2e}")
print(f"{'Newton-CG':<12s} {result_ncg.nit:>8d} {result_ncg.nfev:>8d} "
      f"{result_ncg.fun:>12.2e}")
```

> **핵심 직관**: L-BFGS는 "Hessian의 저랭크 근사"를 사용합니다. $n \times n$ 행렬 대신 $m$개의 벡터 쌍만 저장하므로 메모리가 $O(mn)$입니다. 파라미터가 수백만 개인 자연어 처리 모델의 미세 조정(fine-tuning)에서도 사용됩니다.

---

## 5. 뉴턴-CG: Hessian 없는 2차 최적화

nm-06에서 배운 CG를 활용하면, Hessian을 명시적으로 저장하지 않고도 뉴턴 방향을 근사할 수 있습니다. **Hessian-벡터 곱** $H\mathbf{v}$만 있으면 CG로 $H\mathbf{d} = -\nabla f$를 풀 수 있기 때문입니다. nm-04에서 배운 자동 미분으로 이 곱을 효율적으로 계산합니다.

```python
import torch

# PyTorch에서 Hessian-벡터 곱
params = torch.tensor([-1.0, 1.0], requires_grad=True)
loss = 100*(params[1]-params[0]**2)**2 + (1-params[0])**2

grad = torch.autograd.grad(loss, params, create_graph=True)[0]
v = torch.randn(2)  # 임의의 방향
Hv = torch.autograd.grad(grad @ v, params)[0]  # H·v (행렬 저장 없이!)

print(f"Hessian-벡터 곱: {Hv}")
# 이 Hv를 CG의 행렬-벡터 곱으로 사용하면 뉴턴-CG 완성
```

---

## 6. co-07과의 연결: ML 최적화 전체 그림

co-07에서 다뤘던 경사 하강법과 그 변형(SGD, Adam)은 1차 방법입니다. 이번 강의의 2차 방법과 비교하면 다음과 같습니다.

| 방법 | 차수 | 반복당 비용 | 수렴 속도 | ML에서의 위치 |
|------|------|-----------|----------|-------------|
| SGD | 1차 | $O(n)$ | 선형 | 대규모 학습의 기본 |
| Adam | 1차 (적응) | $O(n)$ | 선형 | 실전 기본값 |
| L-BFGS | 준-2차 | $O(mn)$ | 초선형 | 소규모 / fine-tuning |
| 뉴턴-CG | 2차 | $O(kn)$ | 이차 | 연구용 |
| 자연 경사법 | 2차 (Fisher) | $O(kn)$ | 이차 | 강화학습 (TRPO) |

> **핵심 직관**: 대규모 ML에서는 1차 방법(SGD/Adam)이 지배적입니다. 미니배치의 노이즈가 2차 정보의 정확한 추정을 어렵게 만들기 때문입니다. 그러나 fine-tuning, 메타학습, 강화학습 등에서는 2차 방법이 여전히 강력합니다.

---

## 핵심 정리

1. **Armijo/Wolfe 선 탐색**은 적절한 스텝 크기를 자동으로 결정하여 수렴 안정성을 보장한다.
2. **신뢰 영역 방법**은 2차 모델이 유효한 범위를 적응적으로 조절하여 선 탐색의 대안을 제공한다.
3. **뉴턴 방법**은 이차 수렴을 제공하지만, Hessian 계산 비용 $O(n^3)$이 대규모 문제에서 걸림돌이다.
4. **L-BFGS**는 최근 그래디언트 이력으로 Hessian을 암묵적으로 근사하여 $O(mn)$ 메모리로 준-뉴턴 방향을 구한다.
5. **뉴턴-CG**는 자동 미분의 Hessian-벡터 곱과 CG를 결합하여 Hessian을 저장하지 않고 2차 최적화를 수행한다.
