# 수치 적분과 몬테카를로

## 왜 수치 적분을 배워야 하는가

확률론과 통계에서 기댓값, 주변 확률, 베이즈 사후 분포 — 이 모든 것이 적분입니다. 그런데 대부분의 적분은 해석적으로 풀 수 없습니다. pt-07에서 다뤘던 베이즈 추론의 핵심 난제인 "증거(evidence)" 계산도 다차원 적분입니다.

$$
p(\theta | D) = \frac{p(D|\theta)p(\theta)}{\int p(D|\theta)p(\theta)\,d\theta}
$$

분모의 적분을 어떻게 계산하느냐에 따라 근사 추론의 방법이 결정됩니다. 이번 강의에서는 결정론적 구적법부터 확률론적 몬테카를로, 그리고 MCMC까지 수치 적분의 핵심 기법을 다룹니다.

---

## 1. 결정론적 구적법

### 1.1 사다리꼴 법칙과 Simpson 법칙

1차원 적분 $I = \int_a^b f(x)\,dx$를 $n$개의 구간으로 나누어 근사합니다.

$$
I_{\text{trap}} = \frac{h}{2}\left[f(a) + 2\sum_{i=1}^{n-1}f(x_i) + f(b)\right], \quad h = \frac{b-a}{n}
$$

$$
I_{\text{Simpson}} = \frac{h}{3}\left[f(a) + 4\sum_{\text{홀수}}f(x_i) + 2\sum_{\text{짝수}}f(x_i) + f(b)\right]
$$

| 방법 | 오차 차수 | 다항식 정확도 |
|------|----------|-------------|
| 사다리꼴 | $O(h^2)$ | 1차 정확 |
| Simpson | $O(h^4)$ | 3차 정확 |
| Gaussian (n점) | $O(h^{2n})$ | $(2n-1)$차 정확 |

```python
import numpy as np
from scipy import integrate

def f(x):
    return np.exp(-x**2)  # 가우시안 — 해석적 적분 불가능

# 정확한 값 (erf 함수 사용)
from scipy.special import erf
exact = np.sqrt(np.pi) / 2 * (erf(3) - erf(-3))

# 사다리꼴 법칙
n_points = [10, 100, 1000]
for n in n_points:
    x = np.linspace(-3, 3, n)
    I_trap = np.trapz(f(x), x)
    print(f"사다리꼴 (n={n:4d}): 오차 = {abs(I_trap - exact):.2e}")

# Simpson 법칙
for n in n_points:
    x = np.linspace(-3, 3, n + 1 if n % 2 == 0 else n)
    I_simp = integrate.simpson(f(x), x=x)
    print(f"Simpson  (n={n:4d}): 오차 = {abs(I_simp - exact):.2e}")
```

### 1.2 Gaussian Quadrature

$n$개의 **최적 평가점**과 **가중치**를 선택하여, $2n-1$차 다항식까지 정확하게 적분합니다.

$$
\int_{-1}^{1} f(x)\,dx \approx \sum_{i=1}^{n} w_i f(x_i)
$$

```python
from numpy.polynomial.legendre import leggauss

# Gauss-Legendre 구적법
for n in [3, 5, 10]:
    nodes, weights = leggauss(n)
    # [-1,1] → [-3,3] 변환
    x = 3 * nodes
    w = 3 * weights
    I_gauss = np.sum(w * f(x))
    print(f"Gauss (n={n:2d}점): 오차 = {abs(I_gauss - exact):.2e}")
```

> **핵심 직관**: Gaussian quadrature는 같은 함수 평가 횟수로 사다리꼴이나 Simpson보다 **지수적으로** 정확합니다. 하지만 이 장점은 1차원에서만 빛납니다. 고차원에서는 "차원의 저주"에 걸립니다.

---

## 2. 차원의 저주와 몬테카를로

$d$차원 적분을 각 축에 $n$개 점으로 구적하면 총 $n^d$개의 함수 평가가 필요합니다.

| 차원 $d$ | $n = 10$ | $n = 100$ |
|---------|---------|----------|
| 1 | 10 | 100 |
| 5 | $10^5$ | $10^{10}$ |
| 10 | $10^{10}$ | $10^{20}$ |
| 100 | $10^{100}$ | 불가능 |

**몬테카를로 적분**은 이 저주를 돌파합니다. 수렴 속도가 차원에 무관하게 $O(1/\sqrt{N})$입니다.

$$
I = \int f(\mathbf{x})\,d\mathbf{x} \approx \frac{V}{N}\sum_{i=1}^{N} f(\mathbf{x}_i), \quad \mathbf{x}_i \sim \text{Uniform}
$$

```python
import numpy as np

# 10차원 구의 부피 계산: V = π^5 / 120 ≈ 2.550
d = 10
true_volume = np.pi**(d/2) / np.math.factorial(d//2)

Ns = [1000, 10000, 100000, 1000000]
for N in Ns:
    # [-1,1]^d 에서 균등 샘플
    x = np.random.uniform(-1, 1, size=(N, d))
    # 원점까지 거리가 1 이하인 점의 비율
    inside = np.sum(np.linalg.norm(x, axis=1) <= 1)
    volume = (2**d) * inside / N  # 2^d = 하이퍼큐브 부피
    print(f"N={N:>8d}: 추정 = {volume:.4f}, "
          f"오차 = {abs(volume - true_volume):.4f}")
```

> **핵심 직관**: 몬테카를로는 "무작위로 점을 뿌리고, 조건을 만족하는 비율을 세는" 것입니다. 수렴이 $O(1/\sqrt{N})$으로 느리지만, 이 속도가 **차원 $d$와 무관**하다는 것이 핵심입니다. $d = 1000$이어도 같은 속도입니다.

---

## 3. 분산 감소: 중요도 샘플링

몬테카를로의 약점은 큰 분산입니다. **중요도 샘플링(importance sampling)** 은 $f$가 큰 영역에 더 많은 샘플을 집중하여 분산을 줄입니다.

$$
I = \int f(\mathbf{x}) p(\mathbf{x})\,d\mathbf{x} = \int \frac{f(\mathbf{x}) p(\mathbf{x})}{q(\mathbf{x})} q(\mathbf{x})\,d\mathbf{x} \approx \frac{1}{N}\sum_{i=1}^{N} \frac{f(\mathbf{x}_i) p(\mathbf{x}_i)}{q(\mathbf{x}_i)}
$$

여기서 $\mathbf{x}_i \sim q(\mathbf{x})$이고, $w_i = p(\mathbf{x}_i)/q(\mathbf{x}_i)$를 **중요도 가중치**라 합니다.

```python
import numpy as np

# 목표: E[e^{-x}] where x ~ Exp(1), x > 5 영역
# 즉, P(X > 5) = e^{-5} ≈ 0.00674

N = 100000

# 방법 1: 순수 몬테카를로 (Exp(1)에서 샘플)
x_naive = np.random.exponential(1.0, N)
estimates_naive = (x_naive > 5).astype(float)
print(f"순수 MC:     {np.mean(estimates_naive):.6f} "
      f"(표준편차 {np.std(estimates_naive)/np.sqrt(N):.6f})")

# 방법 2: 중요도 샘플링 (x > 5 영역에 집중)
# q(x) = Exp(1) shifted to start at 5
x_is = 5.0 + np.random.exponential(1.0, N)
# 중요도 가중치: p(x)/q(x) = exp(-x) / exp(-(x-5)) = exp(-5)
weights = np.exp(-5.0) * np.ones(N)
print(f"중요도 샘플링: {np.mean(weights):.6f} "
      f"(표준편차 {np.std(weights)/np.sqrt(N):.6f})")

print(f"정확한 값:    {np.exp(-5):.6f}")
```

---

## 4. MCMC: Metropolis-Hastings 알고리즘

pt-07에서 베이즈 추론을 위해 사후 분포 $p(\theta|D)$로부터 샘플링해야 한다고 배웠습니다. 정규화 상수를 모르는 분포에서 샘플링하는 도구가 **Markov Chain Monte Carlo (MCMC)** 입니다.

### Metropolis-Hastings 알고리즘

1. 현재 상태 $\theta$에서 제안 분포 $q$로 후보 $\theta'$을 생성
2. 수용 확률 계산: $\alpha = \min\left(1, \frac{p(\theta'|D)\,q(\theta|\theta')}{p(\theta|D)\,q(\theta'|\theta)}\right)$
3. 확률 $\alpha$로 이동, $1-\alpha$로 제자리

```python
import numpy as np

def metropolis_hastings(log_target, x0, proposal_std, n_samples, burn_in=1000):
    """Metropolis-Hastings MCMC 샘플러"""
    x = x0
    samples = []
    accepted = 0

    for i in range(n_samples + burn_in):
        # 대칭 제안 분포 (Random Walk)
        x_proposal = x + proposal_std * np.random.randn(*np.shape(x))

        # 로그 수용 비율
        log_alpha = log_target(x_proposal) - log_target(x)

        # 수용/거절
        if np.log(np.random.rand()) < log_alpha:
            x = x_proposal
            accepted += 1

        if i >= burn_in:
            samples.append(x.copy())

    rate = accepted / (n_samples + burn_in)
    return np.array(samples), rate

# 예시: 2차원 가우시안 혼합에서 샘플링
def log_target(x):
    """두 가우시안의 혼합: 0.3 * N(-2,1) + 0.7 * N(3,0.5)"""
    log_p1 = -0.5 * (x - (-2))**2
    log_p2 = -0.5 * ((x - 3)**2) / 0.25
    return np.logaddexp(np.log(0.3) + log_p1, np.log(0.7) + log_p2)

samples, accept_rate = metropolis_hastings(
    log_target, x0=np.array(0.0), proposal_std=1.0,
    n_samples=50000, burn_in=5000
)
print(f"수용률: {accept_rate:.2%}")
print(f"샘플 평균: {np.mean(samples):.3f}")  # 이론값에 근사
print(f"샘플 표준편차: {np.std(samples):.3f}")
```

> **핵심 직관**: MCMC는 "비정규화 확률 밀도의 비율만 계산하면 된다"는 점이 핵심입니다. 정규화 상수 $Z = \int p(\theta|D)\,d\theta$를 계산할 필요 없이 사후 분포에서 샘플링할 수 있습니다. 이것이 베이즈 추론을 실용적으로 만든 돌파구입니다.

---

## 5. 수치 적분 방법 비교

| 방법 | 수렴 속도 | 차원 확장성 | 정규화 상수 | 사용처 |
|------|----------|-----------|-----------|-------|
| 사다리꼴 | $O(h^2)$ | $O(n^d)$ 지수적 | 필요 | 저차원 |
| Simpson | $O(h^4)$ | $O(n^d)$ 지수적 | 필요 | 저차원 |
| Gaussian | $O(h^{2n})$ | $O(n^d)$ 지수적 | 필요 | 저차원 |
| 몬테카를로 | $O(1/\sqrt{N})$ | **차원 무관** | 필요 | 고차원 |
| 중요도 샘플링 | $O(1/\sqrt{N})$ 이상 | **차원 무관** | 필요 | 고차원 + 희귀 사건 |
| MCMC | 이론적으로 정확 | **차원 무관** | **불필요** | 베이즈 추론 |

---

## 6. 실전 팁

```python
# scipy의 적분 도구 모음
from scipy import integrate

# 1차원 적분
result, error = integrate.quad(lambda x: np.exp(-x**2), -np.inf, np.inf)
print(f"∫exp(-x²)dx = {result:.6f} (오차 {error:.2e})")  # √π

# 2차원 적분
result, error = integrate.dblquad(
    lambda y, x: np.exp(-(x**2 + y**2)),
    -5, 5, -5, 5
)
print(f"∫∫exp(-(x²+y²))dxdy = {result:.6f}")  # π

# 고차원 (d≥3)은 몬테카를로가 유일한 실용적 방법
```

---

## 핵심 정리

1. **결정론적 구적법** (사다리꼴, Simpson, Gaussian)은 저차원에서 고정밀도를 제공하지만, 고차원에서 차원의 저주에 빠진다.
2. **몬테카를로 적분**의 수렴 속도 $O(1/\sqrt{N})$은 차원에 무관하여 고차원 적분의 유일한 실용적 방법이다.
3. **중요도 샘플링**은 피적분 함수가 큰 영역에 집중 샘플링하여 몬테카를로의 분산을 줄인다.
4. **MCMC (Metropolis-Hastings)** 는 정규화 상수 없이 비정규화 밀도에서 샘플링할 수 있어, 베이즈 추론의 핵심 도구이다.
5. 저차원(1~2차원)에서는 `scipy.integrate.quad`가, 고차원에서는 몬테카를로/MCMC가 올바른 선택이다.
