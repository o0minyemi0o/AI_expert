# ML을 위한 수치 계산

## 왜 이 강의가 필요한가

지금까지 nm-01부터 nm-09까지 부동소수점, 오차 분석, 자동 미분, 수치 선형대수, 반복법, 몬테카를로, 근사 이론, 수치 최적화를 배웠습니다. 이 마지막 강의에서는 이 모든 내용이 **현대 딥러닝 실전**에서 어떻게 결합되는지 종합합니다.

혼합 정밀도 학습, gradient checkpointing, 수치 안정한 softmax와 손실 함수 — 이 기법들은 수십억 파라미터 모델을 학습시키는 엔지니어가 매일 마주하는 수치 해석 문제입니다.

---

## 1. 혼합 정밀도 학습 (Mixed Precision Training)

nm-01에서 FP16, FP32, FP64의 정밀도 차이를 배웠습니다. **혼합 정밀도 학습**은 연산은 FP16으로 빠르게, 그래디언트 누적은 FP32로 정확하게 수행합니다.

| 형식 | 비트 | 범위 | 정밀도 | 용도 |
|------|------|------|--------|------|
| FP32 | 32 | $\pm 3.4 \times 10^{38}$ | ~7자리 | 마스터 가중치 |
| FP16 | 16 | $\pm 6.5 \times 10^4$ | ~3자리 | 전방/역방향 패스 |
| BF16 | 16 | $\pm 3.4 \times 10^{38}$ | ~2자리 | Google TPU 기본 |

```
혼합 정밀도 학습 흐름:

  FP32 마스터 가중치 ──→ FP16으로 복사 ──→ 전방 패스 (FP16)
         ↑                                     │
         │                                     ↓
  FP32로 업데이트 ←── Loss Scaling ←── 역방향 패스 (FP16)
```

BF16은 FP32와 같은 8비트 지수를 사용해 overflow/underflow 위험이 적으며, 그래디언트 scale이 극단적인 딥러닝에서 FP16보다 안전합니다.

```python
import torch

# PyTorch에서의 혼합 정밀도 학습 루프
model = torch.nn.Linear(1024, 512).cuda()
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
scaler = torch.amp.GradScaler('cuda')  # Loss Scaling

x = torch.randn(32, 1024).cuda()
target = torch.randn(32, 512).cuda()

with torch.amp.autocast('cuda', dtype=torch.float16):
    output = model(x)               # FP16으로 전방 패스
    loss = torch.nn.functional.mse_loss(output, target)

scaler.scale(loss).backward()        # 스케일링된 역방향 패스
scaler.step(optimizer)               # FP32로 가중치 업데이트
scaler.update()                      # 스케일 팩터 조정
```

> **핵심 직관**: FP16의 최소 양수는 약 $6 \times 10^{-8}$입니다. 그래디언트가 이보다 작으면 0으로 변해 학습이 멈춥니다. **Loss Scaling**은 손실을 큰 수로 곱해서 그래디언트를 FP16 범위 안으로 끌어올린 뒤, 업데이트 시 다시 나누는 기법입니다.

---

## 2. Gradient Checkpointing

nm-04에서 역방향 모드 AD는 전방 패스의 모든 중간값을 저장해야 한다고 배웠습니다. **Gradient checkpointing**은 일부만 저장하고, 역방향 시 나머지를 재계산합니다.

```
일반 역전파:              Gradient Checkpointing:
  L1 → L2 → L3 → L4       L1 → L2 → L3 → L4
  ■    ■    ■    ■         ■              ■
  메모리: O(N)             메모리: O(√N), 계산: ~1.33×
```

```python
import torch
from torch.utils.checkpoint import checkpoint

class Block(torch.nn.Module):
    def __init__(self, dim):
        super().__init__()
        self.net = torch.nn.Sequential(
            torch.nn.Linear(dim, dim), torch.nn.ReLU(),
            torch.nn.Linear(dim, dim), torch.nn.ReLU())
    def forward(self, x):
        return self.net(x) + x

block = Block(512)
x = torch.randn(32, 512, requires_grad=True)
y = checkpoint(block, x, use_reentrant=False)  # 중간값 저장 안 함
y.sum().backward()  # 역방향 시 전방 패스를 다시 계산
```

> **핵심 직관**: Gradient checkpointing은 **메모리와 계산의 트레이드오프**입니다. 메모리를 $O(N)$에서 $O(\sqrt{N})$로 줄이는 대신 계산이 약 33% 증가합니다. GPU 메모리가 한정된 상황에서 더 큰 모델이나 배치를 사용할 수 있게 합니다.

---

## 3. Log-Sum-Exp Trick과 안정한 Softmax

nm-01에서 softmax의 overflow 문제를 살펴보았습니다. 핵심 아이디어는 **최대값을 빼는 것**입니다.

$$
\text{softmax}(z_i) = \frac{e^{z_i - z_{\max}}}{\sum_j e^{z_j - z_{\max}}}, \quad \log\sum_i e^{z_i} = z_{\max} + \log\sum_i e^{z_i - z_{\max}}
$$

```python
import numpy as np

def logsumexp_stable(z):
    z_max = np.max(z)
    return z_max + np.log(np.sum(np.exp(z - z_max)))

def softmax_stable(z):
    z_shifted = z - np.max(z)
    exp_z = np.exp(z_shifted)
    return exp_z / np.sum(exp_z)

# 극단적 값에서도 안정
z = np.array([1000.0, 1001.0, 1002.0])
print(f"LSE:     {logsumexp_stable(z):.4f}")
print(f"Softmax: {softmax_stable(z)}")  # overflow 없음!
```

> **핵심 직관**: $z_i - z_{\max} \leq 0$이므로 $e^{z_i - z_{\max}} \leq 1$, overflow가 원리적으로 불가능합니다. 수학적으로 동치인 변환이 수치적 안정성을 완전히 바꿉니다.

---

## 4. 수치적으로 안정한 손실 함수

### Cross-Entropy: logit에서 직접 계산

$$
\mathcal{L} = -\sum_i y_i \log\left(\frac{e^{z_i}}{\sum_j e^{z_j}}\right) = -\sum_i y_i (z_i - \text{LSE}(z))
$$

"softmax → log"처럼 exp와 log를 연속으로 적용하면 불안정합니다. **log-softmax를 한 번에 계산**하면 overflow를 방지합니다.

```python
import numpy as np

def cross_entropy_stable(logits, targets):
    """안정: logits에서 직접 계산"""
    log_probs = logits - logsumexp_stable(logits)
    return -np.sum(targets * log_probs)

# PyTorch의 F.cross_entropy가 정확히 이 방식
logits = np.array([100.0, 200.0, 300.0])
targets = np.array([0.0, 0.0, 1.0])
print(f"Stable CE: {cross_entropy_stable(logits, targets):.6f}")
```

### Binary Cross-Entropy: 안정한 공식

$$
\text{BCE}(z, y) = \max(z, 0) - zy + \log(1 + e^{-|z|})
$$

```python
def bce_stable(logit, target):
    return np.maximum(logit, 0) - logit * target + np.log1p(np.exp(-np.abs(logit)))

for z in [-1000, 0, 1000]:
    print(f"z={z:5d}: BCE = {bce_stable(z, 1.0):.6f}")  # 모두 안정
```

> **핵심 직관**: PyTorch의 `F.cross_entropy`는 logit을 직접 받고, `F.binary_cross_entropy_with_logits`는 sigmoid를 거치지 않고 logit에서 직접 계산합니다. 항상 **logit 버전**을 사용하십시오.

---

## 5. 실전 수치 안정화 패턴 총정리

| 불안정한 패턴 | 안정한 대안 | 관련 강의 |
|-------------|-----------|----------|
| `np.exp(x)` overflow | log 도메인에서 계산 | nm-01 |
| 큰 수 - 큰 수 (소거) | 수학적 변환으로 뺄셈 회피 | nm-02 |
| $A^{-1}b$ | `np.linalg.solve(A, b)` | nm-05 |
| $\log(\text{softmax}(z))$ | log-sum-exp trick | nm-10 |
| BCE via sigmoid → log | $\max(z,0) - zy + \log(1+e^{-|z|})$ | nm-10 |
| $\text{Var} = E[X^2] - (E[X])^2$ | Welford 알고리즘 | nm-02 |
| FP16 gradient underflow | Loss Scaling | nm-10 |
| 메모리 부족 | Gradient Checkpointing | nm-10 |
| 유한 차분 미분 | 자동 미분 | nm-03, nm-04 |

---

## 6. 수치 디버깅 체크리스트

학습이 실패할 때 확인해야 할 수치 해석 체크포인트입니다.

```python
import torch

def numerical_health_check(model, loss_val):
    """모델의 수치적 건강 상태 점검"""
    # 1. 손실값 검사
    if torch.isnan(loss_val): print("NaN 손실 — lr/데이터 확인")
    if torch.isinf(loss_val): print("Inf 손실 — overflow, 클리핑 필요")

    # 2. 그래디언트 & 가중치 검사
    for name, p in model.named_parameters():
        if p.grad is not None:
            gn = p.grad.norm().item()
            if torch.any(torch.isnan(p.grad)): print(f"NaN grad: {name}")
            if gn > 1e6: print(f"큰 grad: {name} (norm={gn:.1e})")
            if 0 < gn < 1e-12: print(f"작은 grad: {name} (norm={gn:.1e})")
        if torch.any(torch.isnan(p.data)): print(f"NaN 가중치: {name}")

# 더 강력한 디버깅: torch.autograd.detect_anomaly()
# NaN이 처음 발생한 연산을 자동으로 추적
```

> **핵심 직관**: 수치 디버깅의 핵심은 "NaN/Inf를 조기에 감지하고 원인을 추적하는 것"입니다. `torch.autograd.detect_anomaly()`를 사용하면 NaN이 처음 발생한 연산을 자동으로 찾아줍니다.

---

## 7. 과목 전체 종합

```
수치 해석 로드맵:

nm-01 부동소수점 ──→ nm-02 오차 분석 ──→ nm-10 실전 안정화
  │                     │
  └─ nm-03 전방 AD ──→ nm-04 역방향 AD ──→ nm-10 체크포인팅
                         │
nm-05 수치 선형대수 ──→ nm-06 반복법 ──→ nm-09 수치 최적화
  │                                       │
  └─ nm-08 근사 이론 ─────────────────→ nm-10 혼합 정밀도
                                          │
nm-07 수치 적분/MC ──────────────────→ nm-10 종합
```

이 과목의 핵심 메시지는 하나입니다. **컴퓨터에서의 수학은 수학 그 자체가 아닙니다.** 유한 정밀도, 유한 메모리, 유한 시간 안에서 "충분히 좋은 답"을 구하는 것이 수치 해석이고, 이것이 ML 시스템의 신뢰성과 성능을 결정합니다.

---

## 핵심 정리

1. **혼합 정밀도 학습**은 FP16으로 속도를, FP32로 정확도를 확보하며, Loss Scaling이 FP16 underflow를 방지한다.
2. **Gradient checkpointing**은 메모리를 $O(N)$에서 $O(\sqrt{N})$로 줄이는 대신 계산을 ~33% 추가한다.
3. **Log-sum-exp trick**과 **안정한 softmax**는 exp/log의 overflow/underflow를 방지하는 핵심 수치 기법이다.
4. 손실 함수는 중간 단계(softmax, sigmoid)를 거치지 않고 **logit에서 직접 계산**해야 수치적으로 안정하다.
5. 수치 해석의 모든 주제 — 부동소수점, 오차, 자동 미분, 선형대수, 최적화 — 는 **현대 딥러닝 시스템의 신뢰성**을 좌우하는 실전 도구이다.
