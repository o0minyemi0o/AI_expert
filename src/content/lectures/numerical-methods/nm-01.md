# 부동소수점 산술

## 왜 부동소수점을 배워야 하는가

컴퓨터는 실수를 정확히 표현할 수 없습니다. 우리가 당연하게 쓰는 `0.1`조차 메모리 안에서는 무한 이진 소수로 근사될 뿐입니다. 신경망의 가중치 수백만 개가 모두 이 근사된 수로 저장되고, 수십억 번의 곱셈과 덧셈을 거칩니다. 이 과정에서 오차가 어떻게 발생하고, 얼마나 축적되는지 이해하지 못하면 학습이 발산하거나 결과를 신뢰할 수 없게 됩니다.

수치 해석의 첫 번째 강의로, 컴퓨터가 실수를 다루는 방식인 **IEEE 754 부동소수점 표준**을 살펴보겠습니다.

---

## 1. IEEE 754 표준

컴퓨터에서 실수를 표현하는 국제 표준인 **IEEE 754**는 하나의 수를 세 부분으로 나눕니다.

$$
(-1)^{s} \times 1.m \times 2^{e - \text{bias}}
$$

| 형식 | 부호(s) | 지수(e) | 가수(m) | 전체 비트 | bias |
|------|---------|---------|---------|-----------|------|
| **Single (FP32)** | 1 bit | 8 bits | 23 bits | 32 bits | 127 |
| **Double (FP64)** | 1 bit | 11 bits | 52 bits | 64 bits | 1023 |
| **Half (FP16)** | 1 bit | 5 bits | 10 bits | 16 bits | 15 |

```
FP64 (double precision) 비트 배치:

 [S][  지수 11 bits  ][        가수 52 bits                  ]
  1   11111111111       1001100110011001100110011001100110011001100110011010
  ^       ^                              ^
 부호   지수부                         가수부 (숨은 1 포함)
```

> **핵심 직관**: 부동소수점은 "과학적 표기법의 이진 버전"입니다. $6.022 \times 10^{23}$처럼, $1.m \times 2^e$ 형태로 매우 크거나 작은 수를 효율적으로 표현합니다.

---

## 2. Machine Epsilon

**Machine epsilon** $\epsilon_{\text{mach}}$은 $1 + \epsilon \neq 1$을 만족하는 가장 작은 양수입니다. 이 값은 부동소수점의 **상대 정밀도 한계**를 결정합니다.

$$
\epsilon_{\text{mach}} = 2^{-(p-1)}
$$

여기서 $p$는 유효 자릿수(가수 비트 + 숨은 비트 1)입니다.

| 형식 | 유효 자릿수 $p$ | $\epsilon_{\text{mach}}$ | 대략적 십진 정밀도 |
|------|----------------|--------------------------|-------------------|
| FP16 | 11 | $2^{-10} \approx 9.77 \times 10^{-4}$ | ~3자리 |
| FP32 | 24 | $2^{-23} \approx 1.19 \times 10^{-7}$ | ~7자리 |
| FP64 | 53 | $2^{-52} \approx 2.22 \times 10^{-16}$ | ~16자리 |

임의의 실수 $x$를 부동소수점으로 반올림한 값 $\text{fl}(x)$에 대해 다음이 보장됩니다.

$$
\frac{|\text{fl}(x) - x|}{|x|} \leq \epsilon_{\text{mach}}
$$

---

## 3. 부동소수점의 한계

### 3.1 0.1 + 0.2 ≠ 0.3

$0.1_{10}$은 이진법으로 $0.0\overline{0011}_2$, 즉 무한 순환소수입니다. 유한 비트로 잘라내면 오차가 발생합니다.

```python
import numpy as np

# 가장 유명한 부동소수점 함정
print(0.1 + 0.2)           # 0.30000000000000004
print(0.1 + 0.2 == 0.3)    # False

# 올바른 비교 방법
print(np.isclose(0.1 + 0.2, 0.3))  # True
print(abs(0.1 + 0.2 - 0.3) < 1e-15)  # True
```

### 3.2 큰 수와 작은 수의 덧셈

```python
big = 1e16
small = 1.0

# 작은 수가 "흡수"됨 (absorption)
print(big + small == big)   # True!
print(big + small - big)    # 0.0 (1.0이 사라짐)

# 누적 합산에서의 문제
total_naive = 0.0
for _ in range(10_000_000):
    total_naive += 1e-7
print(f"단순 합산: {total_naive:.15f}")  # 오차 누적
print(f"정확한 값: {1.0:.15f}")
```

> **핵심 직관**: 크기가 매우 다른 두 수를 더하면, 작은 수의 하위 비트가 잘려 나갑니다. 이를 **흡수(absorption)** 현상이라 합니다. 신경망 학습에서 learning rate가 매우 작을 때 가중치 업데이트가 사라지는 원인이 됩니다.

---

## 4. 특수 값: Inf, NaN, Denormals

IEEE 754는 일반적인 수 외에 특수한 값을 정의합니다.

| 패턴 | 의미 | 예시 |
|------|------|------|
| 지수 = 모두 1, 가수 = 0 | $\pm\infty$ | `1.0 / 0.0` |
| 지수 = 모두 1, 가수 ≠ 0 | NaN (Not a Number) | `0.0 / 0.0` |
| 지수 = 모두 0, 가수 ≠ 0 | 비정규수 (denormal) | 매우 작은 양수 |

```python
import numpy as np

# 특수 값 실험
print(np.float64(1.0) / np.float64(0.0))   # inf
print(np.float64(0.0) / np.float64(0.0))   # nan

# NaN의 독특한 성질: 자기 자신과도 같지 않음
x = np.nan
print(x == x)       # False
print(np.isnan(x))  # True

# overflow와 underflow
print(np.finfo(np.float64).max)    # ~1.8e+308
print(np.finfo(np.float64).tiny)   # ~2.2e-308 (최소 정규수)
```

---

## 5. Kahan 보상 합산

오차 누적을 줄이는 고전적 기법이 **Kahan 합산(compensated summation)** 알고리즘입니다.

```python
def kahan_sum(values):
    """Kahan 보상 합산: 반올림 오차를 보상 변수 c로 추적"""
    s = 0.0  # 누적 합
    c = 0.0  # 보상 (잃어버린 하위 비트)
    for x in values:
        y = x - c        # 보상된 입력
        t = s + y        # 중간 합 (하위 비트 손실 발생)
        c = (t - s) - y  # 손실된 비트 계산
        s = t
    return s

# 비교 실험
import numpy as np
values = [1e-7] * 10_000_000

naive = sum(values)
kahan = kahan_sum(values)
fsum = np.float64(1.0)  # 정확한 값

print(f"단순 합산:  {naive:.15f}")
print(f"Kahan 합산: {kahan:.15f}")
print(f"math.fsum:  {__import__('math').fsum(values):.15f}")
```

> **핵심 직관**: Kahan 합산은 매번 "잃어버린 오차"를 기억해 두었다가 다음 덧셈에서 보상합니다. 이 아이디어는 단순하지만 효과적이며, PyTorch의 내부 구현에서도 유사한 기법이 활용됩니다.

---

## 6. ML에서의 실전적 의미

부동소수점 오차는 다음과 같은 실전 문제를 일으킵니다.

| 문제 | 원인 | 해결 |
|------|------|------|
| 학습 발산 | gradient overflow ($\infty$) | gradient clipping |
| 학습 정체 | gradient underflow (0에 가까움) | 혼합 정밀도 학습 |
| 재현 불가 | 병렬 합산 순서 차이 | deterministic 모드 |
| softmax overflow | $e^{1000} = \infty$ | log-sum-exp trick (nm-10 참조) |

```python
import numpy as np

# softmax overflow 시연
logits = np.array([1000.0, 1001.0, 1002.0])
print(np.exp(logits))  # [inf, inf, inf] — overflow!

# 안정한 softmax
logits_shifted = logits - np.max(logits)
print(np.exp(logits_shifted) / np.sum(np.exp(logits_shifted)))
# [0.09003057, 0.24472847, 0.66524096] — 정상 출력
```

---

## 핵심 정리

1. **IEEE 754**는 실수를 부호 $\times$ 가수 $\times$ $2^{\text{지수}}$ 형태로 표현하며, FP64는 약 16자리 십진 정밀도를 제공한다.
2. **Machine epsilon** $\epsilon_{\text{mach}} \approx 2.22 \times 10^{-16}$ (FP64)은 상대 반올림 오차의 상한이다.
3. 크기가 다른 수의 덧셈에서 **흡수 현상**이 발생하며, Kahan 합산으로 완화할 수 있다.
4. **NaN, Inf, denormal**은 IEEE 754의 특수 값으로, 디버깅 시 반드시 확인해야 한다.
5. ML에서 overflow/underflow는 학습 실패의 주된 원인이며, log-sum-exp trick, gradient clipping 등으로 대응한다.
