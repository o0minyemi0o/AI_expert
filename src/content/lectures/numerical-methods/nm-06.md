# 반복법과 대규모 선형 시스템

## 왜 반복법을 배워야 하는가

nm-05에서 배운 LU, Cholesky 분해는 **직접법(direct method)** 입니다. 정확한 해를 유한 단계에 구하지만, $O(n^3)$의 연산량이 필요합니다. $n = 10{,}000$이면 $10^{12}$번의 연산 — 이미 부담스럽습니다. 대규모 추천 시스템의 행렬은 $n$이 수백만에 달하며, 이때 직접법은 사실상 불가능합니다.

**반복법(iterative method)** 은 정확한 해 대신 **충분히 좋은 근사해**를 훨씬 적은 비용으로 구합니다. 특히 행렬이 **희소(sparse)** 할 때 — 0이 아닌 원소가 $n^2$보다 훨씬 적을 때 — 반복법은 직접법을 압도합니다.

---

## 1. 직접법 vs 반복법

| 기준 | 직접법 (LU, Cholesky) | 반복법 (CG, GMRES) |
|------|---------------------|-------------------|
| 연산량 | $O(n^3)$ 고정 | $O(k \cdot \text{nnz})$, $k$=반복 횟수 |
| 메모리 | $O(n^2)$ (밀집 저장) | $O(\text{nnz})$ (희소 저장) |
| 정확도 | 기계 정밀도 | 허용 오차까지 수렴 |
| 희소 행렬 | 채움(fill-in) 발생 | 행렬-벡터 곱만 필요 |
| 대규모 문제 | $n > 10^4$이면 비현실적 | $n = 10^6$도 가능 |

여기서 $\text{nnz}$는 0이 아닌 원소의 수입니다. 희소 행렬에서 $\text{nnz} \ll n^2$이므로 반복법이 유리합니다.

> **핵심 직관**: 반복법은 행렬 $A$를 "블랙박스"로 취급합니다. $A$의 내부 구조를 알 필요 없이, $A\mathbf{v}$ (행렬-벡터 곱)만 계산할 수 있으면 됩니다. 이 관점이 대규모 ML에서 강력한 무기가 됩니다.

---

## 2. 정치 반복법의 기본 아이디어

$A\mathbf{x} = \mathbf{b}$를 $\mathbf{x} = G\mathbf{x} + \mathbf{c}$ 형태로 변환한 뒤, 반복합니다.

$$
\mathbf{x}^{(k+1)} = G\mathbf{x}^{(k)} + \mathbf{c}
$$

수렴 조건은 반복 행렬 $G$의 **스펙트럴 반지름** $\rho(G) < 1$입니다.

### 야코비 반복과 가우스-자이델 반복

```python
import numpy as np

def jacobi(A, b, x0=None, tol=1e-10, max_iter=1000):
    """야코비 반복법"""
    n = len(b)
    x = x0 if x0 is not None else np.zeros(n)
    D_inv = 1.0 / np.diag(A)

    for k in range(max_iter):
        x_new = D_inv * (b - (A @ x - np.diag(A) * x))
        if np.linalg.norm(x_new - x) / np.linalg.norm(x_new) < tol:
            print(f"야코비: {k+1}번 반복에서 수렴")
            return x_new
        x = x_new
    print("수렴 실패")
    return x

# 대각 우세 행렬 (수렴 보장)
n = 100
A = np.random.randn(n, n)
A = A + A.T + 2*n * np.eye(n)  # 대각 우세
b = np.random.randn(n)

x_jacobi = jacobi(A, b)
x_exact = np.linalg.solve(A, b)
print(f"야코비 오차: {np.linalg.norm(x_jacobi - x_exact):.2e}")
```

---

## 3. 켤레 경사법 (Conjugate Gradient, CG)

**대칭 양의 정부호(SPD)** 행렬에 대한 가장 중요한 반복법입니다. $A\mathbf{x} = \mathbf{b}$를 풀기 위해 다음 이차 형식을 최소화합니다.

$$
\phi(\mathbf{x}) = \frac{1}{2}\mathbf{x}^T A \mathbf{x} - \mathbf{b}^T \mathbf{x}
$$

$\nabla \phi(\mathbf{x}) = A\mathbf{x} - \mathbf{b} = \mathbf{0}$이므로, 이 최소화 문제와 연립방정식은 동치입니다. co-07에서 다뤘던 경사 하강법의 선형대수 특수 사례라고 볼 수 있습니다.

### CG가 경사 하강법보다 나은 이유

경사 하강법은 **지그재그** 경로를 따르지만, CG는 **켤레 방향**으로 탐색하여 $n$차원에서 최대 $n$번 반복이면 정확한 해를 찻습니다.

```
경사 하강법 경로:           켤레 경사법 경로:

  ╲  ╱╲  ╱╲               ╲
   ╲╱  ╲╱  ╲               ╲────────→ *
    지그재그                  2번에 도달!
```

```python
import numpy as np

def conjugate_gradient(A, b, x0=None, tol=1e-10, max_iter=None):
    """켤레 경사법 (CG)"""
    n = len(b)
    max_iter = max_iter or n
    x = x0 if x0 is not None else np.zeros(n)
    r = b - A @ x        # 잔차
    p = r.copy()          # 탐색 방향
    rs_old = r @ r

    residuals = [np.sqrt(rs_old)]

    for k in range(max_iter):
        Ap = A @ p
        alpha = rs_old / (p @ Ap)
        x = x + alpha * p
        r = r - alpha * Ap
        rs_new = r @ r
        residuals.append(np.sqrt(rs_new))

        if np.sqrt(rs_new) < tol:
            print(f"CG: {k+1}번 반복에서 수렴")
            return x, residuals

        beta = rs_new / rs_old
        p = r + beta * p
        rs_old = rs_new

    return x, residuals

# SPD 행렬로 테스트
n = 500
B = np.random.randn(n, n)
A = B.T @ B + 0.1 * np.eye(n)  # SPD
b = np.random.randn(n)

x_cg, res = conjugate_gradient(A, b)
x_exact = np.linalg.solve(A, b)
print(f"CG 오차: {np.linalg.norm(x_cg - x_exact):.2e}")
print(f"CG 반복 횟수: {len(res)-1}")
```

> **핵심 직관**: CG의 수렴 속도는 조건수 $\kappa(A)$에 의존합니다. $k$번 반복 후 오차는 다음과 같이 감소합니다: $\|\mathbf{e}^{(k)}\|_A \leq 2 \left(\frac{\sqrt{\kappa}-1}{\sqrt{\kappa}+1}\right)^k \|\mathbf{e}^{(0)}\|_A$. 조건수가 작을수록 빠르게 수렴합니다.

---

## 4. GMRES (비대칭 시스템)

CG는 SPD 행렬에만 적용됩니다. 비대칭 행렬에 대해서는 **GMRES (Generalized Minimal Residual)** 를 사용합니다.

$$
\mathbf{x}^{(k)} = \arg\min_{\mathbf{x} \in \mathcal{K}_k} \|A\mathbf{x} - \mathbf{b}\|_2
$$

여기서 $\mathcal{K}_k = \text{span}\{\mathbf{b}, A\mathbf{b}, A^2\mathbf{b}, \ldots, A^{k-1}\mathbf{b}\}$는 **크릴로프 부분 공간(Krylov subspace)** 입니다.

```python
from scipy.sparse.linalg import gmres, cg
from scipy.sparse import random as sparse_random

# 희소 비대칭 행렬
n = 10000
A_sparse = sparse_random(n, n, density=0.01, format='csr')
A_sparse = A_sparse + 5.0 * np.eye(n)  # 대각 우세로 수렴 보장
b = np.random.randn(n)

# GMRES
x_gmres, info = gmres(A_sparse, b, atol=1e-10)
print(f"GMRES 수렴 여부: {'성공' if info == 0 else '실패'}")
print(f"잔차: {np.linalg.norm(A_sparse @ x_gmres - b):.2e}")
```

| 방법 | 행렬 조건 | 메모리 | 수렴 보장 |
|------|---------|-------|---------|
| CG | SPD | $O(n)$ | 예 (최대 $n$회) |
| GMRES | 일반 | $O(kn)$ 증가 | 조건부 |
| BiCGSTAB | 일반 | $O(n)$ | 조건부 |

---

## 5. 전처리 (Preconditioning)

반복법의 수렴 속도는 조건수 $\kappa(A)$에 의존합니다. **전처리(preconditioning)** 는 $A$의 조건수를 개선하는 행렬 $M \approx A$를 찾아, 대신 $M^{-1}A\mathbf{x} = M^{-1}\mathbf{b}$를 풉니다.

$$
\kappa(M^{-1}A) \ll \kappa(A)
$$

```python
from scipy.sparse.linalg import LinearOperator, cg
from scipy.sparse import diags
import numpy as np

# 조건수가 큰 SPD 행렬
n = 1000
eigvals = np.logspace(0, 4, n)  # 조건수 = 10^4
Q, _ = np.linalg.qr(np.random.randn(n, n))
A = Q @ np.diag(eigvals) @ Q.T
b = np.random.randn(n)

# 전처리 없이 CG
x_no_precond, info1 = cg(A, b, tol=1e-10, maxiter=200)

# 대각 전처리 (가장 간단)
M_diag = diags(1.0 / np.diag(A))
x_precond, info2 = cg(A, b, tol=1e-10, maxiter=200, M=M_diag)

print(f"전처리 없음 — 수렴: {'성공' if info1==0 else f'미수렴(info={info1})'}")
print(f"대각 전처리 — 수렴: {'성공' if info2==0 else f'미수렴(info={info2})'}")
```

> **핵심 직관**: 이상적인 전처리 행렬은 $M = A$ 자체입니다 (그러면 $M^{-1}A = I$이므로 1번에 수렴). 하지만 그러면 $M^{-1}$을 구하는 비용이 원래 문제를 푸는 것과 같습니다. 좋은 전처리는 "$A$를 잘 근사하면서도 $M^{-1}\mathbf{v}$를 빠르게 계산할 수 있는" 행렬을 찾는 것입니다.

---

## 6. ML에서의 응용

| 응용 | 사용되는 반복법 | 이유 |
|------|-------------|------|
| Hessian-벡터 곱 | CG (뉴턴-CG) | Hessian을 명시적으로 저장하지 않고 2차 최적화 (nm-09 참조) |
| 가우시안 프로세스 | 전처리 CG | 커널 행렬 $K$가 SPD이고 대규모 |
| 그래프 라플라시안 | Lanczos / CG | 희소 행렬의 고유값 계산 |
| 자연 경사법 | CG | Fisher 행렬의 역행렬-벡터 곱 |

```python
import numpy as np

# Hessian-free 최적화의 핵심: Hv 곱을 유한 차분으로 근사
def hessian_vector_product(f_grad, x, v, eps=1e-5):
    """
    Hessian-벡터 곱을 그래디언트 2번 평가로 근사:
    H·v ≈ (∇f(x + εv) - ∇f(x)) / ε
    """
    g_plus = f_grad(x + eps * v)
    g = f_grad(x)
    return (g_plus - g) / eps

# CG를 사용하면 Hx = g 를 풀 수 있음 (H를 저장하지 않고!)
# 이것이 nm-09에서 다룰 뉴턴-CG 방법의 핵심
```

---

## 핵심 정리

1. **직접법**은 $O(n^3)$ 비용으로 정확한 해를 구하고, **반복법**은 $O(k \cdot \text{nnz})$로 근사해를 구한다.
2. **켤레 경사법(CG)** 은 SPD 행렬에 대한 최적의 반복법이며, $n$번 이내에 수렴이 보장된다.
3. **GMRES**는 비대칭 행렬에 적용 가능하지만, 메모리가 반복마다 증가하는 단점이 있다.
4. **전처리**는 조건수를 개선하여 반복법의 수렴을 가속하며, 좋은 전처리 설계가 실전의 핵심이다.
5. ML에서 반복법은 Hessian-벡터 곱, 가우시안 프로세스, 자연 경사법 등 **대규모 행렬의 역행렬을 명시적으로 구하지 않아야 하는** 상황에서 필수적이다.
