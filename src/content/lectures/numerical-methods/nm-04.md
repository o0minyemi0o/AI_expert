# 역방향 모드 자동 미분

## 왜 역방향 모드를 배워야 하는가

nm-03에서 전방 모드 AD를 배웠습니다. 이중수를 사용해 정확한 미분을 구할 수 있었지만, 한 가지 치명적 약점이 있었습니다. 파라미터 $n$개에 대한 그래디언트를 구하려면 전방 패스를 $n$번 반복해야 한다는 것입니다.

현대 신경망은 파라미터가 수백만~수십억 개이고, 손실 함수 출력은 스칼라 1개입니다. 이런 $\mathbb{R}^n \to \mathbb{R}^1$ 구조에서는 **역방향 모드(reverse mode)** AD가 단 한 번의 역방향 패스로 모든 파라미터의 그래디언트를 계산합니다. 이것이 바로 **역전파(backpropagation)** 의 수학적 본질입니다.

---

## 1. 계산 그래프 (Computational Graph)

모든 수학 연산은 **계산 그래프**로 분해할 수 있습니다. 예를 들어 $f(x_1, x_2) = x_1 x_2 + \sin(x_1)$을 살펴봅시다.

```
전방 패스 (값 계산):

  x1=2 ──┬──→ [×] ──→ v1=x1·x2=6 ──→ [+] ──→ y=v1+v2=6.909
         │     ↑                        ↑
  x2=3 ──┘     │                        │
               │                        │
  x1=2 ──────→ [sin] ──→ v2=sin(x1)=0.909
```

전방 패스에서는 입력부터 출력까지 **순서대로** 값을 계산하고, 중간 결과를 모두 저장합니다.

---

## 2. 수반 변수 (Adjoint)와 역방향 패스

**수반 변수(adjoint)** $\bar{v}_i$는 최종 출력 $y$에 대한 중간 변수 $v_i$의 편미분입니다.

$$
\bar{v}_i = \frac{\partial y}{\partial v_i}
$$

역방향 패스에서는 출력부터 입력까지 **거꾸로** 체인 룰을 적용하여 수반 변수를 전파합니다.

```
역방향 패스 (수반 전파):

  ȳ = 1 (시드)

  [+]의 역방향: v̄1 = ȳ · 1 = 1,   v̄2 = ȳ · 1 = 1

  [×]의 역방향: x̄1 += v̄1 · x2 = 3,   x̄2 = v̄1 · x1 = 2

  [sin]의 역방향: x̄1 += v̄2 · cos(x1) = cos(2) ≈ -0.416

  결과: x̄1 = 3 + (-0.416) = 2.584,   x̄2 = 2
```

> **핵심 직관**: 역방향 모드는 "최종 출력이 1만큼 변하면, 각 중간 변수는 얼마나 기여했는가?"를 거꾸로 추적합니다. co-04에서 배운 체인 룰을 그래프 위에서 역순으로 적용하는 것입니다.

---

## 3. 벡터-야코비안 곱 (VJP)

nm-03의 전방 모드가 **야코비안-벡터 곱(JVP)** $J\mathbf{v}$를 계산했다면, 역방향 모드는 **벡터-야코비안 곱(VJP)** $\mathbf{u}^T J$를 계산합니다.

$$
\text{VJP}: \quad \mathbf{u}^T J = \mathbf{u}^T \frac{\partial \mathbf{f}}{\partial \mathbf{x}}, \quad \mathbf{u} \in \mathbb{R}^m
$$

| 모드 | 계산하는 것 | 한 번의 비용 | 전체 야코비안 비용 |
|------|-----------|------------|------------------|
| 전방 (JVP) | $J \mathbf{v}$ (야코비안의 열) | $O(\text{forward})$ | $O(n \cdot \text{forward})$ |
| 역방향 (VJP) | $\mathbf{u}^T J$ (야코비안의 행) | $O(\text{forward})$ | $O(m \cdot \text{forward})$ |

스칼라 출력($m=1$)이면 $\mathbf{u} = 1$로 놓아 **한 번의 역방향 패스**로 $\nabla f$ 전체를 얻습니다.

$$
\bar{\mathbf{x}} = \mathbf{u}^T J \Big|_{\mathbf{u}=1} = \nabla f(\mathbf{x})^T
$$

> **핵심 직관**: 신경망 학습에서 출력은 손실 스칼라 1개, 입력은 파라미터 수백만 개입니다. 역방향 모드는 이 구조에 완벽히 맞아서, 파라미터 수에 관계없이 **한 번의 역방향 패스**로 모든 그래디언트를 구합니다.

---

## 4. Python 구현: 간단한 역방향 AD

```python
import numpy as np

class Var:
    """역방향 모드 AD를 위한 계산 그래프 노드"""
    def __init__(self, value, children=(), grad_fns=()):
        self.value = value
        self.grad = 0.0
        self.children = children
        self.grad_fns = grad_fns

    def __add__(self, other):
        other = other if isinstance(other, Var) else Var(other)
        out = Var(self.value + other.value,
                  children=(self, other),
                  grad_fns=(lambda g: g, lambda g: g))
        return out

    def __mul__(self, other):
        other = other if isinstance(other, Var) else Var(other)
        out = Var(self.value * other.value,
                  children=(self, other),
                  grad_fns=(lambda g, ov=other.value: g * ov,
                            lambda g, sv=self.value: g * sv))
        return out

    def __rmul__(self, other):
        return self.__mul__(other)

    def sin(self):
        out = Var(np.sin(self.value),
                  children=(self,),
                  grad_fns=(lambda g, sv=self.value: g * np.cos(sv),))
        return out

    def backward(self):
        """역방향 패스: 위상 정렬 후 역순으로 수반 전파"""
        topo = []
        visited = set()
        def build(v):
            if v not in visited:
                visited.add(v)
                for child in v.children:
                    build(child)
                topo.append(v)
        build(self)

        self.grad = 1.0  # 시드: dy/dy = 1
        for node in reversed(topo):
            for child, grad_fn in zip(node.children, node.grad_fns):
                child.grad += grad_fn(node.grad)

# 예시: f(x1, x2) = x1*x2 + sin(x1), x1=2, x2=3
x1 = Var(2.0)
x2 = Var(3.0)
y = x1 * x2 + x1.sin()

y.backward()
print(f"f(x1,x2)   = {y.value:.4f}")
print(f"∂f/∂x1     = {x1.grad:.4f}")   # x2 + cos(x1) = 3 + cos(2)
print(f"∂f/∂x2     = {x2.grad:.4f}")   # x1 = 2
```

---

## 5. PyTorch의 autograd

PyTorch의 `autograd`는 역방향 모드 AD의 산업용 구현입니다.

```python
import torch

x1 = torch.tensor(2.0, requires_grad=True)
x2 = torch.tensor(3.0, requires_grad=True)
y = x1 * x2 + torch.sin(x1)

y.backward()
print(f"∂f/∂x1 = {x1.grad:.4f}")  # x2 + cos(x1)
print(f"∂f/∂x2 = {x2.grad:.4f}")  # x1

# 신경망 전체의 그래디언트도 동일한 원리
model = torch.nn.Linear(100, 1)
x = torch.randn(32, 100)
loss = model(x).sum()
loss.backward()

# 모든 파라미터의 그래디언트가 한 번에 계산됨
print(f"가중치 그래디언트 shape: {model.weight.grad.shape}")  # (1, 100)
print(f"편향 그래디언트 shape:   {model.bias.grad.shape}")    # (1,)
```

---

## 6. 전방 vs 역방향 총정리

```
함수: f: R^n → R^m

전방 모드 (JVP)                  역방향 모드 (VJP)
─────────────────                ──────────────────
입력 → 출력 순서                  출력 → 입력 역순
한 패스 = 야코비안 1열             한 패스 = 야코비안 1행
n번 패스 → 전체 야코비안           m번 패스 → 전체 야코비안
n이 작을 때 유리                  m이 작을 때 유리
메모리: O(1) 추가                 메모리: O(연산 수) — 중간값 저장 필요
```

| 관점 | 전방 모드 | 역방향 모드 |
|------|---------|-----------|
| 계산 방향 | 입력 → 출력 | 출력 → 입력 |
| 한 패스 결과 | $J \mathbf{v}$ (1열) | $\mathbf{u}^T J$ (1행) |
| ML에서 비용 | $O(n) \times$ forward | $O(1) \times$ forward |
| 메모리 | 추가 없음 | 중간값 저장 필요 |
| 대표 구현 | JAX `jvp` | PyTorch `autograd` |

> **핵심 직관**: 역방향 모드의 대가는 **메모리**입니다. 전방 패스의 모든 중간값을 저장해야 역방향에서 사용할 수 있습니다. 이 메모리-계산 트레이드오프를 해결하기 위한 **gradient checkpointing**은 nm-10에서 다룹니다.

---

## 7. 고차 미분과 하이퍼-그래디언트

역방향 모드를 반복 적용하면 고차 미분도 가능합니다.

```python
import torch

x = torch.tensor(1.0, requires_grad=True)
y = torch.sin(x) * x**2

# 1차 미분
dy_dx = torch.autograd.grad(y, x, create_graph=True)[0]
print(f"f'(x) = {dy_dx.item():.4f}")

# 2차 미분 (Hessian의 스칼라 버전)
d2y_dx2 = torch.autograd.grad(dy_dx, x)[0]
print(f"f''(x) = {d2y_dx2.item():.4f}")
```

la-05에서 다뤘던 Hessian 행렬도 자동 미분으로 효율적으로 근사할 수 있으며, 이는 nm-09에서 다룰 뉴턴 방법의 핵심입니다.

---

## 핵심 정리

1. **역방향 모드 AD**는 출력에서 입력으로 체인 룰을 적용하여 벡터-야코비안 곱(VJP)을 계산한다.
2. 스칼라 출력($m=1$)이면 **한 번의 역방향 패스**로 모든 파라미터의 그래디언트를 얻는다.
3. **역전파(backpropagation)** 는 역방향 모드 AD의 신경망 특수 사례이다.
4. 역방향 모드의 대가는 **메모리**: 전방 패스의 중간값을 모두 저장해야 한다.
5. PyTorch `autograd`, JAX `jax.grad`는 역방향 모드 AD의 산업용 구현이며, `create_graph=True`로 고차 미분도 가능하다.
