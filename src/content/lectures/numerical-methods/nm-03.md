# 전방 모드 자동 미분

## 왜 자동 미분을 배워야 하는가

미분은 ML의 핵심입니다. 손실 함수의 그래디언트를 구해야 가중치를 업데이트할 수 있습니다. 그런데 미분을 구하는 방법은 세 가지가 있습니다.

1. **손으로 유도** — 정확하지만 복잡한 모델에서는 비현실적
2. **수치 미분 (유한 차분)** — nm-02에서 봤듯이 정밀도와 효율 모두 문제
3. **자동 미분 (Automatic Differentiation)** — 정확하고 효율적

이번 강의에서는 자동 미분의 **전방 모드(forward mode)**를 다룹니다. 핵심 아이디어는 놀랍도록 우아한 **이중수(dual numbers)** 입니다.

---

## 1. 유한 차분법의 한계

nm-02에서 실험했듯이, 전방 차분과 중심 차분은 근본적인 한계가 있습니다.

$$
f'(x) \approx \frac{f(x+h) - f(x)}{h} + O(h) \quad \text{(전방 차분)}
$$

$$
f'(x) \approx \frac{f(x+h) - f(x-h)}{2h} + O(h^2) \quad \text{(중심 차분)}
$$

| 방법 | 절단 오차 | 반올림 오차 | 함수 평가 횟수 ($n$개 변수) |
|------|----------|-----------|---------------------------|
| 전방 차분 | $O(h)$ | $O(\epsilon/h)$ | $n + 1$ |
| 중심 차분 | $O(h^2)$ | $O(\epsilon/h)$ | $2n$ |
| **자동 미분** | **0 (정확)** | **반올림만** | **1 패스** |

> **핵심 직관**: 유한 차분은 $h$가 크면 절단 오차, 작으면 반올림 오차가 문제입니다. 자동 미분은 $h$를 잡을 필요 자체가 없습니다. 절단 오차가 원리적으로 0입니다.

---

## 2. 이중수 (Dual Numbers)

복소수가 $a + bi$ ($i^2 = -1$)인 것처럼, **이중수**는 다음과 같이 정의됩니다.

$$
a + b\epsilon, \quad \epsilon^2 = 0, \quad \epsilon \neq 0
$$

$\epsilon$은 "무한소처럼 행동하지만 제곱하면 사라지는" 수입니다.

### 이중수의 산술

$$
(a + b\epsilon) + (c + d\epsilon) = (a+c) + (b+d)\epsilon
$$

$$
(a + b\epsilon) \times (c + d\epsilon) = ac + (ad + bc)\epsilon + bd\epsilon^2 = ac + (ad + bc)\epsilon
$$

### 핵심: 함수에 이중수를 넣으면?

$f(x)$를 $x = a + \epsilon$에서 테일러 전개합니다.

$$
f(a + \epsilon) = f(a) + f'(a)\epsilon + \frac{f''(a)}{2}\epsilon^2 + \cdots = f(a) + f'(a)\epsilon
$$

$\epsilon^2 = 0$이므로 고차항이 모두 사라집니다!

> **핵심 직관**: 이중수 $a + 1 \cdot \epsilon$을 함수에 넣으면, 실수부에 $f(a)$, 이중부에 $f'(a)$가 자동으로 나옵니다. 절단 오차가 원리적으로 없는 "마법의 수체계"입니다.

---

## 3. 전방 모드 AD 구현

```python
class Dual:
    """이중수: value + deriv * epsilon"""
    def __init__(self, value, deriv=0.0):
        self.value = value  # 함수값 (실수부)
        self.deriv = deriv  # 도함수값 (이중부)

    def __add__(self, other):
        other = other if isinstance(other, Dual) else Dual(other)
        return Dual(self.value + other.value, self.deriv + other.deriv)

    def __radd__(self, other):
        return self.__add__(other)

    def __mul__(self, other):
        other = other if isinstance(other, Dual) else Dual(other)
        return Dual(self.value * other.value,
                    self.value * other.deriv + self.deriv * other.value)

    def __rmul__(self, other):
        return self.__mul__(other)

    def __sub__(self, other):
        other = other if isinstance(other, Dual) else Dual(other)
        return Dual(self.value - other.value, self.deriv - other.deriv)

    def __truediv__(self, other):
        other = other if isinstance(other, Dual) else Dual(other)
        return Dual(self.value / other.value,
                    (self.deriv * other.value - self.value * other.deriv)
                    / other.value**2)

    def __repr__(self):
        return f"Dual({self.value:.6f}, {self.deriv:.6f})"


import math

def dual_sin(x):
    return Dual(math.sin(x.value), x.deriv * math.cos(x.value))

def dual_exp(x):
    e = math.exp(x.value)
    return Dual(e, x.deriv * e)

def dual_log(x):
    return Dual(math.log(x.value), x.deriv / x.value)
```

```python
# 예시: f(x) = x^2 * sin(x) 를 x=2에서 미분
def f(x):
    return x * x * dual_sin(x)

x = Dual(2.0, 1.0)  # x = 2, dx/dx = 1
result = f(x)
print(f"f(2)  = {result.value:.6f}")   # 함수값
print(f"f'(2) = {result.deriv:.6f}")   # 도함수값

# 검증: f'(x) = 2x*sin(x) + x^2*cos(x)
import math
exact = 2*2*math.sin(2) + 4*math.cos(2)
print(f"정확값 = {exact:.6f}")
```

---

## 4. 야코비안-벡터 곱 (JVP)

다변수 함수 $\mathbf{f}: \mathbb{R}^n \to \mathbb{R}^m$에 대해, 전방 모드 AD는 **야코비안-벡터 곱 (Jacobian-Vector Product, JVP)** 을 계산합니다.

$$
\text{JVP}: \quad J \cdot \mathbf{v} = \frac{\partial \mathbf{f}}{\partial \mathbf{x}} \mathbf{v}, \quad \mathbf{v} \in \mathbb{R}^n
$$

la-05에서 다뤘던 야코비안 $J \in \mathbb{R}^{m \times n}$을 명시적으로 구성하지 않고, 방향 $\mathbf{v}$에 대한 **방향 도함수**를 한 번의 전방 패스로 얻습니다.

```python
import numpy as np

def forward_mode_jvp(f, x, v):
    """
    전방 모드 AD로 JVP 계산
    f: R^n -> R^m
    x: 입력 벡터
    v: 방향 벡터 (seed)
    """
    duals = [Dual(xi, vi) for xi, vi in zip(x, v)]
    result = f(duals)
    if isinstance(result, list):
        return ([r.value for r in result],
                [r.deriv for r in result])
    return result.value, result.deriv

# 예시: f(x1, x2) = x1*x2 + sin(x1)
def g(x):
    return x[0] * x[1] + dual_sin(x[0])

x = [1.0, 2.0]

# e1 방향 시드 → ∂f/∂x1
_, jvp1 = forward_mode_jvp(g, x, [1.0, 0.0])
# e2 방향 시드 → ∂f/∂x2
_, jvp2 = forward_mode_jvp(g, x, [0.0, 1.0])

print(f"∂f/∂x1 = {jvp1:.6f}")  # x2 + cos(x1) = 2 + cos(1)
print(f"∂f/∂x2 = {jvp2:.6f}")  # x1 = 1
```

---

## 5. 전방 모드의 계산 비용

전방 모드 AD 한 번의 패스로 야코비안의 **열 하나**(또는 임의의 방향 도함수)를 얻습니다.

```
전체 야코비안 J ∈ R^{m×n} 계산:

전방 모드: n번의 전방 패스 필요 (열마다 한 번)

  v = e1 → J·e1 = J의 1열
  v = e2 → J·e2 = J의 2열
  ...
  v = en → J·en = J의 n열

비용: O(n) × 전방 패스 비용
```

| 상황 | $n$ (입력) | $m$ (출력) | 효율적인 모드 |
|------|-----------|-----------|-------------|
| $\mathbb{R}^n \to \mathbb{R}$ (스칼라 출력) | 큼 | 1 | **역방향** (nm-04) |
| $\mathbb{R} \to \mathbb{R}^m$ (스칼라 입력) | 1 | 큼 | **전방** |
| $\mathbb{R}^n \to \mathbb{R}^n$ (정방) | $n$ | $n$ | 상황에 따라 |

> **핵심 직관**: 전방 모드는 입력 차원 $n$에 비례하는 비용이 듭니다. 신경망은 입력(파라미터)이 수백만 개이고 출력(손실)이 1개이므로, 전방 모드는 비효율적입니다. 이것이 nm-04에서 다룰 역방향 모드가 필요한 이유입니다.

---

## 6. JAX의 전방 모드

실전에서는 JAX 라이브러리가 전방 모드 AD를 `jax.jvp`로 제공합니다.

```python
import jax
import jax.numpy as jnp

def f(x):
    return jnp.sin(x[0]) * x[1]**2

x = jnp.array([1.0, 3.0])
v = jnp.array([1.0, 0.0])  # x[0] 방향

# JVP 계산
primals, tangents = jax.jvp(f, (x,), (v,))
print(f"f(x)    = {primals:.4f}")    # sin(1) * 9
print(f"JVP     = {tangents:.4f}")   # cos(1) * 9 (∂f/∂x1)

# 전체 야코비안도 가능
J = jax.jacfwd(f)(x)  # 전방 모드로 야코비안 계산
print(f"Jacobian = {J}")
```

---

## 핵심 정리

1. **유한 차분법**은 절단 오차와 반올림 오차의 트레이드오프가 있으며, 자동 미분은 이를 근본적으로 해결한다.
2. **이중수** $a + b\epsilon$ ($\epsilon^2 = 0$)에 함수를 적용하면, 실수부에 함수값, 이중부에 도함수가 자동으로 나온다.
3. **전방 모드 AD**는 한 번의 패스로 야코비안-벡터 곱(JVP)을 계산한다.
4. 전방 모드의 비용은 **입력 차원 $n$에 비례**하므로, $n \gg m$인 신경망 학습에는 비효율적이다.
5. JAX는 `jax.jvp`와 `jax.jacfwd`로 전방 모드 AD를 제공하며, 물리 시뮬레이션 등 $m \gg n$인 문제에서 유용하다.
