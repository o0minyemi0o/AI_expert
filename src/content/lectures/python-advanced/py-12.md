# 프로파일링과 최적화

## 왜 프로파일링을 배워야 하는가

"이 코드가 느리다"는 느낌만으로 최적화하면 잘못된 곳을 최적화합니다. **측정 없는 최적화는 추측일 뿐**입니다. Donald Knuth의 유명한 격언처럼 "조기 최적화는 모든 악의 근원"이지만, **프로파일링 후의 최적화는 공학**입니다. Python에서 CPU 시간, 메모리 사용, I/O 대기를 정확히 측정하는 방법을 익힙니다.

---

## 1. 최적화의 원칙

### 순서가 중요하다

```
1. 올바르게 작동하는 코드를 작성한다
2. 프로파일링하여 병목을 찾는다
3. 병목만 최적화한다
4. 1번으로 돌아간다
```

> **핵심 직관**: 코드의 90%의 시간은 10%의 코드에서 소모됩니다. 그 10%를 찾는 것이 프로파일링입니다.

---

## 2. timeit: 마이크로 벤치마크

코드 조각의 실행 시간을 정확히 측정합니다.

```python
import timeit

# 문자열 연결 방법 비교
n = 10000

# 방법 1: + 연산자
t1 = timeit.timeit(
    'result = ""\nfor s in items:\n    result += s',
    setup='items = [str(i) for i in range(1000)]',
    number=100,
)

# 방법 2: join
t2 = timeit.timeit(
    '"".join(items)',
    setup='items = [str(i) for i in range(1000)]',
    number=100,
)

print(f"+= : {t1:.4f}s")    # ~0.15s
print(f"join: {t2:.4f}s")   # ~0.01s (15x 빠름)
```

### 컨텍스트 매니저 방식

```python
from contextlib import contextmanager
import time

@contextmanager
def timer(label=""):
    start = time.perf_counter()
    yield
    elapsed = time.perf_counter() - start
    print(f"{label}: {elapsed:.4f}s")

with timer("List comprehension"):
    result = [x ** 2 for x in range(1_000_000)]

with timer("Map"):
    result = list(map(lambda x: x ** 2, range(1_000_000)))
```

---

## 3. cProfile: CPU 프로파일링

함수별 실행 시간과 호출 횟수를 측정합니다.

```python
import cProfile

def slow_function():
    total = 0
    for i in range(1_000_000):
        total += expensive_calc(i)
    return total

def expensive_calc(n):
    return sum(range(n % 100))

# 프로파일링 실행
cProfile.run('slow_function()', sort='cumulative')
```

출력:

```
   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.312    0.312    2.845    2.845 example.py:3(slow_function)
  1000000    2.533    0.000    2.533    0.000 example.py:8(expensive_calc)
```

| 컬럼 | 의미 |
|------|------|
| `ncalls` | 호출 횟수 |
| `tottime` | 해당 함수 자체의 총 시간 (하위 호출 제외) |
| `cumtime` | 해당 함수 + 하위 호출의 총 시간 |
| `percall` | 호출당 평균 시간 |

### 프로그래밍적 사용

```python
import cProfile
import pstats
import io

profiler = cProfile.Profile()
profiler.enable()

# 측정할 코드
result = slow_function()

profiler.disable()

# 결과 분석
stream = io.StringIO()
stats = pstats.Stats(profiler, stream=stream)
stats.sort_stats('cumulative')
stats.print_stats(20)  # 상위 20개
print(stream.getvalue())
```

### 데코레이터로 프로파일링

```python
import cProfile
import functools

def profile(func):
    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        profiler = cProfile.Profile()
        try:
            return profiler.runcall(func, *args, **kwargs)
        finally:
            profiler.print_stats(sort='cumulative')
    return wrapper

@profile
def my_function():
    # ...
    pass
```

---

## 4. line_profiler: 줄 단위 프로파일링

함수 내부의 **각 줄이 얼마나 걸리는지** 측정합니다.

```python
# pip install line_profiler

@profile  # line_profiler의 데코레이터
def process_data(data):
    result = []                          #  1%
    for item in data:                    #  2%
        cleaned = item.strip().lower()   # 30%
        if cleaned:                      #  2%
            tokens = cleaned.split()     # 25%
            result.extend(tokens)        # 40%
    return result
```

```bash
kernprof -l -v script.py
```

출력:

```
Line #  Hits       Time  Per Hit  % Time  Line Contents
     3                                    def process_data(data):
     4     1        2.0      2.0     0.0      result = []
     5  1001     1234.0      1.2     2.0      for item in data:
     6  1000    18500.0     18.5    30.0          cleaned = item.strip().lower()
     7  1000     1100.0      1.1     1.8          if cleaned:
     8   950    15200.0     16.0    24.7              tokens = cleaned.split()
     9   950    25500.0     26.8    41.4              result.extend(tokens)
```

이 결과에서 `result.extend(tokens)`가 41.4%로 가장 큰 병목입니다.

---

## 5. memory_profiler: 메모리 프로파일링

```python
# pip install memory_profiler

@profile  # memory_profiler의 데코레이터
def memory_intensive():
    a = [i for i in range(1_000_000)]     # +38 MB
    b = {i: i**2 for i in range(500_000)} # +40 MB
    del a                                  # -38 MB
    return b
```

```bash
python -m memory_profiler script.py
```

출력:

```
Line #    Mem usage    Increment  Line Contents
     3     45.2 MiB     45.2 MiB   def memory_intensive():
     4     83.4 MiB     38.2 MiB       a = [i for i in range(1_000_000)]
     5    123.6 MiB     40.2 MiB       b = {i: i**2 for i in ...}
     6     85.4 MiB    -38.2 MiB       del a
     7     85.4 MiB      0.0 MiB       return b
```

---

## 6. 흔한 성능 패턴과 안티패턴

### 리스트 조작

```python
# 느림: 리스트 앞에 삽입 O(n)
lst = []
for i in range(100000):
    lst.insert(0, i)  # 매번 전체 이동

# 빠름: deque 사용 O(1)
from collections import deque
d = deque()
for i in range(100000):
    d.appendleft(i)
```

### 딕셔너리 vs 리스트 검색

```python
# 느림: 리스트에서 검색 O(n)
items = list(range(100000))
99999 in items  # ~1.5ms

# 빠름: set/dict 검색 O(1)
items_set = set(range(100000))
99999 in items_set  # ~0.00005ms (30000x 빠름)
```

### 문자열 포맷팅

```python
import timeit

name = "Alice"
age = 30

# f-string (가장 빠름)
timeit.timeit('f"Name: {name}, Age: {age}"', globals=globals())

# format() (느림)
timeit.timeit('"Name: {}, Age: {}".format(name, age)', globals=globals())

# % 포맷팅 (중간)
timeit.timeit('"Name: %s, Age: %d" % (name, age)', globals=globals())

# f-string이 가장 빠름 (바이트코드 레벨에서 최적화)
```

### 전역 변수 vs 지역 변수

```python
# 느림: 전역 변수 접근 (LOAD_GLOBAL)
x = 10
def slow():
    total = 0
    for i in range(1_000_000):
        total += x  # 매번 전역 딕셔너리 검색

# 빠름: 지역 변수 (LOAD_FAST)
def fast():
    local_x = x  # 한 번만 전역에서 가져옴
    total = 0
    for i in range(1_000_000):
        total += local_x  # 인덱스 접근
```

---

## 7. NumPy 벡터화

루프를 NumPy 연산으로 대체하는 것이 Python 최적화의 가장 효과적인 방법입니다.

```python
import numpy as np
import time

n = 1_000_000

# 순수 Python 루프
def python_distance(a, b):
    total = 0
    for i in range(len(a)):
        total += (a[i] - b[i]) ** 2
    return total ** 0.5

# NumPy 벡터화
def numpy_distance(a, b):
    return np.linalg.norm(a - b)

a_list = list(range(n))
b_list = list(range(n, 2*n))
a_np = np.array(a_list, dtype=float)
b_np = np.array(b_list, dtype=float)

start = time.time()
python_distance(a_list, b_list)
print(f"Python: {time.time() - start:.4f}s")  # ~0.5s

start = time.time()
numpy_distance(a_np, b_np)
print(f"NumPy: {time.time() - start:.4f}s")   # ~0.002s (250x 빠름)
```

### 벡터화 규칙

1. **루프를 제거**하고 NumPy 배열 연산으로 대체
2. **인덱싱 대신 슬라이싱**과 **브로드캐스팅** 활용
3. **조건문 대신 `np.where`** 사용
4. 불가피한 루프는 `np.vectorize` 또는 Numba `@njit` 사용

---

## 8. 최적화 체크리스트

```
□ 1. 올바르게 동작하는가? (최적화 전 테스트 작성)
□ 2. 프로파일링했는가? (cProfile로 병목 확인)
□ 3. 알고리즘을 개선할 수 있는가? (O(n²) → O(n log n))
□ 4. 적절한 자료구조를 사용하고 있는가? (list → set/dict)
□ 5. 내장 함수/라이브러리를 활용하고 있는가? (sum, map, itertools)
□ 6. NumPy 벡터화가 가능한가?
□ 7. 캐싱이 도움이 되는가? (functools.lru_cache)
□ 8. 그래도 느리면 C 확장을 고려한다
```

### functools.lru_cache

```python
from functools import lru_cache

@lru_cache(maxsize=128)
def expensive_api_call(query):
    # 같은 query에 대해 한 번만 실행
    import time
    time.sleep(1)  # 느린 API 호출
    return f"Result for {query}"

expensive_api_call("hello")  # 1초 소요
expensive_api_call("hello")  # 즉시 반환 (캐시)

# 캐시 정보
print(expensive_api_call.cache_info())
# CacheInfo(hits=1, misses=1, maxsize=128, currsize=1)
```

---

## 9. ML에서의 의미

### 학습 루프 프로파일링

```python
import torch.profiler

with torch.profiler.profile(
    activities=[
        torch.profiler.ProfilerActivity.CPU,
        torch.profiler.ProfilerActivity.CUDA,
    ],
    schedule=torch.profiler.schedule(wait=1, warmup=1, active=3),
    on_trace_ready=torch.profiler.tensorboard_trace_handler("./log"),
) as prof:
    for step, batch in enumerate(dataloader):
        train_step(model, batch)
        prof.step()
```

### 데이터 로딩 병목

학습 시간의 상당 부분이 데이터 로딩에 소모될 수 있습니다. `num_workers`, `pin_memory`, `prefetch_factor` 튜닝이 중요합니다.

### 모델 추론 최적화

```python
# 추론 시 그래디언트 비활성화
with torch.no_grad():
    output = model(input)

# 반정밀도(FP16)로 메모리와 속도 개선
model = model.half()
```

---

## 핵심 정리

1. **"측정하고, 최적화하고, 측정한다"**가 최적화의 원칙이다. 추측으로 최적화하지 않는다.
2. **cProfile**은 함수별 CPU 시간을, **line_profiler**는 줄별 시간을, **memory_profiler**는 줄별 메모리를 측정한다.
3. 가장 효과적인 최적화 순서: **알고리즘 개선 → 자료구조 변경 → 벡터화 → 캐싱 → C 확장**.
4. **NumPy 벡터화**는 Python 루프 대비 100~1000배 성능 향상을 제공하며, C 확장 전에 먼저 시도해야 한다.
5. `lru_cache`, `__slots__`, f-string, 지역 변수 활용 등 **작은 습관**이 누적되어 큰 차이를 만든다.
