# C 확장과 ctypes

## 왜 C 확장을 배워야 하는가

Python은 느립니다. 순수 Python 루프는 C보다 10~100배 느릴 수 있습니다. NumPy, PyTorch, TensorFlow가 빠른 이유는 핵심 연산이 **C/C++/CUDA로 작성**되어 있기 때문입니다. `ctypes`, `cffi`, `Cython`, `pybind11` 등으로 C 코드를 Python에서 호출하는 방법을 이해하면, 성능 병목을 직접 해결할 수 있습니다.

---

## 1. Python이 느린 이유

### 동적 타이핑의 비용

```python
# Python: 매 연산마다 타입 체크
def add(a, b):
    return a + b  # a와 b가 int? float? str? 런타임에 결정

# C: 컴파일 타임에 타입 확정
# int add(int a, int b) { return a + b; }
```

| 요인 | Python | C |
|------|--------|---|
| 타입 결정 | 런타임 | 컴파일 타임 |
| 메모리 관리 | GC + refcount | 수동 |
| 정수 표현 | 임의 정밀도 객체 (~28 bytes) | 고정 크기 (4-8 bytes) |
| 루프 | 바이트코드 해석 | 기계어 직접 실행 |
| 함수 호출 | 객체 생성 + 딕셔너리 조회 | 스택 프레임만 |

---

## 2. ctypes: 가장 간단한 FFI

`ctypes`는 표준 라이브러리로, C 공유 라이브러리를 Python에서 직접 호출합니다.

### C 코드 컴파일

```c
// math_ops.c
#include <math.h>

double fast_norm(double* arr, int n) {
    double sum = 0.0;
    for (int i = 0; i < n; i++) {
        sum += arr[i] * arr[i];
    }
    return sqrt(sum);
}

int fibonacci(int n) {
    if (n <= 1) return n;
    int a = 0, b = 1;
    for (int i = 2; i <= n; i++) {
        int temp = a + b;
        a = b;
        b = temp;
    }
    return b;
}
```

```bash
# Linux/macOS에서 공유 라이브러리로 컴파일
gcc -shared -fPIC -o libmath_ops.so math_ops.c -lm
# macOS
gcc -shared -fPIC -o libmath_ops.dylib math_ops.c -lm
```

### Python에서 호출

```python
import ctypes
import os

# 라이브러리 로드
lib = ctypes.CDLL(os.path.join(os.getcwd(), "libmath_ops.so"))

# 함수 시그니처 설정
lib.fibonacci.argtypes = [ctypes.c_int]
lib.fibonacci.restype = ctypes.c_int

lib.fast_norm.argtypes = [ctypes.POINTER(ctypes.c_double), ctypes.c_int]
lib.fast_norm.restype = ctypes.c_double

# 호출
print(lib.fibonacci(30))  # 832040

# 배열 전달
import numpy as np
arr = np.array([3.0, 4.0], dtype=np.float64)
arr_ptr = arr.ctypes.data_as(ctypes.POINTER(ctypes.c_double))
print(lib.fast_norm(arr_ptr, len(arr)))  # 5.0
```

### ctypes 타입 매핑

| C 타입 | ctypes 타입 | Python 타입 |
|--------|-----------|------------|
| `int` | `c_int` | int |
| `double` | `c_double` | float |
| `char*` | `c_char_p` | bytes |
| `void*` | `c_void_p` | int 또는 None |
| `int*` | `POINTER(c_int)` | — |

---

## 3. cffi: 더 Pythonic한 FFI

`cffi`는 C 선언을 거의 그대로 복사하여 사용할 수 있습니다.

```python
from cffi import FFI

ffi = FFI()

# C 함수 선언 (헤더에서 복사)
ffi.cdef("""
    double fast_norm(double* arr, int n);
    int fibonacci(int n);
""")

# 라이브러리 로드
lib = ffi.dlopen("./libmath_ops.so")

# 호출
print(lib.fibonacci(30))

# 배열 전달
arr = ffi.new("double[]", [3.0, 4.0])
print(lib.fast_norm(arr, 2))  # 5.0
```

### ctypes vs cffi

| 특성 | ctypes | cffi |
|------|--------|------|
| 설치 | 표준 라이브러리 | `pip install cffi` |
| 선언 방식 | Python 코드로 | C 선언 복사 |
| 성능 | 보통 | ctypes보다 빠름 |
| 콜백 | 지원 | 더 나은 지원 |
| ABI/API 모드 | ABI만 | ABI + API (컴파일) |

---

## 4. Cython: Python과 C의 하이브리드

Cython은 Python 문법에 C 타입 선언을 추가하여 컴파일하는 언어입니다.

```python
# fib.pyx (Cython 파일)
def fibonacci_py(int n):
    """순수 Python보다 빠르지만 최적이 아님"""
    cdef int a = 0, b = 1, i, temp
    for i in range(2, n + 1):
        temp = a + b
        a = b
        b = temp
    return b

# cdef: C 레벨 함수 (Python에서 직접 호출 불가)
cdef double c_norm(double* arr, int n):
    cdef double s = 0.0
    cdef int i
    for i in range(n):
        s += arr[i] * arr[i]
    return s ** 0.5

# cpdef: C + Python 양쪽에서 호출 가능
cpdef double norm_wrapper(list data):
    cdef int n = len(data)
    cdef double* arr = <double*>malloc(n * sizeof(double))
    for i in range(n):
        arr[i] = data[i]
    result = c_norm(arr, n)
    free(arr)
    return result
```

### 빌드

```python
# setup.py
from setuptools import setup
from Cython.Build import cythonize

setup(
    ext_modules=cythonize("fib.pyx")
)
```

```bash
python setup.py build_ext --inplace
```

### Cython의 장점

1. **점진적 최적화**: 순수 Python에서 시작하여 타입 선언을 추가하며 점진적으로 속도 개선
2. **NumPy 통합**: `cimport numpy`로 NumPy 배열에 직접 접근
3. **GIL 해제**: `with nogil:`로 멀티스레드 성능 확보

---

## 5. pybind11: C++을 위한 현대적 바인딩

C++ 라이브러리를 Python에 바인딩하는 데 가장 널리 사용됩니다.

```cpp
// example.cpp
#include <pybind11/pybind11.h>
#include <pybind11/numpy.h>
#include <cmath>

namespace py = pybind11;

double fast_norm(py::array_t<double> arr) {
    auto buf = arr.request();
    double* ptr = static_cast<double*>(buf.ptr);
    int n = buf.size;

    double sum = 0.0;
    for (int i = 0; i < n; i++) {
        sum += ptr[i] * ptr[i];
    }
    return std::sqrt(sum);
}

PYBIND11_MODULE(example, m) {
    m.doc() = "Example C++ extension";
    m.def("fast_norm", &fast_norm, "Compute L2 norm");
}
```

```python
# Python에서 사용
import example
import numpy as np

arr = np.array([3.0, 4.0])
print(example.fast_norm(arr))  # 5.0
```

PyTorch의 C++ 확장이 pybind11을 사용합니다.

---

## 6. 성능 비교

피보나치 수열 계산 (n=35) 벤치마크:

| 방법 | 실행 시간 | 배율 |
|------|----------|------|
| 순수 Python | ~2.5s | 1x |
| Cython (타입 없음) | ~1.2s | 2x |
| Cython (타입 추가) | ~0.008s | 300x |
| ctypes (C) | ~0.006s | 400x |
| pybind11 (C++) | ~0.006s | 400x |

타입 선언만 추가해도 Cython은 극적인 성능 향상을 보입니다.

---

## 7. NumPy C API와의 관계

NumPy가 빠른 이유는 **벡터화된 C 연산**을 사용하기 때문입니다.

```python
import numpy as np

# 느림: Python 루프
def slow_sum(arr):
    total = 0
    for x in arr:
        total += x
    return total

# 빠름: NumPy (C 코드 호출)
def fast_sum(arr):
    return np.sum(arr)

arr = np.random.randn(1_000_000)
# slow_sum: ~150ms
# fast_sum: ~0.5ms (300x 빠름)
```

직접 C 확장을 작성하기 전에, **NumPy의 벡터 연산으로 해결할 수 있는지** 먼저 확인하는 것이 중요합니다. 대부분의 수치 계산은 NumPy만으로 충분히 빠릅니다.

---

## 8. 어떤 도구를 선택할 것인가

```
"Python 코드가 느리다"
         │
    NumPy 벡터화로 해결 가능?
    ├── Yes → NumPy 사용 (가장 간단)
    │
    ├── No → 기존 C 라이브러리 호출?
    │         ├── Yes → ctypes 또는 cffi
    │         └── No → 새 코드 작성 필요
    │
    └── 새 코드 작성
         ├── Python 코드를 점진적으로 최적화 → Cython
         ├── C++ 라이브러리와 통합 → pybind11
         └── CUDA GPU 코드 → PyTorch C++ extension 또는 Numba
```

| 도구 | 적합한 경우 | 학습 곡선 |
|------|-----------|----------|
| NumPy | 배열 연산 | 낮음 |
| ctypes | 기존 C 라이브러리 호출 | 중간 |
| cffi | 기존 C 라이브러리 (헤더 복잡) | 중간 |
| Cython | Python 코드의 점진적 최적화 | 중간~높음 |
| pybind11 | C++ 라이브러리 바인딩 | 높음 |
| Numba | 수치 계산 JIT 컴파일 | 낮음 |

---

## 9. ML에서의 의미

### PyTorch Custom Operators

```python
# torch.utils.cpp_extension으로 C++ 커스텀 연산 작성
from torch.utils.cpp_extension import load

custom_op = load(
    name="custom_op",
    sources=["custom_op.cpp"],
    extra_cflags=["-O3"],
)
```

### CUDA 커널

GPU에서 실행되는 커스텀 커널을 C++/CUDA로 작성하고 Python에서 호출합니다. Flash Attention, 커스텀 활성화 함수 등이 이 방식입니다.

### Numba: JIT 컴파일

```python
from numba import njit

@njit
def fast_function(arr):
    total = 0.0
    for i in range(len(arr)):
        total += arr[i] ** 2
    return total ** 0.5

# 첫 호출 시 JIT 컴파일, 이후 C 수준 속도
```

Numba는 데코레이터 하나로 Python 함수를 기계어로 컴파일합니다. C 코드 작성 없이 C 수준 성능을 얻습니다.

---

## 핵심 정리

1. Python이 느린 근본 원인은 **동적 타이핑, 객체 오버헤드, 바이트코드 해석**이다.
2. **ctypes**는 표준 라이브러리로, 기존 C 공유 라이브러리를 Python에서 직접 호출할 수 있다.
3. **Cython**은 Python 문법에 C 타입 선언을 추가하여 컴파일하는 방식으로, **점진적 최적화**에 적합하다.
4. **pybind11**은 C++ 라이브러리를 Python에 바인딩하는 현대적 도구이며, PyTorch가 이를 사용한다.
5. 최적화 전에 항상 **NumPy 벡터화 → 프로파일링 → 병목 확인 → C 확장** 순서로 접근해야 한다.
