# 동시성 패턴

## 왜 동시성을 깊이 이해해야 하는가

ML 파이프라인에서 데이터 전처리, 모델 추론, API 호출은 종종 **동시에** 이루어져야 합니다. Python의 동시성은 **threading**, **multiprocessing**, **asyncio** 세 가지 모델이 있으며, 각각의 적용 영역이 다릅니다. 특히 **GIL(Global Interpreter Lock)**의 영향을 정확히 이해해야 올바른 도구를 선택할 수 있습니다.

---

## 1. 동시성 vs 병렬성

| 개념 | 의미 | 비유 |
|------|------|------|
| **동시성** (Concurrency) | 여러 작업을 **번갈아** 처리 | 요리사 한 명이 여러 요리를 돌아가며 |
| **병렬성** (Parallelism) | 여러 작업을 **동시에** 처리 | 요리사 여러 명이 각자 요리를 |

```python
# 동시성: 단일 코어에서 작업 전환
# I/O 대기 시간을 활용
Task A: [실행][대기.......][실행]
Task B:       [실행][대기..][실행]

# 병렬성: 여러 코어에서 동시 실행
Core 1: [Task A 실행중.................]
Core 2: [Task B 실행중.................]
```

> **핵심 직관**: I/O 바운드 작업은 **동시성**으로 해결하고, CPU 바운드 작업은 **병렬성**으로 해결합니다.

---

## 2. GIL (Global Interpreter Lock)

CPython에는 한 번에 **하나의 스레드만** Python 바이트코드를 실행할 수 있게 하는 전역 락이 있습니다.

```python
import threading
import time

counter = 0

def increment(n):
    global counter
    for _ in range(n):
        counter += 1  # 원자적이지 않음!

# 두 스레드가 동시에 실행
t1 = threading.Thread(target=increment, args=(1_000_000,))
t2 = threading.Thread(target=increment, args=(1_000_000,))
t1.start(); t2.start()
t1.join(); t2.join()

print(counter)  # 2,000,000이 아닐 수 있음! (경합 조건)
```

### GIL의 영향

| 작업 유형 | threading 효과 | 이유 |
|----------|---------------|------|
| I/O 바운드 | 효과적 | I/O 대기 중 GIL 해제 |
| CPU 바운드 | **효과 없음** (오히려 느릴 수 있음) | GIL이 병렬 실행 차단 |

```python
import threading
import time

def cpu_task():
    """CPU 바운드 작업"""
    total = 0
    for i in range(50_000_000):
        total += i

# 순차 실행
start = time.time()
cpu_task()
cpu_task()
print(f"Sequential: {time.time() - start:.2f}s")  # ~5s

# 스레드 병렬 (GIL 때문에 빨라지지 않음)
start = time.time()
t1 = threading.Thread(target=cpu_task)
t2 = threading.Thread(target=cpu_task)
t1.start(); t2.start()
t1.join(); t2.join()
print(f"Threaded: {time.time() - start:.2f}s")  # ~5s (같거나 더 느림!)
```

### GIL이 해제되는 경우

1. **I/O 연산**: 파일 읽기/쓰기, 네트워크, `time.sleep()`
2. **C 확장**: NumPy 연산, 대부분의 C 라이브러리 호출
3. **명시적 해제**: Cython의 `with nogil:`

---

## 3. threading: I/O 바운드 동시성

### 기본 사용

```python
import threading
import time

def download(url, delay):
    print(f"Downloading {url}...")
    time.sleep(delay)  # I/O 시뮬레이션 (GIL 해제)
    print(f"Done: {url}")
    return f"Data from {url}"

# 순차: 3초
# 스레드: ~1초 (가장 긴 delay만큼)
threads = []
for url, delay in [("a.com", 1), ("b.com", 0.5), ("c.com", 0.3)]:
    t = threading.Thread(target=download, args=(url, delay))
    threads.append(t)
    t.start()

for t in threads:
    t.join()  # 모든 스레드 완료 대기
```

### Lock: 경합 조건 방지

```python
import threading

lock = threading.Lock()
counter = 0

def safe_increment(n):
    global counter
    for _ in range(n):
        with lock:  # 한 번에 한 스레드만
            counter += 1

t1 = threading.Thread(target=safe_increment, args=(1_000_000,))
t2 = threading.Thread(target=safe_increment, args=(1_000_000,))
t1.start(); t2.start()
t1.join(); t2.join()
print(counter)  # 정확히 2,000,000
```

### 데드락 주의

```python
lock_a = threading.Lock()
lock_b = threading.Lock()

def worker1():
    with lock_a:           # lock_a 획득
        time.sleep(0.1)
        with lock_b:       # lock_b 대기 → 데드락!
            pass

def worker2():
    with lock_b:           # lock_b 획득
        time.sleep(0.1)
        with lock_a:       # lock_a 대기 → 데드락!
            pass

# 해결: 항상 같은 순서로 락을 획득
```

---

## 4. multiprocessing: CPU 바운드 병렬성

별도의 **프로세스**를 사용하므로 GIL의 영향을 받지 않습니다.

```python
import multiprocessing
import time

def cpu_task(n):
    """CPU 집약적 작업"""
    total = 0
    for i in range(n):
        total += i * i
    return total

if __name__ == "__main__":
    # 순차 실행
    start = time.time()
    results = [cpu_task(10_000_000) for _ in range(4)]
    print(f"Sequential: {time.time() - start:.2f}s")

    # 병렬 실행 (4 프로세스)
    start = time.time()
    with multiprocessing.Pool(4) as pool:
        results = pool.map(cpu_task, [10_000_000] * 4)
    print(f"Parallel: {time.time() - start:.2f}s")
    # 4코어 머신에서 약 4배 빠름
```

### 프로세스 간 통신

```python
from multiprocessing import Process, Queue

def worker(queue, data):
    result = sum(x ** 2 for x in data)
    queue.put(result)  # 결과를 큐에 전달

if __name__ == "__main__":
    queue = Queue()
    data_chunks = [range(i, i + 1000) for i in range(0, 4000, 1000)]

    processes = []
    for chunk in data_chunks:
        p = Process(target=worker, args=(queue, chunk))
        processes.append(p)
        p.start()

    for p in processes:
        p.join()

    results = [queue.get() for _ in processes]
    print(f"Total: {sum(results)}")
```

### multiprocessing의 비용

| 항목 | threading | multiprocessing |
|------|-----------|----------------|
| 메모리 | 공유 (같은 프로세스) | 복사 (별도 프로세스) |
| 시작 비용 | 낮음 | **높음** (fork/spawn) |
| 데이터 전달 | 직접 참조 | **직렬화 필요** (pickle) |
| GIL | 영향 받음 | 영향 없음 |

---

## 5. concurrent.futures: 통합 인터페이스

`ThreadPoolExecutor`와 `ProcessPoolExecutor`를 동일한 API로 사용합니다.

```python
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
import time

def fetch(url):
    time.sleep(1)  # I/O 시뮬레이션
    return f"Data from {url}"

def compute(n):
    return sum(i * i for i in range(n))

# I/O 바운드: ThreadPoolExecutor
with ThreadPoolExecutor(max_workers=10) as executor:
    urls = [f"url_{i}" for i in range(10)]
    futures = [executor.submit(fetch, url) for url in urls]
    results = [f.result() for f in futures]
    # ~1초 (10개 동시)

# CPU 바운드: ProcessPoolExecutor
with ProcessPoolExecutor(max_workers=4) as executor:
    futures = [executor.submit(compute, 10_000_000) for _ in range(4)]
    results = [f.result() for f in futures]

# map 인터페이스
with ProcessPoolExecutor(max_workers=4) as executor:
    results = list(executor.map(compute, [10_000_000] * 4))
```

### as_completed: 완료 순서대로 처리

```python
from concurrent.futures import as_completed

with ThreadPoolExecutor(max_workers=5) as executor:
    future_to_url = {executor.submit(fetch, url): url for url in urls}

    for future in as_completed(future_to_url):
        url = future_to_url[future]
        try:
            result = future.result()
            print(f"{url}: {result}")
        except Exception as e:
            print(f"{url} failed: {e}")
```

---

## 6. 동기화 프리미티브

### Event: 스레드 간 신호

```python
import threading

event = threading.Event()

def worker():
    print("Worker waiting...")
    event.wait()  # event가 set될 때까지 대기
    print("Worker proceeding!")

t = threading.Thread(target=worker)
t.start()
time.sleep(1)
event.set()  # worker에게 신호
t.join()
```

### Semaphore: 동시 접근 제한

```python
import threading

sem = threading.Semaphore(3)  # 최대 3개 동시 접근

def limited_worker(name):
    with sem:
        print(f"{name} acquired")
        time.sleep(1)
        print(f"{name} released")

threads = [threading.Thread(target=limited_worker, args=(f"W{i}",))
           for i in range(10)]
for t in threads:
    t.start()
# 3개씩 동시 실행
```

### Queue: 생산자-소비자 패턴

```python
import threading
import queue

q = queue.Queue(maxsize=10)

def producer():
    for i in range(20):
        q.put(i)
        print(f"Produced: {i}")
    q.put(None)  # 종료 신호

def consumer():
    while True:
        item = q.get()
        if item is None:
            break
        print(f"Consumed: {item}")
        q.task_done()

t1 = threading.Thread(target=producer)
t2 = threading.Thread(target=consumer)
t1.start(); t2.start()
t1.join(); t2.join()
```

---

## 7. 세 가지 모델 비교

| | threading | multiprocessing | asyncio |
|------|-----------|----------------|---------|
| **적합한 작업** | I/O 바운드 | CPU 바운드 | I/O 바운드 (대량) |
| **GIL 영향** | 받음 | 안 받음 | 받음 |
| **메모리 공유** | 쉬움 | 어려움 (pickle) | 쉬움 |
| **시작 비용** | 낮음 | 높음 | 매우 낮음 |
| **디버깅** | 어려움 (경합) | 중간 | 비교적 쉬움 |
| **확장성** | ~수십 스레드 | ~CPU 코어 수 | ~수만 코루틴 |

### 선택 가이드

```
작업이 I/O 바운드인가?
├── Yes → 동시 연결이 수백 개 이상?
│         ├── Yes → asyncio
│         └── No → threading
│
└── No (CPU 바운드) → multiprocessing
```

---

## 8. Python 3.12+: GIL 없는 Python

PEP 703으로 GIL을 선택적으로 비활성화할 수 있는 "free-threaded" 빌드가 실험적으로 도입되었습니다.

```bash
# GIL 비활성화 빌드 (실험적)
python3.13t  # "t" suffix = free-threaded
```

이것이 안정화되면 threading으로도 CPU 바운드 작업의 병렬 실행이 가능해집니다. 하지만 기존 C 확장의 스레드 안전성 보장이 필요하므로, 전환에는 시간이 걸릴 것입니다.

---

## 9. ML에서의 의미

### 데이터 로딩 병렬화

```python
from torch.utils.data import DataLoader

# num_workers > 0이면 multiprocessing으로 데이터 로딩
loader = DataLoader(
    dataset,
    batch_size=32,
    num_workers=4,      # 4개 프로세스로 병렬 로딩
    prefetch_factor=2,  # 미리 2배치씩 준비
)
```

### 모델 추론 서버

```python
# ThreadPoolExecutor로 동시 추론 요청 처리
with ThreadPoolExecutor(max_workers=4) as executor:
    futures = [executor.submit(model.predict, batch)
               for batch in batches]
    predictions = [f.result() for f in futures]
```

### 하이퍼파라미터 탐색

```python
from concurrent.futures import ProcessPoolExecutor

def train_with_params(params):
    model = create_model(**params)
    return train_and_evaluate(model)

param_grid = [{"lr": lr, "hidden": h}
              for lr in [0.01, 0.001]
              for h in [64, 128, 256]]

with ProcessPoolExecutor(max_workers=4) as executor:
    results = list(executor.map(train_with_params, param_grid))
```

---

## 핵심 정리

1. **GIL**은 CPython에서 한 번에 하나의 스레드만 Python 바이트코드를 실행하게 하며, CPU 바운드 작업의 스레드 병렬화를 차단한다.
2. **threading**은 I/O 바운드 작업에 적합하다. I/O 대기 중 GIL이 해제되므로 동시성 효과가 있다.
3. **multiprocessing**은 별도 프로세스를 사용하여 GIL을 우회하며, CPU 바운드 작업의 **진정한 병렬성**을 제공한다.
4. **`concurrent.futures`**는 `ThreadPoolExecutor`와 `ProcessPoolExecutor`의 **통합 인터페이스**를 제공한다.
5. 선택 기준: I/O 바운드 소규모 → threading, I/O 바운드 대규모 → asyncio, CPU 바운드 → multiprocessing.
