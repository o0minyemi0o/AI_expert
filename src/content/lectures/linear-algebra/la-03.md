# 고유값 분해

## 왜 고유값 분해를 배워야 하는가

PCA(주성분 분석)를 한 줄로 요약하면 "공분산 행렬의 고유값 분해"입니다. Google의 PageRank는 웹 그래프 행렬의 최대 고유벡터를 찾는 알고리즘이고, 양자역학의 관측 가능량은 에르미트 행렬의 고유값입니다. 고유값 분해는 행렬의 **본질적 구조를 드러내는 가장 강력한 도구**입니다.

이번 강의에서는 고유값과 고유벡터의 정의부터 시작해, 대각화 조건, 스펙트럴 정리, 그리고 ML에서의 핵심 응용까지 다룹니다.

---

## 1. 고유값과 고유벡터의 정의

정방행렬 $A \in \mathbb{R}^{n \times n}$에 대해, 다음을 만족하는 **영이 아닌** 벡터 $\mathbf{v}$와 스칼라 $\lambda$가 존재하면,

$$
A\mathbf{v} = \lambda \mathbf{v}
$$

$\lambda$를 **고유값(eigenvalue)**, $\mathbf{v}$를 대응하는 **고유벡터(eigenvector)**라 합니다.

> **핵심 직관**: 대부분의 벡터는 $A$를 곱하면 방향이 바뀝니다. 그런데 고유벡터는 방향이 변하지 않고, 크기만 $\lambda$배 됩니다. 고유벡터는 행렬 $A$가 작용하는 "자연스러운 축"입니다.

### 기하학적 의미

$A\mathbf{v} = \lambda\mathbf{v}$는 다음을 뜻합니다.

- $\lambda > 1$: 고유벡터 방향으로 **늘어남**
- $0 < \lambda < 1$: 고유벡터 방향으로 **줄어듦**
- $\lambda < 0$: 고유벡터 방향이 **반전됨**
- $\lambda = 0$: 고유벡터가 영 공간에 속함 (정보 소실)

---

## 2. 특성 다항식

$A\mathbf{v} = \lambda\mathbf{v}$를 정리하면,

$$
(A - \lambda I)\mathbf{v} = \mathbf{0}
$$

$\mathbf{v} \neq \mathbf{0}$인 해가 존재하려면 $(A - \lambda I)$가 **특이(singular)**해야 합니다. 즉,

$$
\det(A - \lambda I) = 0
$$

이것이 **특성 방정식(characteristic equation)**이고, 좌변을 $\lambda$에 대해 전개한 것이 **특성 다항식(characteristic polynomial)**입니다.

### 예시: 2×2 행렬

$$
A = \begin{pmatrix} 4 & 1 \\ 2 & 3 \end{pmatrix}
$$

$$
\det(A - \lambda I) = \det\begin{pmatrix} 4-\lambda & 1 \\ 2 & 3-\lambda \end{pmatrix} = (4-\lambda)(3-\lambda) - 2
$$

$$
= \lambda^2 - 7\lambda + 10 = (\lambda - 5)(\lambda - 2) = 0
$$

따라서 $\lambda_1 = 5$, $\lambda_2 = 2$입니다.

**고유벡터 구하기** ($\lambda_1 = 5$):

$$
(A - 5I)\mathbf{v} = \begin{pmatrix} -1 & 1 \\ 2 & -2 \end{pmatrix}\mathbf{v} = \mathbf{0} \implies \mathbf{v}_1 = \begin{pmatrix} 1 \\ 1 \end{pmatrix}
$$

**고유벡터 구하기** ($\lambda_2 = 2$):

$$
(A - 2I)\mathbf{v} = \begin{pmatrix} 2 & 1 \\ 2 & 1 \end{pmatrix}\mathbf{v} = \mathbf{0} \implies \mathbf{v}_2 = \begin{pmatrix} 1 \\ -2 \end{pmatrix}
$$

---

## 3. 대각화 (Diagonalization)

$n \times n$ 행렬 $A$가 $n$개의 선형 독립인 고유벡터를 가지면, $A$는 **대각화 가능(diagonalizable)**합니다.

고유벡터를 열로 모은 행렬 $P$와 고유값을 대각 원소로 한 행렬 $D$를 정의하면,

$$
P = \begin{pmatrix} \mathbf{v}_1 & \mathbf{v}_2 & \cdots & \mathbf{v}_n \end{pmatrix}, \quad D = \begin{pmatrix} \lambda_1 & & \\ & \ddots & \\ & & \lambda_n \end{pmatrix}
$$

$$
A = PDP^{-1}
$$

이것이 **고유값 분해(eigendecomposition)**입니다.

### 대각화가 유용한 이유

**행렬 거듭제곱**이 간단해집니다.

$$
A^k = PD^kP^{-1} = P\begin{pmatrix} \lambda_1^k & & \\ & \ddots & \\ & & \lambda_n^k \end{pmatrix}P^{-1}
$$

직접 $A$를 $k$번 곱하면 $O(n^3 k)$이지만, 대각화하면 $O(n^3 + nk)$입니다.

### 대각화 불가능한 경우

행렬 $A = \begin{pmatrix} 1 & 1 \\ 0 & 1 \end{pmatrix}$은 고유값 $\lambda = 1$ (중복도 2)이지만, 고유벡터는 $\begin{pmatrix} 1 \\ 0 \end{pmatrix}$ 하나뿐입니다. 선형 독립인 고유벡터가 부족하므로 대각화 불가능합니다.

> **핵심 직관**: 대각화란 "좋은 좌표계(고유벡터 기저)에서 보면 행렬이 단순히 각 축을 늘이고 줄이는 것"으로 바뀐다는 뜻입니다.

---

## 4. 대칭 행렬과 스펙트럴 정리

ML에서 가장 자주 만나는 행렬은 **대칭 행렬(symmetric matrix)** $A = A^T$입니다. 공분산 행렬, 그래프 라플라시안, 커널 행렬 등이 모두 대칭입니다.

### 스펙트럴 정리 (Spectral Theorem)

실대칭 행렬 $A = A^T \in \mathbb{R}^{n \times n}$에 대해,

1. **모든 고유값이 실수**이다.
2. **서로 다른 고유값의 고유벡터는 직교**한다.
3. $A$는 항상 **직교 대각화(orthogonal diagonalization)** 가능하다.

$$
A = Q\Lambda Q^T
$$

여기서 $Q$는 직교 행렬($Q^TQ = I$), $\Lambda$는 고유값 대각 행렬입니다.

| 일반 행렬 | 대칭 행렬 |
|-----------|-----------|
| $A = PDP^{-1}$ | $A = Q\Lambda Q^T$ |
| $P$: 가역이면 됨 | $Q$: 직교 행렬 |
| 고유값이 복소수일 수 있음 | 고유값이 항상 실수 |
| 대각화 불가능할 수 있음 | 항상 대각화 가능 |

> **핵심 직관**: 대칭 행렬은 "가장 잘 행동하는(nicely behaved)" 행렬입니다. 고유벡터가 자동으로 직교 좌표계를 이루므로, $Q^{-1} = Q^T$라는 계산적 이점도 있습니다.

### 스펙트럴 분해

$A = Q\Lambda Q^T$를 전개하면,

$$
A = \lambda_1 \mathbf{q}_1\mathbf{q}_1^T + \lambda_2 \mathbf{q}_2\mathbf{q}_2^T + \cdots + \lambda_n \mathbf{q}_n\mathbf{q}_n^T
$$

행렬 $A$는 고유값으로 가중된 **랭크-1 행렬의 합**입니다. 고유값이 큰 성분이 $A$의 주요 구조를 결정합니다.

---

## 5. 고유값의 성질

| 성질 | 수식 |
|------|------|
| 고유값의 합 = 대각합(trace) | $\sum_{i} \lambda_i = \text{tr}(A)$ |
| 고유값의 곱 = 행렬식 | $\prod_{i} \lambda_i = \det(A)$ |
| $A$가 가역 ↔ | 모든 $\lambda_i \neq 0$ |
| $A^{-1}$의 고유값 | $1/\lambda_i$ (고유벡터 동일) |
| $A^k$의 고유값 | $\lambda_i^k$ (고유벡터 동일) |
| $A + cI$의 고유값 | $\lambda_i + c$ (고유벡터 동일) |

이 성질들은 증명 없이 외울 게 아니라, $A\mathbf{v} = \lambda\mathbf{v}$에서 직접 유도할 수 있습니다. 예를 들어 $A^2\mathbf{v} = A(A\mathbf{v}) = A(\lambda\mathbf{v}) = \lambda(A\mathbf{v}) = \lambda^2\mathbf{v}$입니다.

---

## 6. NumPy로 확인하기

```python
import numpy as np

# 예시 행렬
A = np.array([[4, 1],
              [2, 3]])

# 고유값 분해
eigenvalues, eigenvectors = np.linalg.eig(A)
print(f"고유값: {eigenvalues}")        # [5. 2.]
print(f"고유벡터:\n{eigenvectors}")    # 열 벡터

# 검증: A @ v = λ * v
for i in range(len(eigenvalues)):
    v = eigenvectors[:, i]
    lam = eigenvalues[i]
    print(f"A @ v{i} = {A @ v}")
    print(f"λ{i} * v{i} = {lam * v}")
    print()

# 대각화 검증: A = P D P^{-1}
P = eigenvectors
D = np.diag(eigenvalues)
A_reconstructed = P @ D @ np.linalg.inv(P)
print(f"복원:\n{A_reconstructed}")  # 원래 A와 같음

# 대칭 행렬의 스펙트럴 정리
S = np.array([[2, 1],
              [1, 3]])
eigenvalues_s, Q = np.linalg.eigh(S)  # eigh: 대칭 행렬 전용
print(f"\n대칭 행렬 고유값: {eigenvalues_s}")
print(f"직교성 확인 (Q^T Q):\n{Q.T @ Q}")  # 단위행렬

# 행렬 거듭제곱
k = 10
A_power = P @ np.diag(eigenvalues**k) @ np.linalg.inv(P)
print(f"\nA^{k} (대각화):\n{A_power}")
print(f"A^{k} (직접 계산):\n{np.linalg.matrix_power(A, k)}")
```

---

## 7. ML에서의 의미

### PCA = 공분산 행렬의 고유값 분해

데이터 $X \in \mathbb{R}^{n \times d}$ (중심화 완료)의 공분산 행렬은,

$$
C = \frac{1}{n-1}X^TX
$$

$C$는 대칭이고 양의 반정부호(PSD)입니다. 스펙트럴 정리에 의해,

$$
C = Q\Lambda Q^T
$$

- **고유벡터 $\mathbf{q}_i$**: 주성분 방향 (분산이 최대인 축)
- **고유값 $\lambda_i$**: 해당 방향의 분산

상위 $k$개 고유벡터만 취하면 $d$차원 → $k$차원 차원 축소가 됩니다.

### 그래프 라플라시안과 스펙트럴 클러스터링

그래프의 라플라시안 행렬 $L = D - W$도 대칭 PSD입니다. $L$의 가장 작은 고유값들에 대응하는 고유벡터가 그래프의 클러스터 구조를 드러내며, 이것이 **스펙트럴 클러스터링**입니다.

### 수렴 분석

반복 알고리즘의 수렴 속도는 행렬의 고유값에 의해 결정됩니다. 경사하강법에서 Hessian 행렬의 최대 고유값과 최소 고유값의 비 $\kappa = \lambda_{\max}/\lambda_{\min}$이 **조건수**이며, 이 값이 클수록 수렴이 느립니다.

---

## 핵심 정리

1. **고유값 분해** $A\mathbf{v} = \lambda\mathbf{v}$는 행렬이 작용하는 "자연스러운 축"과 그 축 방향의 스케일링을 찾는 것이다.
2. **특성 다항식** $\det(A - \lambda I) = 0$으로 고유값을 구하고, $(A - \lambda I)\mathbf{v} = \mathbf{0}$을 풀어 고유벡터를 얻는다.
3. **대각화** $A = PDP^{-1}$은 거듭제곱, 지수함수 등의 계산을 극적으로 단순화한다.
4. **스펙트럴 정리**: 대칭 행렬은 항상 직교 대각화 가능하며, 고유값이 실수이고 고유벡터가 직교한다.
5. **PCA**는 공분산 행렬의 고유값 분해이며, 고유값이 큰 방향이 데이터의 주요 변동 방향이다.
