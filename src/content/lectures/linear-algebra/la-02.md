# 선형 변환

## 왜 선형 변환을 배워야 하는가

신경망의 각 층(layer)은 본질적으로 **선형 변환 + 비선형 활성화 함수**입니다. 입력 벡터 $\mathbf{x}$에 가중치 행렬 $W$를 곱하는 $W\mathbf{x}$가 바로 선형 변환입니다. 따라서 선형 변환의 성질을 이해하면, 신경망이 데이터를 어떻게 변형하는지 기하학적으로 볼 수 있게 됩니다.

이번 강의에서는 선형 변환의 엄밀한 정의, 행렬과의 관계, 기저 변환, 그리고 핵(kernel)과 상(image)의 구조를 다룹니다.

---

## 1. 선형 변환의 정의

벡터 공간 $V$에서 $W$로의 함수 $T: V \to W$가 **선형 변환(linear transformation)**이려면, 모든 $\mathbf{u}, \mathbf{v} \in V$와 스칼라 $c \in \mathbb{F}$에 대해 다음 두 조건을 만족해야 합니다.

$$
T(\mathbf{u} + \mathbf{v}) = T(\mathbf{u}) + T(\mathbf{v}) \quad \text{(가법성)}
$$

$$
T(c\mathbf{v}) = cT(\mathbf{v}) \quad \text{(동차성)}
$$

이 두 조건을 하나로 합치면 다음과 같습니다.

$$
T(c_1\mathbf{v}_1 + c_2\mathbf{v}_2) = c_1 T(\mathbf{v}_1) + c_2 T(\mathbf{v}_2)
$$

> **핵심 직관**: 선형 변환은 "직선을 직선으로, 원점을 원점으로" 보내는 함수입니다. 격자(grid)를 생각하면, 선형 변환 후에도 격자선이 평행하고 등간격을 유지합니다.

### 선형 변환이 아닌 예시

- $T(\mathbf{x}) = \mathbf{x} + \mathbf{b}$ (평행이동): $T(\mathbf{0}) = \mathbf{b} \neq \mathbf{0}$이므로 선형이 아닙니다.
- $T(x) = x^2$: $T(2x) = 4x^2 \neq 2x^2 = 2T(x)$이므로 선형이 아닙니다.
- ReLU $T(x) = \max(0, x)$: $T(-1 + 2) = 1$이지만 $T(-1) + T(2) = 0 + 2 = 2$이므로 선형이 아닙니다.

---

## 2. 행렬은 곧 선형 변환이다

유한 차원 벡터 공간에서 선형 변환과 행렬은 **일대일 대응**합니다. $T: \mathbb{R}^n \to \mathbb{R}^m$인 선형 변환은 항상 어떤 행렬 $A \in \mathbb{R}^{m \times n}$로 표현됩니다.

$$
T(\mathbf{x}) = A\mathbf{x}
$$

행렬 $A$의 각 열은 표준 기저 벡터의 **상(image)**입니다.

$$
A = \begin{pmatrix} T(\mathbf{e}_1) & T(\mathbf{e}_2) & \cdots & T(\mathbf{e}_n) \end{pmatrix}
$$

### 예시: 2D 선형 변환들

| 변환 | 행렬 | 기하학적 의미 |
|------|------|--------------|
| 항등 | $\begin{pmatrix} 1 & 0 \\ 0 & 1 \end{pmatrix}$ | 아무 변화 없음 |
| 스케일링 | $\begin{pmatrix} s_x & 0 \\ 0 & s_y \end{pmatrix}$ | 축 방향으로 확대/축소 |
| 반시계 회전 ($\theta$) | $\begin{pmatrix} \cos\theta & -\sin\theta \\ \sin\theta & \cos\theta \end{pmatrix}$ | 원점 중심 회전 |
| 전단 (shear) | $\begin{pmatrix} 1 & k \\ 0 & 1 \end{pmatrix}$ | x축 방향으로 기울임 |
| 반사 (x축) | $\begin{pmatrix} 1 & 0 \\ 0 & -1 \end{pmatrix}$ | x축 대칭 |

> **핵심 직관**: 행렬의 열 벡터를 보면 변환이 무엇인지 즉시 알 수 있습니다. 첫 번째 열은 $(1, 0)$이 어디로 가는지, 두 번째 열은 $(0, 1)$이 어디로 가는지를 나타냅니다.

---

## 3. 기저 변환 (Change of Basis)

같은 선형 변환이라도 **어떤 기저에서 바라보느냐**에 따라 행렬이 달라집니다.

기저 $B = \{\mathbf{b}_1, \ldots, \mathbf{b}_n\}$에서 표준 기저로 바꾸는 행렬을 $P$라 하면,

$$
P = \begin{pmatrix} \mathbf{b}_1 & \mathbf{b}_2 & \cdots & \mathbf{b}_n \end{pmatrix}
$$

표준 기저에서의 행렬 $A$가 기저 $B$에서는 다음처럼 표현됩니다.

$$
A_B = P^{-1}AP
$$

### 왜 중요한가

대각화(diagonalization)가 바로 이 기저 변환입니다. 고유벡터 기저로 바꾸면 행렬이 대각 행렬이 됩니다. 복잡한 변환을 "좋은 기저"에서 바라보면 단순해지는 것입니다.

### 예시

$$
A = \begin{pmatrix} 2 & 1 \\ 0 & 3 \end{pmatrix}, \quad
P = \begin{pmatrix} 1 & 1 \\ 0 & 1 \end{pmatrix}
$$

$$
P^{-1} = \begin{pmatrix} 1 & -1 \\ 0 & 1 \end{pmatrix}
$$

$$
A_B = P^{-1}AP = \begin{pmatrix} 1 & -1 \\ 0 & 1 \end{pmatrix}\begin{pmatrix} 2 & 1 \\ 0 & 3 \end{pmatrix}\begin{pmatrix} 1 & 1 \\ 0 & 1 \end{pmatrix} = \begin{pmatrix} 2 & 0 \\ 0 & 3 \end{pmatrix}
$$

기저를 잘 고르면 행렬이 대각화되어 해석이 쉬워집니다.

---

## 4. 핵과 상 (Kernel & Image)

선형 변환 $T: V \to W$에 대해 두 가지 핵심 부분 공간이 존재합니다.

### 핵 (Kernel, Null Space)

$$
\ker(T) = \{\mathbf{v} \in V : T(\mathbf{v}) = \mathbf{0}\}
$$

$T$에 의해 **영벡터로 짓눌리는** 모든 벡터의 집합입니다. $\ker(T)$는 $V$의 부분 공간입니다.

### 상 (Image, Range)

$$
\text{Im}(T) = \{T(\mathbf{v}) : \mathbf{v} \in V\}
$$

$T$가 **실제로 도달할 수 있는** 모든 벡터의 집합입니다. $\text{Im}(T)$는 $W$의 부분 공간입니다.

> **핵심 직관**: 핵(kernel)은 "변환이 잃어버리는 정보"이고, 상(image)은 "변환이 만들어내는 결과의 범위"입니다.

### 단사, 전사, 동형

| 성질 | 조건 | 의미 |
|------|------|------|
| 단사(injective) | $\ker(T) = \{\mathbf{0}\}$ | 정보 손실 없음 |
| 전사(surjective) | $\text{Im}(T) = W$ | 모든 출력 가능 |
| 동형(isomorphism) | 단사 + 전사 | 완전한 일대일 대응 |

---

## 5. Rank-Nullity 정리

선형 변환 $T: V \to W$에 대한 가장 근본적인 정리입니다.

$$
\dim(\ker(T)) + \dim(\text{Im}(T)) = \dim(V)
$$

행렬 $A \in \mathbb{R}^{m \times n}$의 언어로 다시 쓰면,

$$
\text{nullity}(A) + \text{rank}(A) = n
$$

> **핵심 직관**: 입력 공간의 차원은 "잃어버리는 차원(nullity)" + "보존되는 차원(rank)"으로 나뉩니다. 둘의 합은 항상 입력 차원과 같습니다.

### 예시

$$
A = \begin{pmatrix} 1 & 2 & 1 \\ 2 & 4 & 2 \end{pmatrix}
$$

$\text{rank}(A) = 1$ (두 번째 행이 첫 번째의 2배, 세 번째 열이 첫 번째와 같음)

$$
\text{nullity}(A) = 3 - 1 = 2
$$

영 공간의 기저를 구하면,

$$
A\mathbf{x} = \mathbf{0} \implies x_1 = -2x_2 - x_3
$$

$$
\ker(A) = \text{span}\left\{\begin{pmatrix} -2 \\ 1 \\ 0 \end{pmatrix}, \begin{pmatrix} -1 \\ 0 \\ 1 \end{pmatrix}\right\}
$$

차원 확인: $\text{nullity}(2) + \text{rank}(1) = 3 = n$ ✓

---

## 6. 합성과 역변환

### 합성 (Composition)

두 선형 변환 $T_1: U \to V$와 $T_2: V \to W$의 합성 $T_2 \circ T_1$은 행렬의 곱에 대응합니다.

$$
(T_2 \circ T_1)(\mathbf{x}) = B(A\mathbf{x}) = (BA)\mathbf{x}
$$

행렬 곱의 순서가 **오른쪽에서 왼쪽**인 이유가 여기에 있습니다. $A$를 먼저 적용하고, $B$를 나중에 적용합니다.

### 역변환 (Inverse)

$T$가 동형사상(isomorphism)이면 역변환 $T^{-1}$이 존재합니다. 행렬로는 $A^{-1}$에 대응합니다.

역행렬이 존재할 조건:
- $\det(A) \neq 0$
- $\text{rank}(A) = n$ (full rank)
- $\ker(A) = \{\mathbf{0}\}$

이 세 조건은 모두 동치입니다.

---

## 7. NumPy로 확인하기

```python
import numpy as np

# 회전 변환 (45도)
theta = np.pi / 4
R = np.array([
    [np.cos(theta), -np.sin(theta)],
    [np.sin(theta),  np.cos(theta)]
])

# (1, 0)을 45도 회전
v = np.array([1, 0])
print(f"회전 결과: {R @ v}")  # [0.707, 0.707]

# 기저 변환
A = np.array([[2, 1], [0, 3]])
P = np.array([[1, 1], [0, 1]])
P_inv = np.linalg.inv(P)
A_B = P_inv @ A @ P
print(f"기저 변환 후:\n{A_B}")  # [[2, 0], [0, 3]]

# Rank-Nullity 확인
B = np.array([[1, 2, 1], [2, 4, 2]])
rank = np.linalg.matrix_rank(B)
n = B.shape[1]
nullity = n - rank
print(f"rank={rank}, nullity={nullity}, n={n}")  # 1, 2, 3

# 영 공간 구하기 (SVD 활용)
U, S, Vt = np.linalg.svd(B)
null_mask = np.zeros(B.shape[1], dtype=bool)
null_mask[rank:] = True  # 특이값이 0인 부분
null_space = Vt[null_mask]
print(f"영 공간 기저:\n{null_space}")
```

---

## 8. ML에서의 의미

### 신경망의 각 층은 선형 변환이다

완전연결 층(fully connected layer)의 연산은 다음과 같습니다.

$$
\mathbf{h} = \sigma(W\mathbf{x} + \mathbf{b})
$$

여기서 $W\mathbf{x}$가 선형 변환, $\sigma$가 비선형 활성화 함수입니다. 선형 변환만으로는 아무리 층을 쌓아도 하나의 행렬 곱과 같습니다.

$$
W_3(W_2(W_1\mathbf{x})) = (W_3 W_2 W_1)\mathbf{x}
$$

비선형 함수 없이는 깊은 네트워크가 의미 없다는 것을 합성의 성질이 증명합니다.

### 차원 축소와 Rank

$W \in \mathbb{R}^{m \times n}$에서 $m < n$이면, 입력의 차원이 줄어듭니다. 이는 정보 압축이며, $\ker(W) \neq \{\mathbf{0}\}$이므로 반드시 정보 손실이 발생합니다. 오토인코더(autoencoder)의 병목층이 바로 이 원리입니다.

### Attention은 동적 선형 변환

Transformer의 self-attention에서 $\text{softmax}(QK^T/\sqrt{d_k})V$를 보면, $\text{softmax}(\cdots)$는 데이터에 따라 달라지는 "동적 행렬"입니다. 즉, 입력에 따라 선형 변환 자체가 바뀌는 구조입니다.

---

## 핵심 정리

1. **선형 변환**은 가법성과 동차성을 만족하는 함수이며, 유한 차원에서는 **행렬**과 일대일 대응한다.
2. **행렬의 열**은 표준 기저 벡터의 상(image)이다 — 열을 보면 변환을 알 수 있다.
3. **기저 변환** $A_B = P^{-1}AP$는 같은 변환을 다른 좌표계에서 표현한 것이며, 대각화의 핵심이다.
4. **Rank-Nullity 정리**: $\text{nullity} + \text{rank} = n$ — 잃어버리는 차원과 보존되는 차원의 합은 항상 입력 차원이다.
5. 신경망의 각 층은 선형 변환이며, **비선형 활성화 없이는** 층을 아무리 쌓아도 단일 선형 변환과 동일하다.
