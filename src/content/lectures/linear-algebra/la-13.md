# 딥러닝에서의 응용

## 왜 선형대수의 딥러닝 응용을 배워야 하는가

지금까지 배운 선형대수가 딥러닝에서 어떻게 쓰이는지 구체적으로 연결하는 강의입니다. Xavier/Kaiming 초기화의 수학적 유도, Attention 메커니즘의 행렬 해석, LoRA의 저랭크 구조, 그리고 임베딩 레이어의 본질을 선형대수의 언어로 설명합니다.

---

## 1. 가중치 초기화: 분산 보존 조건

### 문제: 왜 초기화가 중요한가

$L$개의 층을 가진 네트워크에서 $\mathbf{h}_l = W_l\mathbf{h}_{l-1}$이면(활성화 함수 무시),

$$
\mathbf{h}_L = W_L W_{L-1} \cdots W_1 \mathbf{x}
$$

각 층에서 분산이 증폭되면 출력이 폭발하고, 감소하면 소실됩니다.

### Xavier/Glorot 초기화

**조건**: 각 층의 출력 분산이 입력 분산과 같아야 합니다.

$W \in \mathbb{R}^{n_{\text{out}} \times n_{\text{in}}}$이고, 입력 $x_j$가 독립이며 $\text{Var}(x_j) = v$이면,

$$
\text{Var}(h_i) = \text{Var}\left(\sum_{j=1}^{n_\text{in}} w_{ij}x_j\right) = n_{\text{in}} \cdot \text{Var}(w_{ij}) \cdot v
$$

$\text{Var}(h_i) = v$가 되려면,

$$
\text{Var}(w_{ij}) = \frac{1}{n_{\text{in}}}
$$

역전파에서의 분산 보존까지 고려하면(fan-in과 fan-out의 조화),

$$
\text{Var}(w_{ij}) = \frac{2}{n_{\text{in}} + n_{\text{out}}}
$$

이것이 **Xavier 초기화**입니다.

$$
W_{ij} \sim \mathcal{N}\left(0, \frac{2}{n_{\text{in}} + n_{\text{out}}}\right) \quad \text{또는} \quad \text{Uniform}\left(-\sqrt{\frac{6}{n_{\text{in}}+n_{\text{out}}}}, \sqrt{\frac{6}{n_{\text{in}}+n_{\text{out}}}}\right)
$$

### Kaiming/He 초기화

ReLU 활성화를 사용하면 음수가 0이 되므로, 분산이 절반으로 줄어듭니다.

$$
\text{Var}(h_i) = \frac{n_{\text{in}}}{2} \cdot \text{Var}(w_{ij}) \cdot v
$$

따라서,

$$
\text{Var}(w_{ij}) = \frac{2}{n_{\text{in}}}
$$

이것이 **Kaiming 초기화**입니다.

> **핵심 직관**: 초기화는 "신호의 분산이 층을 통과할 때 보존되도록" 가중치의 스케일을 조절하는 것입니다. 본질적으로 분산 분석, 즉 선형대수입니다.

---

## 2. Attention 메커니즘의 행렬 해석

Transformer의 self-attention을 행렬 연산으로 분해합니다.

### 기본 수식

$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
$$

여기서 $Q, K, V \in \mathbb{R}^{T \times d_k}$ (시퀀스 길이 $T$, 차원 $d_k$)

### 행렬별 역할

| 행렬 연산 | 크기 | 의미 |
|-----------|------|------|
| $QK^T$ | $T \times T$ | 토큰 간 유사도 (Gram 행렬의 변형) |
| $\text{softmax}(\cdot)$ | $T \times T$ | 확률 분포로 정규화 (각 행의 합 = 1) |
| $\text{softmax}(\cdot) \cdot V$ | $T \times d_k$ | 가중 평균 — 관련 토큰의 정보를 혼합 |

### 투영 관점

$Q = XW^Q$, $K = XW^K$, $V = XW^V$이므로,

$$
QK^T = XW^Q(W^K)^TX^T
$$

$W^Q(W^K)^T$는 $d \times d$ 행렬로, 입력 공간에서의 **유사도 함수를 학습**하는 것입니다. 이는 Mahalanobis 거리 $\mathbf{x}^TM\mathbf{y}$에서 $M = W^Q(W^K)^T$를 학습하는 것과 같습니다.

### $\sqrt{d_k}$ 스케일링의 이유

$Q$와 $K$의 원소가 $\mathcal{N}(0, 1)$이면, $\mathbf{q}^T\mathbf{k} = \sum_{i=1}^{d_k} q_ik_i$의 분산은 $d_k$입니다.

$$
\text{Var}(\mathbf{q}^T\mathbf{k}) = d_k
$$

$d_k$로 나누면 분산이 1이 되어 softmax가 너무 뾰족해지는 것을 방지합니다.

---

## 3. LoRA: 저랭크 적응

### 동기

GPT-3의 파라미터는 1750억 개입니다. 전체를 파인튜닝하려면 막대한 GPU 메모리가 필요합니다. **LoRA (Low-Rank Adaptation)**는 가중치 업데이트를 저랭크로 제한합니다.

### 수식

기존 가중치 $W_0 \in \mathbb{R}^{d \times d}$를 동결하고, 업데이트를 다음과 같이 분해합니다.

$$
W = W_0 + \Delta W = W_0 + BA
$$

- $B \in \mathbb{R}^{d \times r}$, $A \in \mathbb{R}^{r \times d}$, $r \ll d$

### 왜 작동하는가

Aghajanyan et al. (2020)의 연구에 따르면, 사전학습된 모델의 파인튜닝 시 가중치 변화 $\Delta W$는 **내재 차원(intrinsic dimensionality)**이 낮습니다. 즉, $\Delta W$가 사실상 저랭크입니다.

SVD의 Eckart-Young 정리가 이론적 근거입니다: 랭크-$r$ 행렬이 $\Delta W$의 최적 근사입니다.

### 파라미터 절약

| 방법 | 파라미터 수 | $d=4096$, $r=16$ 예시 |
|------|-----------|---------------------|
| 전체 파인튜닝 | $d^2$ | 16,777,216 |
| LoRA | $2dr$ | 131,072 (**128배 절약**) |

```python
import torch
import torch.nn as nn

class LoRALinear(nn.Module):
    def __init__(self, in_features, out_features, rank=16):
        super().__init__()
        self.W = nn.Linear(in_features, out_features, bias=False)
        self.W.weight.requires_grad_(False)  # 동결

        self.A = nn.Linear(in_features, rank, bias=False)
        self.B = nn.Linear(rank, out_features, bias=False)
        nn.init.kaiming_normal_(self.A.weight)
        nn.init.zeros_(self.B.weight)

    def forward(self, x):
        return self.W(x) + self.B(self.A(x))

# 사용
layer = LoRALinear(4096, 4096, rank=16)
total = sum(p.numel() for p in layer.parameters() if p.requires_grad)
print(f"학습 파라미터: {total:,}")  # 131,072
```

---

## 4. 임베딩 레이어 = 행렬 곱

임베딩 레이어 $E \in \mathbb{R}^{V \times d}$ (어휘 크기 $V$, 임베딩 차원 $d$)에서 토큰 $i$의 임베딩을 꺼내는 것은,

$$
\mathbf{e}_i = E^T\mathbf{o}_i
$$

여기서 $\mathbf{o}_i$는 원-핫 벡터입니다. 즉, 임베딩 룩업은 **행렬-벡터 곱의 특수한 경우**입니다.

출력 층에서 로짓(logit)을 계산하는 것은,

$$
\text{logits} = E\mathbf{h}
$$

이는 은닉 벡터 $\mathbf{h}$와 모든 단어 벡터의 **내적(유사도)**을 구하는 것입니다. 가장 유사한 단어가 높은 확률을 얻습니다.

> **핵심 직관**: 임베딩과 최종 분류 층은 같은 행렬 $E$를 공유하기도 합니다 (weight tying). 이는 "의미 공간에서의 거리"를 입력과 출력 모두에 일관되게 사용하는 것입니다.

---

## 5. 배치 정규화의 선형대수

BatchNorm은 미니배치의 통계를 이용합니다.

$$
\hat{x}_i = \frac{x_i - \mu_B}{\sqrt{\sigma_B^2 + \epsilon}}
$$

이를 벡터 형태로 쓰면, 중심화(평균 빼기)는 **투영**과 관련됩니다.

$$
\hat{\mathbf{x}} = \left(I - \frac{1}{n}\mathbf{1}\mathbf{1}^T\right)\mathbf{x} \cdot \frac{1}{\sigma}
$$

$I - \frac{1}{n}\mathbf{1}\mathbf{1}^T$는 $\mathbf{1}$에 직교하는 부분 공간으로의 투영 행렬입니다. la-08에서 배운 투영의 직접적인 응용입니다.

---

## 핵심 정리

1. **Xavier/Kaiming 초기화**는 각 층에서 분산이 보존되도록 가중치 스케일을 설정하며, 본질적으로 분산 분석이다.
2. **Attention**의 $QK^T$는 학습된 유사도 행렬이며, $\sqrt{d_k}$ 스케일링은 내적의 분산을 1로 맞추는 것이다.
3. **LoRA**는 가중치 업데이트를 $\Delta W = BA$ (랭크 $r$)로 제한하며, Eckart-Young 정리가 이론적 근거이다.
4. **임베딩 룩업**은 원-핫 벡터와의 행렬 곱이고, 출력 로짓은 임베딩과의 내적(유사도)이다.
5. BatchNorm의 중심화는 **부분 공간 투영**이며, 선형대수의 기하학적 해석이 직접 적용된다.
