# 행렬 미적분

## 왜 행렬 미적분을 배워야 하는가

신경망의 학습은 결국 **손실 함수의 그래디언트를 구하는 것**입니다. 스칼라 하나를 수천만 개의 파라미터로 미분해야 합니다. 고등학교에서 배운 미분은 $f: \mathbb{R} \to \mathbb{R}$이지만, 실전에서는 $f: \mathbb{R}^{n} \to \mathbb{R}$이거나 $f: \mathbb{R}^{m \times n} \to \mathbb{R}$입니다.

행렬 미적분(matrix calculus)은 이러한 다변수/행렬 함수의 미분을 체계적으로 다루는 도구입니다. 역전파(backpropagation)의 수학적 본질이 바로 여기에 있습니다.

---

## 1. 그래디언트: 스칼라를 벡터로 미분

스칼라 함수 $f: \mathbb{R}^n \to \mathbb{R}$에 대해, **그래디언트(gradient)**는 각 성분에 대한 편미분을 모은 벡터입니다.

$$
\nabla_{\mathbf{x}} f = \frac{\partial f}{\partial \mathbf{x}} = \begin{pmatrix} \frac{\partial f}{\partial x_1} \\ \frac{\partial f}{\partial x_2} \\ \vdots \\ \frac{\partial f}{\partial x_n} \end{pmatrix} \in \mathbb{R}^n
$$

> **핵심 직관**: 그래디언트는 $f$가 **가장 빠르게 증가하는 방향**을 가리킵니다. 경사하강법은 이 반대 방향으로 이동합니다.

### 자주 쓰는 공식

| 함수 $f(\mathbf{x})$ | 그래디언트 $\nabla_\mathbf{x} f$ |
|-----------------------|----------------------------------|
| $\mathbf{a}^T\mathbf{x}$ | $\mathbf{a}$ |
| $\mathbf{x}^T\mathbf{x} = \|\mathbf{x}\|^2$ | $2\mathbf{x}$ |
| $\mathbf{x}^TA\mathbf{x}$ | $(A + A^T)\mathbf{x}$ |
| $\mathbf{x}^TA\mathbf{x}$ ($A$ 대칭) | $2A\mathbf{x}$ |

이 공식들은 암기하는 것보다 유도할 줄 아는 것이 중요합니다.

### 유도 예시: $f(\mathbf{x}) = \mathbf{x}^TA\mathbf{x}$

$f = \sum_i\sum_j x_i A_{ij} x_j$이므로,

$$
\frac{\partial f}{\partial x_k} = \sum_j A_{kj}x_j + \sum_i x_i A_{ik} = (A\mathbf{x})_k + (A^T\mathbf{x})_k
$$

벡터로 모으면,

$$
\nabla_\mathbf{x} f = A\mathbf{x} + A^T\mathbf{x} = (A + A^T)\mathbf{x}
$$

$A$가 대칭이면 $A = A^T$이므로 $\nabla f = 2A\mathbf{x}$가 됩니다.

---

## 2. Jacobian 행렬: 벡터를 벡터로 미분

벡터 함수 $\mathbf{f}: \mathbb{R}^n \to \mathbb{R}^m$에 대해, **Jacobian 행렬**은 모든 편미분을 모은 $m \times n$ 행렬입니다.

$$
J = \frac{\partial \mathbf{f}}{\partial \mathbf{x}} = \begin{pmatrix} \frac{\partial f_1}{\partial x_1} & \cdots & \frac{\partial f_1}{\partial x_n} \\ \vdots & \ddots & \vdots \\ \frac{\partial f_m}{\partial x_1} & \cdots & \frac{\partial f_m}{\partial x_n} \end{pmatrix} \in \mathbb{R}^{m \times n}
$$

### 자주 쓰는 공식

| 함수 $\mathbf{f}(\mathbf{x})$ | Jacobian |
|-------------------------------|----------|
| $A\mathbf{x}$ | $A$ |
| $\mathbf{x}$ | $I$ |
| $\mathbf{x} \odot \mathbf{y}$ (원소별 곱, $\mathbf{y}$ 상수) | $\text{diag}(\mathbf{y})$ |

> **핵심 직관**: Jacobian의 $(i, j)$ 원소는 "출력의 $i$번째 성분이 입력의 $j$번째 성분에 얼마나 민감한가"를 나타냅니다.

---

## 3. Hessian 행렬: 그래디언트를 다시 미분

스칼라 함수 $f: \mathbb{R}^n \to \mathbb{R}$의 **Hessian**은 그래디언트의 Jacobian, 즉 이차 편미분의 행렬입니다.

$$
H = \nabla^2 f = \begin{pmatrix} \frac{\partial^2 f}{\partial x_1^2} & \frac{\partial^2 f}{\partial x_1 \partial x_2} & \cdots \\ \frac{\partial^2 f}{\partial x_2 \partial x_1} & \frac{\partial^2 f}{\partial x_2^2} & \cdots \\ \vdots & \vdots & \ddots \end{pmatrix} \in \mathbb{R}^{n \times n}
$$

Hessian은 항상 **대칭 행렬**입니다 ($f$가 충분히 매끄러우면).

| Hessian의 성질 | 의미 |
|----------------|------|
| 양의 정부호 (PD) | 극소점 (볼록 함수의 최소점) |
| 음의 정부호 (ND) | 극대점 |
| 부정부호 (indefinite) | 안장점 (saddle point) |

> **핵심 직관**: 그래디언트가 "어느 방향으로 가야 하는가"를 알려주면, Hessian은 "그 방향의 곡률이 얼마인가"를 알려줍니다. Newton's method는 Hessian을 이용해 더 정확한 걸음을 내딛습니다.

---

## 4. 행렬에 대한 미분

스칼라 함수 $f: \mathbb{R}^{m \times n} \to \mathbb{R}$을 행렬 $X$로 미분하면,

$$
\frac{\partial f}{\partial X} = \begin{pmatrix} \frac{\partial f}{\partial X_{11}} & \cdots & \frac{\partial f}{\partial X_{1n}} \\ \vdots & \ddots & \vdots \\ \frac{\partial f}{\partial X_{m1}} & \cdots & \frac{\partial f}{\partial X_{mn}} \end{pmatrix}
$$

### 자주 쓰는 공식

| 함수 $f(X)$ | $\frac{\partial f}{\partial X}$ |
|-------------|--------------------------------|
| $\text{tr}(X)$ | $I$ |
| $\text{tr}(AX)$ | $A^T$ |
| $\text{tr}(X^TAX)$ | $(A + A^T)X$ |
| $\text{tr}(AXB)$ | $A^TXB^T$... 아니, $A^TB^T$... |

정확한 공식을 외우기보다, **trace trick**을 사용하는 것이 실전적입니다.

### Trace Trick

스칼라는 자신의 trace와 같습니다: $f = \text{tr}(f)$. 이 성질과 trace의 순환 성질 $\text{tr}(ABC) = \text{tr}(CAB) = \text{tr}(BCA)$를 조합하면, 복잡한 행렬 미분을 기계적으로 풀 수 있습니다.

---

## 5. 체인 룰 (Chain Rule)

### 벡터 체인 룰

$\mathbf{y} = g(\mathbf{x})$이고 $f = h(\mathbf{y})$이면,

$$
\frac{\partial f}{\partial \mathbf{x}} = \frac{\partial f}{\partial \mathbf{y}} \cdot \frac{\partial \mathbf{y}}{\partial \mathbf{x}} = J_g^T \nabla_\mathbf{y} f
$$

여기서 $J_g$는 $g$의 Jacobian입니다.

### 역전파의 본질

$L$개의 층으로 이루어진 신경망에서, 출력에서 입력으로 체인 룰을 반복 적용하면,

$$
\frac{\partial \mathcal{L}}{\partial W_1} = \frac{\partial \mathcal{L}}{\partial \mathbf{h}_L} \cdot \frac{\partial \mathbf{h}_L}{\partial \mathbf{h}_{L-1}} \cdots \frac{\partial \mathbf{h}_2}{\partial \mathbf{h}_1} \cdot \frac{\partial \mathbf{h}_1}{\partial W_1}
$$

각 $\frac{\partial \mathbf{h}_{l+1}}{\partial \mathbf{h}_l}$이 Jacobian 행렬입니다. 이것이 **역전파(backpropagation)**입니다.

> **핵심 직관**: 역전파는 "행렬 체인 룰을 뒤에서부터 효율적으로 계산하는 알고리즘"입니다. 앞에서부터 계산하면(forward mode) 비효율적이지만, 뒤에서부터 계산하면(reverse mode) 한 번의 패스로 모든 파라미터의 그래디언트를 얻습니다.

---

## 6. MSE 손실의 그래디언트 유도

가장 기본적인 예시로, 선형 회귀의 MSE 손실을 완전히 유도해 봅시다.

$$
\mathcal{L}(\mathbf{w}) = \|X\mathbf{w} - \mathbf{y}\|^2 = (X\mathbf{w} - \mathbf{y})^T(X\mathbf{w} - \mathbf{y})
$$

전개하면,

$$
\mathcal{L} = \mathbf{w}^TX^TX\mathbf{w} - 2\mathbf{y}^TX\mathbf{w} + \mathbf{y}^T\mathbf{y}
$$

각 항을 미분합니다.

$$
\frac{\partial}{\partial \mathbf{w}}(\mathbf{w}^TX^TX\mathbf{w}) = 2X^TX\mathbf{w} \quad \text{($X^TX$는 대칭)}
$$

$$
\frac{\partial}{\partial \mathbf{w}}(2\mathbf{y}^TX\mathbf{w}) = 2X^T\mathbf{y}
$$

$$
\frac{\partial}{\partial \mathbf{w}}(\mathbf{y}^T\mathbf{y}) = \mathbf{0}
$$

합치면,

$$
\nabla_\mathbf{w}\mathcal{L} = 2X^TX\mathbf{w} - 2X^T\mathbf{y} = 2X^T(X\mathbf{w} - \mathbf{y})
$$

그래디언트를 0으로 놓으면 **정규방정식(normal equation)**을 얻습니다.

$$
X^TX\mathbf{w} = X^T\mathbf{y} \implies \mathbf{w}^* = (X^TX)^{-1}X^T\mathbf{y}
$$

> **핵심 직관**: 선형 회귀의 해석적 해(closed-form solution)는 행렬 미적분의 직접적 결과입니다. $(X^TX)^{-1}X^T$는 바로 의사역행렬 $X^+$입니다.

---

## 7. NumPy로 확인하기

```python
import numpy as np

# 선형 회귀 그래디언트 검증
np.random.seed(42)
n, d = 100, 5
X = np.random.randn(n, d)
w_true = np.array([1, -2, 3, 0.5, -1])
y = X @ w_true + 0.1 * np.random.randn(n)

# 해석적 해 (정규방정식)
w_analytical = np.linalg.solve(X.T @ X, X.T @ y)
print(f"해석적 해: {w_analytical.round(3)}")

# 경사하강법으로 검증
w = np.zeros(d)
lr = 0.01
for i in range(1000):
    grad = 2 * X.T @ (X @ w - y) / n  # MSE 그래디언트
    w = w - lr * grad

print(f"경사하강법: {w.round(3)}")

# Jacobian 수치 검증
def f(x):
    return np.array([x[0]**2 + x[1], x[0]*x[1]])

x0 = np.array([1.0, 2.0])
eps = 1e-7
J_numerical = np.zeros((2, 2))
for j in range(2):
    x_plus = x0.copy()
    x_plus[j] += eps
    J_numerical[:, j] = (f(x_plus) - f(x0)) / eps

J_analytical = np.array([
    [2*x0[0], 1],     # ∂f1/∂x1, ∂f1/∂x2
    [x0[1], x0[0]]    # ∂f2/∂x1, ∂f2/∂x2
])

print(f"\nJacobian (수치): \n{J_numerical.round(4)}")
print(f"Jacobian (해석): \n{J_analytical}")
```

---

## 8. ML에서의 의미

### 자동 미분 (Automatic Differentiation)

PyTorch, JAX 같은 프레임워크는 행렬 미적분을 자동으로 수행합니다. 내부적으로는 계산 그래프의 각 노드에서 Jacobian을 구하고, 체인 룰로 연결하는 것입니다. 행렬 미적분을 모르면 자동 미분의 결과를 검증할 수 없습니다.

### Hessian과 최적화

2차 최적화(Newton's method, L-BFGS)는 Hessian 정보를 활용합니다. Hessian의 고유값이 모두 양수(PD)이면 현재 점이 극소점 근처이고, 음수 고유값이 있으면 안장점입니다. 대규모 신경망에서는 Hessian을 직접 구할 수 없어 근사 방법(K-FAC, Fisher information matrix)을 사용합니다.

### 기울기 소실/폭발

체인 룰에서 Jacobian을 반복 곱하면, Jacobian의 특이값에 따라 그래디언트가 기하급수적으로 줄거나(소실) 커집니다(폭발). 이것이 RNN에서 심각한 문제였고, 잔차 연결(residual connection)과 적절한 초기화가 해결책입니다.

---

## 핵심 정리

1. **그래디언트** $\nabla f$는 함수가 가장 빠르게 증가하는 방향이며, 경사하강법의 기반이다.
2. **Jacobian**은 벡터 함수의 "국소 선형 근사"이며, 역전파에서 각 층의 미분을 나타낸다.
3. **Hessian**은 이차 미분 정보로, 극소점/안장점 판별과 2차 최적화에 사용된다.
4. **체인 룰의 행렬 버전**이 역전파의 수학적 본질이며, Jacobian의 반복 곱으로 표현된다.
5. MSE 손실의 그래디언트 유도 → 정규방정식 $\mathbf{w}^* = (X^TX)^{-1}X^T\mathbf{y}$는 행렬 미적분의 대표적 응용이다.
