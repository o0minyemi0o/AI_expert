# 텐서 대수

## 왜 텐서 대수를 배워야 하는가

딥러닝 프레임워크에서 모든 데이터는 **텐서(tensor)**로 표현됩니다. 이미지 배치는 4D 텐서 `(batch, channel, height, width)`, Transformer의 attention score는 `(batch, heads, seq_len, seq_len)`입니다. `np.einsum`과 `torch.einsum`을 자유자재로 쓰려면 텐서 연산의 원리를 이해해야 합니다.

---

## 1. 텐서란 무엇인가

### 프로그래밍 관점

텐서는 **다차원 배열(multidimensional array)**입니다.

| 차수(order) | 이름 | 예시 |
|-------------|------|------|
| 0 | 스칼라 | 손실값 $\mathcal{L} = 0.35$ |
| 1 | 벡터 | 특성 벡터 $\mathbf{x} \in \mathbb{R}^d$ |
| 2 | 행렬 | 가중치 $W \in \mathbb{R}^{m \times n}$ |
| 3 | 3차 텐서 | 배치 데이터 $X \in \mathbb{R}^{B \times T \times d}$ |
| 4 | 4차 텐서 | 이미지 배치 $\in \mathbb{R}^{B \times C \times H \times W}$ |

### 수학적 관점

엄밀하게는, 텐서는 다중선형 사상(multilinear map)입니다. 하지만 ML에서는 대부분 "기저를 고정한 좌표 표현", 즉 다차원 배열로 다루면 충분합니다.

> **핵심 직관**: 행렬이 2개의 인덱스로 접근하는 데이터라면, 텐서는 $k$개의 인덱스로 접근하는 데이터입니다. 각 인덱스를 **모드(mode)** 또는 **축(axis)**이라 합니다.

---

## 2. 텐서 연산의 기본

### 원소별 연산 (Element-wise)

같은 크기의 텐서끼리 원소별 덧셈, 곱셈 등을 수행합니다. 활성화 함수 적용이 대표적입니다.

$$
C_{ijk} = A_{ijk} + B_{ijk}
$$

### 텐서 곱 (Tensor Product / Outer Product)

두 텐서의 텐서 곱은 차수가 합쳐집니다.

벡터 $\mathbf{a} \in \mathbb{R}^m$, $\mathbf{b} \in \mathbb{R}^n$의 외적:

$$
(\mathbf{a} \otimes \mathbf{b})_{ij} = a_i b_j \in \mathbb{R}^{m \times n}
$$

일반적으로, 차수 $p$ 텐서와 차수 $q$ 텐서의 텐서 곱은 차수 $p+q$ 텐서입니다.

### 텐서 축약 (Contraction)

공유하는 인덱스를 합산하여 차수를 줄입니다. 행렬 곱이 대표적인 축약입니다.

$$
C_{ij} = \sum_k A_{ik} B_{kj}
$$

인덱스 $k$를 축약(summation)하여 차수가 줄었습니다.

---

## 3. Einstein 합산 표기법

Einstein 표기법의 규칙은 단순합니다: **반복되는 인덱스는 합산한다**.

### 기본 예시

| 연산 | 표준 표기 | Einstein 표기 |
|------|-----------|--------------|
| 내적 | $\sum_i a_i b_i$ | $a_i b_i$ |
| 외적 | $a_i b_j$ | $a_i b_j$ (반복 없음) |
| 행렬-벡터 곱 | $\sum_j A_{ij} x_j$ | $A_{ij} x_j$ |
| 행렬 곱 | $\sum_k A_{ik} B_{kj}$ | $A_{ik} B_{kj}$ |
| Trace | $\sum_i A_{ii}$ | $A_{ii}$ |
| 행렬 원소합 | $\sum_i\sum_j A_{ij}$ | 별도 표기 필요 |

> **핵심 직관**: Einstein 표기법은 "어떤 인덱스가 살아남고 어떤 인덱스가 합산되는가"를 간결하게 표현합니다. 합산 기호 $\sum$를 생략하는 것이 핵심입니다.

---

## 4. np.einsum 마스터하기

NumPy의 `einsum`은 Einstein 표기법을 그대로 코드로 옮긴 것입니다.

```python
import numpy as np

a = np.array([1, 2, 3])
b = np.array([4, 5, 6])
A = np.random.randn(3, 4)
B = np.random.randn(4, 5)
X = np.random.randn(2, 3, 4)  # 배치 행렬

# 내적: a_i b_i → scalar
dot = np.einsum('i,i->', a, b)
print(f"내적: {dot}")  # = np.dot(a, b)

# 외적: a_i b_j → (3, 3)
outer = np.einsum('i,j->ij', a, b)
print(f"외적 shape: {outer.shape}")  # = np.outer(a, b)

# 행렬 곱: A_ik B_kj → C_ij
C = np.einsum('ik,kj->ij', A, B)
print(f"행렬곱 shape: {C.shape}")  # = A @ B

# Trace: A_ii → scalar
M = np.random.randn(4, 4)
tr = np.einsum('ii->', M)
print(f"Trace: {tr:.4f}")  # = np.trace(M)

# 배치 행렬 곱: X_bij Y_bjk → Z_bik
Y = np.random.randn(2, 4, 5)
Z = np.einsum('bij,bjk->bik', X, Y)
print(f"배치 행렬곱 shape: {Z.shape}")  # (2, 3, 5)

# 대각 원소 추출: A_ii → vector
diag = np.einsum('ii->i', M)
print(f"대각: {diag}")  # = np.diag(M)

# 전치: A_ij → A_ji
A_T = np.einsum('ij->ji', A)
print(f"전치 shape: {A_T.shape}")  # = A.T

# 원소별 곱의 합 (Frobenius 내적)
frob = np.einsum('ij,ij->', A, A)
print(f"Frobenius norm²: {frob:.4f}")  # = np.sum(A**2)
```

### einsum이 강력한 이유

1. **가독성**: 연산의 수학적 의미가 인덱스 표기에 그대로 드러남
2. **유연성**: 행렬 곱, trace, 전치, 외적 등을 하나의 함수로 통일
3. **효율성**: 내부적으로 최적화된 경로로 계산
4. **배치 처리**: 배치 차원을 자연스럽게 처리

---

## 5. 딥러닝에서의 텐서 연산

### Attention Score 계산

Multi-head attention에서 $Q, K \in \mathbb{R}^{B \times H \times T \times d_k}$일 때,

$$
\text{score}_{bhij} = \sum_k Q_{bhik} K_{bhjk}
$$

```python
# Attention score (배치 + 멀티헤드)
Q = np.random.randn(2, 8, 10, 64)  # (batch, heads, seq_len, d_k)
K = np.random.randn(2, 8, 10, 64)

scores = np.einsum('bhik,bhjk->bhij', Q, K)
print(f"Attention scores shape: {scores.shape}")  # (2, 8, 10, 10)
```

### Bilinear Layer

바이리니어 레이어 $y = \mathbf{x}_1^T W \mathbf{x}_2$는 3차 텐서 연산입니다.

$$
y_k = \sum_i \sum_j x_{1i} \, W_{ijk} \, x_{2j}
$$

```python
x1 = np.random.randn(5)
x2 = np.random.randn(7)
W = np.random.randn(5, 7, 3)

y = np.einsum('i,ijk,j->k', x1, W, x2)
print(f"Bilinear output shape: {y.shape}")  # (3,)
```

### Convolution의 텐서 표현

1D convolution도 einsum으로 표현 가능합니다.

$$
y_{bo} = \sum_i \sum_k x_{b,i+k} \cdot w_{o,i,k}
$$

실제로는 `im2col` 같은 최적화를 사용하지만, 수학적으로는 텐서 축약입니다.

---

## 6. 텐서 분해 (개요)

행렬에 SVD가 있듯, 텐서에도 분해 방법이 있습니다.

| 분해 | 설명 |
|------|------|
| CP 분해 | 랭크-1 텐서의 합: $\mathcal{T} \approx \sum_r \mathbf{a}_r \otimes \mathbf{b}_r \otimes \mathbf{c}_r$ |
| Tucker 분해 | 핵심 텐서 + 각 모드의 인자 행렬 |
| Tensor Train (TT) | 3차 텐서들의 연쇄 곱 |

텐서 분해는 모델 압축(큰 가중치 텐서를 저랭크로 근사)에 사용됩니다.

---

## 7. NumPy vs PyTorch einsum

```python
# NumPy
import numpy as np
result_np = np.einsum('ij,jk->ik', A, B)

# PyTorch (동일한 문법)
import torch
A_t = torch.randn(3, 4)
B_t = torch.randn(4, 5)
result_pt = torch.einsum('ij,jk->ik', A_t, B_t)

# PyTorch에서 자동 미분도 지원
A_t.requires_grad_(True)
result_pt = torch.einsum('ij,jk->ik', A_t, B_t)
loss = result_pt.sum()
loss.backward()  # A_t.grad 사용 가능
```

---

## 8. ML에서의 의미

### 왜 einsum을 써야 하는가

복잡한 텐서 연산을 `reshape`, `transpose`, `matmul`의 조합으로 쓰면 코드가 난해해집니다. einsum은 수학 논문의 인덱스 표기를 그대로 코드로 옮길 수 있어, 논문 구현 시 실수를 줄여줍니다.

### 모델 압축

대형 모델의 가중치를 텐서 분해로 근사하면, 파라미터 수와 연산량을 줄일 수 있습니다. LoRA도 행렬의 저랭크 분해이고, 이를 고차 텐서로 확장한 것이 텐서 분해 기반 압축입니다.

---

## 핵심 정리

1. **텐서**는 다차원 배열이며, 딥러닝의 모든 데이터와 파라미터는 텐서로 표현된다.
2. **텐서 축약(contraction)**은 공유 인덱스의 합산이며, 행렬 곱이 그 특수한 경우이다.
3. **Einstein 표기법**은 반복되는 인덱스를 자동 합산하는 규칙으로, 복잡한 텐서 연산을 간결하게 표현한다.
4. **`np.einsum` / `torch.einsum`**은 Einstein 표기를 그대로 코드로 옮긴 것이며, 행렬곱·trace·외적·배치 연산을 통일한다.
5. Attention score, bilinear layer, convolution 등 딥러닝의 핵심 연산이 모두 **텐서 축약**으로 표현된다.
