# 벡터 공간과 부분 공간

## 왜 벡터 공간을 배워야 하는가

머신러닝의 거의 모든 연산은 벡터와 행렬 위에서 일어납니다. 데이터 한 행은 벡터이고, 신경망의 가중치는 행렬이며, 학습이란 결국 고차원 공간에서 최적의 점을 찾는 일입니다. 이 공간의 구조를 정확히 이해하지 못하면, 알고리즘이 **왜** 작동하는지 설명할 수 없습니다.

이번 강의에서는 벡터 공간의 엄밀한 정의부터 시작해, 기저(basis)와 차원(dimension), 그리고 행렬의 네 가지 근본 부분 공간(four fundamental subspaces)까지 다룹니다.

---

## 1. 벡터 공간의 정의

벡터 공간(vector space) $V$는 **체(field)** $\mathbb{F}$(보통 $\mathbb{R}$ 또는 $\mathbb{C}$) 위에서 정의된 집합으로, 다음 두 연산이 닫혀 있어야 합니다.

- **벡터 덧셈**: $\mathbf{u} + \mathbf{v} \in V$ (for all $\mathbf{u}, \mathbf{v} \in V$)
- **스칼라 곱**: $c\mathbf{v} \in V$ (for all $c \in \mathbb{F}$, $\mathbf{v} \in V$)

그리고 다음 **8가지 공리**를 만족해야 합니다.

| 공리 | 수식 |
|------|------|
| 덧셈 교환법칙 | $\mathbf{u} + \mathbf{v} = \mathbf{v} + \mathbf{u}$ |
| 덧셈 결합법칙 | $(\mathbf{u} + \mathbf{v}) + \mathbf{w} = \mathbf{u} + (\mathbf{v} + \mathbf{w})$ |
| 영벡터 존재 | $\exists\, \mathbf{0} \in V : \mathbf{v} + \mathbf{0} = \mathbf{v}$ |
| 역원 존재 | $\forall\, \mathbf{v} \in V,\; \exists\, (-\mathbf{v}) : \mathbf{v} + (-\mathbf{v}) = \mathbf{0}$ |
| 스칼라 곱 결합법칙 | $a(b\mathbf{v}) = (ab)\mathbf{v}$ |
| 항등원 | $1 \cdot \mathbf{v} = \mathbf{v}$ |
| 분배법칙 1 | $a(\mathbf{u} + \mathbf{v}) = a\mathbf{u} + a\mathbf{v}$ |
| 분배법칙 2 | $(a + b)\mathbf{v} = a\mathbf{v} + b\mathbf{v}$ |

> **핵심 직관**: 벡터 공간이란 "더하고 상수배 해도 밖으로 튀어나가지 않는 집합"입니다.

### 대표적 예시

- $\mathbb{R}^n$: 우리가 가장 많이 다루는 공간
- 모든 $m \times n$ 행렬의 집합 $\mathbb{R}^{m \times n}$
- 다항식의 집합 $\mathcal{P}_n$ (차수 $n$ 이하)
- 연속함수의 집합 $C[a, b]$

---

## 2. 부분 공간 (Subspace)

벡터 공간 $V$의 부분집합 $W$가 **부분 공간**이 되려면 세 가지만 확인하면 됩니다.

1. **영벡터 포함**: $\mathbf{0} \in W$
2. **덧셈에 닫힘**: $\mathbf{u}, \mathbf{v} \in W \Rightarrow \mathbf{u} + \mathbf{v} \in W$
3. **스칼라 곱에 닫힘**: $c \in \mathbb{F},\; \mathbf{v} \in W \Rightarrow c\mathbf{v} \in W$

### 예시: 부분 공간인 것과 아닌 것

$\mathbb{R}^2$에서 원점을 지나는 직선 $W = \{t(1, 2) : t \in \mathbb{R}\}$은 부분 공간입니다.

반면, $W = \{(x, y) : x + y = 1\}$은 원점 $(0, 0)$을 포함하지 않으므로 부분 공간이 **아닙니다**.

> **ML에서의 의미**: 선형 회귀에서 예측값 $\hat{\mathbf{y}} = X\boldsymbol{\beta}$는 $X$의 열 공간(column space)이라는 부분 공간 위의 점입니다. 학습이란 이 부분 공간에서 실제 $\mathbf{y}$에 가장 가까운 점을 찾는 것입니다.

---

## 3. 선형 결합, 생성, 선형 독립

### 선형 결합 (Linear Combination)

벡터 $\mathbf{v}_1, \mathbf{v}_2, \ldots, \mathbf{v}_k$의 선형 결합은 다음과 같습니다.

$$
c_1 \mathbf{v}_1 + c_2 \mathbf{v}_2 + \cdots + c_k \mathbf{v}_k, \quad c_i \in \mathbb{F}
$$

### 생성 (Span)

벡터 집합 $S = \{\mathbf{v}_1, \ldots, \mathbf{v}_k\}$의 **생성(span)**은 가능한 모든 선형 결합의 집합입니다.

$$
\text{span}(S) = \left\{ \sum_{i=1}^k c_i \mathbf{v}_i \;\middle|\; c_i \in \mathbb{F} \right\}
$$

$\text{span}(S)$는 항상 부분 공간입니다.

### 선형 독립 (Linear Independence)

벡터 $\{\mathbf{v}_1, \ldots, \mathbf{v}_k\}$가 **선형 독립**이라 함은 다음을 만족하는 유일한 해가 $c_1 = c_2 = \cdots = c_k = 0$뿐이라는 것입니다.

$$
c_1 \mathbf{v}_1 + c_2 \mathbf{v}_2 + \cdots + c_k \mathbf{v}_k = \mathbf{0}
$$

> **직관**: 선형 독립이란 "어떤 벡터도 나머지 벡터들의 조합으로 만들 수 없다"는 뜻입니다. 각 벡터가 정보를 하나씩 새로 추가하는 것입니다.

---

## 4. 기저와 차원

### 기저 (Basis)

벡터 공간 $V$의 **기저**는 다음 두 조건을 동시에 만족하는 벡터 집합 $B$입니다.

1. $B$는 **선형 독립**이다.
2. $\text{span}(B) = V$ — $B$가 $V$를 **생성**한다.

$\mathbb{R}^3$의 표준 기저(standard basis)는 다음과 같습니다.

$$
\mathbf{e}_1 = \begin{pmatrix} 1 \\ 0 \\ 0 \end{pmatrix}, \quad
\mathbf{e}_2 = \begin{pmatrix} 0 \\ 1 \\ 0 \end{pmatrix}, \quad
\mathbf{e}_3 = \begin{pmatrix} 0 \\ 0 \\ 1 \end{pmatrix}
$$

하지만 기저는 유일하지 않습니다. 다음도 $\mathbb{R}^2$의 기저입니다.

$$
\left\{ \begin{pmatrix} 1 \\ 1 \end{pmatrix}, \begin{pmatrix} 1 \\ -1 \end{pmatrix} \right\}
$$

### 차원 (Dimension)

벡터 공간 $V$의 **차원** $\dim(V)$은 기저 벡터의 개수입니다. 중요한 정리가 있습니다.

> **정리**: $V$의 모든 기저는 같은 수의 벡터를 가진다.

따라서 차원은 기저 선택에 무관하게 잘 정의됩니다.

- $\dim(\mathbb{R}^n) = n$
- $\dim(\mathbb{R}^{m \times n}) = mn$
- $\dim(\mathcal{P}_n) = n + 1$

---

## 5. 행(Rank)과 영공간(Null Space)

행렬 $A \in \mathbb{R}^{m \times n}$에 대해 두 가지 핵심 부분 공간을 정의합니다.

### 열 공간 (Column Space)

$$
\mathcal{C}(A) = \{A\mathbf{x} : \mathbf{x} \in \mathbb{R}^n\} \subseteq \mathbb{R}^m
$$

$A$의 열 벡터들의 모든 선형 결합입니다. $A\mathbf{x} = \mathbf{b}$가 해를 가질 조건은 $\mathbf{b} \in \mathcal{C}(A)$입니다.

### 영 공간 (Null Space)

$$
\mathcal{N}(A) = \{\mathbf{x} \in \mathbb{R}^n : A\mathbf{x} = \mathbf{0}\} \subseteq \mathbb{R}^n
$$

$A$를 곱했을 때 영벡터가 되는 모든 벡터의 집합입니다.

### 행(Rank)

$$
\text{rank}(A) = \dim(\mathcal{C}(A))
$$

행(rank)은 행렬이 담고 있는 "독립적인 정보의 수"입니다.

### 예시

$$
A = \begin{pmatrix} 1 & 2 & 3 \\ 2 & 4 & 6 \end{pmatrix}
$$

두 번째 행은 첫 번째 행의 2배입니다. 따라서 $\text{rank}(A) = 1$입니다. 열 공간은 $\mathbb{R}^2$의 직선이고, 영 공간은 $\mathbb{R}^3$의 2차원 평면입니다.

---

## 6. 네 가지 근본 부분 공간

Gilbert Strang이 강조하는 선형대수의 **핵심 그림**입니다. 행렬 $A \in \mathbb{R}^{m \times n}$에 대해 네 부분 공간이 존재합니다.

| 부분 공간 | 기호 | 소속 공간 | 차원 |
|-----------|------|-----------|------|
| 열 공간 (Column Space) | $\mathcal{C}(A)$ | $\mathbb{R}^m$ | $r$ |
| 행 공간 (Row Space) | $\mathcal{C}(A^T)$ | $\mathbb{R}^n$ | $r$ |
| 영 공간 (Null Space) | $\mathcal{N}(A)$ | $\mathbb{R}^n$ | $n - r$ |
| 좌영 공간 (Left Null Space) | $\mathcal{N}(A^T)$ | $\mathbb{R}^m$ | $m - r$ |

여기서 $r = \text{rank}(A)$입니다. 핵심 관계는 다음 두 가지입니다.

$$
\mathcal{C}(A^T) \oplus \mathcal{N}(A) = \mathbb{R}^n \quad \text{(직교 보공간)}
$$

$$
\mathcal{C}(A) \oplus \mathcal{N}(A^T) = \mathbb{R}^m \quad \text{(직교 보공간)}
$$

> **핵심 직관**: $A\mathbf{x} = \mathbf{b}$를 풀 때, $\mathbf{x}$는 행 공간 성분(유일한 특수해)과 영 공간 성분(자유도)으로 분해됩니다. 영 공간이 클수록 해가 많습니다.

### Rank-Nullity 정리

$$
\text{rank}(A) + \text{nullity}(A) = n
$$

여기서 $\text{nullity}(A) = \dim(\mathcal{N}(A))$입니다. 이 정리는 "열의 수 = 독립 정보 + 자유도"를 말합니다.

---

## 7. NumPy로 확인하기

```python
import numpy as np

A = np.array([
    [1, 2, 3],
    [2, 4, 6],
    [0, 1, 1]
])

# Rank
print(f"rank = {np.linalg.matrix_rank(A)}")  # 2

# Null space (SVD 활용)
U, S, Vt = np.linalg.svd(A)
null_mask = S < 1e-10
null_space = Vt[null_mask]  # 영 공간의 기저
print(f"null space basis:\n{null_space}")

# Rank-Nullity 확인
n = A.shape[1]  # 열의 수 = 3
rank = np.linalg.matrix_rank(A)
nullity = n - rank
print(f"rank({rank}) + nullity({nullity}) = {n}")
```

---

## 8. ML에서의 의미

### 과적합과 영 공간

학습 데이터 $X \in \mathbb{R}^{n \times d}$에서 **샘플 수 $n$ < 특성 수 $d$** 이면 $\mathcal{N}(X)$가 비자명(nontrivial)합니다. 즉, $X\mathbf{w} = \mathbf{0}$인 $\mathbf{w} \neq \mathbf{0}$가 존재합니다. 이는 전혀 다른 $\mathbf{w}$가 같은 예측을 만든다는 뜻이고, 이것이 바로 **과적합의 근본 원인**입니다.

정규화(regularization)는 이 영 공간에서의 자유도를 제한하는 것입니다.

### PCA와 열 공간

PCA는 데이터의 열 공간에서 분산이 가장 큰 방향을 찾는 것입니다. 데이터가 원래 $d$차원이지만 실제로는 $r$차원 부분 공간 근처에 놓여 있다면, 나머지 $d - r$차원은 노이즈입니다.

### 임베딩과 차원

Word2Vec, BERT 같은 모델이 단어를 300차원, 768차원 벡터로 매핑하는 것은 "의미"라는 추상적 공간을 $\mathbb{R}^d$의 부분 공간으로 근사하는 것입니다.

---

## 핵심 정리

1. **벡터 공간**은 덧셈과 스칼라 곱에 닫힌 집합이다.
2. **기저**는 선형 독립이면서 공간을 생성하는 최소 집합이고, 그 개수가 **차원**이다.
3. 행렬의 **네 가지 근본 부분 공간**은 $A\mathbf{x} = \mathbf{b}$의 해 구조를 완전히 설명한다.
4. **Rank-Nullity 정리**: $\text{rank}(A) + \text{nullity}(A) = n$ — 독립 정보와 자유도의 합은 항상 열의 수이다.
5. ML에서 영 공간은 과적합, 열 공간은 차원 축소와 직결된다.
