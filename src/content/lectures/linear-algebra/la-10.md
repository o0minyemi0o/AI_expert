# 랜덤화 선형대수

## 왜 랜덤화 선형대수를 배워야 하는가

Netflix의 사용자-영화 평점 행렬은 수억 개의 원소를 가집니다. 이런 행렬에 정확한 SVD를 적용하면 시간이 너무 오래 걸립니다. **랜덤화 알고리즘(randomized algorithms)**은 정확도를 약간 희생하고 속도를 극적으로 높이는 접근법입니다. Johnson-Lindenstrauss 보조정리는 "랜덤 투영이 거리를 보존한다"는 놀라운 결과를 보장합니다.

---

## 1. 랜덤 투영 (Random Projection)

고차원 벡터 $\mathbf{x} \in \mathbb{R}^d$를 저차원 $\mathbb{R}^k$ ($k \ll d$)로 보내되, **거리 관계를 대략 보존**하는 것이 목표입니다.

랜덤 투영 행렬 $R \in \mathbb{R}^{k \times d}$를 사용합니다.

$$
\mathbf{z} = R\mathbf{x} \in \mathbb{R}^k
$$

$R$의 원소를 $R_{ij} \sim \mathcal{N}(0, 1/k)$로 설정하면, 기댓값으로 노름이 보존됩니다.

$$
\mathbb{E}[\|R\mathbf{x}\|^2] = \|\mathbf{x}\|^2
$$

> **핵심 직관**: 고차원에서는 랜덤 방향이 "거의 직교"합니다. 따라서 랜덤하게 고른 부분 공간도 원래 구조를 잘 보존합니다. 이것이 **차원의 축복(blessing of dimensionality)**입니다.

---

## 2. Johnson-Lindenstrauss 보조정리

**JL Lemma**: $n$개의 점 $\mathbf{x}_1, \ldots, \mathbf{x}_n \in \mathbb{R}^d$에 대해, $\epsilon \in (0, 1)$이면 다음을 만족하는 선형 사상 $f: \mathbb{R}^d \to \mathbb{R}^k$가 존재합니다.

$$
(1-\epsilon)\|\mathbf{x}_i - \mathbf{x}_j\|^2 \leq \|f(\mathbf{x}_i) - f(\mathbf{x}_j)\|^2 \leq (1+\epsilon)\|\mathbf{x}_i - \mathbf{x}_j\|^2
$$

단, $k = O(\epsilon^{-2} \log n)$이면 됩니다.

### 핵심 포인트

- 목표 차원 $k$는 **원래 차원 $d$에 무관**하고, 데이터 포인트 수 $n$의 로그에만 비례합니다.
- $d = 10{,}000$이고 $n = 1{,}000{,}000$이어도, $\epsilon = 0.1$이면 $k \approx 1{,}400$ 정도면 충분합니다.
- 랜덤 가우시안 행렬이 이 조건을 높은 확률로 만족합니다.

### 증명 스케치

핵심 아이디어는 **집중 부등식(concentration inequality)**입니다.

고정된 벡터 $\mathbf{u}$에 대해, $R\mathbf{u}$의 노름의 제곱은 $\chi^2$ 분포를 따르며, $k$가 충분히 크면 평균 근처에 집중합니다.

$$
P\left[\left|\|R\mathbf{u}\|^2 - \|\mathbf{u}\|^2\right| > \epsilon\|\mathbf{u}\|^2\right] \leq 2\exp\left(-\frac{k\epsilon^2}{4}\right)
$$

$\binom{n}{2}$개의 쌍에 union bound를 적용하면, $k = O(\epsilon^{-2}\log n)$에서 모든 쌍이 동시에 보존됩니다.

---

## 3. 더 빠른 랜덤 투영

가우시안 행렬의 곱은 $O(kd)$입니다. 더 빠른 대안들이 있습니다.

| 방법 | 시간 복잡도 | 설명 |
|------|------------|------|
| 가우시안 | $O(kd)$ | $R_{ij} \sim \mathcal{N}(0, 1/k)$ |
| 희소 랜덤 | $O(\text{nnz}(X))$ | $R_{ij} \in \{-1, 0, 1\}$ (대부분 0) |
| SRHT | $O(d \log k)$ | Subsampled Randomized Hadamard Transform |
| Count Sketch | $O(\text{nnz}(X))$ | 해시 기반, 매우 희소 |

SRHT는 Hadamard 행렬($H$)의 빠른 곱셈($O(d\log d)$, FFT와 유사)을 활용합니다.

---

## 4. 랜덤화 SVD

Halko, Martinsson, Tropp (2011)의 알고리즘으로, 대규모 행렬의 근사 SVD를 효율적으로 구합니다.

### 알고리즘 (목표: 랭크 $k$ 근사)

**Stage 1 — 랜덤 샘플링으로 열 공간 근사**

1. 랜덤 행렬 $\Omega \in \mathbb{R}^{n \times (k+p)}$ 생성 ($p$는 oversampling, 보통 5~10)
2. $Y = A\Omega$ 계산 ($m \times (k+p)$ 행렬)
3. $Y$의 QR 분해: $Y = QR$ → $Q \in \mathbb{R}^{m \times (k+p)}$

이제 $Q$의 열 공간이 $A$의 상위 특이벡터 공간을 근사합니다.

**Stage 2 — 작은 행렬의 정확한 SVD**

4. $B = Q^TA$ 계산 ($(k+p) \times n$의 작은 행렬)
5. $B$의 SVD: $B = \tilde{U}\Sigma V^T$
6. $U = Q\tilde{U}$

최종 근사: $A \approx U\Sigma V^T$

### 계산 복잡도

- 정확한 SVD: $O(mn \cdot \min(m,n))$
- 랜덤화 SVD: $O(mn(k+p))$ — $k \ll \min(m,n)$이면 극적 개선

---

## 5. NumPy로 확인하기

```python
import numpy as np

# Johnson-Lindenstrauss 실험
np.random.seed(42)
d = 5000
n_points = 200
X = np.random.randn(n_points, d)

# 원래 거리 계산
from scipy.spatial.distance import pdist
original_dists = pdist(X)

# 랜덤 투영
for k in [50, 100, 500]:
    R = np.random.randn(k, d) / np.sqrt(k)
    X_proj = (R @ X.T).T
    projected_dists = pdist(X_proj)

    ratios = projected_dists / original_dists
    print(f"k={k:4d}: 비율 평균={ratios.mean():.4f}, "
          f"std={ratios.std():.4f}, "
          f"범위=[{ratios.min():.3f}, {ratios.max():.3f}]")

# 랜덤화 SVD
def randomized_svd(A, k, p=5):
    m, n = A.shape
    Omega = np.random.randn(n, k + p)
    Y = A @ Omega
    Q, _ = np.linalg.qr(Y)
    B = Q.T @ A
    U_tilde, S, Vt = np.linalg.svd(B, full_matrices=False)
    U = Q @ U_tilde
    return U[:, :k], S[:k], Vt[:k, :]

# 테스트
m, n, true_rank = 1000, 500, 10
A_low = np.random.randn(m, true_rank) @ np.random.randn(true_rank, n)
A = A_low + 0.01 * np.random.randn(m, n)  # 저랭크 + 노이즈

U_rand, S_rand, Vt_rand = randomized_svd(A, k=10)
A_approx = U_rand @ np.diag(S_rand) @ Vt_rand

# 정확한 SVD와 비교
U_exact, S_exact, Vt_exact = np.linalg.svd(A, full_matrices=False)
A_exact_k = U_exact[:, :10] @ np.diag(S_exact[:10]) @ Vt_exact[:10, :]

print(f"\n랜덤화 SVD 오차: {np.linalg.norm(A - A_approx):.4f}")
print(f"정확한 SVD 오차: {np.linalg.norm(A - A_exact_k):.4f}")
```

---

## 6. ML에서의 의미

### 대규모 추천 시스템

사용자-아이템 행렬에 정확한 SVD를 적용하면 $O(mn\min(m,n))$입니다. 랜덤화 SVD로 상위 $k$개 잠재 요인만 추출하면 $O(mnk)$로 충분하며, 실무에서 거의 동일한 추천 품질을 얻습니다.

### 차원 축소의 전처리

scikit-learn의 `TruncatedSVD`는 내부적으로 랜덤화 SVD를 사용합니다. 텍스트 데이터의 TF-IDF 행렬처럼 대규모 희소 행렬에 PCA를 적용할 때 필수적입니다.

### 랜덤 투영을 이용한 근사 최근접 이웃

JL 보조정리를 이용하면, 고차원 데이터의 최근접 이웃 검색을 저차원에서 수행할 수 있습니다. Locality-Sensitive Hashing (LSH)의 이론적 기반이 됩니다.

---

## 핵심 정리

1. **랜덤 투영**은 $O(\epsilon^{-2}\log n)$차원으로 줄여도 쌍별 거리를 $(1\pm\epsilon)$ 범위로 보존한다 (JL Lemma).
2. 목표 차원은 **원래 차원 $d$에 무관**하고 데이터 수의 로그에만 의존한다.
3. **랜덤화 SVD**는 랜덤 투영 → QR → 작은 행렬 SVD의 2단계로, 정확한 SVD 대비 극적으로 빠르다.
4. 특이값이 빠르게 감소하는 행렬(실제 데이터 대부분)에서 랜덤화 SVD는 정확한 SVD와 **거의 동일한 결과**를 준다.
5. 추천 시스템, 텍스트 분석, 근사 최근접 이웃 등 대규모 ML의 핵심에 랜덤화 기법이 있다.
