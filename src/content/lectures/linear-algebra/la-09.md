# 행렬 노름과 조건수

## 왜 행렬 노름과 조건수를 배워야 하는가

"이론적으로 맞는 코드인데 결과가 이상하다"는 경험이 있다면, 그 원인은 대부분 **수치 안정성(numerical stability)** 문제입니다. 조건수(condition number)가 크면 작은 입력 변화가 출력에서 크게 증폭됩니다. 또한 ML에서 L1, L2, nuclear norm 정규화의 의미를 이해하려면 노름의 정의부터 알아야 합니다.

---

## 1. 벡터 노름

벡터 $\mathbf{x} \in \mathbb{R}^n$의 $\ell_p$ 노름은 다음과 같습니다.

$$
\|\mathbf{x}\|_p = \left(\sum_{i=1}^n |x_i|^p\right)^{1/p}
$$

| 노름 | 수식 | 기하학적 의미 |
|------|------|--------------|
| $\ell_1$ | $\sum_i \|x_i\|$ | 마름모 (다이아몬드) |
| $\ell_2$ | $\sqrt{\sum_i x_i^2}$ | 원 (유클리드 거리) |
| $\ell_\infty$ | $\max_i \|x_i\|$ | 정사각형 |

> **핵심 직관**: $\ell_1$ 노름의 단위 구(unit ball)는 꼭짓점이 축 위에 있어 **희소 해(sparse solution)**를 유도합니다. 이것이 L1 정규화(Lasso)가 변수 선택을 하는 기하학적 이유입니다.

---

## 2. 행렬 노름

### Frobenius 노름

모든 원소의 제곱합의 제곱근입니다. 벡터의 $\ell_2$ 노름을 행렬로 확장한 것입니다.

$$
\|A\|_F = \sqrt{\sum_{i,j} A_{ij}^2} = \sqrt{\text{tr}(A^TA)} = \sqrt{\sum_i \sigma_i^2}
$$

마지막 등호에서 **특이값과의 관계**가 드러납니다.

### 스펙트럴 노름 (연산자 2-노름)

행렬이 벡터를 **최대한 얼마나 늘일 수 있는가**를 측정합니다.

$$
\|A\|_2 = \max_{\|\mathbf{x}\|_2=1} \|A\mathbf{x}\|_2 = \sigma_1 \quad \text{(최대 특이값)}
$$

### 핵 노름 (Nuclear Norm)

모든 특이값의 합입니다.

$$
\|A\|_* = \sum_i \sigma_i
$$

| 노름 | 수식 | 특이값 관계 | ML에서의 용도 |
|------|------|------------|--------------|
| Frobenius | $\sqrt{\sum \sigma_i^2}$ | 특이값의 $\ell_2$ | Weight decay |
| Spectral | $\sigma_1$ | 특이값의 $\ell_\infty$ | Spectral normalization |
| Nuclear | $\sum \sigma_i$ | 특이값의 $\ell_1$ | 저랭크 행렬 유도 |

> **핵심 직관**: Frobenius 노름은 "모든 원소가 작은가", spectral 노름은 "가장 큰 방향이 얼마나 큰가", nuclear 노름은 "얼마나 많은 방향이 활성화되어 있는가"를 측정합니다.

---

## 3. 조건수 (Condition Number)

가역 행렬 $A$의 **조건수**는 다음과 같습니다.

$$
\kappa(A) = \|A\| \cdot \|A^{-1}\|
$$

$\ell_2$ 노름 기준으로는,

$$
\kappa_2(A) = \frac{\sigma_{\max}}{\sigma_{\min}} = \frac{\sigma_1}{\sigma_n}
$$

### 조건수의 의미

$A\mathbf{x} = \mathbf{b}$에서 $\mathbf{b}$에 작은 섭동(perturbation) $\delta\mathbf{b}$가 있을 때,

$$
\frac{\|\delta\mathbf{x}\|}{\|\mathbf{x}\|} \leq \kappa(A) \cdot \frac{\|\delta\mathbf{b}\|}{\|\mathbf{b}\|}
$$

조건수가 $10^k$이면, 입력의 작은 오차가 출력에서 **최대 $10^k$배** 증폭될 수 있습니다.

| $\kappa(A)$ | 상태 | 의미 |
|-------------|------|------|
| $\approx 1$ | Well-conditioned | 안정적 |
| $10^3 \sim 10^6$ | Moderately ill-conditioned | 주의 필요 |
| $> 10^{10}$ | Severely ill-conditioned | 결과 신뢰 불가 |
| $\infty$ | Singular | 역행렬 불존재 |

> **핵심 직관**: 조건수는 "행렬이 얼마나 원에 가까운 타원을 만드는가"입니다. 조건수가 크면 매우 납작한 타원 → 한 방향으로는 안정적이지만 다른 방향으로는 불안정합니다.

---

## 4. 수치 안정성 예시

### Hilbert 행렬: 악명 높은 ill-conditioned 행렬

$$
H_{ij} = \frac{1}{i + j - 1}
$$

```python
import numpy as np

for n in [5, 10, 15]:
    H = np.array([[1/(i+j+1) for j in range(n)] for i in range(n)])
    kappa = np.linalg.cond(H)
    print(f"n={n:2d}: κ = {kappa:.2e}")
```

출력 (대략):
```
n= 5: κ = 4.77e+05
n=10: κ = 1.60e+13
n=15: κ = 3.67e+17
```

$n=15$이면 조건수가 $10^{17}$ 수준으로, 64비트 부동소수점(약 16자리 정밀도)으로는 의미 있는 계산이 불가능합니다.

---

## 5. NumPy로 확인하기

```python
import numpy as np

A = np.array([[1, 2], [3, 4]], dtype=float)

# 벡터 노름
x = np.array([3, -4])
print(f"ℓ1: {np.linalg.norm(x, 1)}")       # 7
print(f"ℓ2: {np.linalg.norm(x, 2)}")       # 5
print(f"ℓ∞: {np.linalg.norm(x, np.inf)}")  # 4

# 행렬 노름
print(f"\nFrobenius: {np.linalg.norm(A, 'fro'):.4f}")
print(f"Spectral:  {np.linalg.norm(A, 2):.4f}")

# SVD로 확인
U, S, Vt = np.linalg.svd(A)
print(f"특이값: {S}")
print(f"Frobenius (특이값): {np.sqrt(np.sum(S**2)):.4f}")
print(f"Spectral (σ_max):   {S[0]:.4f}")
print(f"Nuclear (특이값 합): {np.sum(S):.4f}")

# 조건수
print(f"\n조건수: {np.linalg.cond(A):.4f}")
print(f"σ_max/σ_min: {S[0]/S[-1]:.4f}")  # 동일

# 조건수가 큰 행렬에서의 불안정성
np.random.seed(42)
n = 10
H = np.array([[1/(i+j+1) for j in range(n)] for i in range(n)])
x_true = np.ones(n)
b = H @ x_true
b_noisy = b + 1e-10 * np.random.randn(n)  # 아주 작은 노이즈

x_clean = np.linalg.solve(H, b)
x_noisy = np.linalg.solve(H, b_noisy)
print(f"\n조건수: {np.linalg.cond(H):.2e}")
print(f"입력 오차: {np.linalg.norm(b - b_noisy):.2e}")
print(f"출력 오차: {np.linalg.norm(x_clean - x_noisy):.2e}")  # 크게 증폭됨
```

---

## 6. ML에서의 의미

### 정규화와 노름

| 정규화 | 노름 | 효과 |
|--------|------|------|
| L1 (Lasso) | $\|\mathbf{w}\|_1$ | 희소 해 → 변수 선택 |
| L2 (Ridge) | $\|\mathbf{w}\|_2^2$ | 작은 가중치 → 과적합 방지 |
| Nuclear norm | $\|W\|_*$ | 저랭크 해 → 행렬 완성 |
| Spectral norm | $\|W\|_2$ | Lipschitz 제약 → GAN 안정화 |

### Spectral Normalization

GAN의 판별자(discriminator)에서 각 층의 가중치를 spectral norm으로 나누어, Lipschitz 상수를 1로 제한합니다.

$$
\bar{W} = W / \sigma_1(W)
$$

이를 통해 학습이 안정화됩니다.

### Gradient Clipping과 조건수

학습 중 그래디언트 폭발은 Hessian의 조건수가 클 때 발생합니다. Gradient clipping은 그래디언트 벡터의 노름을 제한하여 이를 완화합니다.

$$
\mathbf{g} \leftarrow \mathbf{g} \cdot \min\left(1, \frac{c}{\|\mathbf{g}\|}\right)
$$

---

## 핵심 정리

1. **벡터 노름** $\ell_1$, $\ell_2$, $\ell_\infty$는 각각 희소성, 크기, 최대값을 측정하며, 정규화의 기하학적 근거이다.
2. **행렬 노름**은 특이값으로 표현되며, Frobenius(전체 크기), spectral(최대 방향), nuclear(활성 방향 수)로 나뉜다.
3. **조건수** $\kappa = \sigma_{\max}/\sigma_{\min}$은 입력 오차의 최대 증폭 배율이며, 수치 안정성의 핵심 지표이다.
4. Hilbert 행렬처럼 $\kappa$가 큰 행렬은 부동소수점 환경에서 **신뢰할 수 없는 결과**를 낼 수 있다.
5. ML에서 L1/L2/nuclear/spectral norm 정규화는 각각 다른 구조적 제약을 부과하며, 이론적 근거가 모두 노름에 있다.
