# 특이값 분해

## 왜 특이값 분해를 배워야 하는가

고유값 분해는 정방행렬에만 적용됩니다. 하지만 현실의 데이터 행렬은 거의 항상 직사각형입니다 — 1만 명의 사용자 × 500개 영화, 10만 문서 × 3만 단어. **특이값 분해(SVD)**는 **모든 행렬**에 적용 가능한 분해이며, 추천 시스템, 이미지 압축, 잠재 의미 분석, 의사역행렬까지 관통하는 핵심 도구입니다.

---

## 1. SVD의 정의

임의의 행렬 $A \in \mathbb{R}^{m \times n}$에 대해, 다음 분해가 **항상** 존재합니다.

$$
A = U\Sigma V^T
$$

| 행렬 | 크기 | 성질 |
|------|------|------|
| $U$ | $m \times m$ | 직교 행렬 ($U^TU = I$), 좌 특이벡터 |
| $\Sigma$ | $m \times n$ | 대각 행렬, 특이값 $\sigma_1 \geq \sigma_2 \geq \cdots \geq 0$ |
| $V$ | $n \times n$ | 직교 행렬 ($V^TV = I$), 우 특이벡터 |

> **핵심 직관**: SVD는 "모든 선형 변환은 **회전 → 스케일링 → 회전**으로 분해된다"는 것을 말합니다. $V^T$가 입력 공간을 회전하고, $\Sigma$가 각 축을 늘이거나 줄이고, $U$가 출력 공간을 회전합니다.

---

## 2. SVD와 고유값 분해의 관계

SVD는 고유값 분해로부터 자연스럽게 유도됩니다.

$$
A^TA = (U\Sigma V^T)^T(U\Sigma V^T) = V\Sigma^T\Sigma V^T = V\begin{pmatrix} \sigma_1^2 & & \\ & \ddots & \\ & & \sigma_n^2 \end{pmatrix}V^T
$$

$$
AA^T = U\Sigma\Sigma^T U^T = U\begin{pmatrix} \sigma_1^2 & & \\ & \ddots & \\ & & \sigma_m^2 \end{pmatrix}U^T
$$

따라서,
- **우 특이벡터** $V$: $A^TA$의 고유벡터
- **좌 특이벡터** $U$: $AA^T$의 고유벡터
- **특이값** $\sigma_i$: $A^TA$의 고유값의 양의 제곱근, $\sigma_i = \sqrt{\lambda_i(A^TA)}$

> **핵심 직관**: 고유값 분해는 "자기 자신과의 관계"이고, SVD는 "입력 공간과 출력 공간의 관계"를 분해합니다.

---

## 3. Full SVD vs Compact SVD

$\text{rank}(A) = r$일 때, $\Sigma$에서 $r$개만 양수이고 나머지는 0입니다.

### Full SVD

$$
A = U_{m \times m} \Sigma_{m \times n} V^T_{n \times n}
$$

모든 차원을 포함합니다. 영 공간과 좌영 공간 정보까지 담고 있습니다.

### Compact (Thin) SVD

$$
A = U_r \Sigma_r V_r^T
$$

$U_r \in \mathbb{R}^{m \times r}$, $\Sigma_r \in \mathbb{R}^{r \times r}$, $V_r \in \mathbb{R}^{n \times r}$

0이 아닌 특이값에 대응하는 부분만 남깁니다. 저장 공간이 크게 줄어듭니다.

### Truncated SVD (저랭크 근사)

상위 $k$개($k < r$)의 특이값만 사용합니다.

$$
A_k = U_k \Sigma_k V_k^T = \sum_{i=1}^{k} \sigma_i \mathbf{u}_i \mathbf{v}_i^T
$$

이것이 ML에서 가장 자주 사용하는 형태입니다.

---

## 4. Eckart-Young 정리

**최적 저랭크 근사(best low-rank approximation)**에 대한 정리입니다.

> 랭크 $k$ 이하인 모든 행렬 $B$ 중에서, $A_k = \sum_{i=1}^{k}\sigma_i\mathbf{u}_i\mathbf{v}_i^T$가 $\|A - B\|$를 최소화한다.

Frobenius 노름의 경우,

$$
\|A - A_k\|_F = \sqrt{\sigma_{k+1}^2 + \sigma_{k+2}^2 + \cdots + \sigma_r^2}
$$

Spectral 노름의 경우,

$$
\|A - A_k\|_2 = \sigma_{k+1}
$$

> **핵심 직관**: 특이값이 빠르게 감소하면, 적은 수의 성분으로 행렬을 잘 근사할 수 있습니다. 이것이 차원 축소와 데이터 압축의 이론적 근거입니다.

---

## 5. 의사역행렬 (Pseudoinverse)

직사각행렬이나 특이 행렬은 역행렬이 없습니다. SVD를 이용한 **Moore-Penrose 의사역행렬**이 이를 대체합니다.

$$
A^+ = V\Sigma^+ U^T
$$

여기서 $\Sigma^+$는 $\Sigma$의 양수 대각 원소를 역수로 바꾸고 전치한 것입니다.

$$
\Sigma = \begin{pmatrix} \sigma_1 & & \\ & \ddots & \\ & & \sigma_r \\ & & & 0 \end{pmatrix}
\implies
\Sigma^+ = \begin{pmatrix} 1/\sigma_1 & & \\ & \ddots & \\ & & 1/\sigma_r \\ & & & 0 \end{pmatrix}^T
$$

의사역행렬로 구한 $\mathbf{x} = A^+\mathbf{b}$는 최소 노름 최소제곱해입니다.

$$
\mathbf{x} = A^+\mathbf{b} = \arg\min_{\mathbf{x}} \|A\mathbf{x} - \mathbf{b}\|_2 \quad \text{(해가 여러 개면, } \|\mathbf{x}\| \text{ 최소)}
$$

---

## 6. SVD와 네 가지 근본 부분 공간

SVD는 la-01에서 다룬 네 부분 공간을 직접 제공합니다.

| 부분 공간 | SVD에서의 표현 |
|-----------|---------------|
| 열 공간 $\mathcal{C}(A)$ | $U$의 처음 $r$개 열 |
| 좌영 공간 $\mathcal{N}(A^T)$ | $U$의 나머지 $m-r$개 열 |
| 행 공간 $\mathcal{C}(A^T)$ | $V$의 처음 $r$개 열 |
| 영 공간 $\mathcal{N}(A)$ | $V$의 나머지 $n-r$개 열 |

하나의 분해로 행렬의 모든 구조적 정보를 얻을 수 있습니다.

---

## 7. 이미지 압축 예시

$512 \times 512$ 그레이스케일 이미지를 행렬 $A$로 봅니다.

- 원본 저장: $512 \times 512 = 262{,}144$개 값
- 랭크 $k$ 근사: $U_k(512 \times k) + \Sigma_k(k) + V_k(512 \times k) = 1025k$개 값
- $k = 50$이면 $51{,}250$개 값 → 약 **80% 압축**

특이값이 빠르게 감소하는 이미지(부드러운 사진)는 적은 $k$로도 원본과 거의 구분 불가능합니다.

---

## 8. NumPy로 확인하기

```python
import numpy as np

# 예시 행렬
A = np.array([
    [1, 0, 0, 0, 2],
    [0, 0, 3, 0, 0],
    [0, 0, 0, 0, 0],
    [0, 2, 0, 0, 0]
])

# Full SVD
U, S, Vt = np.linalg.svd(A, full_matrices=True)
print(f"특이값: {S}")               # [3. 2.236 2. 0.]
print(f"U shape: {U.shape}")        # (4, 4)
print(f"Vt shape: {Vt.shape}")      # (5, 5)

# 복원 검증
Sigma = np.zeros_like(A, dtype=float)
np.fill_diagonal(Sigma, S)
A_reconstructed = U @ Sigma @ Vt
print(f"복원 오차: {np.linalg.norm(A - A_reconstructed):.2e}")

# Truncated SVD (랭크 2 근사)
k = 2
A_k = U[:, :k] @ np.diag(S[:k]) @ Vt[:k, :]
print(f"\n랭크-2 근사 오차: {np.linalg.norm(A - A_k):.4f}")
print(f"이론값 (σ₃): {S[2]:.4f}")  # 일치해야 함

# 의사역행렬
A_pinv = np.linalg.pinv(A)
print(f"\n의사역행렬 shape: {A_pinv.shape}")  # (5, 4)

# 이미지 압축 시뮬레이션
np.random.seed(42)
img = np.random.randn(100, 100)
U_img, S_img, Vt_img = np.linalg.svd(img, full_matrices=False)

for k in [5, 10, 20, 50]:
    approx = U_img[:, :k] @ np.diag(S_img[:k]) @ Vt_img[:k, :]
    error = np.linalg.norm(img - approx) / np.linalg.norm(img)
    storage = k * (100 + 1 + 100)
    print(f"k={k:3d}: 상대 오차={error:.4f}, 저장량={storage} (원본 10000)")
```

---

## 9. ML에서의 의미

### SVD로 PCA 하기

공분산 행렬을 명시적으로 구하지 않고, 데이터 행렬 $X$에 직접 SVD를 적용할 수 있습니다.

$$
X = U\Sigma V^T
$$

- 주성분 방향: $V$의 열
- 주성분 점수: $U\Sigma$ (또는 $XV$)
- 설명된 분산: $\sigma_i^2 / (n-1)$

$X^TX$를 구하는 것보다 수치적으로 안정적입니다.

### 잠재 의미 분석 (LSA)

문서-단어 행렬에 Truncated SVD를 적용하면, 의미적으로 비슷한 단어/문서가 가까운 벡터로 매핑됩니다. Word2Vec 이전의 고전적 임베딩 방법입니다.

### 추천 시스템

사용자-아이템 평점 행렬에 SVD를 적용하면 빈 칸(미관측 평점)을 채울 수 있습니다. Netflix Prize에서 사용된 핵심 기법입니다.

### LoRA와 저랭크 구조

대형 언어 모델의 파인튜닝에서 LoRA는 가중치 업데이트를 $\Delta W = BA$ ($B \in \mathbb{R}^{d \times r}$, $A \in \mathbb{R}^{r \times d}$)로 제한합니다. 이는 $\Delta W$가 최대 랭크 $r$인 저랭크 행렬이 되도록 하는 것이며, SVD의 Eckart-Young 정리가 이 접근의 이론적 토대입니다.

---

## 핵심 정리

1. **SVD** $A = U\Sigma V^T$는 모든 행렬에 적용 가능하며, 선형 변환을 **회전→스케일→회전**으로 분해한다.
2. 특이값 $\sigma_i$는 $A^TA$의 고유값의 제곱근이며, **행렬이 각 방향으로 얼마나 늘이는지**를 나타낸다.
3. **Eckart-Young 정리**: 상위 $k$개 특이값을 사용한 $A_k$가 최적의 랭크-$k$ 근사이다.
4. **의사역행렬** $A^+ = V\Sigma^+U^T$는 최소 노름 최소제곱해를 제공한다.
5. PCA, 추천 시스템, 이미지 압축, LoRA 등 ML의 핵심 기법들이 SVD에 기반한다.
