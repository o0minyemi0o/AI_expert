# 스키마 진화와 버전 관리

## 왜 스키마 진화가 중요한가

비즈니스는 변합니다. 새 컬럼이 추가되고, 타입이 바뀌고, 필드가 삭제됩니다. 하지만 파이프라인은 24시간 돌아가고, 이미 저장된 데이터는 과거 스키마를 따릅니다. **스키마 변경이 기존 시스템을 깨뜨리지 않으면서 진화**할 수 있어야 합니다. dp-06의 데이터 계약을 기술적으로 구현하는 것이 스키마 진화입니다.

> **핵심 직관**: 스키마 진화의 핵심 질문은 "새 스키마로 쓴 데이터를, 옛 스키마의 코드가 읽을 수 있는가?"입니다. 이것이 **호환성(Compatibility)**이며, 직렬화 포맷 선택의 가장 중요한 기준입니다.

## 1. 직렬화 포맷 비교

```
주요 직렬화 포맷:

  JSON:
  └─ 스키마 없음, 사람이 읽기 쉬움
     크기 큼, 파싱 느림
     스키마 진화: 암묵적 (필드 추가/무시)

  Avro:
  └─ 스키마 별도 관리 (JSON으로 정의)
     바이너리, 크기 작음
     스키마 진화: 명시적 호환성 규칙
     사용: Kafka, Hadoop 에코시스템

  Protobuf:
  └─ .proto 파일로 스키마 정의
     바이너리, 매우 효율적
     필드 번호 기반 → 이름 변경 안전
     사용: gRPC, 내부 서비스 통신

  Parquet:
  └─ 열 저장 + 스키마 내장
     분석 쿼리에 최적
     사용: 데이터 레이크, DW
```

| 포맷 | 크기 | 속도 | 스키마 | 진화 | 적합 |
|------|------|------|--------|------|------|
| JSON | 큼 | 느림 | 없음 | 암묵적 | API, 로그 |
| Avro | 작음 | 빠름 | 있음 | 명시적 | Kafka, 이벤트 |
| Protobuf | 매우 작음 | 매우 빠름 | 있음 | 명시적 | gRPC, 서비스 |
| Parquet | 작음 | 빠름 | 내장 | 컬럼 추가 | 분석, 레이크 |

## 2. Avro 스키마 진화

```
Avro 스키마 예시:

  v1:
  {
    "type": "record",
    "name": "Order",
    "fields": [
      {"name": "order_id", "type": "long"},
      {"name": "amount", "type": "double"},
      {"name": "status", "type": "string"}
    ]
  }

  v2 (필드 추가):
  {
    "type": "record",
    "name": "Order",
    "fields": [
      {"name": "order_id", "type": "long"},
      {"name": "amount", "type": "double"},
      {"name": "status", "type": "string"},
      {"name": "currency", "type": "string", "default": "KRW"}
    ]
  }

  v2로 쓴 데이터를 v1 코드가 읽으면:
  → currency 필드를 무시 (하위 호환)

  v1으로 쓴 데이터를 v2 코드가 읽으면:
  → currency에 기본값 "KRW" 사용 (상위 호환)
```

## 3. 호환성 규칙

```
호환성 유형:

  하위 호환 (Backward Compatible):
  └─ 새 코드가 옛 데이터를 읽을 수 있음
     허용: 필드 추가 (기본값 필수), 필드 삭제
     금지: 타입 변경, 필수 필드 추가

  상위 호환 (Forward Compatible):
  └─ 옛 코드가 새 데이터를 읽을 수 있음
     허용: 필드 삭제, 필드 추가 (기본값 있으면)
     금지: 타입 변경

  완전 호환 (Full Compatible):
  └─ 양방향 호환
     허용: 기본값 있는 필드 추가/삭제만
     가장 안전하지만 제한적

  비호환 (Breaking Change):
  └─ 타입 변경, 필드 이름 변경, 필수 필드 추가
     → 새 토픽/테이블 생성 필요
```

```
Confluent Schema Registry:

  Kafka 토픽의 스키마를 중앙 관리

  Producer → [Schema Registry] → 스키마 등록/검증
  Consumer → [Schema Registry] → 스키마 조회

  동작:
  1. Producer가 새 스키마 등록 요청
  2. Registry가 호환성 규칙 검증
  3. 호환 → 등록 (schema_id 부여)
  4. 비호환 → 거부! (파이프라인 보호)
  5. Consumer가 schema_id로 디시리얼라이즈

  설정:
  BACKWARD (기본) / FORWARD / FULL / NONE
```

> **핵심 직관**: Schema Registry는 dp-06의 데이터 계약을 **자동으로 강제**하는 도구입니다. 비호환 스키마 변경을 Producer가 배포하려는 순간 차단하여, 하류 파이프라인이 깨지는 것을 원천 방지합니다.

## 4. Protobuf 스키마 진화

```
Protobuf의 핵심: 필드 번호

  syntax = "proto3";

  message Order {
    int64 order_id = 1;    // 필드 번호 1
    double amount = 2;     // 필드 번호 2
    string status = 3;     // 필드 번호 3
  }

  진화 v2:
  message Order {
    int64 order_id = 1;
    double amount = 2;
    string status = 3;
    string currency = 4;   // 새 필드, 번호 4
    // int32 old_field = 5; // 삭제: 번호 5는 재사용 금지!
    reserved 5;
  }

  규칙:
  - 필드 번호는 절대 변경/재사용 금지
  - 필드 이름은 변경 가능 (번호로 식별)
  - 삭제된 번호는 reserved로 표시
  - 타입 변경: 호환되는 경우만 (int32 → int64)
```

## 5. Parquet 스키마 진화

```
Parquet의 스키마 진화:

  Parquet 파일 자체에 스키마가 내장
  → 파일마다 스키마가 다를 수 있음

  Spark에서의 처리:
  spark.read.option("mergeSchema", "true").parquet("s3://data/")
  → 모든 파일의 스키마를 병합

  허용되는 변경:
  ├─ 컬럼 추가: 기존 파일에서는 NULL
  ├─ 컬럼 삭제: 새 파일에서 해당 컬럼 없음
  └─ 호환 타입 변경: int → long

  Delta Lake/Iceberg의 스키마 진화:
  └─ 테이블 레벨 스키마 관리
     ALTER TABLE ADD COLUMN
     schema enforcement: 호환되지 않는 쓰기 차단
```

## 6. 데이터 버전 관리

```
코드 vs 데이터 버전 관리:

  코드: Git (가볍고 빠름)
  데이터: Git으로는 부적합 (파일 크기)

  DVC (Data Version Control):
  └─ Git과 연동하여 데이터 버전 관리
     실제 데이터는 S3/GCS에, 메타데이터만 Git에

  dvc add data/training_set.parquet
  → data/training_set.parquet.dvc (메타데이터, Git 추적)
  → data/training_set.parquet (실제 파일, .gitignore)

  git commit -m "training data v2"
  dvc push  # S3에 업로드

  과거 버전 복원:
  git checkout v1.0
  dvc checkout  # 해당 시점의 데이터 복원


  Delta Lake Time Travel:
  -- 특정 버전의 데이터 조회
  SELECT * FROM orders VERSION AS OF 5;
  -- 특정 시점의 데이터 조회
  SELECT * FROM orders TIMESTAMP AS OF '2024-01-15';
```

> **핵심 직관**: "재현 가능한 ML"을 위해서는 코드, 데이터, 모델 세 가지 모두 버전 관리가 필요합니다. DVC는 데이터를, MLflow는 모델을 관리하며, Git이 이 모든 것의 메타데이터를 추적합니다.

스키마 진화는 dp-03의 Kafka 이벤트 스키마, dp-09의 레이크하우스 테이블 스키마와 직접 연결됩니다.

## 핵심 정리

- **Avro**는 Kafka 에코시스템의 표준이며, Schema Registry와 결합하여 호환성을 자동 검증합니다
- **호환성 규칙**(하위/상위/완전)은 "새 코드↔옛 데이터" 또는 "옛 코드↔새 데이터" 간의 읽기 가능 여부를 정의합니다
- **Protobuf**는 필드 번호 기반으로 스키마를 관리하며, 번호를 재사용하지 않는 것이 핵심 규칙입니다
- **Parquet**의 스키마 진화는 컬럼 추가/삭제를 지원하며, Delta Lake/Iceberg가 테이블 레벨 관리를 제공합니다
- **DVC**는 Git과 연동하여 대용량 데이터의 버전 관리를 가능하게 하며, ML 재현성의 기반입니다
