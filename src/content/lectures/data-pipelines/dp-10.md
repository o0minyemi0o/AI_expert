# 피처 스토어

## 왜 피처 스토어가 필요한가

ML 모델의 성능은 **피처 엔지니어링**에 크게 좌우됩니다. 하지만 실무에서 피처 관련 문제가 끊이지 않습니다: 학습과 서빙의 피처가 달라 성능이 떨어지고, 같은 피처를 여러 팀이 중복 개발하고, 과거 시점의 피처를 정확히 재현하지 못합니다. **피처 스토어**는 이 문제들을 체계적으로 해결하는 인프라입니다.

> **핵심 직관**: 피처 스토어의 핵심 가치는 **"학습 때 본 피처와 서빙 때 사용하는 피처가 동일함을 보장"**하는 것입니다. 이 Training-Serving Skew가 ML 서비스의 가장 흔하고 디버깅하기 어려운 문제입니다.

## 1. Training-Serving Skew

```
문제: Training-Serving Skew

  학습 파이프라인:
  [DW] → SQL 쿼리 → Pandas로 피처 계산 → 학습
  "최근 30일 평균 구매액"
  = SELECT AVG(amount) FROM orders
    WHERE user_id = X AND created_at > NOW() - 30

  서빙 파이프라인:
  [API 요청] → Python 코드로 피처 계산 → 추론
  "최근 30일 평균 구매액"
  = Redis에서 조회 (배치로 미리 계산)

  Skew 원인:
  ├─ 코드 중복: SQL vs Python으로 같은 로직 2번 구현
  ├─ 시간 기준 차이: 학습은 과거 시점, 서빙은 현재 시점
  ├─ 데이터 소스 차이: DW vs 실시간 DB
  └─ 버전 불일치: 피처 정의가 변경되었는데 한쪽만 반영


피처 스토어의 해결:

  [피처 정의 (1곳)]
      ↓
  [피처 스토어]
  ├─ 오프라인 저장소 → 학습 (배치)
  └─ 온라인 저장소  → 서빙 (실시간)

  같은 정의 → 같은 계산 → Skew 제거
```

## 2. 오프라인/온라인 저장소

```
오프라인 저장소 (Offline Store):
  목적: 학습 데이터 생성
  특성: 대량, 과거 전체 이력
  구현: S3/Parquet, DW, 레이크하우스
  지연: 초~분 (배치 조회)

온라인 저장소 (Online Store):
  목적: 실시간 서빙
  특성: 최신 값, 키-값 조회
  구현: Redis, DynamoDB
  지연: ms 단위

  예시: "사용자 X의 최근 30일 피처"
  ├─ 오프라인: 전체 사용자의 2년치 피처 히스토리
  └─ 온라인: 각 사용자의 현재 피처 값 (최신만)

  동기화:
  배치 파이프라인이 오프라인 저장소에서 계산
  → 결과를 온라인 저장소에 적재 (materialize)
```

## 3. Feast

```
Feast (Feature Store):
  오픈소스 피처 스토어의 표준

  프로젝트 구조:
  feature_repo/
  ├── feature_store.yaml   # 설정
  ├── entities.py          # 엔티티 정의
  └── features.py          # 피처 정의
```

```python
# entities.py
from feast import Entity
user = Entity(name="user_id", join_keys=["user_id"])

# features.py
from feast import FeatureView, Field
from feast.types import Float64, Int64
from feast.infra.offline_stores.file_source import FileSource

user_stats_source = FileSource(
    path="s3://features/user_stats.parquet",
    timestamp_field="event_timestamp",
)

user_stats = FeatureView(
    name="user_stats",
    entities=[user],
    schema=[
        Field(name="avg_purchase_30d", dtype=Float64),
        Field(name="total_orders_30d", dtype=Int64),
        Field(name="days_since_last_order", dtype=Int64),
    ],
    source=user_stats_source,
    online=True,  # 온라인 저장소에도 적재
    ttl=timedelta(days=1),
)
```

```python
# 학습 시: 오프라인 저장소에서 시점 정합성 있는 피처 조회
from feast import FeatureStore

store = FeatureStore(repo_path="feature_repo/")

# entity_df: user_id + event_timestamp (라벨 시점)
training_df = store.get_historical_features(
    entity_df=entity_df,
    features=["user_stats:avg_purchase_30d",
              "user_stats:total_orders_30d"],
).to_df()

# 서빙 시: 온라인 저장소에서 최신 피처 조회
features = store.get_online_features(
    features=["user_stats:avg_purchase_30d"],
    entity_rows=[{"user_id": 12345}],
).to_dict()
```

> **핵심 직관**: Feast의 `get_historical_features`가 피처 스토어의 가장 중요한 기능입니다. 각 학습 샘플의 **라벨 시점에 알 수 있었던 피처만** 정확히 가져옵니다. 이것이 **시점 정합성(Point-in-Time Correctness)**이며, 데이터 유출(data leakage) 방지의 핵심입니다.

## 4. 시점 정합성 (Point-in-Time Join)

```
문제: 일반 JOIN은 미래 정보를 유출

  라벨: user_1, 2024-01-15에 이탈

  잘못된 방식:
  SELECT f.*
  FROM labels l
  JOIN user_features f ON l.user_id = f.user_id
  → 2024-01-20에 계산된 피처가 포함될 수 있음! (Data Leakage)

  올바른 방식 (Point-in-Time Join):
  각 라벨의 event_timestamp 이전에 계산된
  가장 최근 피처만 매칭

  labels:
  | user_id | event_timestamp | label |
  | 1       | 2024-01-15      | 1     |

  features:
  | user_id | feature_timestamp | avg_purchase |
  | 1       | 2024-01-10        | 50.0         | ← 이것을 매칭
  | 1       | 2024-01-20        | 45.0         | ← 미래, 제외!

  Feast가 자동으로 수행:
  store.get_historical_features(
      entity_df=labels_with_timestamps,
      features=[...]
  )
```

## 5. 피처 공유와 거버넌스

```
피처 중복 문제:

  추천 팀: "최근 30일 평균 구매액" → 자체 계산
  사기 탐지 팀: "최근 30일 평균 구매액" → 또 자체 계산
  마케팅 팀: "최근 30일 평균 구매액" → 또 또 자체 계산

  문제: 3번 계산, 3가지 다른 구현, 미묘한 차이

  피처 스토어의 해결:
  [user_stats 피처 뷰] → 한 번 정의, 모두가 사용

  피처 카탈로그:
  ├─ 피처 이름, 설명, 타입
  ├─ 소유 팀 (owner)
  ├─ 데이터 소스
  ├─ 최신 업데이트 시간
  ├─ 사용 중인 모델 목록
  └─ 데이터 품질 지표

  거버넌스:
  ├─ 접근 제어: PII 피처는 승인 필요
  ├─ 감사 로그: 누가 어떤 피처를 사용했는지
  └─ 폐기 절차: 사용 중인 모델 확인 후 폐기
```

## 6. 실전 피처 파이프라인

```
전형적인 피처 파이프라인:

  [원천 DB] → [CDC/Kafka] → [Spark 배치] → [오프라인 저장소]
                                ↓                 ↓
                          [온라인 적재]       [학습 데이터]
                                ↓
                          [온라인 저장소]
                                ↓
                          [모델 서빙 API]

  실시간 피처 (선택적):
  [Kafka] → [Flink] → [온라인 저장소]
  예: "최근 1시간 거래 횟수" (사기 탐지)

  배치 + 실시간 결합:
  서빙 시 온라인 저장소에서 배치 피처 + 실시간 피처를
  함께 조회하여 모델에 전달
```

> **핵심 직관**: 모든 피처를 실시간으로 계산할 필요는 없습니다. **"이 피처가 몇 시간 전 값이어도 괜찮은가?"**를 기준으로 배치 vs 실시간을 결정합니다. 대부분의 피처는 일일 배치로 충분하며, 실시간이 필요한 피처는 보통 전체의 10-20%입니다.

피처 스토어는 dp-05의 데이터 모델에서 피처 소스를 가져오고, dp-08의 Airflow에서 배치 파이프라인을 스케줄링합니다.

## 핵심 정리

- **Training-Serving Skew**는 학습과 서빙의 피처 불일치이며, 피처 스토어가 이를 단일 정의로 해결합니다
- **오프라인 저장소**는 학습용(대량, 과거), **온라인 저장소**는 서빙용(최신, 저지연)으로 분리됩니다
- **시점 정합성(Point-in-Time Join)**은 라벨 시점에 알 수 있었던 피처만 사용하여 Data Leakage를 방지합니다
- **Feast**는 오픈소스 피처 스토어 표준으로, 피처 정의 → 오프라인/온라인 제공을 통합합니다
- 대부분의 피처는 **일일 배치로 충분**하며, 실시간 피처는 사기 탐지 등 시간 민감한 경우에만 필요합니다
